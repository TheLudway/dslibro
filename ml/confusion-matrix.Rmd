## Métricas de evaluación

Antes de comenzar a describir enfoques para optimizar la forma en que construimos algoritmos, primero debemos definir a qué nos referimos cuando decimos que un enfoque es mejor que otro. En esta sección, nos centramos en describir las formas en que se evalúan los algoritmos de _machine learning_. Específicamente, necesitamos cuantificar lo que queremos decir con "mejor".

Para nuestra primera introducción a los conceptos de _machine learning_, comenzaremos con un ejemplo aburrido y sencillo:  cómo predecir sexo basado altura. A medida que explicamos _machine learning_ paso a paso, este ejemplo nos permitirá establecer el primer componente básico. Muy pronto, estaremos atacando desafíos más interesantes. Utilizamos el paquete __caret__, que tiene varias funciones útiles para construir y evaluar métodos de _machine learning_. Presentamos los detalles de este paquete en la Sección \@ref(caret).

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
```

Como primer ejemplo, usamos los datos de altura en __dslabs__:

```{r}
library(dslabs)
data(heights)
```

Comenzamos definiendo el resultado y los predictores.

```{r}
y <- heights$sex
x <- heights$height
```

En este caso, solo tenemos un predictor, altura, mientras que `y` es claramente un resultado categórico ya que los valores observados son `Male` o `Female`. Sabemos que no podremos predecir $Y$ de forma precisa basado en $X$ porque las alturas promedio masculinas y femeninas no son tan diferentes en relación con la variabilidad dentro del grupo. ¿Pero podemos hacerlo mejor que simplemente adivinar? Para responder a esta pregunta, necesitamos una definición cuantitativa de "mejor".

### Sets de entrenamiento y de evaluación

En última instancia, un algoritmo de _machine learning_ se evalúa basado en cómo funciona en el mundo real con sets de datos completamente nuevos. Sin embargo, cuando desarrollamos un algoritmo, generalmente tenemos un set de datos para el que conocemos los resultados, como lo hacemos con las alturas: sabemos el sexo de cada estudiante en nuestro set de datos. Por lo tanto, para imitar el proceso de evaluación final, generalmente dividimos los datos en dos partes y actuamos como si no supiéramos el resultado de una de estas. Dejamos de fingir que no conocemos el resultado para evaluar el algoritmo, pero solo _después_ de haber terminado de construirlo. Nos referimos al grupo para el que conocemos el resultado  y que usamos para desarrollar el algoritmo como el _set de entrenamiento_ (_training set_ en inglés). Nos referimos al grupo para el que pretendemos que no conocer el resultado como el _set de evaluación_ (_test set_ en inglés).

Una forma estándar de generar los sets de entrenamiento y de evaluación es dividiendo aleatoriamente los datos. El paquete __caret__ incluye la función `createDataPartition` que nos ayuda a generar índices para dividir aleatoriamente los datos en sets de entrenamiento y de evaluación:


```{r}
set.seed(2007)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
```

El argumento `times` se usa para definir cuántas muestras aleatorias de índices devolver, el argumento `p` se utiliza para definir qué proporción de los datos está representada por el índice y el argumento `list` se usa para decidir si queremos que los índices se devuelvan como una lista o no. Podemos usar el resultado de la llamada a la función `createDataPartition` para definir los sets de entrenamiento y de evaluación de esta manera:

```{r}
test_set <- heights[test_index, ]
train_set <- heights[-test_index, ]
```

Ahora desarrollaremos un algoritmo usando **solo** el set de entrenamiento. Una vez que hayamos terminado de desarrollar el algoritmo, lo _congelaremos_ y lo evaluaremos utilizando el set de evaluación. La forma más sencilla de evaluar el algoritmo cuando los resultados son categóricos es simplemente informar la proporción de casos que se predijeron correctamente **en el set de evaluación**. Esta métrica generalmente se conoce como _exactitud general_ (_overall accuracy_ en inglés).

### Exactitud general

Para demostrar el uso de la exactidud general, crearemos dos algoritmos diferentes y los compararemos.

Comencemos desarrollando el algoritmo más sencillo posible: adivinar el resultado.

```{r}
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE)
```

Tengan en cuenta que estamos ignorando completamente el predictor y simplemente adivinando el sexo.

En las aplicaciones de _machine learning_, es útil usar factores para representar los resultados categóricos porque las funciones de R desarrolladas para _machine learning_, como las del paquete __caret__, requieren o recomiendan que los resultados categóricos se codifiquen como factores.  Para convertir `y_hat` a factores podemos usar la función `factor`:

```{r}
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE) %>%
factor(levels = levels(test_set$sex))
```

La _exactidud general_ se define simplemente como la proporción general que se predice correctamente:

```{r}
mean(y_hat == test_set$sex)
```

No es sorprendente que nuestra exactidud sea 50%. ¡Estamos adivinando!

¿Podemos mejorarla? El análisis de datos exploratorios sugiere que sí porque, en promedio, los hombres son un poco más altos que las mujeres:

```{r}
heights %>% group_by(sex) %>% summarize(mean(height), sd(height))
```

Pero, ¿cómo usamos esta información? Probemos con otro enfoque sencillo: predecir `Male` si la altura está dentro de dos desviaciones estándar del hombre promedio.

```{r}
y_hat <- ifelse(x > 62, "Male", "Female") %>%
factor(levels = levels(test_set$sex))
```

La exactidud aumenta de 0.50 a aproximadamente 0.80:

```{r}
mean(y == y_hat)
```

¿Pero podemos mejorarla aún más? En el ejemplo anterior, utilizamos un umbral de 62, pero podemos examinar la exactidud obtenida para otros umbrales y luego elegir el valor que provee los mejores resultados. Sin embargo,recuerden, **es importante que optimicemos el umbral utilizando solo el set de entrenamiento**: el set de evaluación es solo para evaluación. Aunque para este ejemplo simplista no es un problema, más adelante aprenderemos que evaluar un algoritmo en el set de entrenamiento puede conducir a un _sobreajuste_ (_overfitting_ en inglés), que a menudo resulta en evaluaciones peligrosamente sobre optimistas.

Aquí examinamos la exactidud de 10 umbrales diferentes y elegimos el que produce el mejor resultado:

```{r}
cutoff <- seq(61, 70)
accuracy <- map_dbl(cutoff, function(x){
y_hat <- ifelse(train_set$height > x, "Male", "Female") %>%
factor(levels = levels(test_set$sex))
mean(y_hat == train_set$sex)
})
```

Podemos hacer un gráfico que muestra la exactitud obtenida en el set de entrenamiento para hombres y mujeres:

```{r accuracy-vs-cutoff, echo=FALSE}
data.frame(cutoff, accuracy) %>%
ggplot(aes(cutoff, accuracy)) +
geom_point() +
geom_line()
```

Vemos que el valor máximo es:

```{r}
max(accuracy)
```

que es mucho más grande que 0.5. El umbral que resulta en esta exactitud es:

```{r}
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff
```

Ahora podemos evaluar el uso de este umbral en nuestro set de evaluaciones para asegurarnos de que nuestra exactitud no sea demasiado optimista:

```{r}
y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>%
factor(levels = levels(test_set$sex))
y_hat <- factor(y_hat)
mean(y_hat == test_set$sex)
```

Vemos que es un poco más baja que la exactitud observada para el set de entrenamiento, pero aún es mejor que adivinar. Y al probar en un set de datos en el que no entrenamos, sabemos que nuestro resultado no se debe a que se haya elegido para dar un buen resultado en el set de evaluación.

### La matriz de confusión

La regla de predicción que desarrollamos en la sección anterior predice `Male` si el alumno es más alto que `r best_cutoff` pulgadas. Dado que la mujer promedio es aproximadamente `r best_cutoff` pulgadas, esta regla de predicción parece incorrecta. ¿Que pasó? Si la altura de un estudiante es la de la mujer promedio, ¿no deberíamos predecir `Female`?

En términos generales, la exactitud general puede ser una medida engañosa. Para ver esto, comenzaremos construyendo lo que se conoce como _matriz de confusión_ (_confusion matrix_ en inglés), que básicamente tabula cada combinación de predicción y valor real. Podemos hacer esto en R usando la función `table`:

```{r}
table(predicted = y_hat, actual = test_set$sex)
```

Si estudiamos esta tabla detenidamente, revela un problema. Si calculamos la exactitud por separado para cada sexo, obtenemos:

```{r}
test_set %>%
mutate(y_hat = y_hat) %>%
group_by(sex) %>%
summarize(accuracy = mean(y_hat == sex))
```

Hay un desequilibrio en la exactitud para hombres y mujeres: se predice que demasiadas mujeres son hombres. ¡Estamos prediciendo que casi la mitad de las mujeres son hombres! ¿Cómo es que nuestra exactitud general sea tan alta? Esto se debe a que la _prevalencia_ de los hombres en este set de datos es alta. Estas alturas se obtuvieron de tres cursos de ciencias de datos, dos de los cuales tenían más hombres matriculados:


```{r}
prev <- mean(y == "Male")
prev
```

Entonces, al calcular la exactitud general, el alto porcentaje de errores cometidos para las mujeres se ve superado por las ganancias en las predicciones acertadas para los hombres. **Esto puede ser un gran problema en _machine learning_.** Si sus datos de entrenamiento están sesgados de alguna manera, es probable que también desarrolle algoritmos sesgados. El hecho de que hayamos utilizado un set de evaluación no importa porque también se deriva del  original set de datos sesgado. Esta es una de las razones por las que observamos métricas distintas de la exactitud general al evaluar un algoritmo de _machine learning_.

Hay varias métricas que podemos usar para evaluar un algoritmo de manera que la prevalencia no afecte nuestra evaluación, y todas estas pueden derivarse de la matriz de confusión. Una forma general de mejorar el uso de la exactitud general es estudiar _sensibilidad_ y _especificidad_ por separado.

### Sensibilidad y especificidad

Para definir la sensibilidad y especificidad, necesitamos un resultado binario. Cuando los resultados son categóricos, podemos definir estos términos para una categoría específica. En el ejemplo de dígitos, podemos pedir la especificidad en el caso de predecir correctamente 2 en lugar de algún otro dígito. Una vez que especificamos una categoría de interés, podemos hablar sobre resultados positivos, $Y=1$ y resultados negativos, $Y=0$.

En general, la _sensibilidad_ se define como la capacidad de un algoritmo para predecir un resultado positivo cuando el resultado real es positivo: $\hat{Y}=1$ cuando $Y=1$. Un algoritmo que predice que todo es positivo ( $\hat{Y}=1$ pase lo que pase) tiene una sensibilidad perfecta, pero esta métrica por sí sola no es suficiente para evaluar un algoritmo. Por esta razón, también examinamos la _especificidad_, que generalmente se define como la capacidad de un algoritmo para no predecir un resultado positivo $\hat{Y}=0$ cuando el resultado real no es positivo $Y=0$. Podemos resumir de la siguiente manera:

* Alta sensibilidad: $Y=1 \implies \hat{Y}=1$
* Alta especificidad: $Y=0 \implies \hat{Y} = 0$

Aunque lo anterior a menudo se considera la definición de especificidad, otra forma de pensar en la especificidad es por la proporción de predicciones positivas que realmente son positivas:

* Alta especificidad: $\hat{Y}=1 \implies Y=1$.

Para ofrecer definiciones precisas, nombramos las cuatro entradas de la matriz de confusión:

```{r, echo=FALSE}
mat <- matrix(c("True positives (TP)", "False negatives (FN)",
"False positives (FP)", "True negatives (TN)"), 2, 2)
colnames(mat) <- c("Actually Positive", "Actually Negative")
rownames(mat) <- c("Predicted positive", "Predicted negative")
tmp <- as.data.frame(mat)
if(knitr::is_html_output()){
knitr::kable(tmp, "html") %>%
kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
knitr::kable(tmp, "latex", booktabs = TRUE) %>%
kableExtra::kable_styling(font_size = 8)
}
```

La sensibilidad se cuantifica típicamente por $TP/(TP+FN)$, la proporción de positivos reales (la primera columna = $TP+FN$) que se predicen ser positivos ($TP$). Esta cantidad se conoce como la _tasa positiva verdadera_ (_true positive rate_ o TPR por sus siglas en inglés) o _recall_.

La especificidad se define como $TN/(TN+FP)$ o la proporción de negativos (la segunda columna = $FP+TN$) que se llaman negativos ($TN$). Esta cantidad también se denomina la _tasa negativa verdadera_ (_true negative rate_ o TNR por sus siglas en inglés). Hay otra forma de cuantificar la especificidad que es $TP/(TP+FP)$ o la proporción de resultados que se llaman positivos (la primera fila o $TP+FP$) que realmente son positivos ($TP$). Esta cantidad se conoce como _valor predictivo positivo_ (_positive predictive value_ o PPV por sus siglas en inglés) y también como _precisión_ (_precision_ en inglés). Tengan en cuenta que, a diferencia de TPR y TNR, la precisión depende de la prevalencia, ya que una mayor prevalencia implica que pueden obtener una mayor precisión incluso cuando adivinando.

Los diferentes nombres pueden ser confusos, por lo que incluimos una tabla para ayudarnos a recordar los términos. La tabla incluye una columna que muestra la definición si pensamos en las proporciones como probabilidades.


| Medida de | Nombre 1 | Nombre 2 | Definición | Representación de probabilidad |
|---------|-----|----------|--------|------------------|
sensibilidad | TPR | Recall | $\frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}$ | $\mbox{Pr}(\hat{Y}=1 \mid Y=1)$ |
especificidad | TNR | 1-FPR | $\frac{\mbox{TN}}{\mbox{TN}+\mbox{FP}}$ | $\mbox{Pr}(\hat{Y}=0 \mid Y=0)$ |
especificidad | PPV | Precisión | $\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}$ | $\mbox{Pr}(Y=1 \mid \hat{Y}=1)$|

Aquí, TPR es tasa de positivos verdaderos, FPR es tasa de positivos falsos y PPV es valor predictivo positivo. La función `confusionMatrix` del paquete __caret__ calcula todas estas métricas para nosotros una vez que definimos qué categoría es "positiva". La función espera factores como entrada, y el primer nivel se considera el resultado positivo o $Y=1$. En nuestro ejemplo, `Female` es el primer nivel porque viene antes de `Male` alfabéticamente. Si escriben esto en R, verán varias métricas que incluyen exactitud, sensibilidad, especificidad y PPV.

```{r}
cm <- confusionMatrix(data = y_hat, reference = test_set$sex)
```

Puede acceder a estos directamente, por ejemplo, así:

```{r}
cm$overall["Accuracy"]
cm$byClass[c("Sensitivity","Specificity", "Prevalence")]
```

Podemos ver que la alta exactitud general es posible a pesar de la sensibilidad relativamente baja. Como sugerimos anteriormente, la razón por la que esto sucede es debido a la baja prevalencia (0.23): la proporción de mujeres es baja. Como la prevalencia es baja, no predecir mujeres reales como mujeres (baja sensibilidad) no disminuye la exactitud tanto como no predecir hombres reales como hombres (baja especificidad). Este es un ejemplo de por qué es importante examinar la sensibilidad y la especificidad y no solo la exactitud. Antes de aplicar este algoritmo a sets de datos generales, debemos preguntarnos si la prevalencia será la misma.


### Exactitud equilibrada y medida $F_1$

Aunque generalmente recomendamos estudiar tanto la especificidad como la sensibilidad, a menudo es útil tener un resumen de un número, por ejemplo, para fines de optimización. Una medida que se prefiere sobre la exactitud general es el promedio de especificidad y sensibilidad, conocida como _exactitud equilibrada_ (_balanced accuracy_ en inglés). Debido a que la especificidad y la sensibilidad son tasas, es más apropiado calcular la _media armónica_ (_harmonic average_ en inglés). De hecho, la _medida $F_1$_ (_$F_1$-score_ en inglés), un resumen de un número ampliamente utilizado, es la media armónica de precisión y _recall_:

$$
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} +
\frac{1}{\mbox{precision}}\right) }
$$

Debido a que es más fácil de escribir, a menudo se ve esta media armónica reescrita como:

$$
2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
$$

al definir $F_1$.

Recuerden que, según el contexto, algunos tipos de errores son más costosos que otros. Por ejemplo, en el caso de la seguridad de los aviones, es mucho más importante maximizar la sensibilidad sobre la especificidad: no predecir el mal funcionamiento de un avión antes de que se estrelle es un error mucho más costoso que aterrizar un avión cuando, de hecho, el avión está en perfectas condiciones. En un caso criminal de asesinato capital, lo contrario es cierto ya que un falso positivo puede llevar a ejecutar a una persona inocente. La medida $F_1$ se puede adaptar para pesar la especificidad y la sensibilidad de manera diferente. Para hacer esto, definimos $\beta$ para representar cuánto más importante es la sensibilidad en comparación con la especificidad y consideramos una media armónica ponderada:

$$
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} +
\frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
$$


La función `F_meas` en el paquete __caret__ calcula este resumen con un valor de `beta` igual a 1 por defecto.


Reconstruyamos nuestro algoritmo de predicción, pero esta vez maximizando la medida F en lugar de la exactitud general:

```{r}
cutoff <- seq(61, 70)
F_1 <- map_dbl(cutoff, function(x){
y_hat <- ifelse(train_set$height > x, "Male", "Female") %>%
factor(levels = levels(test_set$sex))
F_meas(data = y_hat, reference = factor(train_set$sex))
})
```

Como antes, podemos trazar estas medidas $F_1$ versus los umbrales:

```{r f_1-vs-cutoff, echo=FALSE}
data.frame(cutoff, F_1) %>%
ggplot(aes(cutoff, F_1)) +
geom_point() +
geom_line()
```

Vemos que el maximo de la medida $F_1$ es:

```{r}
max(F_1)
```

Este máximo se logra cuando usamos el siguiente umbral:
```{r}
best_cutoff <- cutoff[which.max(F_1)]
best_cutoff
```

Un umbral de 65 tiene más sentido que 64. Además, equilibra la especificidad y la sensibilidad de nuestra matriz de confusión:

```{r}
y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>%
factor(levels = levels(test_set$sex))
sensitivity(data = y_hat, reference = test_set$sex)
specificity(data = y_hat, reference = test_set$sex)
```

Ahora vemos que obtenemos mejores resultados que adivinando, que tanto la sensibilidad como la especificidad son relativamente altas, y que hemos construido nuestro primer algoritmo de _machine learning_. Este toma altura como predictor y predice mujeres si la persona mide 65 pulgadas o menos.


### La prevalencia importa en la práctica

Un algoritmo de _machine learning_ con sensibilidad y especificidad muy altas puede ser inútil en la práctica cuando la prevalencia se acerca a 0 o 1. Para ver esto, considere el caso de una doctora que se especializa en una enfermedad rara y que está interesada en desarrollar un algoritmo para predecir quién tiene la enfermedad. La doctora comparte los datos con ustedes, que entonces desarrollan un algoritmo con una sensibilidad muy alta. Explican que esto significa que si un paciente tiene la enfermedad, es muy probable que el algoritmo prediga correctamente. También le dicen a la doctora que están preocupados porque, según el set de datos que analizaron, la mitad de los pacientes tienen la enfermedad: $\mbox{Pr}(\hat{Y}=1)$. La doctora no está preocupada ni impresionada y explica que lo importante es la precisión de la evaluación: $\mbox{Pr}(Y=1 | \hat{Y}=1)$. Usando el teorema de Bayes, podemos conectar las dos medidas:

$$ \mbox{Pr}(Y = 1\mid \hat{Y}=1) = \mbox{Pr}(\hat{Y}=1 \mid Y=1) \frac{\mbox{Pr}(Y=1)}{\mbox{Pr}(\hat{Y}=1)}$$

La doctora sabe que la prevalencia de la enfermedad es de 5 en 1,000, lo que implica que $\mbox{Pr}(Y=1) \,/ \,\mbox{Pr}(\hat{Y}=1) = 1/100$ y, por lo tanto, la precisión de su algoritmo es inferior a 0.01. La doctora no tiene mucho uso para su algoritmo.


### Curvas ROC y precision-recall

Al comparar los dos métodos (adivinar versus usar un umbral de altura), comparamos la exactitud y $F_1$. El segundo método claramente superó al primero. Sin embargo, si bien consideramos varios umbrales para el segundo método, para el primero solo consideramos un enfoque: adivinar con igual probabilidad. Noten que adivinar `Male` con mayor probabilidad nos daría una mayor exactitud debido al sesgo en la muestra:


```{r}
p <- 0.9
n <- length(test_index)
y_hat <- sample(c("Male", "Female"), n, replace = TRUE, prob=c(p, 1-p)) %>%
factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex)
```

Pero, como se describió anteriormente, esto tendría el costo de una menor sensibilidad. Las curvas que describimos en esta sección nos ayudarán a ver esto.

Recuerden que para cada uno de estos parámetros,  podemos obtener una sensibilidad y especificidad diferente. Por esta razón, un enfoque muy común para evaluar métodos es compararlos gráficamente trazando ambos.

Un gráfico ampliamente utilizado que hace esto es la curva _Característica Operativa del Receptor_ (_Receiver Operating Characteristic_ o ROC por sus siglas en inglés). Para aprender más sobre el origen del nombre, pueden consultar la página de Wikipedia Curva ROC^[https://es.wikipedia.org/wiki/Curva_ROC].

La curva ROC representa la sensibilidad (TPR) frente a la especificidad 1 o la tasa de falsos positivos (FPR). Aquí calculamos el TPR y el FPR necesarios para diferentes probabilidades [fix check eng] de adivinar `Male`:

```{r roc-1}
probs <- seq(0, 1, length.out = 10)
guessing <- map_df(probs, function(p){
y_hat <-
sample(c("Male", "Female"), n, replace = TRUE, prob=c(p, 1-p)) %>%
factor(levels = c("Female", "Male"))
list(method = "Guessing",
FPR = 1 - specificity(y_hat, test_set$sex),
TPR = sensitivity(y_hat, test_set$sex))
})
```

Podemos usar un código similar para calcular estos valores para nuestro segundo enfoque. Al graficar ambas curvas juntas, podemos comparar la sensibilidad para diferentes valores de especificidad:

<!--We can construct an ROC curve for the height-based approach:-->

```{r, echo=FALSE}
cutoffs <- c(50, seq(60, 75), 80)
height_cutoff <- map_df(cutoffs, function(x){
y_hat <- ifelse(test_set$height > x, "Male", "Female") %>%
factor(levels = c("Female", "Male"))
list(method = "Height cutoff",
FPR = 1-specificity(y_hat, test_set$sex),
TPR = sensitivity(y_hat, test_set$sex))
})
```
<!--
```{r roc-2, echo=FALSE}
bind_rows(guessing, height_cutoff) %>%
ggplot(aes(FPR, TPR, color = method)) +
geom_line() +
geom_point() +
xlab("1 - Specificity") +
ylab("Sensitivity")
```
-->

```{r roc-3, echo=FALSE, fig.width=6, fig.height=3}
library(ggrepel)
tmp_1 <- map_df(cutoffs, function(x){
y_hat <- ifelse(test_set$height > x, "Male", "Female") %>%
factor(levels = c("Female", "Male"))
list(method = "Height cutoff",
cutoff = x,
FPR = 1-specificity(y_hat, test_set$sex),
TPR = sensitivity(y_hat, test_set$sex))
})
tmp_2 <- map_df(probs, function(p){
y_hat <-
sample(c("Male", "Female"), n, replace = TRUE, prob=c(p, 1-p)) %>%
factor(levels = c("Female", "Male"))
list(method = "Guessing",
cutoff = round(p,1),
FPR = 1 - specificity(y_hat, test_set$sex),
TPR = sensitivity(y_hat, test_set$sex))
})

bind_rows(tmp_1, tmp_2) %>%
ggplot(aes(FPR, TPR, label = cutoff, color = method)) +
geom_line() +
geom_point() +
geom_text_repel(nudge_x = 0.01, nudge_y = -0.01, show.legend = FALSE)
```

Vemos que obtenemos una mayor sensibilidad con este enfoque para todos los valores de especificidad, lo que implica que es un método mejor. [fix eng too? "for guessing"??] Tengan en cuenta que si adivinamos, las curvas ROC  caen en la línea de identidad. También noten que cuando haciendo curvas ROC, a menudo es bueno agregar el umbral asociado con cada punto.

Los paquetes __pROC__ y __plotROC__ son útiles para generar estos gráficos.

Las curvas ROC tienen una debilidad y es que ninguna de las medidas graficadas depende de la prevalencia. En los casos en que la prevalencia es importante, en su lugar podemos hacer un gráfico _precision-recall_. La idea es similar, pero en cambio graficamos la precisión versus el _recall_:


```{r precision-recall-1, warning=FALSE, message=FALSE, echo=FALSE}
guessing <- map_df(probs, function(p){
y_hat <- sample(c("Male", "Female"), length(test_index),
replace = TRUE, prob=c(p, 1-p)) %>%
factor(levels = c("Female", "Male"))
list(method = "Guess",
recall = sensitivity(y_hat, test_set$sex),
precision = precision(y_hat, test_set$sex))
})

height_cutoff <- map_df(cutoffs, function(x){
y_hat <- ifelse(test_set$height > x, "Male", "Female") %>%
factor(levels = c("Female", "Male"))
list(method = "Height cutoff",
recall = sensitivity(y_hat, test_set$sex),
precision = precision(y_hat, test_set$sex))
})
tmp_1 <- bind_rows(guessing, height_cutoff) %>% mutate(Positive = "Y = 1 if Female")

guessing <- map_df(probs, function(p){
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE,
prob=c(p, 1-p)) %>%
factor(levels = c("Male", "Female"))
list(method = "Guess",
recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})

height_cutoff <- map_df(cutoffs, function(x){
y_hat <- ifelse(test_set$height > x, "Male", "Female") %>%
factor(levels = c("Male", "Female"))
list(method = "Height cutoff",
recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})
tmp_2 <- bind_rows(guessing, height_cutoff) %>% mutate(Positive = "Y = 1 if Male")

bind_rows(tmp_1, tmp_2) %>%
ggplot(aes(recall, precision, color = method)) +
geom_line() +
geom_point() +
facet_wrap(~ Positive)
```

De este gráfico inmediatamente vemos que la precisión de adivinar no es alta. Esto se debe a que la prevalencia es baja. También vemos que si cambiamos los positivos para que signifiquen Male en lugar de Female, la curva ROC permanece igual, pero el gráfico _precision-recall_ cambia.


### La función de pérdida {#loss-function}

Hasta ahora hemos descrito métricas de evaluación que se aplican exclusivamente a datos categóricos. Específicamente, para los resultados binarios, hemos descrito cómo la sensibilidad, especificidad, precisión y $F_1$ se pueden utilizar como cuantificación. Sin embargo, estas métricas no son útiles para resultados continuos. En esta sección, describimos cómo el enfoque general para definir "mejor" en _machine learning_ es definir una _función de pérdida_ (_loss function_ en inglés), que puede aplicarse tanto a datos categóricos como continuos.

La función de pérdida más utilizada es la función de pérdida al cuadrado. Si $\hat{y}$ es nuestro predictor e $y$ es el resultado observado, la función de pérdida al cuadrado es simplemente:

$$
(\hat{y} - y)^2
$$

Debido a que frecuentemente tenemos un set de evaluaciones con muchas observaciones, digamos $N$, usamos el error cuadrático medio (_mean squared error_ o MSE por sus siglas en inglés):

$$
\mbox{MSE} = \frac{1}{N} \mbox{RSS} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
$$

En la práctica, a menudo indicamos la raiz cudadrada de la desviación cuadrática media (_root mean squared error_ o RMSE por sus siglas en inglés), que es $\sqrt{\mbox{MSE}}$, porque está en las mismas unidades que los resultados. Pero hacer las matemáticas muchas veces es más fácil con el MSE y, por lo tanto, se usa más en los libros de texto, ya que estos generalmente describen las propiedades teóricas de los algoritmos.

Si los resultados son binarios, tanto RMSE como MSE son equivalentes a la exactitud menos uno, ya que $(\hat{y} - y)^2$ es 0 si la predicción fue correcta y 1 en caso contrario. En general, nuestro objetivo es construir un algoritmo que minimice la pérdida para que esté lo más cerca posible de 0.

Debido a que nuestros datos son generalmente una muestra aleatoria, podemos pensar en el MSE como una variable aleatoria y el MSE observado puede considerarse como una estimación del MSE esperado, que en notación matemática escribimos así:

$$
\mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
$$

Este es un concepto teórico porque en la práctica solo tenemos un set de datos con el cual trabajar. [fix eng is not great, trans bad too] Pero en teoría, pensamos en tener un gran número de muestras aleatorias (llámen este número $B$), aplicar nuestro algoritmo a cada una, obtener un MSE para cada muestra aleatoria y pensar en el MSE esperado como:

$$
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2
$$

con $y_{i}^b$ denotando la observación $i$ en la muestra aleatoria $b$ y $\hat{y}_i^b$ la predicción resultante obtenida de aplicar exactamente el mismo algoritmo a la muestra aleatoria $b$. Nuevamente, en la práctica solo observamos una muestra aleatoria, por lo que el MSE esperado es solo teórico. Sin embargo, [fix eng too seccion?] en el capítulo \@ref(cross-validation) describimos un enfoque para estimar el MSE que trata de imitar esta cantidad teórica.

Tengan en cuenta que hay funciones de pérdida distintas de la función de pérdida cuadrática. Por ejemplo, el _Error Medio Absoluto_ (_Mean Absolute Error_ en inglés) utiliza valores absolutos, $|\hat{Y}_i - Y_i|$ en lugar de cuadrar los errores
$(\hat{Y}_i - Y_i)^2$. Sin embargo, en este libro nos enfocamos en minimizar la función de pérdida cuadrática ya que es la más utilizada.


## Ejercicios


[fix should we clarify university?] Los sets de datos `reported_height` y `height` se recopilaron de tres clases impartidas en los Departamentos de Ciencias Computacionales y Bioestadística, así como de forma remota a través de la Escuela de Extensión. La clase de bioestadística se impartió en 2016 junto con una versión en línea ofrecida por la Escuela de Extensión. El 25 de enero de 2016 a las 8:15 a.m., durante una de las clases, los instructores le pidieron a los estudiantes que completaran el cuestionario de sexo y altura que poblaba el set de datos `reported_height`. Los estudiantes en línea completaron la encuesta durante los próximos días, después de que la conferencia se publicara en línea. Podemos usar esta información para definir una variable, llamarla `type`, para denotar el tipo de estudiante, `inclass` (presenciales) o `online` (en línea):

```{r, eval=FALSE}
library(lubridate)
data("reported_heights")
dat <- mutate(reported_heights, date_time = ymd_hms(time_stamp)) %>%
filter(date_time >= make_date(2016, 01, 25) &
date_time < make_date(2016, 02, 1)) %>%
mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 &
between(minute(date_time), 15, 30),
"inclass", "online")) %>% select(sex, type)
x <- dat$type
y <- factor(dat$sex, c("Female", "Male"))
```


1\. Muestre estadísticas de resumen que indican que el `type` es predictivo del sexo.


2\. En lugar de usar la altura para predecir el sexo, use el variable `type`.


3\. Muestre la matriz de confusión.


4\. Utilice la función `confusionMatrix` en el paquete __caret__ para informar la exactitud.

5\. Ahora use las funciones `sensitivity` y `specificity` para informar especificidad y sensibilidad.


6\. ¿Cuál es la prevalencia (% de mujeres) en el set de datos `dat` definido anteriormente?


