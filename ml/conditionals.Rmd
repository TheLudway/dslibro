## Probabilidades y expectativas condicionales

En las aplicaciones de aprendizaje automático, rara vez podemos predecir los resultados perfectamente. Por ejemplo, los detectores de spam a menudo pierden correos electrónicos que son claramente spam, Siri a menudo no entiende las palabras que estamos diciendo, y su banco a veces piensa que su tarjeta fue robada cuando no fue así. La razón más común para no poder construir algoritmos perfectos es que es imposible. Para ver esto, tenga en cuenta que la mayoría de los conjuntos de datos incluirán grupos de observaciones con los mismos valores exactos observados para todos los predictores, pero con diferentes resultados. Debido a que nuestras reglas de predicción son funciones, las entradas iguales (los predictores) implican salidas iguales (las predicciones). Por lo tanto, para un desafío en el que los mismos predictores se asocian con diferentes resultados en diferentes observaciones individuales, es imposible predecir correctamente para todos estos casos. Vimos un ejemplo simple de esto en la sección anterior: para cualquier altura dada $x$, tendrás hombres y mujeres que son $x$ pulgadas de alto.

Sin embargo, nada de esto significa que no podamos construir algoritmos útiles que sean mucho mejores que adivinar, y en algunos casos mejor que las opiniones de expertos. Para lograr esto de manera óptima, hacemos uso de representaciones probabilísticas del problema basadas en las ideas presentadas en la Sección \@ref(conditional-expectation). Las observaciones con los mismos valores observados para los predictores pueden no ser todas iguales, pero podemos suponer que todas tienen la misma probabilidad de esta clase o de esa clase. Escribiremos esta idea matemáticamente para el caso de datos categóricos.

### Probabilidades condicionales

Usamos la notación $(X_1 = x_1,\dots,X_p=x_p)$ para representar el hecho de que hemos observado valores $x_1,\dots,x_p$ para covariables $X_1, \dots, X_p$. Esto no implica que el resultado $Y$ tomará un valor específico. En cambio, implica una probabilidad específica. En particular, denotamos las _probabilidades condicionales_ para cada clase $k$:

$$
\mbox{Pr}(Y=k \mid X_1 = x_1,\dots,X_p=x_p), \, \mbox{for}\,k=1,\dots,K
$$

Para evitar escribir todos los predictores, usaremos las letras en negrita como esta: $\mathbf{X} \equiv (X_1,\dots,X_p)$ y $\mathbf{x} \equiv (x_1,\dots,x_p)$. También usaremos la siguiente notación para la probabilidad condicional de ser clase $k$:

$$
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
$$

Nota: utilizaremos el $p(x)$ notación para representar probabilidades condicionales como funciones de los predictores. No lo confundas con el $p$ eso representa el número de predictores.

Estas probabilidades guían la construcción de un algoritmo que hace la mejor predicción: para cualquier $\mathbf{x}$, vamos a predecir la clase $k$ con la mayor probabilidad entre $p_1(x), p_2(x), \dots p_K(x)$. En notación matemática, lo escribimos así: $\hat{Y} = \max_k p_k(\mathbf{x})$.

En el aprendizaje automático, nos referimos a esto como _Bayes &#39;Rule_. Pero tenga en cuenta que esta es una regla teórica ya que en la práctica no sabemos $p_k(\mathbf{x}), k=1,\dots,K$. De hecho, estimar estas probabilidades condicionales puede considerarse como el principal desafío del aprendizaje automático. Cuanto mejores sean nuestras estimaciones de probabilidad $\hat{p}_k(\mathbf{x})$, mejor será nuestro predictor:

$$\hat{Y} = \max_k \hat{p}_k(\mathbf{x})$$

Entonces, lo que predeciremos depende de dos cosas: 1) qué tan cerca están las $\max_k p_k(\mathbf{x})$ a 1 o 0 (certeza perfecta)
y 2) qué tan cerca están nuestras estimaciones $\hat{p}_k(\mathbf{x})$ son a $p_k(\mathbf{x})$. No podemos hacer nada con respecto a la primera restricción, ya que está determinada por la naturaleza del problema, por lo que nuestra energía busca encontrar formas de estimar mejor las probabilidades condicionales. La primera restricción implica que tenemos límites en cuanto a qué tan bien puede funcionar incluso el mejor algoritmo posible. Debería acostumbrarse a la idea de que, si bien en algunos desafíos podremos lograr una precisión casi perfecta, con lectores de dígitos, por ejemplo, en otros nuestro éxito está restringido por la aleatoriedad del proceso, con recomendaciones de películas, por ejemplo.

Antes de continuar, es importante recordar que definir nuestra predicción maximizando la probabilidad no siempre es óptimo en la práctica y depende del contexto. Como se discutió anteriormente, la sensibilidad y la especificidad pueden diferir en importancia. Pero incluso en estos casos, tener una buena estimación de la $p_k(x), k=1,\dots,K$ nos bastará para construir modelos de predicción óptimos, ya que podemos controlar el equilibrio entre especificidad y sensibilidad como lo deseemos. Por ejemplo, simplemente podemos cambiar los límites utilizados para predecir un resultado u otro. En el ejemplo del plano, podemos aterrizar el plano en cualquier momento en que la probabilidad de mal funcionamiento sea superior a 1 en un millón, en lugar del 1/2 predeterminado utilizado cuando los tipos de error son igualmente indeseados.

### Expectativas condicionales

Para datos binarios, puedes pensar en la probabilidad $\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})$ como la proporción de 1s en el estrato de la población para la cual $\mathbf{X}=\mathbf{x}$. Muchos de los algoritmos que aprenderemos se pueden aplicar tanto a datos categóricos como continuos debido a la conexión entre las probabilidades condicionales y las expectativas condicionales.

Porque la expectativa es el promedio de los valores $y_1,\dots,y_n$ en la población, en el caso en que el $y$ s son 0 o 1, la expectativa es equivalente a la probabilidad de elegir aleatoriamente uno ya que el promedio es simplemente la proporción de unos:

$$
\mbox{E}(Y \mid \mathbf{X}=\mathbf{x})=\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}).
$$

Como resultado, a menudo solo usamos la expectativa para denotar tanto la probabilidad condicional como la expectativa condicional.

Al igual que con los resultados categóricos, en la mayoría de las aplicaciones, los mismos predictores observados no garantizan los mismos resultados continuos. En cambio, suponemos que el resultado sigue la misma distribución condicional. Ahora explicaremos por qué usamos la expectativa condicional para definir nuestros predictores.


### La expectativa condicional minimiza la función de pérdida al cuadrado

¿Por qué nos importa la expectativa condicional en el aprendizaje automático? Esto se debe a que el valor esperado tiene una propiedad matemática atractiva: minimiza el MSE. Específicamente, de todas las predicciones posibles. $\hat{Y}$,

$$
\hat{Y} = \mbox{E}(Y \mid \mathbf{X}=\mathbf{x}) \, \mbox{ minimizes } \, \mbox{E}\{ (\hat{Y} - Y)^2 \mid \mathbf{X}=\mathbf{x} \}
$$

Debido a esta propiedad, una descripción sucinta de la tarea principal del aprendizaje automático es que utilizamos datos para estimar:

$$
f(\mathbf{x}) \equiv \mbox{E}( Y \mid \mathbf{X}=\mathbf{x} )
$$

para cualquier conjunto de características $\mathbf{x} = (x_1, \dots, x_p)$. Por supuesto, esto es más fácil decirlo que hacerlo, ya que esta función puede tomar cualquier forma y $p$ puede ser muy grande. Considere un caso en el que solo tenemos un predictor $x$. La expectativa $\mbox{E}\{ Y \mid X=x \}$ puede ser cualquier función de $x$: una línea, una parábola, una onda sinusoidal, una función de paso, cualquier cosa. Se vuelve aún más complicado cuando consideramos instancias con grandes $p$, en ese caso $f(\mathbf{x})$ es una función de un vector multidimensional $\mathbf{x}$. Por ejemplo, en nuestro ejemplo de lector de dígitos $p = 784$! ** La principal forma en que los algoritmos competitivos de aprendizaje automático difieren es en su enfoque para estimar esta expectativa. **


## Ejercicios

1\. Calcule las probabilidades condicionales de ser hombre para el `heights` conjunto de datos Redondea las alturas a la pulgada más cercana. Trazar la probabilidad condicional estimada $P(x) = \mbox{Pr}(\mbox{Male} | \mbox{height}=x)$ para cada $x$.


2\. En la gráfica que acabamos de hacer, vemos una gran variabilidad para valores bajos de altura. Esto se debe a que tenemos pocos puntos de datos en estos estratos. Esta vez usa el `quantile` función para cuantiles $0.1,0.2,\dots,0.9$ y el NA función para asegurar que cada grupo tenga el mismo número de puntos. Sugerencia: para cualquier vector numérico NA, puede crear grupos basados en cuantiles como este:

```{r, eval=FALSE}
cut(x, quantile(x, seq(0, 1, 0.1)), include.lowest = TRUE)
```

3\. Genere datos a partir de una distribución normal bivariada utilizando el paquete __MASS__ como este:

```{r, eval=FALSE}
Sigma <- 9*matrix(c(1,0.5,0.5,1), 2, 2)
dat <- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) %>%
data.frame() %>% setNames(c("x", "y"))
```

Puede hacer un diagrama rápido de los datos usando `plot(dat)`. Use un enfoque similar al ejercicio anterior para estimar las expectativas condicionales y hacer un diagrama.


