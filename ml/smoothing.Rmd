# Suavización

```{r, echo=FALSE}
img_path <- "ml/img/"
```

Antes de continuar aprendiendo sobre algoritmos de _machine learning_, presentamos el importante concepto de _suavización_ (_smoothing_ en inglés). La suavización es una técnica muy poderosa comúnmente usada en el análisis de datos. Otros nombres dados a esta técnica son _ajustamiento de curvas_ y _filtro de paso bajo_ (_curve fitting_ y _low pass filtering_ en inglés). La suavización está diseñada para detectar tendencias en presencia de datos ruidosos cuando se desconoce la forma real de la tendencia. El nombre _suavización_ proviene del hecho de que para lograr esta hazaña, suponemos que la tendencia es _suave_, como una superficie lisa. En contraste, el ruido (_noise_ en inglés), o la desviación de la tendencia, es impredeciblemente ondulante:

```{r signal-plus-noise-example, message=FALSE, warning=FALSE, fig.height=6, echo=FALSE}
library(tidyverse)
set.seed(1)
n <- 100
x <- seq(-pi*4, pi*4, len = n)
tmp <- data.frame(x = x , f = sin(x) + x/8, e = rnorm(n, 0, 0.5))
p1 <- qplot(x, f, main = "smooth trend", ylim = range(tmp$f+tmp$e), data = tmp, geom = "line")
p2 <- qplot(x, e, main = "noise", ylim = range(tmp$f+tmp$e), data = tmp, geom = "line")
p3 <- qplot(x, f+e, main = "data = smooth trend + noise", ylim = range(tmp$f+tmp$e), data = tmp, geom = "line")
gridExtra::grid.arrange(p1, p2, p3)
```

Parte de lo que explicamos en esta sección son los supuestos que nos permiten extraer la tendencia del ruido.

Para entender por qué cubrimos este tema, recuerden que los conceptos detrás de las técnicas de suavización son extremadamente útiles en _machine learning_ porque las expectativas/probabilidades condicionales que necesitamos estimar pueden considerarse como tendencias de formas desconocidas afectadas por incertidumbre.

Para explicar estos conceptos, nos enfocaremos primero en un problema con un solo predictor. Específicamente, tratamos de estimar la tendencia temporal en el margen de la encuesta de votación popular de 2008 en Estados Unidos (la diferencia entre Obama y McCain).

```{r polls-2008-data, warning=FALSE, message=FALSE}
library(tidyverse)
library(dslabs)
data("polls_2008")
qplot(day, margin, data = polls_2008)
```

Para los fines de este ejemplo, no lo piensen como un problema de pronóstico. En cambio, simplemente estamos interesados en entender la forma de la tendencia *después* de que terminen las elecciones.

Suponemos que para cualquier día $x$, hay una verdadera preferencia entre el electorado $f(x)$, pero debido a la incertidumbre introducida por el sondeo, cada punto de datos viene con un error $\varepsilon$. Un modelo matemático para el margen de encuesta observado $Y_i$ es:

$$
Y_i = f(x_i) + \varepsilon_i
$$

Para pensar en esto como un problema de _machine learning_, recuerden que queremos predecir $Y$ dado un día $x$. Si supiéramos la expectativa condicional $f(x) = \mbox{E}(Y \mid X=x)$, la usaríamos. Pero como no conocemos esta expectativa condicional, tenemos que estimarla. Usemos la regresión, ya que es el único método que hemos aprendido hasta ahora.

```{r linear-regression-not-flexible, echo=FALSE}
resid <- ifelse(lm(margin~day, data = polls_2008)$resid > 0, "+", "-")
polls_2008 %>%
  mutate(resid = resid) %>%
  ggplot(aes(day, margin)) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  geom_point(aes(color = resid), size = 3)
```

La línea que vemos no parece describir muy bien la tendencia. Por ejemplo, el 4 de septiembre (día -62), se celebró la Convención Republicana y los datos sugieren que este evento le dio a John McCain un impulso en las encuestas. Sin embargo, la línea de regresión no captura esta tendencia potencial. Para ver más claramente la _falta de ajuste_, observamos que los puntos por encima de la línea ajustada (azul) y los de abajo (rojo) no se distribuyen uniformemente entre los días. Por lo tanto, necesitamos un enfoque alternativo más flexible.

## Suavización de compartimientos

La idea general de la suavización es agrupar los puntos de datos en estratos en los que el valor de $f(x)$ se puede suponer que es constante. Podemos hacer esta suposición porque pensamos que $f(x)$ cambia lentamente y, como resultado, $f(x)$ es casi constante en pequeñas ventanas de tiempo. Un ejemplo de esta idea para los datos `poll_2008` es suponer que la opinión pública se mantuvo aproximadamente igual en el plazo de una semana. Con este supuesto, tenemos varios puntos de datos con el mismo valor esperado.

Si fijamos un día para estar en el centro de nuestra semana, llámelo $x_0$, entonces para cualquier otro día $x$ tal que $|x - x_0| \leq 3.5$, suponemos que $f(x)$ es una constante $f(x) = \mu$. Esta suposición implica que:
$$
E[Y_i | X_i = x_i ] \approx \mu \mbox{ if } |x_i - x_0| \leq 3.5
$$

En la suavización, llamamos el tamaño del intervalo que satisface $|x_i - x_0| \leq 3.5$, el _tamaño de la ventana_, _parámetro de suavizado_ o _span_. Más adelante, aprenderemos como intentamos optimizar este parámetro.

Esta suposición implica que un buen estimador de $f(x)$ es el promedio de $Y_i$ valores en la ventana. Si definimos $A_0$ como el conjunto de índices $i$ tal que $|x_i - x_0| \leq 3.5$ y $N_0$ como el número de índices en $A_0$, entonces nuestro estimador es:

$$
\hat{f}(x_0) = \frac{1}{N_0} \sum_{i \in A_0} Y_i
$$

La idea detrás de la _suavización de compartimientos_ (_bin smoothing_ en inglés) es hacer este cálculo con cada valor de $x$ como el centro. En el ejemplo de la encuesta, para cada día, calcularíamos el promedio de los valores dentro de una semana con ese día en el centro. Aquí hay dos ejemplos: $x_0 = -125$ y $x_0 = -55$. El segmento azul representa el promedio resultante.

```{r binsmoother-expained, echo=FALSE}
span <- 3.5
tmp <- polls_2008 %>%
  crossing(center = polls_2008$day) %>%
  mutate(dist = abs(day - center)) %>%
  filter(dist <= span)

tmp %>% filter(center %in% c(-125, -55)) %>%
  ggplot(aes(day, margin)) +
  geom_point(data = polls_2008, size = 3, alpha = 0.5, color = "grey") +
  geom_point(size = 2) +
  geom_smooth(aes(group = center),
              method = "lm", formula=y~1, se = FALSE) +
  facet_wrap(~center)
```

Al calcular esta media para cada punto, formamos un estimador de la curva subyacente $f(x)$. A continuación, mostramos el procedimiento que ocurre a medida que avanzamos de -155 a 0. En cada valor de $x_0$, mantenemos el estimador $\hat{f}(x_0)$ y continuamos al siguiente punto:

```{r binsmoother-animation, echo=FALSE, warning=FALSE}
library(gganimate)
span <- 7
fit <- with(polls_2008, ksmooth(day, margin, kernel="box", x.points = day, bandwidth = span))
bin_fit <- data.frame(x = fit$x, .fitted=fit$y)

if(!file.exists(file.path(img_path,"binsmoother-animation.gif"))){
  p <- tmp %>%
    ggplot() +
    geom_smooth(aes(day, margin, group = center, frame = center), method = "lm", formula=y~1, se = FALSE) +
    geom_point(aes(day, margin), data = polls_2008, size = 3, alpha = .5, color = "grey") +
    geom_point(aes(day, margin, frame = center)) +
    geom_line(aes(x=x, y = .fitted, frame = x, cumulative = TRUE), data = bin_fit, color = "red") +
    ggtitle("x0 = ")
  
  gganimate(p, filename = file.path(img_path,"binsmoother-animation.gif"), interval= .1)
}
if(knitr::is_html_output()){
  knitr::include_graphics(file.path(img_path,"binsmoother-animation.gif"))
} else{
  centers <- quantile(tmp$center, seq(1,6)/6)
  tmp_bin_fit <- crossing(center=centers,bin_fit) %>%
    group_by(center) %>%
    filter(x <= center) %>%
    ungroup()
  
  tmp %>% filter(center %in% centers) %>%
    ggplot() +
    geom_smooth(aes(day, margin), method = "lm", formula=y~1, se = FALSE) +
    geom_point(aes(day, margin), data = polls_2008, size = 3, alpha = .5, color = "grey") +
    geom_point(aes(day, margin)) +
    geom_line(aes(x=x, y = .fitted), data = tmp_bin_fit, color = "red") +
    ggtitle("x0 = ") +
    facet_wrap(~center, nrow = 2)
}
```

El código final y el estimador resultante se ven así:

```{r binsmoother-final}
span <- 7
fit <- with(polls_2008,
            ksmooth(day, margin, kernel = "box", bandwidth = span))

polls_2008 %>% mutate(smooth = fit$y) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(day, smooth), color="red")
```

## Kernels

El resultado final de la suavización de compartimientos es bastante ondulante. Una de las razones es que cada vez que la ventana se mueve, cambian dos puntos. Podemos atenuar esto algo tomando promedios ponderados que le dan al punto central más peso que a los puntos lejanos, con los dos puntos en los bordes recibiendo muy poco peso.

Pueden pensar en el enfoque de suavización de compartimiento como un promedio ponderado:

$$
\hat{f}(x_0) = \sum_{i=1}^N w_0(x_i) Y_i
$$

en el que cada punto recibe un peso de $0$ o $1/N_0$, con $N_0$ el número de puntos en la semana. En el código anterior, usamos el argumento `kernel="box"` en nuestra llamada a la función `ksmooth`. Esto se debe a que la _función de peso_ $w_0(x)$ parece una caja. La función `ksmooth` ofrece una opción "más suave" que utiliza la densidad normal para asignar pesos.

```{r gaussian-kernel, echo=FALSE, out.width="80%", fig.height=3, fig.width=6}
x_0 <- -125

p1 <- data.frame(x = polls_2008$day) %>% mutate(w_0 = 1*I(abs(x - x_0)<=span/2)) %>%
  mutate(w_0 = w_0/sum(w_0)) %>%
  ggplot(aes(x, w_0)) +
  geom_step() +
  ggtitle("Box")

tmp <- with(data.frame(day = seq(min(polls_2008$day), max(polls_2008$day), .25)),
            ksmooth(day, 1*I(day == x_0), kernel = "normal", x.points = day, bandwidth = span))
p2 <- data.frame(x = tmp$x, w_0 = tmp$y) %>%
  mutate(w_0 = w_0/sum(w_0)) %>%
  ggplot(aes(x, w_0)) +
  geom_line() +
  ggtitle("Normal")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

<!--
Con esta animación, vemos que los puntos en el borde obtienen menos peso (el tamaño del punto es proporcional a su peso):

```{r kernel-animation, echo=FALSE, warning=FALSE}
tmp <- polls_2008 %>%
crossing(center = polls_2008$day) %>%
mutate(dist = abs(day - center)) %>%
filter(dist <= span) %>%
mutate(weight = dnorm(dist, 0, span/2.54))%>%
mutate(weight = weight/max(weight))

span <- 7
fit <- with(polls_2008, ksmooth(day, margin, kernel="normal", x.points = day, bandwidth = span))
bin_fit <- data.frame(x = fit$x, .fitted=fit$y)

if(!file.exists(file.path(img_path,"kernel-animation.gif"))){
p <- tmp %>%
ggplot() +
geom_smooth(aes(day, margin, group = center, weight = weight, frame = center), method = "lm", formula=y~1, se=FALSE) +
geom_point(aes(day, margin), data = polls_2008, size = 3, alpha = .5, color = "grey") +
geom_point(aes(day, margin, size = weight, frame = center), show.legend = FALSE) +
scale_size(range = c(0, 3)) +
geom_line(aes(x=x, y = .fitted, frame = x, cumulative = TRUE), data = bin_fit, color = "red")

gganimate(p, filename = file.path(img_path,"kernel-animation.gif"), interval= .1)
}
if(knitr::is_html_output()){
knitr::include_graphics(file.path(img_path,"kernel-animation.gif"))
} else{
centers <- quantile(tmp$center, seq(1,6)/6)
tmp_bin_fit <- crossing(center=centers,bin_fit) %>%
group_by(center) %>%
filter(x <= center) %>%
ungroup()

tmp %>% filter(center %in% centers) %>%
ggplot() +
geom_smooth(aes(day, margin), method = "lm", formula=y~1, se = FALSE) +
geom_point(aes(day, margin, size = weight), data = polls_2008, size = 3, alpha = .5, color = "grey") +
geom_point(aes(day, margin)) +
geom_line(aes(x=x, y = .fitted), data = tmp_bin_fit, color = "red") +
facet_wrap(~center, nrow = 2)
}
```
-->

El código final y el gráfico resultante para el _kernel_ normal se ven así:

```{r final-ksmooth-normal-kernel}
span <- 7
fit <- with(polls_2008,
            ksmooth(day, margin, kernel = "normal", bandwidth = span))

polls_2008 %>% mutate(smooth = fit$y) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(day, smooth), color="red")
```

Observen que el estimador final ahora se ve más suave.

Hay varias funciones en R que implementan suavizadores de compartimientos. Un ejemplo es `ksmooth`, que mostramos arriba. En la práctica, sin embargo, generalmente preferimos métodos que usan modelos ligeramente más complejos que ajustar una constante. El resultado final arriba, por ejemplo, todavía es algo ondulante en partes que no esperamos que sea (entre -125 y -75, por ejemplo). Métodos como `loess`, que explicamos a continuación, mejoran esto.

## Regresión ponderada local (loess)

Una limitación del enfoque de suavización de compartimientos que acabamos de describir es que necesitamos ventanas pequeñas para que se cumpla el supuesto de que la función es aproximadamente constante. Como resultado, terminamos con un pequeño número de puntos de datos para promediar y obtener estimaciones imprecisas $\hat{f}(x)$. Aquí describimos cómo la _regresión ponderada local_ (loess o _local weighted regression_ en inglés) nos permite considerar tamaños de ventana más grandes. Para hacer esto, usaremos un resultado matemático, conocido como el teorema de Taylor, que dice que si examinamos muy de cerca cualquier función suave $f(x)$, parecerá una línea. Para ver por qué esto tiene sentido, consideren los bordes curvos que hacen los jardineros con palas rectas:

```{r, echo=FALSE}
knitr::include_graphics(file.path(img_path, "garden.png"))
```

("Downing Street garden path edge"^[https://www.flickr.com/photos/49707497@N06/7361631644] del usuario de Flckr Número 10^[https://www.flickr.com/photos/number10gov/]. Licencia CC-BY 2.0^[https://creativecommons.org/licenses/by/2.0/].)

En lugar de suponer que la función es aproximadamente constante en una ventana, suponemos que la función es localmente lineal. Podemos considerar tamaños de ventana más grandes cuando suponemos que la función es localmente lineal que cuando suponemos que es localmente constante. En lugar de la ventana de una semana, consideramos una ventana más grande en la que la tendencia es aproximadamente lineal. Comenzamos con una ventana de tres semanas y luego consideramos y evaluamos otras opciones:

$$
E[Y_i | X_i = x_i ] = \beta_0 + \beta_1 (x_i-x_0) \mbox{ if } |x_i - x_0| \leq 21
$$

Para cada punto $x_0$, loess define una ventana y ajusta una línea dentro de esa ventana. Aquí hay un ejemplo que muestra los ajustes para $x_0=-125$ y $x_0 = -55$:

```{r loess, echo=FALSE}
span <- 21/diff(range(polls_2008$day))

tmp <- polls_2008 %>%
  crossing(center = polls_2008$day) %>%
  mutate(dist = abs(day - center)) %>%
  filter(rank(dist)/ n() <= span) %>%
  mutate(weight = (1 - (dist/ max(dist))^3)^3)

tmp %>%
  filter(center %in% c(-125, -55)) %>%
  ggplot(aes(day, margin)) +
  scale_size(range = c(0, 3)) +
  geom_smooth(aes(group = center, weight = weight),
              method = "lm", se = FALSE) +
  geom_point(data = polls_2008, size = 3, alpha = .5, color = "grey") +
  geom_point(aes(size = weight)) +
  facet_wrap(~center)
```

El valor ajustado en $x_0$ se convierte en nuestro estimador $\hat{f}(x_0)$. A continuación, mostramos el procedimiento que ocurre mientras cambiamos de -155 a 0.

```{r loess-animation, echo=FALSE, warning=FALSE}
library(broom)
fit <- loess(margin ~ day, degree=1, span = span, data=polls_2008)
loess_fit <- augment(fit)

if(!file.exists(file.path(img_path,"loess-animation.gif"))){
  p <- ggplot(tmp, aes(day, margin)) +
    scale_size(range = c(0, 3)) +
    geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
    geom_point(data = polls_2008, size = 3, alpha = .5, color = "grey") +
    geom_point(aes(size = weight, frame = center)) +
    geom_line(aes(x=day, y = .fitted, frame = day, cumulative = TRUE),
              data = loess_fit, color = "red")
  
  gganimate(p, filename = file.path(img_path,"loess-animation.gif"), interval= .1)
}
if(knitr::is_html_output()){
  knitr::include_graphics(file.path(img_path,"loess-animation.gif"))
} else{
  centers <- quantile(tmp$center, seq(1,6)/6)
  tmp_loess_fit <- crossing(center=centers, loess_fit) %>%
    group_by(center) %>%
    filter(day <= center) %>%
    ungroup()
  
  tmp %>% filter(center %in% centers) %>%
    ggplot() +
    geom_smooth(aes(day, margin), method = "lm", se = FALSE) +
    geom_point(aes(day, margin, size = weight), data = polls_2008, size = 3, alpha = .5, color = "grey") +
    geom_point(aes(day, margin)) +
    geom_line(aes(x=day, y = .fitted), data = tmp_loess_fit, color = "red") +
    facet_wrap(~center, nrow = 2)
}
```

El resultado final es un ajuste más suave que ese producido por la suavización de compartimiento porque utilizamos tamaños de muestra más grandes para estimar nuestros parámetros locales:

```{r final-loess}
total_days <- diff(range(polls_2008$day))
span <- 21/total_days

fit <- loess(margin ~ day, degree=1, span = span, data=polls_2008)

polls_2008 %>% mutate(smooth = fit$fitted) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(day, smooth), color="red")
```

Podemos ver cómo diferentes tamaños de ventanas, _spans_, conducen a diferentes estimadores:

```{r loess-multi-span-animation, echo=FALSE, warning=FALSE, message=FALSE}
spans <- c(.66, 0.25, 0.15, 0.10)

fits <- tibble(span = spans) %>%
  group_by(span) %>%
  do(broom::augment(loess(margin ~ day, degree=1, span = .$span, data=polls_2008)))

tmp <- fits %>%
  crossing(center = polls_2008$day) %>%
  mutate(dist = abs(day - center)) %>%
  filter(rank(dist) / n() <= span) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

if(!file.exists(file.path(img_path, "loess-multi-span-animation.gif"))){
  p <- ggplot(tmp, aes(day, margin)) +
    scale_size(range = c(0, 2)) +
    geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
    geom_point(data = polls_2008, size = 2, alpha = .5, color = "grey") +
    geom_line(aes(x=day, y = .fitted, frame = day, cumulative = TRUE),
              data = fits, color = "red") +
    geom_point(aes(size = weight, frame = center)) +
    facet_wrap(~span)
  
  gganimate(p, filename = file.path(img_path, "loess-multi-span-animation.gif"), interval= .1)
}
if(knitr::is_html_output()){
  knitr::include_graphics(file.path(img_path,"loess-multi-span-animation.gif"))
} else{
  centers <- quantile(tmp$center, seq(1,3)/3)
  tmp_fits <- crossing(center=centers, fits) %>%
    group_by(center) %>%
    filter(day <= center) %>%
    ungroup()
  
  tmp %>% filter(center %in% centers) %>%
    ggplot() +
    geom_smooth(aes(day, margin), method = "lm", se = FALSE) +
    geom_point(aes(day, margin, size = weight), data = polls_2008, size = 3, alpha = .5, color = "grey") +
    geom_point(aes(day, margin)) +
    geom_line(aes(x=day, y = .fitted), data = tmp_fits, color = "red") +
    facet_grid(span ~ center)
}

```

Aquí están los estimadores finales:

```{r loess-final, echo=FALSE}
tmp %>% ggplot(aes(day, margin)) +
  geom_point(size = 2, alpha = .5, color = "grey") +
  geom_line(aes(day, .fitted), data = fits, color = "red") +
  facet_wrap(~span)
```

Hay otras tres diferencias entre `loess` y el típico suavizador de compartimiento.

1\. En vez de mantener el tamaño del compartimiento igual, `loess` mantiene el mismo número de puntos utilizados en el ajuste local. Este número se controla a través del argumento `span`, que espera una proporción. Por ejemplo, si `N` es el número de puntos de datos y `span=0.5`, entonces para un determinado $x$, `loess` usará los `0.5 * N` puntos más cercanos a $x$ para el ajuste.

2\. Al ajustar una línea localmente, `loess` utiliza un enfoque _ponderado_. Básicamente, en lugar de usar mínimos cuadrados, minimizamos una versión ponderada:

$$
\sum_{i=1}^N w_0(x_i) \left[Y_i - \left\{\beta_0 + \beta_1 (x_i-x_0)\right\}\right]^2
$$

Sin embargo, en lugar del _kernel_ gaussiano, loess usa una función llamada el _Tukey tri-weight_:

$$
W(u)= \left( 1 - |u|^3\right)^3 \mbox{ if } |u| \leq 1 \mbox{ and } W(u) = 0 \mbox{ if } |u| > 1
$$

Para definir los pesos, denotamos $2h$ como el tamaño de la ventana y definimos:

$$
w_0(x_i) = W\left(\frac{x_i - x_0}{h}\right)
$$

Este _kernel_ difiere del _kernel_ gaussiano en que más puntos obtienen valores más cercanos al máximo:

```{r triweight-kernel, echo=FALSE, out.width="80%", fig.height=3, fig.width=6}
x_0 <- -125

tmp <- with(data.frame(day = seq(min(polls_2008$day), max(polls_2008$day), .25)),
            ksmooth(day, 1*I(day == x_0), kernel = "normal", x.points = day, bandwidth = 7))
p1 <- data.frame(x = tmp$x, w_0 = tmp$y) %>%
  mutate(w_0 = w_0/sum(w_0)) %>%
  ggplot(aes(x, w_0)) +
  geom_line() +
  ggtitle("Normal")

p2 <- data.frame(x = seq(min(polls_2008$day), max(polls_2008$day), length.out = 100)) %>%
  mutate(w_0 = (1 - (abs(x-x_0)/21)^3)^3*I(abs(x-x_0)<=21)) %>%
  ggplot(aes(x, w_0)) +
  geom_line() +
  ggtitle("Tri-weight")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```


3\. `loess` tiene la opción de ajustar el modelo local _robustamente_. Se implementa un algoritmo iterativo en el que, después de ajustar un modelo en una iteración, se detectan valores atípicos y se ponderan hacia abajo para la siguiente iteración. Para usar esta opción, usamos el argumento `family="symmetric"`.

### Ajustando con parábolas

El teorema de Taylor también nos dice que si miramos cualquier función matemática lo suficientemente cerca, parece una parábola. El teorema además establece que no tienen que mirar tan de cerca cuando se aproxima con parábolas como cuando se aproxima con líneas. Esto significa que podemos hacer que nuestras ventanas sean aún más grandes y ajustar parábolas en lugar de líneas.


$$
E[Y_i | X_i = x_i ] = \beta_0 + \beta_1 (x_i-x_0) + \beta_2 (x_i-x_0)^2 \mbox{ if } |x_i - x_0| \leq h
$$

Este es el procedimiento por defecto de la función `loess`. Es posible que hayan notado que cuando mostramos el código para usar loess, configuramos `degree = 1`. Esto le dice a loess que se ajuste a polinomios de grado 1, un nombre elegante para líneas. Si leen la página de ayuda para loess, verán que, para el argumento `degree`, el valor predeterminado es 2. Por defecto, loess se ajusta a parábolas, no a líneas. Aquí hay una comparación de las líneas de ajuste (guiones rojos) y las parábolas de ajuste (naranja sólido):

```{r polls-2008-parabola-line-loess}
total_days <- diff(range(polls_2008$day))
span <- 28/total_days
fit_1 <- loess(margin ~ day, degree=1, span = span, data=polls_2008)

fit_2 <- loess(margin ~ day, span = span, data=polls_2008)


polls_2008 %>% mutate(smooth_1 = fit_1$fitted, smooth_2 = fit_2$fitted) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(day, smooth_1), color="red", lty = 2) +
  geom_line(aes(day, smooth_2), color="orange", lty = 1)
```

`degree = 2` nos da resultados más ondulantes. Por eso, preferimos `degree = 1` ya que es menos propenso a este tipo de ruido.


### Cuidado con los parámetros de suavización predeterminados

__ggplot2__ utiliza loess en su función `geom_smooth`:

```{r ggplot-loess-default, warning=FALSE, message=FALSE}
polls_2008 %>% ggplot(aes(day, margin)) +
  geom_point() +
  geom_smooth()
```

Pero tengan cuidado con los parámetros predeterminados ya que rara vez son óptimos. Afortunadamente, pueden cambiarlos fácilmente:

```{r ggplot-loess-degree-1, warning=FALSE, message=FALSE}
polls_2008 %>% ggplot(aes(day, margin)) +
  geom_point() +
  geom_smooth(method = "loess", span = 0.15, method.args = list(degree=1))
```

## Conectando la suavización al _machine learning_ {#smoothing-ml-connection}

Para ver cómo la suavización se relaciona con el _machine learning_ usando un ejemplo concreto, consideren nuestro ejemplo introducido en la Sección \@ref(two-or-seven). Si definimos el resultado $Y = 1$ para dígitos que son siete e $Y=0$ para dígitos que son 2, entonces estamos interesados en estimar la probabilidad condicional:

$$
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2).
$$
con $X_1$ y $X_2$ los dos predictores definidos en la Sección \@ref(two-or-seven). En este ejemplo, los 0s y 1s que observamos son "ruidosos" porque para algunas regiones las probabilidades $p(x_1, x_2)$ no están tan cerca de 0 o 1. Por lo tanto, necesitamos estimar $p(x_1, x_2)$. La suavización es una alternativa para lograr esto. En la Sección \@ref(two-or-seven), vimos que la regresión lineal no era lo suficientemente flexible como para capturar la naturaleza no lineal de $p(x_1, x_2)$; los enfoques de suavización, por ende, pueden proveer una mejora. En el siguiente capítulo, describimos un algoritmo popular de _machine learning_, _k vecinos más cercanos_, que se basa en la suavización por compartimientos.

## Ejercicios

1\. En la parte de _wrangling_ de este libro, utilizamos el siguiente código para obtener recuentos de mortalidad para Puerto Rico para 2015-2018.

```{r, eval=FALSE}
library(tidyverse)
library(lubridate)
library(purrr)
library(pdftools)
library(dslabs)

fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf",
                  package="dslabs")
dat <- map_df(str_split(pdf_text(fn), "\n"), function(s){
  s <- str_trim(s)
  header_index <- str_which(s, "2015")[1]
  tmp <- str_split(s[header_index], "\\s+", simplify = TRUE)
  month <- tmp[1]
  header <- tmp[-1]
  tail_index  <- str_which(s, "Total")
  n <- str_count(s, "\\d+")
  out <- c(1:header_index, which(n == 1), 
           which(n >= 28), tail_index:length(s))
  s[-out] %>%  str_remove_all("[^\\d\\s]") %>% str_trim() %>%
    str_split_fixed("\\s+", n = 6) %>% .[,1:5] %>% as_tibble() %>% 
    setNames(c("day", header)) %>%
    mutate(month = month, day = as.numeric(day)) %>%
    pivot_longer(-c(day, month), names_to = "year", values_to = "deaths") %>%
    mutate(deaths = as.numeric(deaths))
}) %>%
  mutate(month = recode(month, 
                        "JAN" = 1, "FEB" = 2, "MAR" = 3, 
                        "APR" = 4, "MAY" = 5, "JUN" = 6, 
                        "JUL" = 7, "AGO" = 8, "SEP" = 9, 
                        "OCT" = 10, "NOV" = 11, "DEC" = 12)) %>%
  mutate(date = make_date(year, month, day)) %>%
  filter(date <= "2018-05-01")
```

Utilice la función `loess` para obtener un estimador uniforme del número esperado de muertes como función de la fecha. Grafique la función suave que resulta. Use un _span_ de dos meses.

2\. Grafique los estimadores suaves como función del día del año, todas en el mismo gráfico pero con diferentes colores.

3\. Suponga que queremos predecir 2s y 7s en nuestro set de datos  `mnist_27` con solo la segunda covariable. ¿Podemos hacer esto? A primera vista parece que los datos no tienen mucho poder predictivo. De hecho, si ajustamos una regresión logística regular, ¡el coeficiente para `x_2` no es significativo!

```{r, eval = FALSE}
library(broom)
library(dslabs)
data("mnist_27")
mnist_27$train %>%
  glm(y ~ x_2, family = "binomial", data = .) %>%
  tidy()
```

Graficar un diagrama de dispersión aquí no es útil ya que `y` es binario:

```{r, eval = FALSE}
qplot(x_2, y, data = mnist_27$train)
```

Ajuste una línea loess a los datos anteriores y grafique los resultados. Observe que hay poder predictivo, excepto que la probabilidad condicional no es lineal.


