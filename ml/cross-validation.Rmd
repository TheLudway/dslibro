# Validación cruzada {#cross-validation}


En este capítulo presentamos la validación cruzada (_cross validation_ en inglés), una de las ideas más importantes de _machine learning_. Aquí nos centramos en los aspectos conceptuales y matemáticos. Describiremos cómo implementar la validación cruzada en la práctica con el paquete __caret__ más adelante, en la Sección \@ref(caret-cv) del proximo capitulo. Para motivar el concepto, utilizaremos los dos datos de dígitos predictores presentados en la Sección \@ref(two-or-seven) e introducir, por primera vez, un algoritmo real de _machine learning_: _k-nearest neighbors_ (kNN).

## Motivación con _k-nearest neighbors_ {#knn-cv-intro}

Comencemos cargando los datos y mostrando una gráfica de los predictores con resultados representados con color.

```{r mnist-27-data, warning=FALSE, message=FALSE}
library(tidyverse)
library(dslabs)
data("mnist_27")
mnist_27$test%>% ggplot(aes(x_1, x_2, color = y)) + geom_point()
```

Utilizaremos estos datos para estimar la función de probabilidad condicional

$$
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2).
$$
como se define en la Sección \@ref(smoothing-ml-connection). Con _k-nearest neighbors_ (kNN) estimamos $p(x_1, x_2)$ de manera similar al alisado de contenedores. Sin embargo, como veremos, kNN es más fácil de adaptar a múltiples dimensiones. Primero definimos la distancia entre todas las observaciones en función de las características. Entonces, por cualquier punto $(x_1,x_2)$ para lo cual queremos una estimación de $p(x_1, x_2)$ buscamos el $k$ puntos más cercanos a $(x_1,x_2)$ y luego tome un promedio de los 0 y 1 asociados con estos puntos. Nos referimos al conjunto de puntos utilizados para calcular el promedio como el barrio. Debido a la conexión que describimos anteriormente entre las expectativas condicionales y las probabilidades condicionales, esto nos da un $\hat{p}(x_1,x_2)$, al igual que el bin suavizador nos dio una estimación de una tendencia. Al igual que con los suavizadores de contenedores, podemos controlar la flexibilidad de nuestra estimación, en este caso a través de $k$ parámetro: más grande $k$ s resultan en estimaciones más suaves, mientras que son más pequeñas $k$ s dan como resultado estimaciones más flexibles y más onduladas.


Para implementar el algoritmo, podemos usar el `knn3` función del paquete __caret__. Mirando el archivo de ayuda para este paquete, vemos que podemos llamarlo de una de dos maneras. Utilizaremos el primero en el que especificamos una _formula_ y un marco de datos. El marco de datos contiene todos los datos que se utilizarán. La fórmula tiene la forma. `outcome ~ predictor_1 + predictor_2 + predictor_3` y así. Por lo tanto, escribiríamos `y ~ x_1 + x_2`. Si vamos a usar todos los predictores, podemos usar el `.` me gusta esto `y ~ .`. La última llamada se ve así:


```{r, eval=FALSE}
library(caret)
knn_fit <- knn3(y ~ ., data = mnist_27$train)
```

Para esta función, también debemos elegir un parámetro: el número de _neighbors_ a incluir. Comencemos con el valor predeterminado $k=5$.

```{r}
knn_fit <- knn3(y ~ ., data = mnist_27$train, k = 5)
```

En este caso, dado que nuestro set de datos es equilibrado y nos preocupamos tanto por la sensibilidad como por la especificidad, utilizaremos la precisión para cuantificar el rendimiento.

Los `predict` función para `knn` produce una probabilidad para cada clase. Mantenemos la probabilidad de ser un 7 como estimación $\hat{p}(x_1, x_2)$

```{r}
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
```

En la sección \@ref(two-or-seven) utilizamos la regresión lineal para generar una estimación.
```{r}
fit_lm <- mnist_27$train %>%
mutate(y = ifelse(y == 7, 1, 0)) %>%
lm(y ~ x_1 + x_2, data = .)
p_hat_lm <- predict(fit_lm, mnist_27$test)
y_hat_lm <- factor(ifelse(p_hat_lm > 0.5, 7, 2))
confusionMatrix(y_hat_lm, mnist_27$test$y)$overall["Accuracy"]
```

Y vemos que kNN, con el parámetro predeterminado, ya supera la regresión. Para ver por qué este es el caso, graficaremos $\hat{p}(x_1, x_2)$ y compararlo con la probabilidad condicional verdadera $p(x_1, x_2)$:


```{r, echo=FALSE}
# We use this function to plot the estimated conditional probabilities
plot_cond_prob <- function(p_hat=NULL){
tmp <- mnist_27$true_p
if(!is.null(p_hat)){
tmp <- mutate(tmp, p=p_hat)
}
tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
geom_raster(show.legend = FALSE) +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
stat_contour(breaks=c(0.5),color="black")
}
```


```{r knn-fit, echo=FALSE, message=FALSE, warning=FALSE, out.width="100%"}
p1 <- plot_cond_prob() + ggtitle("True conditional probability")

p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) +
ggtitle("kNN-5 estimate")
library(gridExtra)

grid.arrange(p2, p1, nrow=1)
```

Vemos que kNN se adapta mejor a la forma no lineal de $p(x_1, x_2)$. Sin embargo, nuestra estimación tiene algunas islas de azul en el área roja, lo que intuitivamente no tiene mucho sentido. Esto se debe a lo que llamamos _over-training_. Describimos el sobreentrenamiento en detalle a continuación. El exceso de entrenamiento es la razón por la que tenemos una mayor precisión en el conjunto de trenes en comparación con el conjunto de prueba:

```{r}
y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class")
confusionMatrix(y_hat_knn, mnist_27$train$y)$overall["Accuracy"]

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]
```


### Sobreentrenamiento

El sobreentrenamiento es peor cuando establecemos $k=1$. Con $k=1$, la estimación para cada $(x_1, x_2)$ en el conjunto de entrenamiento se obtiene solo con el $y$ correspondiente a ese punto. En este caso, si el $(x_1, x_2)$ son únicos, obtendremos una precisión perfecta en el conjunto de entrenamiento porque cada punto se usa para predecirse a sí mismo. Recuerde que si los predictores no son únicos y tienen resultados diferentes para al menos un conjunto de predictores, entonces es imposible predecir perfectamente.

Aquí ajustamos un modelo kNN con $k=1$:

```{r}
knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$train, type = "class")
confusionMatrix(y_hat_knn_1, mnist_27$train$y)$overall[["Accuracy"]]
```

Sin embargo, la precisión del conjunto de prueba es en realidad peor que la regresión logística:

```{r}
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn_1, mnist_27$test$y)$overall["Accuracy"]
```

Podemos ver el problema de sobreajuste en esta figura.
```{r knn-1-overfit, echo=FALSE, out.width="100%"}
p1 <- mnist_27$true_p %>%
mutate(knn = predict(knn_fit_1, newdata = .)[,2]) %>%
ggplot() +
geom_point(data = mnist_27$train, aes(x_1, x_2, color= y),
pch=21, show.legend = FALSE) +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
stat_contour(aes(x_1, x_2, z = knn), breaks=c(0.5), color="black") +
ggtitle("Train set")

p2 <- mnist_27$true_p %>%
mutate(knn = predict(knn_fit_1, newdata = .)[,2]) %>%
ggplot() +
geom_point(data = mnist_27$test, aes(x_1, x_2, color= y),
pch=21, show.legend = FALSE) +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
stat_contour(aes(x_1, x_2, z = knn), breaks=c(0.5), color="black") +
ggtitle("Test set")

grid.arrange(p1, p2, nrow=1)
```

Las curvas negras denotan los límites de la regla de decisión.

El estimado $\hat{p}(x_1, x_2)$ sigue los datos de entrenamiento muy de cerca (izquierda). Puedes ver que en el conjunto de entrenamiento, los límites se han trazado para rodear perfectamente un único punto rojo en un mar azul. Porque la mayoría de los puntos $(x_1, x_2)$ son únicos, la predicción es 1 o 0 y la predicción para ese punto es la etiqueta asociada. Sin embargo, una vez que presentamos el conjunto de entrenamiento (derecha), vemos que muchas de estas pequeñas islas ahora tienen el color opuesto y terminamos haciendo varias predicciones incorrectas.

### Alisado excesivo

Aunque no tan mal como con los ejemplos anteriores, vimos eso con $k=5$ también hemos sobreentrenado. Por lo tanto, debemos considerar una mayor $k$. Probemos, por ejemplo, un número mucho mayor: $k=401$.

```{r}
knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn_401, mnist_27$test$y)$overall["Accuracy"]
```

Esto resulta ser similar a la regresión:
```{r mnist-27-glm-est, echo=FALSE, out.width="100%"}
p_hat <- predict(fit_lm, newdata = mnist_27$true_p)
p_hat <- scales::squish(p_hat, c(0, 1))
p1 <- plot_cond_prob(p_hat) +
ggtitle("Regression")

p2 <- plot_cond_prob(predict(knn_fit_401, mnist_27$true_p)[,2]) +
ggtitle("kNN-401")

grid.arrange(p1, p2, nrow=1)
```

Este tamaño de $k$ es tan grande que no permite suficiente flexibilidad. A esto lo llamamos _over-smoothing_.


### Escogiendo el $k$ en kNN

Entonces, ¿cómo elegimos $k$? En principio queremos elegir el $k$ que maximiza la precisión o minimiza el MSE esperado como se define en \@ref(loss-function). El objetivo de la validación cruzada es estimar estas cantidades para cualquier algoritmo y conjunto de parámetros de ajuste como $k$. Para entender por qué necesitamos un método especial para hacer esto, repitamos lo que hicimos anteriormente pero para diferentes valores de $k$:

```{r}
ks <- seq(3, 251, 2)
```

Hacemos esto usando `map_df` función para repetir lo anterior para cada uno.

```{r, warning=FALSE, message=FALSE}
library(purrr)
accuracy <- map_df(ks, function(k){
fit <- knn3(y ~ ., data = mnist_27$train, k = k)

y_hat <- predict(fit, mnist_27$train, type = "class")
cm_train <- confusionMatrix(y_hat, mnist_27$train$y)
train_error <- cm_train$overall["Accuracy"]

y_hat <- predict(fit, mnist_27$test, type = "class")
cm_test <- confusionMatrix(y_hat, mnist_27$test$y)
test_error <- cm_test$overall["Accuracy"]

tibble(train = train_error, test = test_error)
})
```

Tenga en cuenta que estimamos la precisión utilizando tanto el conjunto de entrenamiento como el conjunto de prueba. Ahora podemos trazar las estimaciones de precisión para cada valor de $k$:

```{r accuracy-vs-k-knn, echo=FALSE}
accuracy %>% mutate(k = ks) %>%
gather(set, accuracy, -k) %>%
mutate(set = factor(set, levels = c("train", "test"))) %>%
ggplot(aes(k, accuracy, color = set)) +
geom_line() +
geom_point()
```

Primero, tenga en cuenta que la estimación obtenida en el conjunto de entrenamiento es generalmente menor que la estimación obtenida con el conjunto de prueba, con una diferencia mayor para valores más pequeños de $k$. Esto se debe al sobreentrenamiento. También tenga en cuenta que la precisión frente a $k$ el gráfico es bastante irregular. No esperamos esto porque pequeños cambios en $k$ no debería afectar demasiado el rendimiento del algoritmo. La irregularidad se explica por el hecho de que la precisión se calcula en una muestra y, por lo tanto, es una variable aleatoria. Esto demuestra por qué preferimos minimizar la pérdida esperada en lugar de la pérdida que observamos con un set de datos.

Si tuviéramos que usar estas estimaciones para elegir $k$ que maximiza la precisión, usaríamos las estimaciones basadas en los datos de la prueba:

```{r}
ks[which.max(accuracy$test)]
max(accuracy$test)
```

Otra razón por la que necesitamos una mejor estimación de precisión es que si usamos el conjunto de prueba para elegir esto $k$, no debemos esperar que la estimación de precisión que se acompaña se extrapole al mundo real. Esto se debe a que incluso aquí rompimos una regla de oro del _machine learning_: seleccionamos el $k$ utilizando el conjunto de prueba. La validación cruzada también proporciona una estimación que tiene esto en cuenta.

## Descripción matemática de validación cruzada

En la Sección \@ref(loss-function), describimos que un objetivo común del _machine learning_ es encontrar un algoritmo que produzca predictores $\hat{Y}$ para un resultado $Y$ que minimiza el MSE:

$$
\mbox{MSE} = \mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
$$
Cuando todo lo que tenemos a nuestra disposición es un set de datos, podemos estimar el MSE con el MSE observado de esta manera:

$$
\hat{\mbox{MSE}} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
$$
Estos dos a menudo se conocen como el "error verdadero" y el "error aparente", respectivamente.

Hay dos características importantes del error aparente que siempre debemos tener en cuenta:

1. Debido a que nuestros datos son aleatorios, el error aparente es una variable aleatoria. Por ejemplo, el set de datos que tenemos puede ser una muestra aleatoria de una población más grande. Un algoritmo puede tener un error aparente menor que otro algoritmo debido a la suerte.

2. Si entrenamos un algoritmo en el mismo set de datos que usamos para calcular el error aparente, podríamos estar entrenando demasiado. En general, cuando hacemos esto, el error aparente será una subestimación del error verdadero. Veremos un ejemplo extremo de esto con _k-nearest neighbors_.

La validación cruzada es una técnica que nos permite aliviar estos dos problemas. Para comprender la validación cruzada, es útil pensar en el error verdadero, una cantidad teórica, como el promedio de muchos errores aparentes obtenidos al aplicar el algoritmo a $B$ nuevas muestras aleatorias de los datos, ninguna de ellas utilizada para entrenar el algoritmo. Como se muestra en un capítulo anterior, pensamos en el verdadero error como:


$$
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2
$$
con $B$ un gran número que puede considerarse prácticamente infinito.
Como ya se mencionó, esta es una cantidad teórica porque solo tenemos disponible un conjunto de resultados: $y_1, \dots, y_n$. La validación cruzada se basa en la idea de imitar la configuración teórica anterior de la mejor manera posible con los datos que tenemos. Para hacer esto, tenemos que generar una serie de diferentes muestras aleatorias. Hay varios enfoques que podemos usar, pero la idea general para todos ellos es generar aleatoriamente conjuntos de datos más pequeños que no se usan para el entrenamiento, y en su lugar se usan para estimar el error verdadero.

## Validación cruzada K-fold


```{r, include=FALSE}
if(knitr::is_html_output()){
knitr::opts_chunk$set(out.width = "500px",
out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"')
} else{
knitr::opts_chunk$set(out.width = "35%")
}
```

El primero que describimos es _K-fold cross validation_.
En términos generales, un desafío de _machine learning_ comienza con un set de datos (azul en la imagen a continuación). Necesitamos construir un algoritmo usando este set de datos que eventualmente se usará en conjuntos de datos completamente independientes (amarillo).

```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-1.png")
```

Pero no podemos ver estos conjuntos de datos independientes.

```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-2.png")
```

Entonces, para imitar esta situación, creamos una parte de nuestro set de datos y pretendemos que sea un set de datos independiente: dividimos el set de datos en un _conjunto de entrenamiento_ (azul) y un _conjunto de prueba_ (rojo). Entrenaremos nuestro algoritmo exclusivamente en el conjunto de entrenamiento y usaremos el conjunto de prueba solo para fines de evaluación.

Por lo general, intentamos seleccionar una pequeña parte del set de datos para tener la mayor cantidad de datos posible para entrenar. Sin embargo, también queremos que el conjunto de pruebas sea grande para que podamos obtener una estimación estable de la pérdida sin ajustar un número poco práctico de modelos. Las opciones típicas son usar del 10% al 20% de los datos para las pruebas.

```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-3.png")
```

Reiteremos que es indispensable que no usemos el conjunto de prueba en absoluto: no para filtrar filas, no para seleccionar características, ¡nada!

Ahora, esto presenta un nuevo problema porque para la mayoría de los algoritmos de _machine learning_ necesitamos seleccionar parámetros, por ejemplo, el número de _neighbors_ $k$ en _k-nearest neighbors_. Aquí, nos referiremos al conjunto de parámetros como $\lambda$. Necesitamos optimizar los parámetros del algoritmo sin usar nuestro conjunto de pruebas y sabemos que si optimizamos y evaluamos en el mismo set de datos, sobreentrenaremos. Aquí es donde la validación cruzada es más útil.

Para cada conjunto de parámetros de algoritmo que se considera, queremos una estimación del MSE y luego elegiremos los parámetros con el MSE más pequeño. La validación cruzada proporciona esta estimación.

Primero, antes de comenzar el procedimiento de validación cruzada, es importante corregir todos los parámetros del algoritmo. Aunque entrenaremos el algoritmo en el conjunto de conjuntos de entrenamiento, los parámetros $\lambda$ será el mismo en todos los conjuntos de entrenamiento. Usaremos $\hat{y}_i(\lambda)$ para denotar los predictores obtenidos cuando usamos parámetros $\lambda$.

Entonces, si vamos a imitar esta definición:


$$
\mbox{MSE}(\lambda) = \frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2
$$

queremos considerar conjuntos de datos que puedan considerarse una muestra aleatoria independiente y queremos hacerlo varias veces. Con la validación cruzada K-fold, lo hacemos $K$ veces. En los dibujos animados, estamos mostrando un ejemplo que usa $K=5$.

Eventualmente terminaremos con $K$ muestras, pero comencemos describiendo cómo construir el primero: simplemente elegimos $M=N/K$ observaciones al azar (redondeamos si $M$ no es un número redondo) y piense en esto como una muestra aleatoria $y_1^b, \dots, y_M^b$, con $b=1$. Llamamos a esto el conjunto de validación:


```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-4.png")
```

Ahora podemos ajustar el modelo en el conjunto de entrenamiento, luego calcular el error aparente en el conjunto independiente:

$$
\hat{\mbox{MSE}}_b(\lambda) = \frac{1}{M}\sum_{i=1}^M \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2
$$

Tenga en cuenta que esta es solo una muestra y, por lo tanto, devolverá una estimación ruidosa del error verdadero. Por eso tomamos $K$ muestras, no solo una. En la validación de K-cross, dividimos aleatoriamente las observaciones en $K$ conjuntos no superpuestos:


```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-5.png")
```


Ahora repetimos el cálculo anterior para cada uno de estos conjuntos $b=1,\dots,K$ y obtener $\hat{\mbox{MSE}}_1(\lambda),\dots, \hat{\mbox{MSE}}_K(\lambda)$. Luego, para nuestra estimación final, calculamos el promedio:

$$
\hat{\mbox{MSE}}(\lambda) = \frac{1}{B} \sum_{b=1}^K \hat{\mbox{MSE}}_b(\lambda)
$$

y obtener un estimado de nuestra pérdida. Un paso final sería seleccionar el $\lambda$ eso minimiza el MSE.

Hemos descrito cómo usar la validación cruzada para optimizar los parámetros. Sin embargo, ahora tenemos que tener en cuenta el hecho de que la optimización se produjo en los datos de entrenamiento y, por lo tanto, necesitamos una estimación de nuestro algoritmo final basado en datos que no se utilizaron para optimizar la elección. Aquí es donde usamos el conjunto de prueba que separamos al principio:


```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-6.png")
```

Podemos hacer una validación cruzada nuevamente:

```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-7.png")
```

y obtener una estimación final de nuestra pérdida esperada. Sin embargo, tenga en cuenta que esto significa que todo nuestro tiempo de cálculo se multiplica por $K$. Pronto aprenderá que realizar esta tarea lleva tiempo porque estamos realizando muchos cálculos complejos. Como resultado, siempre estamos buscando formas de reducir este tiempo. Para la evaluación final, a menudo solo usamos un conjunto de pruebas.

Una vez que estemos satisfechos con este modelo y queramos ponerlo a disposición de otros, podríamos reajustar el modelo en todo el set de datos, sin cambiar los parámetros optimizados.


```{r, echo=FALSE}
knitr::include_graphics("ml/img/cv-8.png")
```


Ahora, ¿cómo elegimos la validación cruzada? $K$? Grandes valores de $K$ son preferibles porque los datos de entrenamiento imitan mejor el set de datos original. Sin embargo, valores mayores de $K$ tendrá un tiempo de cálculo mucho más lento: por ejemplo, la validación cruzada 100 veces será 10 veces más lenta que la validación cruzada 10 veces. Por esta razón, las elecciones de $K=5$ y $K=10$ son populares

Una forma de mejorar la varianza de nuestra estimación final es tomar más muestras. Para hacer esto, ya no necesitaríamos que el conjunto de entrenamiento se dividiera en conjuntos no superpuestos. En cambio, simplemente elegiríamos $K$ conjuntos de algún tamaño al azar.

Una versión popular de esta técnica, en cada pliegue, selecciona observaciones al azar con reemplazo (lo que significa que la misma observación puede aparecer dos veces). Este enfoque tiene algunas ventajas (no se discute aquí) y generalmente se conoce como _bootstrap_. De hecho, este es el enfoque predeterminado en el paquete __caret__. Describimos cómo implementar la validación cruzada con el paquete __caret__ en el próximo capítulo. En la siguiente sección, incluimos una explicación de cómo funciona el _bootstrap_ en general.


## Ejercicios

Genere un conjunto de predictores aleatorios y resultados como este:

```{r, eval=FALSE}
set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n * p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
```

1\. Porque `x` y `y` son completamente independientes, no deberías poder predecir `y` utilizando `x` con precisión mayor que 0.5. Confirme esto ejecutando la validación cruzada utilizando regresión logística para ajustar el modelo. Debido a que tenemos tantos predictores, seleccionamos una muestra aleatoria `x_subset`. Use el subconjunto cuando entrene al modelo. Pista: usa el cursor `train` función. los `results` componente de la salida de `train` te muestra la precisión. Ignora las advertencias.


2\. Ahora, en lugar de una selección aleatoria de predictores, vamos a buscar aquellos que sean más predictivos del resultado. Podemos hacer esto comparando los valores para el $y=1$ grupo para aquellos en el $y=0$ grupo, para cada predictor, utilizando una prueba t. Puede realizar este paso así:

```{r, eval=FALSE}
devtools::install_bioc("genefilter")
install.packages("genefilter")
library(genefilter)
tt <- colttests(x, y)
```

Cree un vector de los valores p y llámelo `pvals`.


3\. Crear un índice `ind` con los números de columna de los predictores que estaban "estadísticamente significativos" asociados con `y`. Utilice un valor de corte p de 0.01 para definir "estadísticamente significativo". ¿Cuántos predictores sobreviven a este corte?


4\. Vuelva a ejecutar la validación cruzada pero después de redefinir `x_subset` ser el subconjunto de `x` definido por las columnas que muestran una asociación "estadísticamente significativa" con `y`. ¿Cuál es la precisión ahora?


5\. Vuelva a ejecutar la validación cruzada nuevamente, pero esta vez usando kNN. Pruebe la siguiente cuadrícula de parámetros de ajuste: `k = seq(101, 301, 25)`. Haz un diagrama de la precisión resultante.

6\. En los ejercicios 3 y 4, vemos que a pesar del hecho de que `x` y `y` son completamente independientes, pudimos predecir `y` con una precisión superior al 70%. Debemos estar haciendo algo mal entonces. ¿Qué es?

a. La función `train` estima la precisión en los mismos datos que usa para entrenar el algoritmo.
si. Estamos ajustando demasiado el modelo al incluir 100 predictores.
c. Utilizamos todo el set de datos para seleccionar las columnas utilizadas en el modelo. Este paso debe incluirse como parte del algoritmo. La validación cruzada se realizó **después de** esta selección.
re. La alta precisión se debe solo a la variabilidad aleatoria.

7\. Avanzado. Vuelva a realizar la validación cruzada, pero esta vez incluya el paso de selección en la validación cruzada. La precisión ahora debería estar cerca del 50%.


8\. Cargaue el set de datos `tissue_gene_expression`. Utilice la función `train` para predecir el tejido a partir de la expresión génica. Usa kNN. Qué `k` ¿funciona mejor?


```{r, include=FALSE}
knitr::opts_chunk$set(out.width = "70%", out.extra = NULL)
```

## Bootstrap

Suponga que la distribución del ingreso de su población es la siguiente:

```{r income-distribution}
set.seed(1995)
n <- 10^6
income <- 10^(rnorm(n, log10(45000), log10(3)))
qplot(log10(income), bins = 30, color = I("black"))
```

La mediana de la población es:

```{r}
m <- median(income)
m
```

Supongamos que no tenemos acceso a toda la población, pero queremos estimar la mediana $m$. Tomamos una muestra de 100 y estimamos la mediana de la población $m$ con la mediana de la muestra $M$:

```{r}
N <- 100
X <- sample(income, N)
median(X)
```

¿Podemos construir un intervalo de confianza? ¿Cuál es la distribución de $M$ ?

Debido a que estamos simulando los datos, podemos usar una simulación de Monte Carlo para aprender la distribución de $M$.

```{r median-is-normal, message=FALSE, warning=FALSE, out.width="100%", fig.width = 6, fig.height = 3}
library(gridExtra)
B <- 10^4
M <- replicate(B, {
X <- sample(income, N)
median(X)
})
p1 <- qplot(M, bins = 30, color = I("black"))
p2 <- qplot(sample = scale(M), xlab = "theoretical", ylab = "sample") +
geom_abline()
grid.arrange(p1, p2, ncol = 2)
```

Si conocemos esta distribución, podemos construir un intervalo de confianza. El problema aquí es que, como ya hemos descrito, en la práctica no tenemos acceso a la distribución. En el pasado, [fix ctl] hemos usado el teorema del límite central (CLT), pero el CLT que estudiamos se aplica a los promedios y aquí estamos interesados en la mediana. Podemos ver que el intervalo de confianza del 95% basado en CLT:

```{r}
median(X) + 1.96 * sd(X)/ sqrt(N) * c(-1, 1)
```

es bastante diferente del intervalo de confianza que generaríamos si conocemos la distribución real de $M$:

```{r}
quantile(M, c(0.025, 0.975))
```

El _bootstrap_ nos permite aproximar una simulación de Monte Carlo sin acceso a toda la distribución. La idea general es relativamente sencilla. Actuamos como si la muestra observada fuera la población. Luego muestreamos (con reemplazo) sets de datos, del mismo tamaño de muestra del set de datos original. Luego calculamos la estadística de resumen, en este caso la mediana, en estas _muestras de bootstrap_.

La teoría nos dice que, en muchas situaciones, la distribución de las estadísticas obtenidas con muestras de _bootstrap_ se aproxima a la distribución de nuestra estadística real. Así es como construimos muestras de _bootstrap_ y una distribución aproximada:


```{r}
B <- 10^4
M_star <- replicate(B, {
X_star <- sample(X, N, replace = TRUE)
median(X_star)
})
```

Tengan en cuenta que un intervalo de confianza construido con _bootstrap_ está mucho más cerca de uno construido con la distribución teórica:

```{r}
quantile(M_star, c(0.025, 0.975))
```

Para obtener más información sobre _bootstrap_, incluyendo las correcciones que se pueden aplicar para mejorar estos intervalos de confianza, consulte el libro _An introduction to the bootstrap_ de Efron y Tibshirani.

*Tengan en cuenta que podemos usar ideas similares a las utilizadas en el _bootstrap_ en la validación cruzada: [fix] en lugar de dividir los datos en particiones iguales, simplemente hacemos _bootstrap_ muchas veces.*

## Ejercicios

1\. La función `createResample` se puede utilizar para crear ejemplos de _bootstrap_. Por ejemplo, podemos crear 10 muestras de _bootstrap_ para el set de datos `mnist_27` así:

```{r, eval=FALSE}
set.seed(1995)
indexes <- createResample(mnist_27$train$y, 10)
```

?Cuántas veces `3`, `4` y `7` aparecen en el primer índice re-muestreado?


2\. Vemos que algunos números aparecen más de una vez y otros no aparecen ninguna vez. Esto tiene que ser así para que cada set de datos sea independiente. Repita el ejercicio para todos los índices [fix]  re-muestreados.


3\. Genere un set de datos aleatorio como este:

```{r, eval=FALSE}
y <- rnorm(100, 0, 1)
```

[fix] Estime el cuantil 75, que sabemos es:


```{r, eval = FALSE}
qnorm(0.75)
```

con el cuantil de muestra:
```{r, eval = FALSE}
quantile(y, 0.75)
```

Ejecute una simulación de Monte Carlo para conocer el valor esperado y el error estándar de esta variable aleatoria.


4\. En la práctica, no podemos ejecutar una simulación de Monte Carlo porque no sabemos si `rnorm` se está utilizando para simular los datos. Use el _bootstrap_ para estimar el error estándar usando solo la muestra inicial `y`. Use 10 muestras de _bootstrap_.



5\. Repita el ejercicio 4, pero con 10,000 muestras de _bootstrap_.
