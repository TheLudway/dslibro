# Machine learning en la práctica

Ahora que hemos aprendido varios métodos y los hemos explorado con ejemplos ilustrativos, los aplicaremos a un ejemplo real: los dígitos MNIST.

Podemos cargar estos datos usando el siguiente paquete de __dslabs__:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
mnist <- read_mnist()
```

El set de datos incluye dos componentes, un set de entrenamiento y un set de evaluación:

```{r}
names(mnist)
```

Cada uno de estos componentes incluye una matriz con atributos en las columnas:

```{r}
dim(mnist$train$images)
```

y un vector con las clases como enteros:

```{r}
class(mnist$train$labels)
table(mnist$train$labels)
```

Como queremos que este ejemplo se ejecute en una computadora portátil pequeña y en menos de una hora, consideraremos un subconjunto del set de datos. Tomaremos muestras de 10,000 filas aleatorias del set de entrenamiento y 1,000 filas aleatorias del set de evaluación:

```{r}
set.seed(1990)
index <- sample(nrow(mnist$train$images), 10000)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])

index <- sample(nrow(mnist$test$images), 1000)
x_test <- mnist$test$images[index,]
y_test <- factor(mnist$test$labels[index])
```

## Preprocesamiento

En _machine learning_, a menudo transformamos predictores antes de ejecutar el algoritmo. También eliminamos predictores que claramente no son útiles. Llamamos a estos pasos _preprocesamiento_ (_preprocessing_ en inglés).

Ejemplos de preprocesamiento incluyen estandarizar los predictores, transformar logarítmicamente algunos predictores, eliminar los predictores que están altamente correlacionados con otros y eliminar los predictores con muy pocos valores no únicos o una variación cercana a cero. Mostramos un ejemplo a continuación.

Podemos ejecutar la función `nearZero` del paquete __caret__ para ver que muchos atributos no varían mucho de una observación a otra. Podemos ver que hay una gran cantidad de atributos con variabilidad 0:

```{r pixel-sds, message=FALSE, warning=FALSE}
library(matrixStats)
sds <- colSds(x)
qplot(sds, bins = 256)
```

Esto se espera porque hay partes de la imagen que raras veces contienen escritura (píxeles oscuros).

El paquete __caret__ incluye una función que recomienda que se eliminen estos atributos debido a que la variación es casi cero:

```{r, message=FALSE, warning=FALSE}
library(caret)
nzv <- nearZeroVar(x)
```

Podemos ver las columnas que se recomiendan elimiar:

```{r, eval=FALSE}
image(matrix(1:784 %in% nzv, 28, 28))
```

```{r near-zero-image, fig.width = 4, fig.height = 4, out.width="50%"}
rafalib::mypar()
image(matrix(1:784 %in% nzv, 28, 28))
```

Entonces nos quedeamos con este número de columnas:

```{r}
col_index <- setdiff(1:ncol(x), nzv)
length(col_index)
```

Ahora estamos listos para adaptarnos a algunos modelos. Antes de comenzar, debemos agregar nombres de columna a las matrices de predictores, ya que estos son requeridos por __caret__:

```{r}
colnames(x) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(x)
```

## k-vecino más cercano y bosque aleatorio

Comencemos con kNN. El primer paso es optimizar para $k$. Tengan en cuenta que cuando ejecutamos el algoritmo, tendremos que calcular una distancia entre cada observación en el set de evaluación y cada observación en el set de entrenamiento. Hay muchos cálculos. Por lo tanto, utilizaremos la validación cruzada _k-fold_ para mejorar la velocidad.

Si ejecutamos el siguiente código, el tiempo de computación en una computadora portátil estándar será de varios minutos.

```{r mnist-knn-fit, eval=FALSE}
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn <- train(x[ ,col_index], y,
                   method = "knn",
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)
train_knn
```

En general, es una buena idea hacer una ejecución de prueba con un subconjunto de datos para tener una idea del tiempo antes de comenzar a ejecutar un código que puede tardar horas en completarse. Podemos hacer esto de la siguiente manera:

```{r, eval = FALSE}
n <- 1000
b <- 2
index <- sample(nrow(x), n)
control <- trainControl(method = "cv", number = b, p = .9)
train_knn <- train(x[index, col_index], y[index],
                   method = "knn",
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)
```

Entonces podemos aumentar `n` y `b` e intentar establecer un patrón de cómo afectan el tiempo de computación para tener una idea de cuánto tiempo tomará el proceso de ajuste para valores mayores de `n` y `b`. Quieren saber si una función tomará horas, o incluso días, antes de ejecutarla.

Una vez que optimicemos nuestro algoritmo, podemos aplicarlo a todo el set de datos:

```{r}
fit_knn <- knn3(x[, col_index], y, k = 3)
```

¡La exactitud es casi 0.95!
```{r}
y_hat_knn <- predict(fit_knn, x_test[, col_index], type="class")
cm <- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall["Accuracy"]
```

Ahora logramos una exactitud de aproximadamente 0.95. De la especificidad y sensibilidad, también vemos que los 8 son los más difíciles de detectar y que el dígito pronosticado incorrectamente con mas frecuencia es el 7.

```{r}
cm$byClass[,1:2]
```

Ahora veamos si podemos hacerlo aún mejor con el algoritmo de bosque aleatorio.

Con bosque aleatorio, el tiempo de cálculo es un reto. Para cada bosque, necesitamos construir cientos de árboles. También tenemos varios parámetros que podemos ajustar.

Debido a que con el bosque aleatorio el ajuste es la parte más lenta del procedimiento en lugar de la predicción (como con kNN), usaremos solo una validación cruzada de cinco pliegues (_folds_ en inglés). También reduciremos la cantidad de árboles que se ajustan ya que aún no estamos construyendo nuestro modelo final.

Finalmente, para calcular en un set de datos más pequeño, tomaremos una muestra aleatoria de las observaciones al construir cada árbol. Podemos cambiar este número con el argumento `nSamp`.


```{r mnist-rf, message=FALSE, warning=FALSE, cache=TRUE}
library(randomForest)
control <- trainControl(method="cv", number = 5)
grid <- data.frame(mtry = c(1, 5, 10, 25, 50, 100))

train_rf <- train(x[, col_index], y,
                  method = "rf",
                  ntree = 150,
                  trControl = control,
                  tuneGrid = grid,
                  nSamp = 5000)

ggplot(train_rf)
train_rf$bestTune
```

Ahora que hemos optimizado nuestro algoritmo, estamos listos para ajustar nuestro modelo final:

```{r, cache=TRUE}
fit_rf <- randomForest(x[, col_index], y,
                       minNode = train_rf$bestTune$mtry)
```

Para verificar que ejecutamos suficientes árboles, podemos usar la función `plot`:

```{r, eval=FALSE}
plot(fit_rf)
```


Vemos que logramos una alta exactitud:

```{r}
y_hat_rf <- predict(fit_rf, x_test[ ,col_index])
cm <- confusionMatrix(y_hat_rf, y_test)
cm$overall["Accuracy"]
```
<!--
Aquí hay algunos ejemplos de las imágenes originales y nuestras llamadas:
```{r mnist-examples-of-calls, echo=FALSE, out.width="100%"}
rafalib::mypar(3,4)
for(i in 1:12){
image(matrix(x_test[i,], 28, 28)[, 28:1],
main = paste("Our prediction:", y_hat_rf[i]),
xaxt="n", yaxt="n")
}
```
-->

Con algunos ajustes adicionales, podemos obtener una exactitud aún mayor.

## Importancia variable

La siguiente función calcula la importancia de cada atributo:

```{r}
imp <- importance(fit_rf)
```

Podemos ver qué atributos se utilizan más al graficar una imagen:


```{r eval=FALSE}
mat <- rep(0, ncol(x))
mat[col_index] <- imp
image(matrix(mat, 28, 28))
```

```{r importance-image, fig.width = 4, fig.height = 4, out.width="50%"}
rafalib::mypar()
mat <- rep(0, ncol(x))
mat[col_index] <- imp
image(matrix(mat, 28, 28))
```

## Evaluaciones visuales

Una parte importante del análisis de datos es visualizar los resultados para determinar por qué estamos fallando. Cómo hacemos esto depende de la aplicación. A continuación mostramos las imágenes de dígitos para los cuales hicimos una predicción incorrecta. Podemos comparar lo que obtenemos con kNN con bosque aleatorio.

Aquí vemos unos errores para el bosque aleatorio:

```{r knn-images, echo=FALSE, out.width="100%", fig.width=6, fig.height=1.65}
p_max <- predict(fit_knn, x_test[,col_index])
p_max <- apply(p_max, 1, max)
ind <- which(y_hat_knn != y_test)
ind <- ind[order(p_max[ind], decreasing = TRUE)]

rafalib::mypar(1,4)
for(i in ind[1:4]){
  image(matrix(x_test[i,], 28, 28)[, 28:1],
        main = paste0("Pr(",y_hat_knn[i],")=",round(p_max[i], 2)," but is a ",y_test[i]),
        xaxt="n", yaxt="n")
}
```


```{r rf-images,, echo=FALSE, out.width="100%", fig.width=6, fig.height=1.65}
p_max <- predict(fit_rf, x_test[,col_index], type = "prob")
p_max <- p_max/ rowSums(p_max)
p_max <- apply(p_max, 1, max)

ind <- which(y_hat_rf != y_test)
ind <- ind[order(p_max[ind], decreasing = TRUE)]

rafalib::mypar(1,4)
for(i in ind[1:4]){
  image(matrix(x_test[i,], 28, 28)[, 28:1],
        main = paste0("Pr(",y_hat_rf[i],")=",round(p_max[i], 2), " but is a ",y_test[i]),
        xaxt="n", yaxt="n")
}
```

Al examinar errores como este, a menudo encontramos debilidades específicas en los algoritmos o en las opciones de parámetros que podemos intentar corregir.

## Conjuntos

La idea de un conjunto (_ensemble_ en inglés) es similar a la idea de combinar datos de diferentes encuestadores para obtener un mejor estimado del verdadero apoyo para cada candidato.

En _machine learning_, generalmente se pueden mejorar los resultados finales combinando los resultados de diferentes algoritmos.

Aquí hay un ejemplo sencillo donde calculamos nuevas probabilidades de clase tomando el promedio de bosque aleatorio y kNN. Podemos ver que la exactitud mejora a 0.96:

```{r}
p_rf <- predict(fit_rf, x_test[,col_index], type = "prob")
p_rf<- p_rf/ rowSums(p_rf)
p_knn <- predict(fit_knn, x_test[,col_index])
p <- (p_rf + p_knn)/2
y_pred <- factor(apply(p, 1, which.max)-1)
confusionMatrix(y_pred, y_test)$overall["Accuracy"]
```

En los ejercicios, vamos a construir varios modelos de _machine learning_ para el set de datos `mnist_27` y luego construir un conjunto.

## Ejercicios

1\. Utilice el set de entrenamiento `mnist_27` para construir un modelo con varios de los modelos disponibles del paquete __caret__. Por ejemplo, puede tratar estos:

```{r, eval = FALSE}
models <- c("glm", "lda", "naive_bayes", "svmLinear", "gamboost",
            "gamLoess", "qda", "knn", "kknn", "loclda", "gam", "rf",
            "ranger","wsrf", "Rborist", "avNNet", "mlp", "monmlp", "gbm",
            "adaboost", "svmRadial", "svmRadialCost", "svmRadialSigma")
```

Aunque no hemos explicado muchos de estos algoritmos, aplíquelos usando `train` con todos los parámetros predeterminados. Guarde los resultados en una lista. Es posible que tenga que instalar algunos paquetes. Tenga en cuenta que probablemente recibirá algunas advertencias.


2\. Ahora que tiene todos los modelos entrenados en una lista, use `sapply` o `map` para crear una matriz de predicciones para el set de evaluación. Debería terminar con una matriz con `length(mnist_27$test$y)` filas y `length(models)` columnas.


3\. Ahora calcule la exactitud para cada modelo en el set de evaluación.

4\. Ahora construya una predicción de conjunto para el voto mayoritario y calcule su exactitud.

5\. Anteriormente calculamos la exactitud de cada método en el set de entrenamiento y notamos que variaban. ¿Qué métodos individuales funcionan mejor que el conjunto?

6\. Es tentador eliminar los métodos que no funcionan bien y volver a hacer el conjunto. El problema con este acercamiento es que estamos utilizando los datos de evaluación para tomar una decisión. Sin embargo, podríamos usar los estimados de exactitud obtenidos de la validación cruzada con los datos de entrenamiento. Obtenga estos estimados y guárdelos en un objeto.


7\. Ahora solo consideremos los métodos con una exactitud estimada de 0.8 al construir el conjunto. ¿Cuál es la exactitud ahora?


8\. __Avanzado__: Si dos métodos dan resultados que son iguales, unirlos no cambiará los resultados en absoluto. Para cada par de métricas,  compare cuán frequentemente predicen lo mismo. Entonces use la función `heatmap` para visualizar los resultados. Sugerencia: use el argumento  `method = "binary"` en la función `dist`.


9\. __Avanzado__: Tenga en cuenta que cada método también puede producir una probabilidad condicional estimada. En lugar del voto mayoritario, podemos tomar el promedio de estas probabilidades condicionales estimadas. Para la mayoría de los métodos, podemos usar el `type = "prob"` en la función `train`. Sin embargo, algunos de los métodos requieren que use el argumento `trControl=trainControl(classProbs=TRUE)` al llamar `train`. Además, estos métodos no funcionan si las clases tienen números como nombres. Sugerencia: cambie los niveles de esta manera:


```{r, eval = FALSE}
dat$train$y <- recode_factor(dat$train$y, "2"="two", "7"="seven")
dat$test$y <- recode_factor(dat$test$y, "2"="two", "7"="seven")
```


10\. En este capítulo, ilustramos un par de algoritmos de _machine learning_ en un subconjunto del set de datos MNIST. Intente ajustar un modelo a todo el set de datos.
