## Estudio de caso: ¿es un 2 o un 7? {#two-or-seven}

En los dos ejemplos anteriores, solo teníamos un predictor. Realmente no consideramos estos retos de _machine learning_, que se caracterizan por casos con muchos predictores. Volvamos al ejemplo de dígitos en el que teníamos 784 predictores. Para fines ilustrativos, comenzaremos simplificando este problema a uno con dos predictores y dos clases. Específicamente, definimos el desafío como construir un algoritmo que pueda determinar si un dígito es un 2 o 7 de los predictores. No estamos del todo listos para construir algoritmos con 784 predictores, por lo que extraeremos dos predictores sencillos de los 784: la proporción de píxeles oscuros que están en el cuadrante superior izquierdo ($X_1$) y el cuadrante inferior derecho ($X_2$).

Entonces seleccionamos una muestra aleatoria de 1,000 dígitos, 500 en el set de entrenamiento y 500 en el set de evaluación. Proveemos este set de datos en el paquete `dslabs`:

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(dslabs)
data("mnist_27")
```

Podemos explorar los datos graficando los dos predictores y usando colores para denotar las etiquetas:

```{r two-or-seven-scatter}
mnist_27$train %>% ggplot(aes(x_1, x_2, color = y)) + geom_point()
```

Inmediatamente vemos algunos patrones. Por ejemplo, si $X_1$ (el panel superior izquierdo) es muy grande, entonces el dígito es probablemente un 7. Además, para valores más pequeños de $X_1$, los 2s parecen estar en los valores de rango medio de $X_2$.

Para ilustrar como interpretar $X_1$ y $X_2$, incluimos cuatro imágenes como ejemplo. A la izquierda están las imágenes originales de los dos dígitos con los valores más grandes y más pequeños para $X_1$ y a la derecha tenemos las imágenes correspondientes a los valores más grandes y más pequeños de $X_2$:

```{r two-or-seven-images-large-x1, echo=FALSE, out.width="100%", fig.height=3, fig.width=6.5}
if(!exists("mnist")) mnist <- read_mnist()
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_1), which.max(mnist_27$train$x_1))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
  expand.grid(Row=1:28, Column=1:28) %>%
    mutate(label=titles[i],
           value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
p1 <- tmp %>% ggplot(aes(Row, Column, fill=value)) +
  geom_raster(show.legend = FALSE) +
  scale_y_reverse() +
  scale_fill_gradient(low="white", high="black") +
  facet_grid(.~label) +
  geom_vline(xintercept = 14.5) +
  geom_hline(yintercept = 14.5) +
  ggtitle("Largest and smallest x_1")



is <- mnist_27$index_train[c(which.min(mnist_27$train$x_2), which.max(mnist_27$train$x_2))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
  expand.grid(Row=1:28, Column=1:28) %>%
    mutate(label=titles[i],
           value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
p2 <- tmp %>% ggplot(aes(Row, Column, fill=value)) +
  geom_raster(show.legend = FALSE) +
  scale_y_reverse() +
  scale_fill_gradient(low="white", high="black") +
  facet_grid(.~label) +
  geom_vline(xintercept = 14.5) +
  geom_hline(yintercept = 14.5) +
  ggtitle("Largest and smallest x_2")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Comenzamos a tener una idea de por qué estos predictores son útiles, pero también por qué el problema será algo desafiante.

Realmente no hemos aprendido ningún algoritmo todavía, así que intentemos construir un algoritmo usando regresión. El modelo es simplemente:

$$
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2) =
\beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

Lo ajustamos así:

```{r}
fit <- mnist_27$train %>%
  mutate(y = ifelse(y==7, 1, 0)) %>%
  lm(y ~ x_1 + x_2, data = .)
```

Ahora podemos construir una regla de decisión basada en el estimador $\hat{p}(x_1, x_2)$:

```{r}
library(caret)
p_hat <- predict(fit, newdata = mnist_27$test)
y_hat <- factor(ifelse(p_hat > 0.5, 7, 2))
confusionMatrix(y_hat, mnist_27$test$y)$overall[["Accuracy"]]
```

Obtenemos una exactidud muy superior al 50%. No está mal para nuestro primer intento. ¿Pero podemos mejorar?

Como construimos el ejemplo `mnist_27` y tuvimos a nuestra disposición 60,000 dígitos solo en el set de datos MNIST, lo usamos para construir la distribución condicional _verdadera_ $p(x_1, x_2)$. Recuerden que esto es algo a lo que no tenemos acceso en la práctica, pero lo incluimos en este ejemplo porque permite comparar $\hat{p}(x_1, x_2)$ con la verdadera $p(x_1, x_2)$. Esta comparación nos enseña las limitaciones de diferentes algoritmos. Hagamos eso aquí. Hemos almacenado el verdadero $p(x_1,x_2)$ en el objeto `mnist_27` y podemos graficar la imagen usando la función `geom_raster()` de __ggplot2__ . Elegimos mejores colores y usamos la  función `stat_contour` para dibujar una curva que separa pares $(x_1,x_2)$ para cual $p(x_1,x_2) > 0.5$ y pares para cual $p(x_1,x_2) < 0.5$:

```{r true-p-better-colors}
mnist_27$true_p %>% ggplot(aes(x_1, x_2, z = p, fill = p)) +
  geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
  stat_contour(breaks=c(0.5), color="black")
```

Arriba vemos un gráfico del verdadero $p(x,y)$. Para comenzar a entender las limitaciones de la regresión logística aquí, primero tengan en cuenta que con la regresión logística $\hat{p}(x,y)$ tiene que ser un plano y, como resultado, el umbral definido por la regla de decisión lo da:
$\hat{p}(x,y) = 0.5$, lo que implica que el umbral no puede ser otra cosa que una línea recta:

$$
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
x_2 = (0.5-\hat{\beta}_0)/\hat{\beta}_2 -\hat{\beta}_1/\hat{\beta}_2 x_1
$$

Noten que, para este umbral, $x_2$ es una función lineal de $x_1$. Esto implica que nuestro enfoque de regresión logística no tiene posibilidades de capturar la naturaleza no lineal de la verdadera $p(x_1,x_2)$. A continuación se muestra una representación visual de $\hat{p}(x_1, x_2)$. Utilizamos la función `squish` del paquete __scales__ para restringir los estimados entre 0 y 1. Podemos ver dónde se cometieron los errores al mostrar también los datos y el umbral. Principalmente provienen de valores bajos $x_1$ que tienen un valor alto o bajo de $x_2$. La regresión no puede detectar esto.

```{r regression-p-hat, echo=FALSE, out.width="100%", fig.height=3, fig.width=7}
p_hat <- predict(fit, newdata = mnist_27$true_p)
p_hat <- scales::squish(p_hat, c(0, 1))
p1 <- mnist_27$true_p %>% mutate(p_hat = p_hat) %>%
  ggplot(aes(x_1, x_2, z=p_hat, fill=p_hat)) +
  geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks=c(0.5), color="black")

p2 <- mnist_27$true_p %>% mutate(p_hat = p_hat) %>%
  ggplot() +
  stat_contour(aes(x_1, x_2, z=p_hat), breaks=c(0.5), color="black") +
  geom_point(mapping = aes(x_1, x_2, color=y), data = mnist_27$test)
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Necesitamos algo más flexible: un método que permita estimadores con formas distintas a un plano.

Vamos a aprender algunos algoritmos nuevos basados en diferentes ideas y conceptos. Pero lo que todos tienen en común es que permiten enfoques más flexibles. Comenzaremos describiendo alogoritmos basados en _nearest neighbor_ o _kernels_. Para introducir los conceptos detrás de estos enfoques, comenzaremos nuevamente con un ejemplo unidimensional sencillo y describiremos el concepto de _suavización_ (_smoothing_ en inglés).

