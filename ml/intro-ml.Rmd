# (PART) Machine Learning {-}

```{r, echo=FALSE}
img_path <- "ml/img/"
```

# Introducción a _machine learning_

Quizás las metodologías de ciencia de datos más populares provienen del campo de _machine learning_. Las historias de éxito _machine learning_ incluyen lectores de códigos postales escritos a mano implementados por el servicio postal, tecnología de reconocimiento de voz como Siri de Apple, sistemas de recomendación de películas, detectores de spam y _malware_, automóviles sin conductor y predictores de precios de viviendas. Aunque hoy en día la Inteligencia Artificial y _machine learning_ se usan indistintamente, hacemos la siguiente distinción: mientras que los primeros algoritmos de inteligencia artificial, como esos utilizados por las máquinas de ajedrez, implementaron la toma de decisiones según reglas programables derivadas de la teoría o de los primeros principios, en _machine learning_ las decisiones de aprendizaje se basan en algoritmos **que se construyen con datos**.

## Notación

En _machine learning_, los datos se presentan en forma de:

1. el _resultado_ (_outcome_ en inglés) que queremos predecir y
2. las _características_ que usaremos para predecir el resultado

Queremos construir un algoritmo que tome los valores de las características como [fix] input/entrada y devuelva una predicción para el resultado cuando no sabemos el resultado. El enfoque de _machine learning_ consiste en entrenar un algoritmo utilizando un set de datos para el que conocemos el resultado, y luego aplicar este algoritmo en el futuro para hacer una predicción cuando no sabemos el resultado.

Aquí usaremos $Y$ para denotar el resultado y $X_1, \dots, X_p$ para denotar características. Tengan en cuenta que las características a veces se denominan predictores o covariables. Consideramos que todos estos son sinónimos.

Los problemas de predicción se pueden dividir en resultados categóricos y continuos. Para resultados categóricos, $Y$ puede ser cualquiera de $K$ clases El número de clases puede variar mucho entre las aplicaciones.
[fix not a sent; eng too] Por ejemplo, en los datos del lector de dígitos, $K=10$ siendo las clases los dígitos 0, 1, 2, 3, 4, 5, 6, 7, 8 y 9. En el reconocimiento de voz, los resultados son todas las palabras o frases posibles que estamos tratando de detectar. La detección de spam tiene dos resultados: spam o no spam. En este libro, denotamos las categorías $K$ con índices $k=1,\dots,K$. Sin embargo, para datos binarios usaremos $k=0,1$ para conveniencias matemáticas que demostraremos más adelante.

La configuración general es la siguiente. Tenemos una serie de características y un resultado desconocido que queremos predecir:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(dslabs)
tmp <- tibble(outcome="?",
'feature 1' = "$X_1$",
'feature 2' = "$X_2$",
'feature 3' = "$X_3$",
'feature 4' = "$X_4$",
'feature 5' = "$X_5$")
if(knitr::is_html_output()){
knitr::kable(tmp, "html", align = "c") %>%
kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
knitr::kable(tmp, "latex", align="c", escape = FALSE, booktabs = TRUE) %>%
kableExtra::kable_styling(font_size = 8)
}
```

Para _construir un modelo_ que provee una predicción para cualquier conjunto de valores observados $X_1=x_1, X_2=x_2, \dots X_5=x_5$, [fix collect] recogemos/recolectamos? datos para los cuales conocemos el resultado:

```{r, echo=FALSE}
n <- 2
tmp <- tibble(outcome = paste0("$y_{", 1:n,"}$"),
'feature 1' = paste0("$x_{",1:n,",1}$"),
'feature 2' = paste0("$x_{",1:n,",2}$"),
'feature 3' = paste0("$x_{",1:n,",3}$"),
'feature 4' = paste0("$x_{",1:n,",4}$"),
'feature 5' = paste0("$x_{",1:n,",5}$"))
tmp_2 <- rbind(c("$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\vdots$", "$\\vdots$"),
c("$y_n$", "$x_{n,1}$","$x_{n,2}$","$x_{n,3}$","$x_{n,4}$","$x_{n,5}$"))
colnames(tmp_2) <- names(tmp)
tmp <- bind_rows(tmp, as_tibble(tmp_2))
if(knitr::is_html_output()){
knitr::kable(tmp, "html") %>%
kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
knitr::kable(tmp, "latex", escape = FALSE, booktabs = TRUE) %>%
kableExtra::kable_styling(font_size = 8)
}
```

Cuando el resultado es continuo, nos referimos a la tarea de _machine learning_ como _predicción_, y el resultado principal del modelo es una función $f$ que produce automáticamente una predicción, denotada con $\hat{y}$, para cualquier conjunto de predictores: $\hat{y} = f(x_1, x_2, \dots, x_p)$. Usamos el término _resultado real_ (_actual outcome_ en inglés) para denotar lo que acabamos observando. Entonces queremos la predicción $\hat{y}$ para que coincida con resultado real $y$ lo mejor posible. Debido a que nuestro resultado es continuo, nuestras predicciones $\hat{y}$ no serán exactamente correctas o incorrectas, sino que determinaremos un _error_ definido como la diferencia entre la predicción y el resultado real $y - \hat{y}$.

Cuando el resultado es categórico, nos referimos a la tarea de _machine learning_ como _clasificación_, y el resultado principal del modelo será una _regla de decisión_ que prescribe cuál de las $K$ clases que debemos predecir. En este escenario, la mayoría de los modelos proporcionan funciones de los predictores para cada clase. $k$, $f_k(x_1, x_2, \dots, x_p)$, que se utilizan para tomar esta decisión. Cuando los datos son binarios, las reglas de decisión típicas se ven así: si $f_1(x_1, x_2, \dots, x_p) > C$, pronostique la categoría 1, si no la otra categoría, con $C$ un límite predeterminado. Debido a que los resultados son categóricos, nuestras predicciones serán correctas o incorrectas.

Tenga en cuenta que estos términos varían entre cursos, libros de texto y otras publicaciones. A menudo, el término _predicción_ se usa tanto para resultados categóricos como continuos, y el término _regresión_ puede usarse para el caso continuo. Aquí evitamos usar _regresión_ para evitar confusiones con nuestro uso previo del término _regresión lineal_. En la mayoría de los casos, estará claro si nuestros resultados son categóricos o continuos, por lo que evitaremos usar estos términos cuando sea posible.

## Un ejemplo

Consideremos el ejemplo del lector de código postal. El primer paso para manejar el correo que se recibe en la oficina de correos es ordenar las letras por código postal:

```{r, echo=FALSE, out.width="40%"}
knitr::include_graphics(file.path(img_path,"how-to-write-a-address-on-an-envelope-how-to-write-the-address-on-an-envelope-write-address-on-envelope-india-finishedenvelope-x69070.png"))
```

Originalmente, los humanos tenían que clasificarlos a mano. Para hacer esto, tuvieron que leer los códigos postales en cada carta. Hoy, gracias a los algoritmos de _machine learning_, una computadora puede leer códigos postales y luego un robot clasifica las letras. En esta parte del libro, aprenderemos cómo construir algoritmos que puedan leer un dígito.

El primer paso para construir un algoritmo es entender
¿cuáles son los resultados y las características? A continuación hay tres imágenes de dígitos escritos. Estos ya han sido leídos por un humano y se les ha asignado un resultado. $Y$. Estos se consideran conocidos y sirven como set de entrenamiento.

```{r digit-images-example, echo=FALSE, cache=TRUE}
mnist <- read_mnist()
tmp <- lapply( c(1,4,5), function(i){
expand.grid(Row=1:28, Column=1:28) %>%
mutate(id=i, label=mnist$train$label[i],
value = unlist(mnist$train$images[i,]))
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
geom_raster(show.legend = FALSE) +
scale_y_reverse() +
scale_fill_gradient(low="white", high="black") +
facet_grid(.~label)
```

Las imágenes se convierten en $28 \times 28 = 784$ píxeles y, para cada píxel, obtenemos una intensidad de escala de grises entre 0 (blanco) y 255 (negro), que consideramos continua por ahora. El siguiente gráfico muestra las características individuales de cada imagen:

```{r example-images, echo=FALSE}
tmp %>% ggplot(aes(Row, Column, fill=value)) +
geom_point(pch=21) +
scale_y_reverse() +
scale_fill_gradient(low="white", high="black") +
facet_grid(.~label)
```

Para cada imagen digitalizada $i$, tenemos un resultado categórico $Y_i$ que puede ser uno de los 10 valores ($0,1,2,3,4,5,6,7,8,9$) y características $X_{i,1}, \dots, X_{i,784}$. Usamos negrita $\mathbf{X}_i = (X_{i,1}, \dots, X_{i,784})$ para distinguir el vector de predictores de los predictores individuales. Cuando nos referimos a un conjunto arbitrario de características en lugar de una imagen específica en nuestro set de datos, descartamos el índice $i$ y usamos $Y$ y $\mathbf{X} = (X_{1}, \dots, X_{784})$. Utilizamos variables en mayúsculas porque, en general, pensamos en los predictores como variables aleatorias. Usamos minúsculas, por ejemplo $\mathbf{X} = \mathbf{x}$, para denotar valores observados. Cuando codificamos usamos minúsculas.

La tarea de _machine learning_ es construir un algoritmo que devuelva una predicción para cualquiera de los posibles valores de las características. Aquí, aprenderemos varios enfoques para construir estos algoritmos. Aunque en este punto puede parecer imposible lograr esto, comenzaremos con ejemplos simples y desarrollaremos nuestro conocimiento hasta que podamos atacar los más complejos. De hecho, comenzamos con un ejemplo artificialmente sencillo con un solo predictor y luego pasamos a un ejemplo un poco más realista con dos predictores. Una vez que comprendamos estos, atacaremos los desafíos de _machine learning_ del mundo real que involucran muchos predictores.

## Ejercicios

1\. Para cada uno de los siguientes, determine si el resultado es continuo o categórico:

a. Lector de dígitos
c. Recomendaciones de películas
c. Filtro de spam
d. Hospitalizaciones
e. Siri (reconocimiento de voz)


2\. ¿Cuántas funciones tenemos disponibles para la predicción en el set de datos de dígitos?


3\. En el ejemplo del lector de dígitos, los resultados se almacenan aquí:

```{r, eval=FALSE}
library(dslabs)
y <- mnist$train$labels
```

¿Las siguientes operaciones tienen un significado práctico?
```{r, eval=FALSE}
y[5] + y[6]
y[5] > y[6]
```

Eliga la mejor respuesta:

a. Sí porque $9 + 2 = 11$ y $9 > 2$.
b. No, porque `y` no es un vector numérico.
c. No, porque 11 no es un dígito. Son dos dígitos.
d. No, porque estas son etiquetas que representan una categoría, no un número. Un `9` representa una clase, no el número 9.

