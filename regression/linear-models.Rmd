## Confusión

Anteriormente, notamos una fuerte relación entre carreras y BB. Si encontramos la línea de regresión para predecir carreras desde bases por bolas, obtendremos una pendiente de:

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(Lahman)
get_slope <- function(x, y) cor(x, y) * sd(y)/ sd(x)

bb_slope <- Teams %>%
  filter(yearID %in% 1961:2001 ) %>%
  mutate(BB_per_game = BB/G, R_per_game = R/G) %>%
  summarize(slope = get_slope(BB_per_game, R_per_game))

bb_slope
```
Entonces, ¿esto significa que si contratamos jugadores de bajo salario con muchos BB y así aumentamos por 2 el número de BB por juego, nuestro equipo marcará `r round(bb_slope*2, 1)` más carreras por juego?

Nuevamente debemos recordar que la asociación no implica la causalidad. Los datos ofrecen evidencia sólida de que un equipo con dos BB más por juego que el equipo promedio, anota `r round(bb_slope*2, 1)` carreras por juego. Pero esto no significa que los BB sean la causa.

Noten que si calculamos la pendiente de la línea de regresión para sencillos obtenemos:

```{r}
singles_slope <- Teams %>%
  filter(yearID %in% 1961:2001 ) %>%
  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %>%
  summarize(slope = get_slope(Singles_per_game, R_per_game))

singles_slope
```

que es un valor más bajo que el que obtenemos para BB.

Además, observen que un sencillo lleva a un jugador a primera base igual que un BB. Aquellos que saben de béisbol señalarán que con un sencillo, los corredores en base tienen una mejor oportunidad de anotar que con un BB. Entonces, ¿cómo puede un BB ser más predictivo de las carreras? La razón por la que esto sucede es por _confusión_. Aquí mostramos la correlación entre HR, BB y sencillos:

```{r}
Teams %>%
  filter(yearID %in% 1961:2001 ) %>%
  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %>%
  summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles))
```

Resulta que los lanzadores, temerosos de los HR, a veces evitarán lanzar _strikes_ a los bateadores de HR. Como resultado, los bateadores de HR tienden a tener más BB y un equipo con muchos bateadores de HR también tendrá más BB. Aunque puede parecer que BB causan carreras, realmente son HR los que causan la mayoría de estas carreras. Decimos que BB están _confundidos_ (_confounded_ en inglés) con HR. Sin embargo, ¿es posible que las BB todavía ayuden? Para averiguar, tenemos que ajustar para el efecto de HR. La regresión también puede ayudar con esto.

### Cómo entender la confusión a través de la estratificación

Un primer acercamiento es mantener los HR fijos a un valor determinado y luego examinar la relación entre BB y las carreras. Como lo hicimos cuando estratificamos a los padres redondeando a la pulgada más cercana, aquí podemos estratificar HR por juego a los diez más cercanos. Filtramos los estratos con pocos puntos para evitar estimadores muy variables:

```{r}
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR_strata = round(HR/G, 1),
         BB_per_game = BB/ G,
         R_per_game = R/ G) %>%
  filter(HR_strata >= 0.4 & HR_strata <=1.2)
```

y luego hacemos un diagrama de dispersión para cada estrato:

```{r runs-vs-bb-by-hr-strata, out.width="80%", fig.height=5}
dat %>%
  ggplot(aes(BB_per_game, R_per_game)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap( ~ HR_strata)
```

Recuerden que la pendiente de regresión para predecir carreras con BB era `r round(bb_slope, 1)`. Una vez que estratificamos por HR, estas pendientes se reducen sustancialmente:

```{r}
dat %>%
  group_by(HR_strata) %>%
  summarize(slope = get_slope(BB_per_game, R_per_game))
```

Las pendientes se reducen, pero no son 0, lo que indica que las BB son útiles para producir carreras, pero no tanto como se pensaba anteriormente. De hecho, los valores anteriores están más cerca de la pendiente que obtuvimos de sencillos, `r round(singles_slope, 2)`, que es más consistente con nuestra intuición. Dado que tanto los sencillos como los BB nos llevan a primera base, deberían tener aproximadamente el mismo poder predictivo.

Aunque nuestra comprensión de la aplicación nos dice que los HR causan BB pero no al revés, aún podemos verificar si la estratificación por BB hace que el efecto de HR disminuya. Para hacer esto, usamos el mismo código, excepto que intercambiamos HR y BB para obtener este gráfico:

```{r runs-vs-hr-by-bb-strata, out.width="100%",echo=FALSE}
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(BB_strata = round(BB/G,1),
         HR_per_game = HR/G,
         R_per_game = R/G) %>%
  filter(BB_strata >= 2.8 & BB_strata <= 3.9)

dat %>%
  ggplot(aes(HR_per_game, R_per_game)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap( ~ BB_strata)
```

En este caso, las pendientes no cambian mucho del valor original:

```{r}
dat %>% group_by(BB_strata) %>%
  summarize(slope = get_slope(HR_per_game, R_per_game))
```

Se reducen un poco, lo que es consistente con el hecho de que BB sí conducen a algunas carreras.


```{r}
hr_slope <- Teams %>%
  filter(yearID %in% 1961:2001 ) %>%
  mutate(HR_per_game = HR/G, R_per_game = R/G) %>%
  summarize(slope = get_slope(HR_per_game, R_per_game))

hr_slope
```

De todos modos, parece que si estratificamos por HR, tenemos una distribución normal de dos variables para carreras versus BB. Del mismo modo, si estratificamos por BB, tenemos una distribución normal de dos variables aproximada para HR versus carreras.

### Regresión lineal múltiple

Es un poco complejo calcular líneas de regresión para cada estrato. Básicamente, estamos ajustando modelos como este:

$$
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1(x_2) x_1 + \beta_2(x_1) x_2
$$

con las pendientes para $x_1$ cambiando para diferentes valores de $x_2$ y viceversa. ¿Pero hay un acercamiento más fácil?

Si tomamos en cuenta la variabilidad aleatoria, las pendientes en los estratos no parecen cambiar mucho. Si estas pendientes son iguales, esto implica que $\beta_1(x_2)$ y $\beta_2(x_1)$ son constantes. Esto a su vez implica que la expectativa de carreras condicionadas en HR y BB se puede escribir así:

$$
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

Este modelo sugiere que si el número de HR se fija en $x_2$, observamos una relación lineal entre carreras y BB con un intercepto de $\beta_0 + \beta_2 x_2$. Nuestro análisis exploratorio de datos sugirió esto. El modelo también sugiere que a medida que aumenta el número de HR, el crecimiento del intercepto también es lineal y está determinado por $\beta_1 x_1$.

En este análisis, denominado _regresión lineal múltiple_ (_multivariable linear regression_ en inglés), a menudo escucharán a la gente decir que la pendiente BB $\beta_1$ está _ajustada_ (_adjusted_ en inglés) para el efecto HR. Si el modelo es correcto, entonces se ha tomado en cuenta la confusión. ¿Pero cómo estimamos $\beta_1$ y $\beta_2$ de los datos? Para esto, aprendemos sobre modelos lineales y estimaciones de mínimos cuadrados.

## Estimaciones de mínimos cuadrados {#lse}

Hemos explicado cómo cuando los datos tienen una distribución normal de dos variables, entonces los valores esperados condicionales siguen la línea de regresión. El hecho de que el valor esperado condicional es una línea no es una suposición adicional, sino más bien un resultado derivado. Sin embargo, en la práctica es común escribir un modelo que describa la relación entre dos o más variables utilizando un _modelo lineal_ (_linear model_ en inglés).

Notamos que "lineal" aquí no se refiere exclusivamente a líneas, sino al hecho de que el valor esperado condicional es una combinación lineal de cantidades conocidas. En matemáticas, cuando multiplicamos cada variable por una constante y luego las sumamos, decimos que formamos una combinación lineal de las variables. Por ejemplo, $3x - 4y + 5z$ es una combinación lineal de $x$, $y$ y $z$. Además, podemos añadir una constante y por eso $2 + 3x - 4y + 5z$ también es una combinación lineal de $x$, $y$ y $z$.

Entonces $\beta_0 + \beta_1 x_1 + \beta_2 x_2$ es una combinación lineal de $x_1$ y $x_2$. El modelo lineal más sencillo es una constante $\beta_0$; el segundo más sencillo es una línea $\beta_0 + \beta_1 x$. Si tuviéramos que especificar un modelo lineal para los datos de Galton, denotaríamos las $N$ alturas de padres observadas con $x_1, \dots, x_n$, entonces modelamos las $N$ alturas de hijos que estamos tratando de predecir con:

$$
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \, i=1,\dots,N.
$$

Aquí $x_i$ es la altura del padre, que es fija (no aleatoria) debido al condicionamiento, y $Y_i$ es la altura aleatoria del hijo que queremos predecir. Suponemos además que $\varepsilon_i$ son independientes entre sí, tienen valor esperado 0 y la desviación estándar, llámenla $\sigma$, no depende de $i$.

En el modelo anterior, sabemos el $x_i$, pero para tener un modelo útil para la predicción, necesitamos $\beta_0$ y $\beta_1$. Los estimamos a partir de los datos. Una vez que hagamos esto, podemos predecir alturas de hijos para cualquier altura de padre $x$. Mostramos cómo hacer esto en la siguiente sección.

Noten que si además suponemos que el $\varepsilon$ tiene una distribución normal, entonces este modelo es exactamente el mismo que obtuvimos anteriormente suponiendo que los datos siguen una distribución normal de dos variables. Una diferencia algo matizada es que en el primer acercamiento suponemos que los datos siguen una distribución normal de dos variables y que no suponemos un modelo lineal, sino que lo derivamos. En la práctica, los modelos lineales son simplemente supuestos y no necesariamente suponemos normalidad: la distribución de $\varepsilon$s no se específica. Sin embargo, si sus datos siguen una distribución normal de dos variables, se cumple el modelo lineal anterior. Si sus datos no siguen una distribución normal de dos variables, necesitarán justificar el modelo de otra forma.

### Interpretando modelos lineales

Una razón por la que los modelos lineales son populares es porque son interpretables. En el caso de los datos de Galton, podemos interpretar los datos de esta manera: debido a los genes heredados, la predicción de la altura del hijo crece por $\beta_1$ para cada pulgada que aumentamos la altura del padre $x$. Porque no todos los hijos con padres de estatura $x$ son de la misma altura, necesitamos el término $\varepsilon$, lo que explica la variabilidad restante. Esta variabilidad restante incluye el efecto genético de la madre, los factores ambientales y otros factores biológicos aleatorios.

Dada la forma en que escribimos el modelo anterior, el intercepto $\beta_0$ no es muy interpretable, ya que es la altura pronosticada de un hijo con un padre que mide 0 pulgadas. Debido a la regresión a la media, la predicción generalmente será un poco más grande que 0. Para hacer que el parámetro de pendiente sea más interpretable, podemos reescribir el modelo como:

$$
Y_i = \beta_0 + \beta_1 (x_i - \bar{x}) + \varepsilon_i, \, i=1,\dots,N
$$


con $\bar{x} = 1/N \sum_{i=1}^N x_i$ el promedio de $x$. En este caso, $\beta_0$ representa la altura cuando $x_i = \bar{x}$, que es la altura del hijo de un padre promedio.

### Estimadores de mínimos cuadrados (LSE)

Para que los modelos lineales sean útiles, tenemos que estimar los $\beta$s desconocidos. El enfoque estándar en la ciencia es encontrar los valores que minimizan la distancia del modelo ajustado a los datos. La siguiente ecuación se llama la ecuación de mínimos cuadrados (LS o _least squares_ en inglés) y la veremos a menudo en este capítulo. Para los datos de Galton, escribiríamos:

$$
RSS = \sum_{i=1}^n \left\{ y_i - \left(\beta_0 + \beta_1 x_i \right)\right\}^2
$$

Esta cantidad se denomina suma de errores cuadrados (_residual sum of squares_ o RSS por sus siglas en inglés). Una vez que encontremos los valores que minimizan el RSS, llamaremos a los valores los estimadores de mínimos cuadrados (_least squares estimates_ o LSE por sus siglas en inglés) y los denotaremos con $\hat{\beta}_0$ y $\hat{\beta}_1$. Demostremos esto con el set de datos previamente definido:

```{r, message=FALSE}
library(HistData)
data("GaltonFamilies")
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
```

Escribamos una función que calcule el RSS para cualquier par de valores $\beta_0$ y $\beta_1$.

```{r}
rss <- function(beta0, beta1, data){
  resid <- galton_heights$son - (beta0+beta1*galton_heights$father)
  return(sum(resid^2))
}
```

Entonces, para cualquier par de valores, obtenemos un RSS. Aquí hay un gráfico del RSS como función de $\beta_1$ cuando mantenemos el $\beta_0$ fijo en 25.

```{r rss-versus-estimate}
beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() +
  geom_line(aes(beta1, rss))
```

Podemos ver un mínimo claro para $\beta_1$ alrededor de 0.65. Sin embargo, este mínimo para $\beta_1$ es para cuando $\beta_0 = 25$, un valor que elegimos arbitrariamente. No sabemos si (25, 0.65) es el par que minimiza la ecuación en todos los pares posibles.

El método de prueba y error no funcionarán en este caso. Podríamos buscar un mínimo dentro de una cuadrícula fina de valores de $\beta_0$ y $\beta_1$, pero esto requiere mucho tiempo innecesariamente ya que podemos usar cálculo: tomen las derivadas parciales, fíjenlas en 0 y resuelvan para $\beta_1$ y $\beta_2$. Por supuesto, si tenemos muchos parámetros, estas ecuaciones pueden volverse bastante complejas. Pero hay funciones en R que hacen estos cálculos por nosotros. Aprenderemos esto a continuación. Para aprender las matemáticas detrás de esto, pueden consultar un libro sobre modelos lineales.

### La función `lm` 

En R, podemos obtener los estimadores de mínimos cuadrados usando la función `lm`. Para ajustar el modelo:

$$
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
$$

con $Y_i$ la altura del hijo y $x_i$ la altura del padre, podemos usar este código para obtener los estimadores de mínimos cuadrados.

```{r}
fit <- lm(son ~ father, data = galton_heights)
fit$coef
```

La forma más común que usamos `lm` es mediante el uso del cáracter `~` para dejar `lm` saber cuál es la variable que estamos prediciendo (a la izquierda de `~`) y que estamos utilizando para predecir (a la derecha de `~`). El intercepto se agrega automáticamente al modelo que se ajustará.

El objeto `fit` incluye más información sobre el ajuste. Podemos usar la función `summary` para extraer más de esta información (que no mostramos):

```{r}
summary(fit)
```

Para entender parte de la información incluida en este resumen, debemos recordar que el LSE consiste de variables aleatorias. La estadística matemática nos da algunas ideas sobre la distribución de estas variables aleatorias.


### El LSE consiste de variables aleatorias

El LSE se deriva de los datos $y_1,\dots,y_N$, que son una realización de variables aleatorias $Y_1, \dots, Y_N$. Esto implica que nuestros estimadores son variables aleatorias. Para ver esto, podemos ejecutar una simulación Monte Carlo en la que suponemos que los datos de altura del hijo y del padre definen una población, tomar una muestra aleatoria de tamaño $N=50$, y calcular el coeficiente de pendiente de regresión para cada uno:

```{r}
B <- 1000
N <- 50
lse <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>%
    lm(son ~ father, data = .) %>%
    .$coef
})
lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,])
```

Podemos ver la variabilidad de los estimadores graficando sus distribuciones:

```{r lse-distributions, out.width="100%", fig.width=6, fig.height=3, echo=FALSE}
library(gridExtra)
p1 <- lse %>% ggplot(aes(beta_0)) +
  geom_histogram(binwidth = 5, color = "black")
p2 <- lse %>% ggplot(aes(beta_1)) +
  geom_histogram(binwidth = 0.1, color = "black")
grid.arrange(p1, p2, ncol = 2)
```

La razón por la que se ven normales es porque el teorema del límite central también aplica aquí: para $N$ suficientemente grande, los estimadores de mínimos cuadrados serán aproximadamente normales con el valor esperado $\beta_0$ y $\beta_1$, respectivamente. Los errores estándar son un poco complicados para calcular, pero la teoría matemática nos permite calcularlos y están incluidos en el resumen proporcionado por la función `lm`. Aquí lo vemos para uno de nuestros sets de datos simulados:

```{r}
sample_n(galton_heights, N, replace = TRUE) %>%
  lm(son ~ father, data = .) %>%
  summary %>% .$coef
```

Pueden ver que los estimadores de errores estándar informados por la función `summary` están cerca de los errores estándar de la simulación:

```{r}
lse %>% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))
```

La función `summary` también informa estadísticas t (`t value`) y valores-p (`Pr(>|t|)`). La estadística t no se basa realmente en el teorema del límite central, sino más bien en la suposición de que los $\varepsilon$s siguen una distribución normal. Bajo este supuesto, la teoría matemática nos dice que el LSE dividido por su error estándar, $\hat{\beta}_0/ \hat{\mbox{SE}}(\hat{\beta}_0 )$ y $\hat{\beta}_1/ \hat{\mbox{SE}}(\hat{\beta}_1 )$, sigue una distribución t con $N-p$ grados de libertad, con $p$ el número de parámetros en nuestro modelo. En el caso de la altura $p=2$, los dos valores-p prueban la hipótesis nula de que $\beta_0 = 0$ y $\beta_1=0$, respectivamente.

Recuerden que, como describimos en la Sección \@ref(t-dist), para $N$ suficientemente grande, el CLT funciona y la distribución t se vuelve casi igual a la distribución normal. Además, tengan en cuenta que podemos construir intervalos de confianza, pero pronto aprenderemos sobre __broom__, un paquete adicional que lo hace fácil.

Aunque no ofrecemos ejemplos en este libro, las pruebas de hipótesis con modelos de regresión se usan comúnmente en epidemiología y economía para hacer afirmaciones como "el efecto de A en B fue estadísticamente significativo después de ajustar por X, Y y Z". Sin embargo, varios supuestos tienen que ser válidos para que estas afirmaciones sean verdaderas.


### Valores pronosticados son variables aleatorias

Una vez que ajustemos nuestro modelo, podemos obtener predicciones de $Y$ usando los estimadores al modelo de regresión. Por ejemplo, si la altura del padre es $x$, entonces nuestra predicción $\hat{Y}$ para la altura del hijo será:

$$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 x$$

Cuando graficamos $\hat{Y}$ versus $x$, vemos la línea de regresión.

Tengan en cuenta que la predicción $\hat{Y}$ también es una variable aleatoria y la teoría matemática nos dice cuáles son los errores estándar. Si suponemos que los errores son normales o tienen un tamaño de muestra lo suficientemente grande, además podemos usar la teoría para construir intervalos de confianza. De hecho, la capa `geom_smooth(method = "lm")` de __ggplot2__ que anteriormente usamos grafica $\hat{Y}$ y lo rodea por intervalos de confianza:

```{r father-son-regression}
galton_heights %>% ggplot(aes(son, father)) +
  geom_point() +
  geom_smooth(method = "lm")
```

La función `predict` de R toma un objeto `lm` como entrada y devuelve la predicción. Si se lo pedimos, también provee los errores estándar y la información necesaria para construir intervalos de confianza:

```{r father-son-predictor}
fit <- galton_heights %>% lm(son ~ father, data = .)

y_hat <- predict(fit, se.fit = TRUE)

names(y_hat)
```

## Ejercicios


Hemos demostrado cómo BB y sencillos tienen un poder predictivo similar para anotar carreras. Otra forma de comparar la utilidad de estas métricas de béisbol es evaluando cuán estables son a lo largo de los años. Dado que tenemos que elegir jugadores a base de sus desempeños anteriores, preferiremos métricas que sean más estables. En estos ejercicios, compararemos la estabilidad de sencillos y BBs.

1\. Antes de comenzar, queremos generar dos tablas. Una para 2002 y otra para el promedio de las temporadas 1999-2001. Queremos definir estadísticas por turnos al bate. Aquí vemos como creamos la tabla para el 2017, quedándonos solo con jugadores con más de 100 turnos al bate.

```{r, eval=FALSE}
library(Lahman)
dat <- Batting %>% filter(yearID == 2002) %>%
  mutate(pa = AB + BB,
         singles = (H - X2B - X3B - HR)/ pa, bb = BB/ pa) %>%
  filter(pa >= 100) %>%
  select(playerID, singles, bb)
```

Ahora calcule una tabla similar pero con tasas calculadas durante 1999-2001.

2\. En la Sección \@ref(joins), aprenderemmos sobre `inner_join`, que se puede usar para poner los datos y promedios de 2001 en la misma tabla:

```{r, eval = FALSE}
dat <- inner_join(dat, avg, by = "playerID")
```

Calcule la correlación entre 2002 y las temporadas anteriores para sencillos y BB.


3\. Note que la correlación es mayor para BB. Para rápidamente tener una idea de la incertidumbre asociada con este estimador de correlación, ajustaremos un modelo lineal y calcularemos los intervalos de confianza para el coeficiente de pendiente. Sin embargo, primero haga diagramas de dispersión para confirmar que es apropiado ajustar un modelo lineal.


4\. Ahora ajuste un modelo lineal para cada métrica y use la  función `confint` para comparar los estimadores.


## Regresión lineal en el tidyverse

Para ver cómo usamos la función `lm` en un análisis más complejo, volvamos al ejemplo del béisbol. En ese caso, estimamos líneas de regresión para predecir carreras para BB en diferentes estratos de HR. Primero construimos un _data frame_ similar a este:

```{r}
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR = round(HR/G, 1),
         BB = BB/G,
         R = R/G) %>%
  select(HR, BB, R) %>%
  filter(HR >= 0.4 & HR<=1.2)
```

Como en ese momento no sabíamos de la función `lm` para calcular la línea de regresión en cada estrato, utilizamos la fórmula así:

```{r, eval=FALSE}
get_slope <- function(x, y) cor(x, y) * sd(y)/ sd(x)
dat %>%
  group_by(HR) %>%
  summarize(slope = get_slope(BB, R))
```

Argumentamos que las pendientes son similares y que las diferencias quizás se debieron a una variación aleatoria. Para ofrecer una defensa más rigurosa de que las pendientes eran las mismas, lo que condujo a nuestro modelo de múltiples variables, pudimos calcular los intervalos de confianza para cada pendiente. No hemos aprendido la fórmula para esto, pero la función `lm` provee suficiente información para construirlos.

Primero, noten que si intentamos usar la función `lm` para obtener la pendiente estimada de esta manera:

```{r}
dat %>%
  group_by(HR) %>%
  lm(R ~ BB, data = .) %>% .$coef
```

no obtenemos el resultado que queremos. La función `lm` ignora el `group_by` ya que `lm` no es parte del __tidyverse__ y no sabe cómo manejar el resultado de un tibble agrupado.

Las funciones de __tidyverse__ saben cómo interpretar los tibbles agrupados. Además, para facilitar la creación de una secuencia de comandos con el _pipe_ `%>%`, las funciones de __tidyverse__ consistentemente devuelven _data frames_, ya que esto asegura que el resultado de una función sea aceptado como la entrada de otra. Pero la mayoría de las funciones de R no reconocen los tibbles agrupados ni devuelven _data frames_. La función `lm`  es un ejemplo. Sin embargo, podemos escribir una función que usa `lm` para calcular y devolver los resúmenes relevantes en un data frame y entonces usar `summarize`: 


```{r}
get_slope <- function(x, y){
  fit <- lm(y ~ x)
  tibble(slope = fit$coefficients[2], 
         se = summary(fit)$coefficient[2,2])
}
dat %>%  
  group_by(HR) %>%
  summarize(get_slope(BB, R))
```

Aquí un ejemplo que devuelve estimadores para ambos parametros:


```{r}
get_lse <- function(x, y){
  fit <- lm(y ~ x)
  data.frame(term = names(fit$coefficients),
             slope = fit$coefficients, 
             se = summary(fit)$coefficient[,2])
}

dat %>%  
  group_by(HR) %>%
  summarize(get_lse(BB, R))
```

Si creen que todo esto es demasiado complicado, no son los únicos. Para simplificar las cosas, presentamos el paquete __broom__ que fue diseñado para facilitar el uso de funciones que ajustan modelos, como `lm`, con el __tidyverse__.

### El paquete __broom__

Nuestra tarea original era proveer un estimador y un intervalo de confianza para los estimadores de la pendiente de cada estrato. El paquete __broom__ hará esto bastante fácil.

El paquete __broom__ tiene tres funciones principales, todas de las cuales extraen información del objeto devuelto por `lm` y lo devuelve en un _data frame_ que __tidyverse__ entiende. Estas funciones son `tidy`, `glance` y `augment`. La función `tidy` devuelve estimadores e información relacionada como un _data frame_:

```{r}
library(broom)
fit <- lm(R ~ BB, data = dat)
tidy(fit)
```

Podemos agregar otros resúmenes importantes, como los intervalos de confianza:

```{r}
tidy(fit, conf.int = TRUE)
```

Debido a que el resultado es un _data frame_, podemos usarlo inmediatamente con `summarize` para unir los comandos que producen la tabla que queremos. Como se devuelve un _data frame_, podemos filtrar y seleccionar las filas y columnas que queramos, que facilita trabajar con __ggplot2__:

```{r do-tidy-example}
dat %>%  
  group_by(HR) %>%
  summarize(tidy(lm(R ~ BB), conf.int = TRUE)) %>%
  filter(term == "BB") %>%
  select(HR, estimate, conf.low, conf.high) %>%
  ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point()
```

Ahora volvemos a discutir nuestra tarea original de determinar si las pendientes cambiaron. El gráfico que acabamos de hacer, usando `summarize` y `tidy`, muestra que los intervalos de confianza se superponen, que provee una buena confirmación visual de que nuestra suposición de que la pendiente no cambia es cierta.

Las otras funciones ofrecidas por __broom__, `glance` y `augment`, se relacionan con resultados específicos del modelo y de la observación, respectivamente. Aquí podemos ver los resúmenes que resultan de ajustar modelos que `glance` devuelve:

```{r}
glance(fit)
```

Pueden obtener más información sobre estos resúmenes en cualquier libro de texto de regresión.

Veremos un ejemplo de `augment` en la siguiente sección.


## Ejercicios


1\. En una sección anterior, calculamos la correlación entre madres e hijas, madres e hijos, padres e hijas, y padres e hijos, y notamos que la correlación más alta es entre padres e hijos y la más baja es entre madres e hijos. Podemos calcular estas correlaciones usando:

```{r, eval=FALSE}
library(HistData)
data("GaltonFamilies")
set.seed(1)
galton_heights <- GaltonFamilies %>%
  group_by(family, gender) %>%
  sample_n(1) %>%
  ungroup()

cors <- galton_heights %>% 
  pivot_longer(father:mother, names_to = "parent", values_to = "parentHeight") %>%
  mutate(child = ifelse(gender == "female", "daughter", "son")) %>%
  unite(pair, c("parent", "child")) %>% 
  group_by(pair) %>%
  summarize(cor = cor(parentHeight, childHeight))
```

¿Son estas diferencias estadísticamente significativas? Para responder, calcularemos las pendientes de la línea de regresión junto con sus errores estándar. Comience usando `lm` y el paquete __broom__ para calcular el LSE de las pendientes y los errores estándar.


2\. Repita el ejercicio anterior, pero calcule también un intervalo de confianza.


3\. Grafique los intervalos de confianza y observe que se superponen, que implica que los datos son consistentes con que la herencia de altura y sexo son independientes.


4\. Debido a que estamos seleccionando niños al azar, podemos hacer algo como una prueba de permutación aquí. Repita el cálculo de correlaciones 100 veces tomando una muestra diferente cada vez. Sugerencia: use un código similar al que usamos con las simulaciones.

5\. Ajuste un modelo de regresión lineal para obtener los efectos de BB y HR en las carreras (a nivel de equipo) para el año 1971. Utilice la función `tidy` del paquete __broom__ para obtener los resultados en un _data frame_.


6\. Ahora repita lo anterior para cada año desde 1961 y haga un gráfico. Utilice `summarize` y el paquete __broom__ para ajustar este modelo para cada año desde 1961.


7\. Use los resultados del ejercicio anterior para graficar los efectos estimados de BB en las carreras.


8\. __Avanzado__. Escriba una función que tome R, HR y BB como argumentos y ajuste dos modelos lineales: `R ~ BB` y `R~BB+HR`. Luego use la función `summarize` para obtener el `BB` para ambos modelos para cada año desde 1961. Finalmente, grafíquelos como función de tiempo y compárelos.


## Estudio de caso: _Moneyball_ (continuación)

Al tratar de responder de cuán bien las BB predicen las carreras, la exploración de datos nos llevó a un modelo:

$$
\mbox{E}[R \mid BB = x_1, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

Aquí, los datos son aproximadamente normales y las distribuciones condicionales también fueron normales. Por lo tanto, tiene sentido usar un modelo lineal:

$$
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
$$

con $Y_i$ representando carreras por juego para el equipo $i$, $x_{i,1}$ representando BB por juego, y $x_{i,2}$ representando HR por juego. Para usar `lm` aquí, necesitamos que la función sepa que tenemos dos variables predictivas. Entonces usamos el símbolo `+` de la siguiente manera:


```{r}
fit <- Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(BB = BB/G, HR = HR/G, R = R/G) %>%
  lm(R ~ BB + HR, data = .)
```

Nosotros podemos usar `tidy` para ver un buen resumen:

```{r}
tidy(fit, conf.int = TRUE)
```


Cuando ajustamos el modelo con una sola variable, las pendientes estimadas fueron `r bb_slope` y `r hr_slope` para BB y HR, respectivamente. Tengan en cuenta que cuando se ajusta el modelo de múltiples variables, ambos disminuyen, con el efecto BB disminuyendo mucho más.

Ahora queremos construir una métrica para elegir jugadores. Tenemos que considerar sencillos, dobles y triples. ¿Podemos construir un modelo que prediga carreras basado en todos estos resultados?

Ahora vamos a dar un "salto de fe" y suponer que estas cinco variables son conjuntamente normales. Esto significa que si elegimos cualquiera de ellas y mantenemos las otras cuatro fijas, la relación con el resultado es lineal y la pendiente no depende de los cuatro valores que se mantienen constantes. Si esto es cierto, entonces un modelo lineal para nuestros datos es:

$$
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3}+ \beta_4 x_{i,4} + \beta_5 x_{i,5} + \varepsilon_i
$$

con $x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}$ representando BB, sencillos, dobles, triples y HR respectivamente.

Utilizando `lm`, podemos encontrar rápidamente el LSE para los parámetros usando:

```{r}
fit <- Teams %>%
  filter(yearID %in% 1961:2001) %>%
  mutate(BB = BB/ G,
         singles = (H - X2B - X3B - HR)/ G,
         doubles = X2B/ G,
         triples = X3B/ G,
         HR = HR/ G,
         R = R/ G) %>%
  lm(R ~ BB + singles + doubles + triples + HR, data = .)
```

Podemos ver los coeficientes usando `tidy`:

```{r}
coefs <- tidy(fit, conf.int = TRUE)

coefs
```

Para ver cuán bien nuestra métrica predice carreras, podemos predecir el número de carreras para cada equipo en 2002 usando la función `predict` y entonces hacer un gráfico:

```{r model-predicts-runs}
Teams %>%
  filter(yearID %in% 2002) %>%
  mutate(BB = BB/G,
         singles = (H-X2B-X3B-HR)/G,
         doubles = X2B/G,
         triples =X3B/G,
         HR=HR/G,
         R=R/G) %>%
  mutate(R_hat = predict(fit, newdata = .)) %>%
  ggplot(aes(R_hat, R, label = teamID)) +
  geom_point() +
  geom_text(nudge_x=0.1, cex = 2) +
  geom_abline()
```

Nuestro modelo hace un buen trabajo, como lo demuestra el hecho de que los puntos del gráfico observado versus los del gráfico previsto caen cerca de la línea de identidad.

Entonces, en lugar de usar el promedio de bateo o solo el número de HR como una medida de selección de jugadores, podemos usar nuestro modelo ajustado para formar una métrica que se relacione más directamente con la producción de carreras. Específicamente, para definir una métrica para un jugador A, imaginamos un equipo compuesto por jugadores como el jugador A y usamos nuestro modelo de regresión ajustado para predecir cuántas carreras produciría este equipo. La fórmula se vería así:
`r coefs$estimate[1]` +
`r coefs$estimate[2]` $\times$ BB +
`r coefs$estimate[3]` $\times$ singles +
`r coefs$estimate[4]` $\times$ dobles +
`r coefs$estimate[5]` $\times$ triples +
`r coefs$estimate[6]` $\times$ HR.

Para definir una métrica específica al jugador, tenemos un poco más de trabajo por hacer. Un reto aquí es que derivamos la métrica para equipos, basada en estadísticas de resumen a nivel de equipo. Por ejemplo, el valor de HR que se ingresa en la ecuación es HR por juego para todo el equipo. Si en cambio calculamos el HR por juego para un jugador, el valor será mucho más bajo dado que ya no son 9 bateadores contribuyendo al total sino un solo jugador. Además, si un jugador solo juega parte del juego y obtiene menos oportunidades que el promedio, todavía se considera un juego jugado. Para los jugadores, una tasa que toma en cuenta oportunidades es la tasa por turnos al bate.

Para hacer que la tasa de equipo por juego sea comparable a la tasa de jugador por turno al bate, calculamos el número promedio de turnos al bate por juego:

```{r}
pa_per_game <- Batting %>% filter(yearID == 2002) %>%
  group_by(teamID) %>%
  summarize(pa_per_game = sum(AB+BB)/max(G)) %>%
  pull(pa_per_game) %>%
  mean
```

Calculamos las tasas de turnos al bate para jugadores disponibles en 2002 con datos de 1997-2001. Para evitar pequeños artefactos de muestra, filtramos jugadores con menos de 200 turnos al bate por año. Aquí está el cálculo completo en una línea:

```{r}
players <- Batting %>% filter(yearID %in% 1997:2001) %>%
  group_by(playerID) %>%
  mutate(PA = BB + AB) %>%
  summarize(G = sum(PA)/pa_per_game,
            BB = sum(BB)/G,
            singles = sum(H-X2B-X3B-HR)/G,
            doubles = sum(X2B)/G,
            triples = sum(X3B)/G,
            HR = sum(HR)/G,
            AVG = sum(H)/sum(AB),
            PA = sum(PA)) %>%
  filter(PA >= 1000) %>%
  select(-G) %>%
  mutate(R_hat = predict(fit, newdata = .))
```

Las carreras que predecimos para cada jugador calculadas aquí se pueden interpretar como el número de carreras que predecimos que un equipo anotará si todos los bateadores son exactamente como ese jugador. La distribución demuestra que existe una gran variabilidad entre los jugadores:

```{r r-hat-hist}
qplot(R_hat, data = players, binwidth = 0.5, color = I("black"))
```

### Añadiendo información sobre salario y posición 

Para realmente construir el equipo, necesitaremos conocer sus salarios y su posición defensiva. Para hacer esto, unimos el _data frame_ `players` que acabamos de crear con el _data frame_ de información del jugador incluido en algunas de las otras tablas de datos de __Lahman__. Aprenderemos más sobre la función `join` en la Sección \@ref(joins).

Comiencen añadiendo los salarios del 2002 para cada jugador:

```{r}
players <- Salaries %>%
  filter(yearID == 2002) %>%
  select(playerID, salary) %>%
  right_join(players, by="playerID")
```

A continuación, agregamos su posición defensiva. Esta es una tarea algo complicada porque los jugadores juegan más de una posición cada año. La tabla `Appearances` del paquete __Lahman__ indica cuántos juegos jugó cada jugador en cada posición y podemos elegir la posición que más se jugó usando `which.max` en cada fila. Usamos `apply` para hacer esto. Sin embargo, debido a que algunos jugadores son intercambiados, aparecen más de una vez en la tabla, por lo que primero sumamos sus turnos al bate en los equipos. Aquí, escogemos la posición en la que más jugó el jugador usando la función `top_n`. Para asegurarnos de que solo elegimos una posición, en el caso de empates, elegimos la primera fila del _data frame_ resultante. También eliminamos la posición `OF` que significa _outfielder_, una generalización de tres posiciones: jardín izquierdo (LF o _left field_ en inglés), jardín central (CF o _center field_ en inglés) y campo derecho (RF o _right field_ en inglés). Además, eliminamos los lanzadores, ya que no batean en la liga en la que juegan los Atléticos.

```{r}
position_names <-
  paste0("G_", c("p","c","1b","2b","3b","ss","lf","cf","rf", "dh"))

tmp <- Appearances %>%
  filter(yearID == 2002) %>%
  group_by(playerID) %>%
  summarize_at(position_names, sum) %>%
  ungroup()

pos <- tmp %>%
  select(position_names) %>%
  apply(., 1, which.max)

players <- tibble(playerID = tmp$playerID, POS = position_names[pos]) %>%
  mutate(POS = str_to_upper(str_remove(POS, "G_"))) %>%
  filter(POS != "P") %>%
  right_join(players, by="playerID") %>%
  filter(!is.na(POS) & !is.na(salary))
```

Finalmente, agregamos su nombre y apellido:

```{r}
players <- Master %>%
  select(playerID, nameFirst, nameLast, debut) %>%
  mutate(debut = as.Date(debut)) %>%
  right_join(players, by="playerID")
```

Si son fanáticos del béisbol, reconocerán a los 10 mejores jugadores:

```{r}
players %>% select(nameFirst, nameLast, POS, salary, R_hat) %>%
  arrange(desc(R_hat)) %>% top_n(10)
```


### Escoger nueve jugadores

En promedio, los jugadores con una métrica más alta tienen salarios más altos:

```{r predicted-runs-vs-salary}
players %>% ggplot(aes(salary, R_hat, color = POS)) +
  geom_point() +
  scale_x_log10()
```

<!--Notice the very high salaries for most players. We do see some low-cost players with very high metrics. These will be great for our team. Some of these are likely young players that have not yet been able to negotiate a salary and are unavailable.

Aquí rehacemos el gráfico sin jugadores que debutaron antes de 1998. Usamos la función __lubridate__ `year`, introducido en la Sección \@ref(lubridate).
```{r predicted-runs-vs-salary-no-rookies, message=FALSE, warning=FALSE}
library(lubridate)
players %>% filter(year(debut) < 1998) %>%
ggplot(aes(salary, R_hat, color = POS)) +
geom_point() +
scale_x_log10()
```
-->
Podemos buscar buenas ofertas mirando a los jugadores que producen muchas más carreras que otros con salarios similares. Podemos usar esta tabla para decidir qué jugadores escoger y mantener nuestro salario total por debajo de los 40 millones de dólares con los que tuvo que trabajar Billy Beane. Esto se puede hacer usando lo que los científicos de la computación llaman programación lineal. Esto no es algo que enseñamos, pero aquí están los jugadores seleccionados con este acercamiento:

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(reshape2)
library(lpSolve)

players <- players %>% filter(lubridate::year(debut) < 1998)
constraint_matrix <- acast(players, POS ~ playerID, fun.aggregate = length)
npos <- nrow(constraint_matrix)
constraint_matrix <- rbind(constraint_matrix, salary = players$salary)
constraint_dir <- c(rep("==", npos), "<=")
constraint_limit <- c(rep(1, npos), 40*10^6)
lp_solution <- lp("max", players$R_hat,
                  constraint_matrix, constraint_dir, constraint_limit,
                  all.int = TRUE)
our_team <- players %>%
  filter(lp_solution$solution == 1) %>%
  arrange(desc(R_hat))
tmp <- our_team %>% select(nameFirst, nameLast, POS, salary, R_hat)
if(knitr::is_html_output()){
  knitr::kable(tmp, "html") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(tmp, "latex", booktabs = TRUE) %>%
    kableExtra::kable_styling(font_size = 8)
}

```

Vemos que todos estos jugadores tienen BB por encima del promedio y la mayoría tienen tasas de HR por encima del promedio, mientras que lo mismo no es cierto para sencillos. Aquí incluimos una tabla con estadísticas estandarizadas para todos los jugadores, de modo que, por ejemplo, los bateadores de HR por encima del promedio tienen valores superiores a 0.

```{r, echo=FALSE}
my_scale <- function(x) (x - median(x))/mad(x)
tmp <- players %>% mutate(BB = my_scale(BB),
                          singles = my_scale(singles),
                          doubles = my_scale(doubles),
                          triples = my_scale(triples),
                          HR = my_scale(HR),
                          AVG = my_scale(AVG),
                          R_hat = my_scale(R_hat)) %>%
  filter(playerID %in% our_team$playerID) %>%
  select(nameLast, BB, singles, doubles, triples, HR, AVG, R_hat) %>%
  arrange(desc(R_hat))
if(knitr::is_html_output()){
  knitr::kable(tmp, "html") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(tmp, "latex", booktabs = TRUE) %>%
    kableExtra::kable_styling(font_size = 8)
}
```

## La falacia de la regresión

Wikipedia define la _maldición de segundo año_ (_sophomore slump_ en inglés) como:

> Una caída de segundo año o maldición de segundo año se refiere a una instancia en la que un segundo esfuerzo, o segundo año, no cumple con los estándares del primer esfuerzo. Se usa comúnmente para referirse a la apatía de los estudiantes (segundo año de secundaria, colegio o universidad), el rendimiento de los atletas (segunda temporada de juego), cantantes/bandas (segundo álbum), programas de televisión (segunda temporada) y películas (secuelas/precuelas).

En las Grandes Ligas de Béisbol, el premio al novato del año (_Rookie of the Year_ o ROY por sus siglas en inglés) se otorga al jugador de primer año que se considera que ha tenido el mejor desempeño. La frase _maldición de segundo año_ se usa para describir la observación de que a los ganadores del premio ROY no les va tan bien durante su segundo año. Por ejemplo, este artículo de Fox Sports^[https://www.foxsports.com/stories/mlb/will-mlbs-tremendous-rookie-class-of-2015-suffer-a-sophomore-slump] que pregunta "¿La impresionante clase de novatos del MLB de 2015 sufrirá una maldición de segundo año?"

¿Los datos confirman la existencia de una maldición de segundo año? Vamos a ver. Al examinar los datos para el promedio de bateo, vemos que esta observación es válida para los ROY de mayor rendimiento:

<!--The data is available in the Lahman library, but we have to do some work to create a table with the statistics for all the ROY. First we create a table with player ID, their names, and their most played position.-->

```{r, echo=FALSE}
library(Lahman)
playerInfo <- Fielding %>%
  group_by(playerID) %>%
  arrange(desc(G)) %>%
  slice(1) %>%
  ungroup %>%
  left_join(Master, by="playerID") %>%
  select(playerID, nameFirst, nameLast, POS)
```

<!--
Ahora, crearemos una tabla con solo los ganadores del premio ROY y agregaremos sus estadísticas de bateo. Filtramos a los lanzadores, ya que los lanzadores no reciben premios por batear y nos vamos a centrar en la ofensiva. Específicamente, nos enfocaremos en el promedio de bateo ya que es el resumen del que la mayoría de los expertos hablan cuando se habla de la caída de segundo año:
-->

```{r, echo=FALSE}
ROY <- AwardsPlayers %>%
  filter(awardID == "Rookie of the Year") %>%
  left_join(playerInfo, by="playerID") %>%
  rename(rookie_year = yearID) %>%
  right_join(Batting, by="playerID") %>%
  mutate(AVG = H/AB) %>%
  filter(POS != "P")
```

<!--
También mantendremos solo las temporadas de novatos y de segundo año y eliminaremos a los jugadores que no jugaron las temporadas de segundo año:
-->
```{r, echo=FALSE}
ROY <- ROY %>%
  filter(yearID == rookie_year | yearID == rookie_year+1) %>%
  group_by(playerID) %>%
  mutate(rookie = ifelse(yearID == min(yearID), "rookie", "sophomore")) %>%
  filter(n() == 2) %>%
  ungroup %>%
  select(playerID, rookie_year, rookie, nameFirst, nameLast, AVG)
```
<!--
Finalmente, usaremos la función `spread` para tener una columna para los promedios de bateo de novato y segundo año
-->

```{r, echo=FALSE}
ROY <- ROY %>% spread(rookie, AVG) %>% arrange(desc(rookie))
tmp <- ROY %>% slice(1:5) %>%
  select(nameFirst, nameLast, rookie_year, rookie, sophomore)
if(knitr::is_html_output()){
  knitr::kable(tmp, "html") %>%
    kableExtra::kable_styling(bootstrap_options = "striped",
                              full_width = FALSE)
} else{
  knitr::kable(tmp, "latex", booktabs = TRUE) %>%
    kableExtra::kable_styling(font_size = 8)
}
```

De hecho, la proporción de jugadores que tienen un promedio de bateo más bajo en su segundo año es `r mean(ROY$sophomore - ROY$rookie<= 0)`.

Entonces, ¿es "nerviosismo" o "maldición"? Para responder a esta pregunta, volvamos nuestra atención a todos los jugadores que jugaron las temporadas 2013 y 2014 y batearon más de 130 veces (mínimo para ganar el ROY).

<!--We perform similar operations to what we did above: -->

```{r, echo=FALSE}
two_years <- Batting %>%
  filter(yearID %in% 2013:2014) %>%
  group_by(playerID, yearID) %>%
  filter(sum(AB) >= 130) %>%
  summarize(AVG = sum(H)/sum(AB)) %>%
  ungroup %>%
  spread(yearID, AVG) %>%
  filter(!is.na(`2013`) & !is.na(`2014`)) %>%
  left_join(playerInfo, by="playerID") %>%
  filter(POS!="P") %>%
  select(-POS) %>%
  arrange(desc(`2013`)) %>%
  select(nameFirst, nameLast, `2013`, `2014`)
```

El mismo patrón surge cuando miramos a los jugadores con el mejor desempeño: los promedios de bateo disminuyen para la mayoría de los mejores jugadores.

```{r, echo=FALSE}
tmp <- two_years %>% slice(1:5)
if(knitr::is_html_output()){
  knitr::kable(tmp, "html") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(tmp, "latex", booktabs = TRUE) %>%
    kableExtra::kable_styling(font_size = 8)
}
```

¡Pero estos no son novatos! Además, miren lo que les sucede a los peores jugadores del 2013:

```{r, echo=FALSE}
tmp <- arrange(two_years, `2013`) %>% slice(1:5)
if(knitr::is_html_output()){
  knitr::kable(tmp, "html") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(tmp, "latex", booktabs = TRUE) %>%
    kableExtra::kable_styling(font_size = 8)
}
```

¡Sus promedios de bateo en su mayoría suben! ¿Es esto una especie de "bendición" de segundo año? No lo es. No hay tal cosa como una maldición de segundo año. Todo esto se explica con un simple hecho estadístico: la correlación para el rendimiento en dos años separados es alta, pero no perfecta:

```{r regression-fallacy, echo=FALSE, fig.height=3, fig.width=3, out.width="40%"}
qplot(`2013`, `2014`, data = two_years)
```

La correlación es `r cor (two_years $"2013",two_years$"2014")` y
los datos se parecen mucho a una distribución normal de dos variables, que significa que predecimos un promedio de bateo $Y$ del 2014 para cualquier jugador que tuviera un promedio de bateo $X$ en el 2013 con:

$$ \frac{Y - .255}{.032} = 0.46 \left( \frac{X - .261}{.023}\right) $$

Como la correlación no es perfecta, la regresión nos dice que, en promedio, esperamos que los jugadores de alto desempeño del 2013 tengan un peor desempeño en 2014. No es una maldición; es solo por casualidad. El ROY se selecciona de los valores superiores de $X$ entonces se espera que $Y$ muestre regresión a la media.


## Modelos de error de medición

Hasta ahora, todos nuestros ejemplos de regresión lineal se han aplicado a dos o más variables aleatorias. Suponemos que los pares siguen una distribución normal de dos variables y lo usamos para motivar un modelo lineal. Este enfoque cubre la mayoría de los ejemplos reales de regresión lineal. La otra aplicación importante proviene de los modelos de errores de medición. En estas aplicaciones, es común tener una covariable no aleatoria, como el tiempo, y la aleatoriedad se introduce por error de medición en lugar de muestreo o variabilidad natural.

Para entender estos modelos, imaginen que son Galileo en el siglo XVI tratando de describir la velocidad de un objeto que cae. Un asistente sube a la torre de Pisa y deja caer una pelota, mientras que otros asistentes registran la posición en diferentes momentos. Simulemos algunos datos usando las ecuaciones que conocemos hoy y agregando algunos errores de medición. La función `rfalling_object` de __dslabs__ genera estas simulaciones:

```{r}
library(dslabs)
falling_object <- rfalling_object()
```

Los asistentes le entregan los datos a Galileo y esto es lo que él ve:

```{r gravity}
falling_object %>%
  ggplot(aes(time, observed_distance)) +
  geom_point() +
  ylab("Distance in meters") +
  xlab("Time in seconds")
```

Galileo no conoce la ecuación exacta, pero al observar el gráfico anterior, deduce que la posición debe seguir una parábola, que podemos escribir así:

$$ f(x) = \beta_0 + \beta_1 x + \beta_2 x^2$$

Los datos no caen exactamente en una parábola. Galileo sabe que esto se debe a un error de medición. Sus ayudantes cometen errores al medir la distancia. Para tomar esto en cuenta, modela los datos con:

$$ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i, i=1,\dots,n $$

con $Y_i$ representando la distancia en metros, $x_i$ representando el tiempo en segundos y $\varepsilon$ tomando en cuenta el error de medición. Se supone que el error de medición sea aleatorio, independiente el uno del otro y que con la misma distribución para cada $i$. También suponemos que no hay sesgo, que significa que el valor esperado $\mbox{E}[\varepsilon] = 0$.

Noten que este es un modelo lineal porque es una combinación lineal de cantidades conocidas ($x$ y $x^2$ son conocidas) y parámetros desconocidos (los $\beta$s son parámetros desconocidos para Galileo). A diferencia de nuestros ejemplos anteriores, aquí $x$ es una cantidad fija; no estamos condicionando.

Para plantear una nueva teoría física y comenzar a hacer predicciones sobre la caída de otros objetos, Galileo necesita números reales, en lugar de parámetros desconocidos. Usar el LSE parece un enfoque razonable. ¿Cómo encontramos el LSE?

Los cálculos del LSE no requieren que los errores sean aproximadamente normales. La función `lm`  encontrará los $\beta$s que minimizarán la suma residual de cuadrados:

```{r}
fit <- falling_object %>%
  mutate(time_sq = time^2) %>%
  lm(observed_distance~time+time_sq, data=.)
tidy(fit)
```

Verifiquemos si la parábola estimada se ajusta a los datos. La función `augment` de __broom__ nos permite hacer esto fácilmente:

```{r falling-object-fit}
augment(fit) %>%
  ggplot() +
  geom_point(aes(time, observed_distance)) +
  geom_line(aes(time, .fitted), col = "blue")
```


Gracias a nuestros maestros de física de escuela secundaria, sabemos que la ecuación para la trayectoria de un objeto que cae es:

$$d = h_0 + v_0 t - 0.5 \times 9.8 t^2$$

con $h_0$ y $v_0$ la altura inicial y la velocidad, respectivamente. Los datos que simulamos anteriormente siguieron esta ecuación y agregaron un error de medición a fin de simular `n` observaciones para dejar caer una pelota $(v_0=0)$ desde la torre de Pisa $(h_0=55.86)$.

Estos son consistentes con los estimadores de los parámetros:

```{r}
tidy(fit, conf.int = TRUE)
```

La altura de la torre de Pisa está dentro del intervalo de confianza para $\beta_0$, la velocidad inicial 0 está en el intervalo de confianza para $\beta_1$ (recuerden que el valor-p es mayor que 0.05) y la constante de aceleración está en un intervalo de confianza para $-2 \times \beta_2$.

## Ejercicios

Desde la década de 1980, los _sabermetricians_ han utilizado una estadística de resumen diferente del promedio de bateo para evaluar a los jugadores. Se dieron cuenta de que las BB eran importantes y que a los dobles, triples y HR se les debe dar más peso que los sencillos. Como resultado, propusieron la siguiente métrica:

$$
\frac{\mbox{BB}}{\mbox{PA}} + \frac{\mbox{Singles} + 2 \mbox{Doubles} + 3 \mbox{Triples} + 4\mbox{HR}}{\mbox{AB}}
$$

Denominaron a la métrica: _on-base-percentage plus slugging percentage_ o OPS. Aunque los _sabermetricians_ probablemente no usaron la regresión, aquí mostramos cómo OPS está cerca de lo que se obtiene con la regresión.

1\. Calcule el OPS para cada equipo en la temporada 2001. Luego grafique carreras por juego versus OPS.


2\. Para cada año desde 1961, calcule la correlación entre carreras por juego y OPS. Entonces grafique estas correlaciones como función del año.



3\. Tenga en cuenta que podemos reescribir OPS como un promedio ponderado de BB, sencillos, dobles, triples y HR. Sabemos que los coeficientes para dobles, triples y HR son 2, 3 y 4 veces mayores que para los sencillos. ¿Pero y los BB? ¿Cuál es el peso para BB en relación con sencillos? Sugerencia: el peso de BB en relación con sencillos será una función de AB y PA.


4\. Tenga en cuenta que el peso para BB, $\frac{\mbox{AB}}{\mbox{PA}}$, cambiará de un equipo a otro. Para ver cuán variable es, calcule y grafique esta cantidad para cada equipo para cada año desde 1961. Luego vuelva a graficarla, pero en lugar de calcularla para cada equipo, calcule y grafique la razón para todo el año. Entonces, una vez que esté claro de que no hay mucha tendencia de tiempo o equipo, indique el promedio general.


5\. Ahora sabemos que la fórmula para OPS es proporcional a $0.91 \times \mbox{BB} + \mbox{singles} + 2 \times \mbox{doubles} + 3 \times \mbox{triples} + 4 \times \mbox{HR}$. Veamos cómo se comparan estos coeficientes con esos obtenidos con la regresión. Ajuste un modelo de regresión a los datos después de 1961, como se hizo anteriormente: usando estadísticas por juego para cada año para cada equipo. Después de ajustar este modelo, indique los coeficientes como pesos relativos al coeficiente para sencillos.


6\. Vemos que nuestros coeficientes del modelo de regresión lineal siguen la misma tendencia general que esos utilizados por OPS, pero con un peso ligeramente menor para las métricas que no son sencillos. Para cada equipo en los años posteriores a 1961, calcule el OPS, las carreras predichas con el modelo de regresión y calcule la correlación entre los dos, así como la correlación con carreras por juego.

7\. Vemos que el uso del enfoque de regresión predice carreras un poco mejor que OPS, pero no tanto. Sin embargo, tenga en cuenta que hemos estado calculando OPS y prediciendo carreras para los equipos cuando estas medidas se utilizan para evaluar a los jugadores. Demostremos que OPS es bastante similar a lo que se obtiene con la regresión a nivel de jugador. Para la temporada de 1961 y las posteriores, calcule el OPS y las carreras previstas de nuestro modelo para cada jugador y grafíquelas. Use la corrección de PA por juego que usamos en el capítulo anterior.

8\. ¿Qué jugadores han mostrado la mayor diferencia entre su rango por carreras predichas y OPS?

