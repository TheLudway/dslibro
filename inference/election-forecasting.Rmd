## Estudio de caso: pronóstico de elecciones {#election-forecasting}

En una sección anterior, generamos las siguientes tablas de datos:
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
polls <- polls_us_election_2016 %>%
filter(state == "U.S." & enddate >= "2016-10-31" &
(grade %in% c("A+","A","A-","B+") | is.na(grade))) %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

one_poll_per_pollster <- polls %>% group_by(pollster) %>%
filter(enddate == max(enddate)) %>%
ungroup()

results <- one_poll_per_pollster %>%
summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
mutate(start = avg - 1.96*se, end = avg + 1.96*se)
```

A continuación, las utilizaremos para nuestro pronóstico.

### Enfoque bayesiano {#bayesian-approach}

Los encuestadores tienden a hacer declaraciones probabilísticas sobre los resultados de las elecciones. Por ejemplo, "La probabilidad de que Obama gane el colegio electoral es 91%" es una declaración probabilística sobre un parámetro que en secciones anteriores hemos denotado con $d$. Mostramos que para las elecciones del 2016, FiveThirtyEight le dio a Clinton una probabilidad de 81.4% de ganar el voto popular. Para hacer esto, utilizaron el enfoque bayesiano que describimos anteriormente.

Suponemos un modelo jerárquico similar al que hicimos para predecir el desempeño de un jugador de béisbol. Los libros de texto estadísticos escribirán el modelo así:

$$
\begin{aligned}
d &\sim N(\mu, \tau^2) \mbox{ describes our best guess had we not seen any polling data}\\
\bar{X} \mid d &\sim N(d, \sigma^2) \mbox{ describes randomness due to sampling and the pollster effect}
\end{aligned}
$$

Para hacer nuestro mejor pronóstico, notamos que antes de que haya datos de encuestas disponibles, podemos usar fuentes de datos que no son datos de encuestas. Un enfoque popular es utilizar lo que los encuestadores llaman _fundamentals_, que se basan en características que históricamente parecen tener un efecto a favor o en contra del partido en poder como, por ejemplo, el estado de la economía. No usaremos estos aquí. En cambio, usaremos $\mu = 0$, que se interpreta como un modelo que no ofrece información sobre quién ganará. Para la desviación estándar, usaremos datos históricos recientes que muestran que el ganador del voto popular tiene una variabilidad promedio de aproximadamente 3.5%. Por lo tanto, fijamos $\tau = 0.035$.

Ahora podemos usar las fórmulas para la distribución a posteriori del parámetro $d$: la probabilidad de que $d>0$ dado los datos de la encuesta observada:

```{r}
mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2/ (sigma^2 + tau^2)

posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

posterior_mean
posterior_se
```

Para hacer una declaración de probabilidad, usamos el hecho de que la distribución a posteriori también es normal. Y tenemos un intervalo de confianza de Bayes de:

```{r}
posterior_mean + c(-1.96, 1.96)*posterior_se
```

La probabilidad a posteriori $\mbox{Pr}(d>0 \mid \bar{X})$ se puede calcular así:

```{r}
1 - pnorm(0, posterior_mean, posterior_se)
```

Esto dice que estamos 100% seguros de que Clinton ganará el voto popular, lo que parece demasiado confiado. Además, no está de acuerdo con el 81.4% de FiveThirtyEight. ¿Qué explica esta diferencia?

### El sesgo general

Una vez finalizadas las elecciones, se puede observar la diferencia entre las predicciones de los encuestadores y el resultado real. Una observación importante que nuestro modelo no considera es que es común ver un sesgo general que afecta a muchos encuestadores de la misma manera, que entonces conduce a que los datos observados estén correlacionados. No hay una buena explicación para esto, pero se observa en datos históricos: en una elección, el promedio de las encuestas favorece a los demócratas por 2%, luego en las siguientes elecciones favorece a los republicanos por 1%, entonces en las próximas elecciones no hay sesgo, luego en la siguiente los republicanos son los favoritos por 3%, y así sucesivamente. En 2016, las encuestas favorecieron a los demócratas por 1-2%.

Aunque sabemos que este sesgo afecta a nuestras encuestas, no tenemos forma de saber cuán grande es este sesgo hasta la noche de las elecciones. Como consecuencia, no podemos corregir nuestras encuestas para tomar este sesgo en cuenta. Lo que podemos hacer es incluir un término en nuestro modelo que explique esta variabilidad.

### Representaciones matemáticas de modelos.

Supongamos que estamos recopilando datos de un encuestador y suponemos que no hay sesgo general. El encuestador recoge varias encuestas con un tamaño de muestra de $N$, por lo que observamos varias mediciones de la variabilidad $X_1, \dots, X_J$. La teoría nos dice que estas variables aleatorias tienen un valor esperado $d$ y un error estándar $2 \sqrt{p(1-p)/N}$. Comencemos usando el siguiente modelo para describir la variabilidad observada:

$$
X_j = d + \varepsilon_j.
$$
Usamos el índice $j$ para representar las diferentes encuestas y definimos $\varepsilon_j$ para ser una variable aleatoria que explica la variabilidad entre encuestas individuales introducida por el error de muestreo. Para hacer esto, suponemos que su promedio es 0 y su error estándar es $2 \sqrt{p(1-p)/N}$. Si $d$ es 2.1 y el tamaño de la muestra para estas encuestas es de 2,000, podemos simular $J=6$ puntos de datos de este modelo así:

```{r}
set.seed(3)
J <- 6
N <- 2000
d <- .021
p <- (d + 1)/2
X <- d + rnorm(J, 0, 2 * sqrt(p * (1 - p)/ N))
```

Ahora supongamos que tenemos $J=6$ puntos de datos de $I=5$ diferentes encuestadores. Para representar esto, necesitamos dos índices, uno para el encuestador y otro para las encuestas que cada encuestador toma. Usamos $X_{ij}$ con $i$ representando al encuestador y $j$ representando la encuesta número $j$ de ese encuestador. Si aplicamos el mismo modelo, escribimos:


$$
X_{i,j} = d + \varepsilon_{i,j}
$$

Para simular datos, ahora tenemos que usar un bucle para simular los datos de cada encuestador:

```{r}
I <- 5
J <- 6
N <- 2000
X <- sapply(1:I, function(i){
d + rnorm(J, 0, 2 * sqrt(p * (1 - p)/ N))
})
```

Los datos simulados realmente no parecen capturar las características de los datos reales:

```{r simulated-data-without-bias, echo=FALSE, message=FALSE, warning=FALSE}
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>% ungroup() %>%
select(pollster, spread) %>%
mutate(type = "Observed data", pollster = as.character(pollster)) %>%
bind_rows(data.frame(spread = as.vector(X) ,
pollster = I(rep(as.character(1:I), each=J)),
type = I("Simulated data"))) %>%
mutate(type = factor(type, levels = c("Simulated data", "Observed data"))) %>%
ggplot(aes(pollster, spread)) +
geom_point() +
coord_flip() +
facet_wrap( ~ type, scales = "free_y")
```

El modelo anterior no toma en cuenta la variabilidad entre encuestadores. Para arreglar esto, agregamos un nuevo término para el efecto de los encuestadores. Usaremos $h_i$ para representar el sesgo del encuestador número $i$. Le añadimos este nuevo término al modelo:

$$
X_{i,j} = d + h_i + \varepsilon_{i,j}
$$

Para simular datos de un encuestador específico, ahora necesitamos escojer un $h_i$ y luego agregar el $\varepsilon$s. Entonces, para un encuestador específico, suponemos que $\sigma_h$ es 0.025:

```{r}
I <- 5
J <- 6
N <- 2000
d <- .021
p <- (d + 1)/ 2
h <- rnorm(I, 0, 0.025)
X <- sapply(1:I, function(i){
d + h[i] + rnorm(J, 0, 2 * sqrt(p * (1 - p)/ N))
})
```

Los datos simulados ahora se parecen más a los datos reales:

```{r simulated-pollster-data, fig.height=3.78, fig.width=3, out.width="35%", echo=FALSE}
data.frame(Spread = as.vector(X) , Pollster = as.factor(rep(1:I, each=J))) %>%
ggplot(aes(Pollster, Spread)) +
geom_point() +
scale_y_continuous(limit = c(-0.056, 0.092)) +
coord_flip()
```

Noten que $h_i$ es común a todas las variabilidades observadas de un encuestador específico. Diferentes encuestadores tienen una $h_i$ diferente, lo que explica por qué cuando vemos los datos de los distintos encuestadores, podemos ver los diferentes grupos de puntos desplazarse hacia arriba y hacia abajo.

Ahora, en el modelo anterior, suponemos que el promedio de los sesgos de los encuestadores es 0. Creemos que para cada encuestador sesgado a favor de nuestro partido, hay otro a favor del otro partido y suponemos que la desviación estándar es $\sigma_h$. Pero históricamente vemos que cada elección tiene un sesgo general que afecta a todas las encuestas. Podemos observar esto con los datos del 2016, pero si recopilamos datos históricos, vemos que el promedio de las encuestas falla por más de lo que predicen modelos como el anterior. Para ver esto, tomaríamos el promedio de las encuestas para cada año electoral y lo compararíamos con el valor real. Si hiciéramos esto, veríamos una diferencia con una desviación estándar de entre 2-3%. Para incorporar esto en el modelo, podemos agregar otro término para explicar esta variabilidad:
$$
X_{i,j} = d + b + h_i + \varepsilon_{i,j}.
$$

Aquí $b$ es una variable aleatoria que explica la variabilidad de elección a elección. Esta variable aleatoria cambia de elección a elección, pero para cualquier elección dada, es la misma para todos los encuestadores y las encuestas dentro de la elección. Por eso no tiene índices. Esto implica que todas las variables aleatorias $X_{i,j}$ para un año electoral están correlacionadas ya que todas tienen $b$ en común.

Una forma de interpretar $b$ es como la diferencia entre el promedio de todas las encuestas de todos los encuestadores y el resultado real de la elección. Como no conocemos el resultado real hasta después de las elecciones, no podemos estimar $b$ hasta entonces. Sin embargo, podemos estimar $b$ de las elecciones anteriores y estudiar la distribución de estos valores. Conforme a este enfoque, suponemos que, a lo largo de los años electorales, $b$ tiene el valor esperado 0 y el error estándar es aproximadamente $\sigma_b = 0.025$.

Una implicación de agregar este término al modelo es que la desviación estándar de $X_{i,j}$ es mayor que lo que llamamos anteriormente $\sigma$, que combina la variabilidad del encuestador y la variabilidad de la muestra, y que se estimó con:

```{r}
sd(one_poll_per_pollster$spread)
```

Esta estimación no incluye la variabilidad introducida por $b$. Tengan en cuenta que como:

$$
\bar{X} = d + b + \frac{1}{N}\sum_{i=1}^N X_i,
$$

la desviación estándar de $\bar{X}$ es:

$$
\sqrt{\sigma^2/N + \sigma_b^2}.
$$
Ya que la misma $b$ está en cada medición, el promedio no reduce la variabilidad introducida por este término. Este es un punto importante: no importa cuántas encuestas realicen, este sesgo no se reduce.

Si rehacemos el cálculo bayesiano tomando en cuenta esta variabilidad, obtenemos un resultado mucho más cercano al de FiveThirtyEight:

```{r}
mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + .025^2)
Y <- results$avg
B <- sigma^2/ (sigma^2 + tau^2)

posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt( 1/ (1/sigma^2 + 1/tau^2))

1 - pnorm(0, posterior_mean, posterior_se)
```


### Prediciendo el colegio electoral

Hasta ahora nos hemos enfocado en el voto popular. Pero en Estados Unidos, las elecciones no se deciden por el voto popular, sino por lo que se conoce como el colegio electoral. Cada estado obtiene una cantidad de votos electorales que dependen, de una manera algo compleja, del tamaño de la población del estado. Aquí están los 5 principales estados clasificados por votos electorales en 2016.

```{r}
results_us_election_2016 %>% top_n(5, electoral_votes)
```

Con algunas excepciones menores que no discutimos, los votos electorales se ganan todo o nada. Por ejemplo, si un candidato gana California con solo 1 voto, aún obtiene los 55 votos electorales. Esto significa que al ganar algunos estados grandes por un amplio margen, pero al perder muchos estados pequeños por pequeños márgenes, se puede ganar el voto popular, pero perder el colegio electoral que es lo que decide el ganador. Esto sucedió en 1876, 1888, 2000 y 2016. La idea detrás de esto es evitar que algunos estados grandes tengan el poder de dominar las elecciones presidenciales. Sin embargo, muchas personas en Estados Unidos consideran que el colegio electoral es injusto y les gustaría abolirlo.

Ahora estamos listos para predecir el resultado del colegio electoral para 2016. Comenzamos agregando los resultados de una encuesta realizada durante la última semana antes de las elecciones. Utilizamos `str_detect`, una función que discutiremos más adelante en la Sección \@ref(stringr), para eliminar encuestas que cubren solo parte de un estado. 

```{r}
results <- polls_us_election_2016 %>%
filter(state!="U.S." &
!str_detect(state, "CD") &
enddate >="2016-10-31" &
(grade %in% c("A+","A","A-","B+") | is.na(grade))) %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
group_by(state) %>%
summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
mutate(state = as.character(state))
```

Aquí están los cinco estados con los resultados más cerrados según las encuestas:

```{r}
results %>% arrange(abs(avg))
```

Ahora utilizaremos el comando `left_join` que nos permitirá agregar fácilmente el número de votos electorales para cada estado del conjunto de datos `us_electoral_votes_2016`. Describiremos esta función en detalle en el capítulo sobre _data wrangling_. Aquí, simplemente diremos que la función combina los dos conjuntos de datos para que la información del segundo argumento se agregue a la información del primero:

```{r}
results <- left_join(results, results_us_election_2016, by = "state")
```

Observen que algunos estados no tienen encuestas porque prácticamente se conoce el ganador:

```{r}
results_us_election_2016 %>% filter(!state %in% results$state) %>%
pull(state)
```

No se realizaron encuestas en DC, Rhode Island, Alaska y Wyoming porque los demócratas seguramente ganarán en los primeros dos y los republicanos en los últimos dos.

Debido a que no podemos estimar la desviación estándar para los estados con una sola encuesta, la calcularemos como la mediana de las desviaciones estándar estimadas para los estados con más de una encuesta:

```{r}
results <- results %>%
mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
```

Para hacer argumentos probabilísticos, utilizaremos una simulación Monte Carlo. Para cada estado, aplicamos el enfoque bayesiano para generar una $d$ para el día de elecciones. Podríamos construir las probabilidades a priori de cada estado basado en la historia reciente. Sin embargo, para simplificar, asignamos una probabilidad a priori a cada estado que supone que no sabemos nada sobre lo que sucederá. Dado que de un año electoral a otro, los resultados de un estado específico no cambian tanto, asignaremos una desviación estándar de 2% o $\tau=0.02$. Por ahora, vamos a suponer, incorrectamente, que los resultados de la encuesta de cada estado son independientes. El código para el cálculo bayesiano bajo estos supuestos se ve así:

```{r, echo=FALSE}
mu <- 0
tau <- 0.02
results %>% mutate(sigma = sd/sqrt(n),
B = sigma^2/ (sigma^2 + tau^2),
posterior_mean = B * mu + (1 - B) * avg,
posterior_se = sqrt(1/ (1/sigma^2 + 1/tau^2)))
```

Las estimaciones basadas en las probabilidades a posteriori mueven las estimaciones hacia 0, aunque los estados con muchas encuestas están menos influenciados. Esto se espera ya que mientras más datos de encuestas recolectamos, más confiamos en esos resultados:

```{r posterior-versus-original-estimates, echo=FALSE}
results %>% mutate(sigma = sd/ sqrt(n),
B = sigma^2/ (sigma^2 + tau^2),
posterior_mean = B * mu + (1 - B) * avg,
posterior_se = sqrt(1/ (1/sigma^2 + 1/tau^2))) %>%
ggplot(aes(avg, posterior_mean, size = n)) + geom_point() +
geom_abline(slope = 1, intercept = 0)
```

Ahora repetimos esto 10,000 veces y generamos un resultado de la probabilidad a posteriori. En cada iteración, hacemos un seguimiento del número total de votos electorales para Clinton. Recuerden que Trump obtiene 270 votos electorales menos los votos para Clinton. También noten que la razón por la que agregamos 7 en el código es para tomar en cuenta Rhode Island y DC:

```{r, cache=TRUE}
B <- 10000
mu <- 0
tau <- 0.02
clinton_EV <- replicate(B, {
results %>% mutate(sigma = sd/sqrt(n),
B = sigma^2/ (sigma^2 + tau^2),
posterior_mean = B * mu + (1 - B) * avg,
posterior_se = sqrt(1/ (1/sigma^2 + 1/tau^2)),
result = rnorm(length(posterior_mean),
posterior_mean, posterior_se),
clinton = ifelse(result > 0, electoral_votes, 0)) %>%
summarize(clinton = sum(clinton)) %>%
pull(clinton) + 7
})

mean(clinton_EV > 269)
```

Este modelo le da a Clinton una probabilidad de ganar mayor que 99%. 
<!--Here is a histogram of the Monte Carlo outcomes:

```{r election-forecast-posterior-no-bias, echo=FALSE}
data.frame(clinton_EV) %>%
ggplot(aes(clinton_EV)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269)
```
-->
El Consorcio Electoral de Princeton hizo una predicción similar. Ahora sabemos que fallaron por mucho. ¿Que pasó?

El modelo anterior ignora el sesgo general y supone que los resultados de diferentes estados son independientes. Después de las elecciones, nos dimos cuenta de que el sesgo general en 2016 no era tan grande: estaba entre 1 y 2%. Pero debido a que la elección estuvo cerrada en varios estados grandes y estos estados tenían una gran cantidad de encuestas, los encuestadores que ignoraron el sesgo general subestimaron considerablemente el error estándar. Utilizando la notación que introducimos, suponieron que el error estándar era $\sqrt{\sigma^2/N}$ que con N grande es bastante más pequeño que la estimación más precisa
$\sqrt{\sigma^2/N + \sigma_b^2}$. FiveThirtyEight, que modela el sesgo general de una manera bastante sofisticada, informó un resultado más cercano. Podemos simular los resultados ahora con un término de sesgo. Para el nivel de estado, el sesgo general puede ser mayor, por lo que lo establecemos en $\sigma_b = 0.03$:


```{r election-forecast-posterior-with-bias, , cache=TRUE}
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
results %>% mutate(sigma = sqrt(sd^2/n + bias_sd^2),
B = sigma^2/ (sigma^2 + tau^2),
posterior_mean = B*mu + (1-B)*avg,
posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)),
result = rnorm(length(posterior_mean),
posterior_mean, posterior_se),
clinton = ifelse(result>0, electoral_votes, 0)) %>%
summarize(clinton = sum(clinton) + 7) %>%
pull(clinton)
})
mean(clinton_EV_2 > 269)
```

Esto nos da una estimación mucho más sensata. Al observar los resultados de la simulación, vemos cómo el término de sesgo agrega variabilidad a los resultados finales.

```{r comparison-forecast-with-and-without-bias, echo=FALSE}
data.frame(no_bias=clinton_EV, with_bias=clinton_EV_2) %>% gather(approach, result) %>%
ggplot(aes(result)) +
geom_histogram(binwidth = 1) +
geom_vline(xintercept = 269) +
facet_grid(approach~., scales="free")
```


El modelo de FiveThirtyEight incluye muchas otras características que no describimos aquí. Una es que modelan la variabilidad con distribuciones que tienen altas probabilidades para eventos extremos en comparación con la distribución normal. Una forma que nosotros podemos hacerlo es cambiando la distribución utilizada en la simulación de una distribución normal a una distribución t. FiveThirtyEight predijo una probabilidad de 71%.

### Pronósticos

A los pronosticadores les gusta hacer predicciones mucho antes de las elecciones. Las predicciones se adaptan a medida que salen nuevas encuestas. Sin embargo, una pregunta importante que deben hacer los pronosticadores es: ¿cuán informativas son las encuestas tomadas varias semanas antes de las elecciones sobre la elección real? Aquí estudiamos la variabilidad de los resultados de las encuestas a lo largo del tiempo.


Para asegurarnos de que la variabilidad que observamos no se deba a efectos del encuestador, estudiemos los datos de un encuestador:

```{r poplular-vote-time-trend}
one_pollster <- polls_us_election_2016 %>%
filter(pollster == "Ipsos" & state == "U.S.") %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Como no hay efecto de encuestador, quizás el error estándar teórico coincide con la desviación estándar derivada de los datos. Calculamos ambos aquí:

```{r}
se <- one_pollster %>%
summarize(empirical = sd(spread),
theoretical = 2 * sqrt(mean(spread) * (1 - mean(spread))/
min(samplesize)))
se
```

Pero la desviación estándar empírica es más alta que la estimación teórica más alta posible. Además, los datos de la variabilidad no se ven normales como la teoría predeciría:

```{r time-trend-variability, echo=FALSE}
qplot(spread, geom = "histogram", binwidth = 0.01, data = one_pollster, color = I("black"))
```

Los modelos que hemos descrito incluyen la variabilidad entre encuestadores y el error de muestreo. Pero este gráfico es para un encuestador y la variabilidad que vemos ciertamente no la explica el error de muestreo. ¿De dónde viene la variabilidad extra? Los siguientes gráficos muestran un fuerte argumento de que esta variabilidad proviene de fluctuaciones de tiempo no explicadas por la teoría que supone que $p$ es fijo:


```{r time-trend-estimate, echo=FALSE}
one_pollster %>% ggplot(aes(enddate, spread)) +
geom_point() +
geom_smooth(method = "loess", span = 0.1)
```

Algunos de los picos y valles que vemos coinciden con eventos como las convenciones de los partidos, que tienden a dar un impulso a los candidatos. Vemos consistencia entre los distintos encuestadores en cuanto a la localización de los picos y valles.

```{r time-trend-estimate-several-pollsters, echo=FALSE}
polls_us_election_2016 %>%
filter(state == "U.S.") %>%
group_by(pollster) %>%
filter(n()>=10) %>%
ungroup() %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
ggplot(aes(enddate, spread)) +
geom_smooth(method = "loess", span = 0.1) +
geom_point(aes(color=pollster), show.legend = FALSE, alpha=0.6)
```

Esto implica que, si vamos a pronosticar, nuestro modelo debe incluir un término que toma en cuenta el efecto temporal. Necesitamos escribir un modelo que incluya un término de sesgo para el tiempo:

$$
Y_{i,j,t} = d + b + h_j + b_t + \varepsilon_{i,j,t}
$$

La desviación estándar de $b_t$ va a depender de $t$ ya que en cuanto más nos acercamos al día de las elecciones, más cerca de 0 debería estar este término de sesgo.

Los encuestadores también intentan estimar las tendencias de estos datos e incorporarlos en sus predicciones. Podemos modelar la tendencia temporal con una función $f(t)$ y reescribir el modelo así: 


$$
Y_{i,j,t} = d + b + h_j + b_t + f(t) + \varepsilon_{i,jt,}
$$

Usualmente vemos el estimado $f(t)$ no para la diferencia, sino para los porcentajes reales para cada candidato así:

```{r trend-estimate-for-all-pollsters, warning=FALSE, message=FALSE, echo=FALSE}
polls_us_election_2016 %>%
filter(state == "U.S." & enddate>="2016-07-01") %>%
select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
gather(candidate, percentage, -enddate, -pollster) %>%
mutate(candidate = factor(candidate, levels = c("Trump","Clinton")))%>%
group_by(pollster) %>%
filter(n()>=10) %>%
ungroup() %>%
ggplot(aes(enddate, percentage, color = candidate)) +
geom_point(show.legend = FALSE, alpha=0.4) +
geom_smooth(method = "loess", span = 0.15) +
scale_y_continuous(limits = c(30,50))
```

Una vez que se seleccione un modelo como el anterior, podemos usar datos históricos y actuales para estimar todos los parámetros necesarios para hacer predicciones. Existe una variedad de métodos para estimar tendencias $f(t)$ que discutimos en la parte de _machine learning_.

## Ejercicios

1\. Crea esta tabla:

```{r, eval=TRUE}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>%
filter(state != "U.S." & enddate >= "2016-10-31") %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Ahora, para cada encuesta, use el CLT para crear un intervalo de confianza de 95% para la diferencia informada por cada encuesta. Llame al objeto resultante `cis` con columnas inferior y superior para los límites de los intervalos de confianza. Utilice la función `select` para mantener las columnas `state, startdate, end date, pollster, grade, spread, lower, upper`.

2\. Puede agregar el resultado final a la tabla `cis` que acaba de crear utilizando la función `right_join` así:

```{r, eval=FALSE}
add <- results_us_election_2016 %>%
mutate(actual_spread = clinton/100 - trump/100) %>%
select(state, actual_spread)
cis <- cis %>%
mutate(state = as.character(state)) %>%
left_join(add, by = "state")
```

Ahora determine con qué frecuencia el intervalo de confianza de 95% incluye el resultado real.

3\. Repita esto, pero muestre la proporción de veces que cada encuestador acierta. Muestre solo encuestadores con más de 5 encuestas y póngalos en orden de mejor a peor. Muestre el número de encuestas realizadas por cada encuestador y la calificación de FiveThirtyEight para cada encuestador. Sugerencia: use `n=n(), grade = grade[1]` en la llamada a `summarize`.


4\. Repita el ejercicio 3, pero en lugar de estratificar por encuestador, estratifique por estado. Tenga en cuenta que aquí no podemos mostrar calificaciones.


5\. Haga un diagrama de barras basado en el resultado del ejercicio 4. Use `coord_flip`.


6\. Para cada encuesta, calcule la diferencia entre la diferencia que predijimos y la diferencia observada. Añada una columna a la tabla `cis`. Entonces, añada otra columna llamada `hit` que es `TRUE` cuando los signos son los mismos. Sugerencia: use la función `sign`. Llame al objeto `resids`.


7\. Cree un gráfico como en el ejercicio 5, pero para la proporción de veces que los signos de la diferencia fueron iguales.

8\. En el ejercicio 7, vemos que para la mayoría de los estados las encuestas acertaron el 100% de las veces. En solo 9 estados las encuestas fallaron más de 25% de las veces. En particular, observe que en Wisconsin todas las encuestas se equivocaron. En Pennsylvania y Michigan, más de 90% de las encuestas tenían los signos incorrectos. Haga un histograma de los errores. ¿Cuál es la mediana de estos errores?


9\. Vemos que a nivel estatal, el error medio fue 3% a favor de Clinton. La distribución no está centrada en 0, sino en 0.03. Este es el sesgo general que describimos en la sección anterior. Cree un diagrama de caja para ver si el sesgo fue general para todos los estados o si afectó a algunos estados de manera diferente. Utilice `filter(grade %in% c("A+","A","A-","B+") | is.na(grade)))` para incluir solo encuestadores con altas calificaciones.


10\. Algunos de estos estados solo tienen unas pocas encuestas. Repita el ejercicio 9, pero solo incluya estados con 5 o más encuestas buenas. Sugerencia: use `group_by`, `filter` y luego `ungroup`. Verá que el Oeste (Washington, Nuevo México, California) subestimó el desempeño de Hillary, mientras que el Medio Oeste (Michigan, Pennsylvania, Wisconsin, Ohio, Missouri) lo sobrestimó. En nuestra simulación, no modelamos este comportamiento ya que agregamos un sesgo general, en lugar de un sesgo regional. Tenga en cuenta que algunos encuestadores ahora pueden modelar la correlación entre estados similares y estimar esta correlación a partir de datos históricos. Para obtener más información sobre esto, puede aprender sobre efectos aleatorios y modelos mixtos.

