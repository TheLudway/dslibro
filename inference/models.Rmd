# Modelos estadísticos {#models}

>> "Todos los modelos están equivocados, pero algunos son útiles". --George EP Box

El día antes de las elecciones presidenciales de 2008, FiveThirtyEight de Nate Silver declaró que "Barack Obama parece preparado para una victoria electoral decisiva". Fueron más lejos y predijeron que Obama ganaría las elecciones con 349 votos electorales a 189, y el voto popular por un margen del 6,1%. FiveThirtyEight también adjuntó una declaración probabilística a su predicción alegando que Obama tenía un 91% de posibilidades de ganar las elecciones. Las predicciones fueron bastante precisas ya que, en los resultados finales, Obama ganó el colegio electoral 365 a 173 y el voto popular por una diferencia de 7.2%. Su actuación en las elecciones de 2008 atrajo a FiveThirtyEight a la atención de expertos políticos y personalidades de la televisión. Cuatro años después, la semana antes de las elecciones presidenciales de 2012, Nate Silver de FiveThirtyEight le estaba dando a Obama una probabilidad del 90% de ganar a pesar de que muchos de los expertos pensaban que los resultados finales estarían más cerca. El comentarista político Joe Scarborough dijo durante su show^[https://www.youtube.com/watch?v=TbKkjm-gheY]:

>> Cualquiera que piense que esta carrera es todo menos una sacudida en este momento es un gran ideólogo ... son bromas.

A lo que Nate Silver respondió a través de Twitter:

>> Si crees que es una sacudida, apuestemos. Si Obama gana, usted dona $ 1,000 a la Cruz Roja Americana. Si Romney gana, yo lo hago. ¿Acuerdo?

En 2016, Silver no estaba tan seguro y le dio a Hillary Clinton solo un 71% de victorias. En contraste, la mayoría de los otros pronosticadores estaban casi seguros de que ella ganaría. Ella perdió. Pero el 71% sigue siendo más del 50%, ¿se equivocó el Sr. Silver? ¿Y qué significa la probabilidad en este contexto de todos modos? ¿Se tiran los dados en alguna parte?

En este capítulo, demostraremos cómo _agregadores de población_, como FiveThirtyEight, recopilaron y combinaron datos informados por diferentes expertos para producir predicciones mejoradas. Introduciremos ideas detrás de los _modelos estadísticos_, también conocidos como _modelos de probabilidad_, que fueron utilizados por los agregadores de encuestas para mejorar los pronósticos electorales más allá del poder de las encuestas individuales. En este capítulo, motivamos los modelos, construyendo sobre los conceptos de inferencia estadística que aprendimos en el Capítulo \@ref(inference). Comenzamos con modelos relativamente simples, dándonos cuenta de que el ejercicio real de la ciencia de datos de pronosticar elecciones involucra algunos bastante complejos, que presentamos al final del capítulo en la Sección \@ref(election-forecasting).

```{r, echo=FALSE, message=FALSE}
set.seed(2)
img_path <- "inference/img/"
```

## Agregadores de encuestas

Como describimos anteriormente, unas semanas antes de las elecciones de 2012, Nate Silver le estaba dando a Obama una probabilidad del 90% de ganar. ¿Cómo tenía tanta confianza el señor Silver? Utilizaremos una simulación de Monte Carlo para ilustrar la idea que el Sr. Silver tuvo y otros no vieron. Para hacer esto, generamos resultados para 12 encuestas realizadas la semana anterior a las elecciones. Imitamos tamaños de muestra de encuestas reales y construimos e informamos intervalos de confianza del 95 \% para cada una de las 12 encuestas. Guardamos los resultados de esta simulación en un marco de datos y agregamos una columna de ID de encuesta.


```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d + 1)/ 2

polls <- map_df(Ns, function(N) {
x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
x_hat <- mean(x)
se_hat <- sqrt(x_hat * (1 - x_hat)/ N)
list(estimate = 2 * x_hat - 1,
low = 2*(x_hat - 1.96*se_hat) - 1,
high = 2*(x_hat + 1.96*se_hat) - 1,
sample_size = N)
}) %>% mutate(poll = seq_along(Ns))

```

Aquí hay una visualización que muestra los intervalos que los encuestadores habrían informado sobre la diferencia entre Obama y Romney:

```{r simulated-polls, message=FALSE, echo=FALSE}
ggplot(polls, aes(poll, estimate, ymin=low, ymax=high)) +
geom_hline(yintercept = 0) +
geom_point(col="#00B6EB")+
geom_errorbar(col="#00B6EB") +
coord_flip() +
scale_x_continuous(breaks=c(1,ncol(polls))) +
scale_y_continuous(limits = c(-0.17, 0.17)) +
geom_hline(yintercept = 2*p-1, lty=2)
```

No es sorprendente que las 12 encuestas reporten intervalos de confianza que incluyen el resultado de la noche electoral (línea discontinua). Sin embargo, las 12 encuestas también incluyen 0 (línea negra sólida) también. Por lo tanto, si se les pide una predicción individualmente, los encuestadores tendrían que decir: es una sacudida. A continuación describimos una idea clave que les falta.

Los agregadores de encuestas, como Nate Silver, se dieron cuenta de que al combinar los resultados de diferentes encuestas, podría mejorar enormemente la precisión. Al hacer esto, estamos llevando a cabo una encuesta con un gran tamaño de muestra. Por lo tanto, podemos informar un intervalo de confianza menor del 95 \% y una predicción más precisa.

Aunque como agregadores no tenemos acceso a los datos de la encuesta sin procesar, podemos usar las matemáticas para reconstruir lo que habríamos obtenido si hubiéramos hecho una encuesta grande con:

```{r}
sum(polls$sample_size)
```

participantes. Básicamente, construimos una estimación de la propagación, llamémosla $d$, con un promedio ponderado de la siguiente manera:

```{r}
d_hat <- polls %>%
summarize(avg = sum(estimate*sample_size)/ sum(sample_size)) %>%
pull(avg)
```

Una vez que tengamos una estimación de $d$, podemos construir una estimación de la proporción de votos para Obama, que luego podemos usar para estimar el error estándar. Una vez que hacemos esto, vemos que nuestro margen de error es `r p_hat <- (1+d_hat)/2; moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size)); moe`.

Por lo tanto, podemos predecir que la propagación será `r round(d_hat*100,1)` más o menos `r round(moe*100 ,1)`, que no solo incluye el resultado real que finalmente observamos en la noche de las elecciones, sino que está bastante lejos de incluir 0. Una vez que combinamos las 12 encuestas, estamos seguros de que Obama ganará el voto popular.

```{r confidence-coverage-2008-election, echo=FALSE}
p_hat <- (1+d_hat)/2
moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size))
new_row <- tibble(d_hat, d_hat-moe, d_hat+moe, sum(polls$sample_size),13)
names(new_row) <- names(polls)
polls2 <- bind_rows(polls, new_row)
polls2$poll<-as.character(polls2$poll);
polls2$poll[13] <- "Avg"
polls2$col <- as.character(c(rep(2,12),1))
ggplot(polls2, aes(poll, estimate, ymin=low, ymax=high, color=col)) +
geom_hline(yintercept = 0) +
geom_point(show.legend = FALSE)+
geom_errorbar(show.legend = FALSE) +
coord_flip() +
scale_y_continuous(limits = c(-0.17, 0.17)) +
geom_hline(yintercept = 2*p-1, lty=2)
```

Por supuesto, esto fue solo una simulación para ilustrar la idea. El ejercicio real de la ciencia de datos de pronosticar elecciones es mucho más complicado e implica modelar. A continuación explicamos cómo los encuestadores ajustan los modelos multinivel a los datos y los utilizan para pronosticar los resultados electorales. En las elecciones presidenciales estadounidenses de 2008 y 2012, Nate Silver utilizó este enfoque para hacer una predicción casi perfecta y silenciar a los expertos.

Desde las elecciones de 2008, otras organizaciones han comenzado su propio grupo de pronóstico de elecciones que, como el de Nate Silver, agrega datos de encuestas y utiliza modelos estadísticos para hacer predicciones. En 2016, los pronosticadores subestimaron las posibilidades de Trump de ganar mucho. El día antes de las elecciones, el _New York Times_ informó^[https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html] las siguientes probabilidades de que Hillary Clinton gane la presidencia:


```{r, echo=FALSE, out.width="100%"}
#knitr::include_graphics(file.path(img_path, "pollster-2016-predictions.png"))
tmp <- data.frame(NYT = " 85%", `538` = " 71%", HuffPost = " 98%", PW = " 89%", PEC = " >99%", DK = " 92%", Cook = " Lean Dem", Roth = " Lean Dem", check.names = FALSE, row.names = "Win Prob")
if(knitr::is_html_output()){
knitr::kable(tmp, "html") %>%
kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
knitr::kable(tmp, "latex", booktabs = TRUE) %>%
kableExtra::kable_styling(font_size = 8)
}
```

<!--(Source: [New York Times](https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html))-->

Por ejemplo, el Consorcio Electoral de Princeton (PEC) le dio a Trump menos del 1% de posibilidades de ganar, mientras que el Huffington Post le dio una probabilidad del 2%. Por el contrario, FiveThirtyEight tenía la probabilidad de que Trump ganara al 29%, más que lanzar dos monedas y obtener dos caras. De hecho, cuatro días antes de las elecciones, FiveThirtyEight publicó un artículo titulado _Trump es solo un error de sondeo normal detrás de Clinton_^[https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind- Clinton/].
Al comprender los modelos estadísticos y cómo estos pronosticadores los usan, comenzaremos a comprender cómo sucedió esto.

Aunque no es tan interesante como predecir el colegio electoral, para fines ilustrativos comenzaremos analizando las predicciones para el voto popular. FiveThirtyEight predijo una ventaja de 3.6% para Clinton^[https://projects.fivethirtyeight.com/2016-election-forecast/], incluyó el resultado real de 2.1% (48.2% a 46.1%) en su intervalo, y fue mucho más confía en que Clinton gane las elecciones, dándole una probabilidad del 81.4%. Su predicción se resumió con un cuadro como este:


```{r fivethirtyeight-densities, echo=FALSE, out.width="80%", fig.height=2}
## knitr::include_graphics(file.path(img_path, "popular-vote-538.png"))

my_dgamma <- function(x, mean = 1, sd = 1){
shape = mean^2/sd^2
scale = sd^2/ mean
dgamma(x, shape = shape, scale = scale)
}

my_qgamma <- function(mean = 1, sd = 1){
shape = mean^2/sd^2
scale = sd^2/ mean
qgamma(c(0.1,0.9), shape = shape, scale = scale)
}

tmp <- tibble(candidate = c("Clinton", "Trump", "Johnson"), avg = c(48.5, 44.9, 5.0), avg_txt = c("48.5%", "44.9%", "5.0%"), sd = rep(2, 3), m = my_dgamma(avg, avg, sd)) %>%
mutate(candidate = reorder(candidate, -avg))

xx <- seq(0, 75, len = 300)
tmp_2 <- map_df(1:3, function(i){
tibble(candidate = tmp$candidate[i],
avg = tmp$avg[i],
sd = tmp$sd[i],
x = xx,
y = my_dgamma(xx, tmp$avg[i], tmp$sd[i]))
})

tmp_3 <- map_df(1:3, function(i){
qq <- my_qgamma(tmp$avg[i], tmp$sd[i])
xx <- seq(qq[1], qq[2], len = 200)
tibble(candidate = tmp$candidate[i],
avg = tmp$avg[i],
sd = tmp$sd[i],
x = xx,
y = my_dgamma(xx, tmp$avg[i], tmp$sd[i]))
})

tmp_2 %>%
ggplot(aes(x, ymax = y, ymin = 0)) +
geom_ribbon(fill = "grey") +
facet_grid(candidate~., switch="y") +
scale_x_continuous(breaks = seq(0, 75, 25), position = "top",
label= paste0(seq(0, 75, 25), "%")) +
geom_abline(intercept = 0, slope = 0) +
xlab("") + ylab("") +
theme_minimal() +
theme(panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
strip.text.y = element_text(angle = 180, size = 11, vjust = 0.2)) +
geom_ribbon(data = tmp_3, mapping = aes(x = x, ymax = y, ymin = 0, fill = candidate), inherit.aes = FALSE, show.legend = FALSE) +
scale_fill_manual(values = c("#3cace4", "#fc5c34", "#fccc2c")) +
geom_point(data = tmp, mapping = aes(x = avg, y = m), inherit.aes = FALSE) +
geom_text(data = tmp, mapping = aes(x = avg, y = m, label = avg_txt), inherit.aes = FALSE, hjust = 0, nudge_x = 1)


```

Las áreas coloreadas representan valores con una probabilidad del 80% de incluir el resultado real, según el modelo FiveThirtyEight.
<!--(Source: [FiveThirtyEight](https://projects.fivethirtyeight.com/2016-election-forecast/))-->

Presentamos datos reales de las elecciones presidenciales de EE. UU. De 2016 para mostrar cómo los modelos están motivados y construidos para producir estas predicciones. Para comprender la declaración de "81.4% de probabilidad" necesitamos describir las estadísticas bayesianas, lo que hacemos en las Secciones \@ref(bayesian-statistics) y \@ref(bayesian-approach).

### Datos de encuesta

Utilizamos datos de encuestas públicas organizados por FiveThirtyEight para las elecciones presidenciales de 2016. Los datos se incluyen como parte del paquete __dslabs__:

```{r}
data(polls_us_election_2016)
```

La tabla incluye los resultados de las encuestas nacionales, así como las encuestas estatales, tomadas durante el año anterior a la elección. Para este primer ejemplo, filtraremos los datos para incluir encuestas nacionales realizadas durante la semana previa a las elecciones. También eliminamos las encuestas que FiveThirtyEight ha determinado que no son confiables y se calificaron con una "B" o menos. Algunas encuestas no han sido calificadas e incluimos aquellas:

```{r}
polls <- polls_us_election_2016 %>%
filter(state == "U.S." & enddate >= "2016-10-31" &
(grade %in% c("A+","A","A-","B+") | is.na(grade)))
```

Agregamos una estimación de spread:

```{r}
polls <- polls %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Para este ejemplo, asumiremos que solo hay dos partes y llamaremos $p$ la proporción de votos para Clinton y $1-p$ la proporción que vota por Trump. Estamos interesados en la difusión $2p-1$. Llamemos al spread $d$ (por diferencia)


Tenemos `r nrow(polls)` estimaciones de la propagación. La teoría que aprendimos nos dice que estas estimaciones son una variable aleatoria con una distribución de probabilidad que es aproximadamente normal. El valor esperado es la propagación de la noche electoral. $d$ y el error estándar es $2\sqrt{p (1 - p)/ N}$. Suponiendo que el modelo de urna que describimos anteriormente es bueno, podemos usar esta información para construir un intervalo de confianza basado en los datos agregados. El spread estimado es:

```{r}
d_hat <- polls %>%
summarize(d_hat = sum(spread * samplesize)/ sum(samplesize)) %>%
pull(d_hat)
```

y el error estándar es:

```{r}
p_hat <- (d_hat+1)/2
moe <- 1.96 * 2 * sqrt(p_hat * (1 - p_hat)/ sum(polls$samplesize))
moe
```

Entonces informamos una extensión de `r round(d_hat*100,2)`\% con un margen de error de `r round(moe*100,2)`\%. En la noche de las elecciones, descubrimos que el porcentaje real era 2.1 \%, que está fuera de un intervalo de confianza del 95 \%. ¿Que pasó?

Un histograma de los spreads informados muestra un problema:
```{r polls-2016-spread-histogram}
polls %>%
ggplot(aes(spread)) +
geom_histogram(color="black", binwidth = .01)
```

Los datos no parecen estar distribuidos normalmente y el error estándar parece ser mayor que `r moe`. La teoría no está funcionando del todo aquí.

### Sesgo de encuestador

Observe que varios encuestadores están involucrados y algunos toman varias encuestas por semana:

```{r}
polls %>% group_by(pollster) %>% summarize(n())
```

Visualicemos los datos de los encuestadores que sondean regularmente:

```{r pollster-bias, echo=FALSE}
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>%
ggplot(aes(pollster, spread)) +
geom_point() +
coord_flip()
```

Esta trama revela un resultado inesperado. Primero, considere que el error estándar predicho por la teoría para cada encuesta:

```{r}
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>%
summarize(se = 2 * sqrt(p_hat * (1-p_hat)/ median(samplesize)))
```

está entre 0.018 y 0.033, lo que está de acuerdo con la variación dentro de la encuesta que vemos. Sin embargo, parece haber diferencias entre las encuestas. Observe, por ejemplo, cómo el encuestador USC Dornsife/ LA Times predice un 4% \win para Trump, mientras que Ipsos predice un triunfo mayor que 5 \% para Clinton. La teoría que aprendimos no dice nada acerca de diferentes encuestadores que producen encuestas con diferentes valores esperados. Todas las encuestas deben tener el mismo valor esperado. FiveThirtyEight se refiere a estas diferencias como "efectos domésticos". También los llamamos sesgo de encuestador.

En la siguiente sección, en lugar de utilizar la teoría del modelo de urna, desarrollaremos un modelo basado en datos.


## Modelos basados en datos {#data-driven-model}

Para cada encuestador, recopilemos su último resultado informado antes de las elecciones:

```{r}
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
filter(enddate == max(enddate)) %>%
ungroup()
```

Aquí hay un histograma de los datos para estos `r nrow(one_poll_per_pollster)` encuestadores:

```{r pollster-bias-histogram}
qplot(spread, data = one_poll_per_pollster, binwidth = 0.01)
```

En la sección anterior, vimos que usar la teoría del modelo de urna para combinar estos resultados podría no ser apropiado debido al efecto de encuestador. En cambio, modelaremos estos datos de propagación directamente.

El nuevo modelo también puede considerarse como un modelo de urna, aunque la conexión no es tan directa. En lugar de 0s (republicanos) y 1s (demócratas), nuestra urna ahora contiene resultados de encuestas de todos los posibles encuestadores. Suponemos que el valor esperado de nuestra urna es la extensión real $d=2p-1$.

Porque en lugar de 0s y 1s, nuestra urna contiene números continuos entre -1 y 1, la desviación estándar de la urna ya no es $\sqrt{p(1-p)}$. En lugar de la variabilidad del muestreo de votantes, el error estándar ahora incluye la variabilidad de encuestador a encuestador. Nuestra nueva urna también incluye la variabilidad de muestreo del sondeo. En cualquier caso, esta desviación estándar es ahora un parámetro desconocido. En los libros de texto de estadística, el símbolo griego $\sigma$ se usa para representar este parámetro.

En resumen, tenemos dos parámetros desconocidos: el valor esperado $d$ y la desviación estándar $\sigma$.

Nuestra tarea es estimar $d$. Porque modelamos los valores observados $X_1,\dots X_N$ como una muestra aleatoria de la urna, el CLT aún podría funcionar en esta situación porque es un promedio de variables aleatorias independientes. Para un tamaño de muestra suficientemente grande $N$, la distribución de probabilidad del promedio de la muestra $\bar{X}$ es aproximadamente normal con el valor esperado $\mu$ y error estándar $\sigma/\sqrt{N}$. Si estamos dispuestos a considerar $N=15$ suficientemente grande, podemos usar esto para construir intervalos de confianza.

Un problema es que no sabemos $\sigma$. Pero la teoría nos dice que podemos estimar el modelo de urna $\sigma$ con la _desviación estándar de muestra_ definida como
$s = \sqrt{ \sum_{i=1}^N (X_i - \bar{X})^2/ (N-1)}$.

A diferencia de la definición de desviación estándar de la población, ahora dividimos por $N-1$. Esto hace $s$ una mejor estimación de $\sigma$. Hay una explicación matemática para esto, que se explica en la mayoría de los libros de texto de estadística, pero no la cubrimos aquí.

Los `sd` la función en R calcula la desviación estándar de la muestra:

```{r}
sd(one_poll_per_pollster$spread)
```

Ahora estamos listos para formar un nuevo intervalo de confianza basado en nuestro nuevo modelo basado en datos:

```{r}
results <- one_poll_per_pollster %>%
summarize(avg = mean(spread),
se = sd(spread)/ sqrt(length(spread))) %>%
mutate(start = avg - 1.96 * se,
end = avg + 1.96 * se)
round(results * 100, 1)
```

Nuestro intervalo de confianza es más amplio ahora ya que incorpora la variabilidad del encuestador. Incluye el resultado de la noche electoral del 2.1%. Además, tenga en cuenta que era lo suficientemente pequeño como para no incluir 0, lo que significa que estábamos seguros de que Clinton ganaría el voto popular.

¿Estamos listos ahora para declarar una probabilidad de que Clinton gane el voto popular? Aún no. En nuestro modelo $d$ es un parámetro fijo, por lo que no podemos hablar de probabilidades. Para proporcionar probabilidades, necesitaremos aprender sobre las estadísticas bayesianas.

## Ejercicios

Hemos estado utilizando modelos de urna para motivar el uso de modelos de probabilidad. La mayoría de las aplicaciones de ciencia de datos no están relacionadas con los datos obtenidos de urnas. Más comunes son los datos que provienen de individuos. La razón por la que la probabilidad juega un papel aquí es porque los datos provienen de una muestra aleatoria. La muestra aleatoria se toma de una población y la urna sirve como analogía para la población.

Volvamos al conjunto de datos de alturas. Supongamos que consideramos a los machos en nuestro curso como la población.

```{r, eval=FALSE}
library(dslabs)
data(heights)
x <- heights %>% filter(sex == "Male") %>%
pull(height)
```

1\. Matemáticamente hablando, `x` es nuestra población Usando la analogía de la urna, tenemos una urna con los valores de `x` en eso. ¿Cuáles son las desviaciones promedio y estándar de nuestra población?


2\. Llame al promedio de población calculado arriba $\mu$ y la desviación estándar $\sigma$. Ahora tome una muestra de tamaño 50, con reemplazo, y construya una estimación para $\mu$ y $\sigma$.

3\. ¿Qué nos dice la teoría sobre el promedio de la muestra? $\bar{X}$ y cómo se relaciona con $\mu$?

a. Es prácticamente idéntico a $\mu$.
si. Es una variable aleatoria con valor esperado. $\mu$ y error estándar $\sigma/\sqrt{N}$.
c. Es una variable aleatoria con valor esperado. $\mu$ y error estándar $\sigma$.
re. No contiene información


4\. Entonces, ¿cómo es esto útil? Vamos a utilizar un ejemplo demasiado simplificado pero ilustrativo. Supongamos que queremos saber la altura promedio de nuestros estudiantes varones, pero solo llegamos a medir 50 de los 708. Usaremos $\bar{X}$ como nuestra estimación. Sabemos por la respuesta al ejercicio 3 que la estimación estándar de nuestro error $\bar{X}-\mu$ es $\sigma/\sqrt{N}$. Queremos calcular esto, pero no sabemos $\sigma$. Según lo que se describe en esta sección, muestre su estimación de $\sigma$.


5\. Ahora que tenemos una estimación de $\sigma$, llamemos a nuestra estimación $s$. Construya un intervalo de confianza del 95% para $\mu$.



6\. Ahora ejecute una simulación de Monte Carlo en la que calcule 10,000 intervalos de confianza como acaba de hacer. ¿Qué proporción de estos intervalos incluyen $\mu$?

7\. En esta sección, hablamos sobre el sesgo de encuestador. Utilizamos la visualización para motivar la presencia de tal sesgo. Aquí le daremos un tratamiento más riguroso. Consideraremos dos encuestadores que realizaron encuestas diarias. Examinaremos las encuestas nacionales del mes anterior a las elecciones.

```{r, eval=FALSE}
data(polls_us_election_2016)
polls <- polls_us_election_2016 %>%
filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research",
"The Times-Picayune/Lucid") &
enddate >= "2016-10-15" &
state == "U.S.") %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Queremos responder la pregunta: ¿hay un sesgo en la encuesta? Haz un diagrama que muestre los diferenciales para cada encuesta.


8\. Los datos parecen sugerir que hay una diferencia. Sin embargo, estos
los datos están sujetos a variabilidad. Quizás las diferencias que observamos se deben al azar.

La teoría del modelo de urna no dice nada sobre el efecto encuestador. Bajo el modelo de urna, ambos encuestadores tienen el mismo valor esperado: la diferencia del día de las elecciones, que llamamos $d$.

Para responder a la pregunta "¿hay un modelo de urna?", Modelaremos los datos observados $Y_{i,j}$ de la siguiente manera:

$$
Y_{i,j} = d + b_i + \varepsilon_{i,j}
$$

con $i=1,2$ indexando los dos encuestadores, $b_i$ el sesgo para el encuestador $i$ y $\varepsilon_ij$ encuesta a encuesta variabilidad casual. Asumimos el $\varepsilon$ son independientes entre sí, tienen valor esperado $0$ y desviación estándar $\sigma_i$ a pesar de $j$.

¿Cuál de las siguientes representa mejor nuestra pregunta?

a. Es $\varepsilon_{i,j}$ = 0?
si. ¿Qué tan cerca están los $Y_{i,j}$ a $d$?
c. Es $b_1 \neq b_2$?
re. Son $b_1 = 0$ y $b_2 = 0$ ?

9\. Solo en el lado derecho de este modelo $\varepsilon_{i,j}$ es una variable aleatoria Los otros dos son constantes. ¿Cuál es el valor esperado de $Y_{1,j}$?


10\. Supongamos que definimos $\bar{Y}_1$ como el promedio de los resultados de la encuesta de la primera encuesta, $Y_{1,1},\dots,Y_{1,N_1}$ con $N_1$ el número de encuestas realizadas por el primer encuestador:

```{r, eval=FALSE}
polls %>%
filter(pollster=="Rasmussen Reports/Pulse Opinion Research") %>%
summarize(N_1 = n())
```

¿Cuáles son los valores esperados? $\bar{Y}_1$?


11\. ¿Cuál es el error estándar de $\bar{Y}_1$ ?

12\. Supongamos que definimos $\bar{Y}_2$ como el promedio de los resultados de la encuesta de la primera encuesta, $Y_{2,1},\dots,Y_{2,N_2}$ con $N_2$ el número de encuestas realizadas por el primer encuestador. ¿Cuál es el valor esperado? $\bar{Y}_2$?

13\. ¿Cuál es el error estándar de $\bar{Y}_2$ ?

14\. Usando lo que aprendimos al responder las preguntas anteriores, ¿cuál es el valor esperado de $\bar{Y}_{2} - \bar{Y}_1$?

15\. Usando lo que aprendimos al responder las preguntas anteriores, ¿cuál es el error estándar de $\bar{Y}_{2} - \bar{Y}_1$?

Dieciséis\. La respuesta a la pregunta anterior depende de $\sigma_1$ y $\sigma_2$, que no sabemos. Aprendimos que podemos estimarlos con la desviación estándar de la muestra. Escribe un código que calcule estas dos estimaciones.


17\. ¿Qué nos dice el CLT sobre la distribución de $\bar{Y}_2 - \bar{Y}_1$?

a. Nada porque este no es el promedio de una muestra.
si. Porque el $Y_{ij}$ son aproximadamente normales, también lo son los promedios.
c. Tenga en cuenta que $\bar{Y}_2$ y $\bar{Y}_1$ son promedios de muestra, así que si asumimos $N_2$ y $N_1$ son lo suficientemente grandes, cada uno es aproximadamente normal. La diferencia de las normales también es normal.
re. Los datos no son 0 o 1, por lo que CLT no se aplica.

18\. Hemos construido una variable aleatoria que tiene un valor esperado. $b_2 - b_1$, la diferencia de sesgo del encuestador. Si nuestro modelo se cumple, entonces esta variable aleatoria tiene una distribución aproximadamente normal y conocemos su error estándar. El error estándar depende de $\sigma_1$ y $\sigma_2$, pero podemos tapar las desviaciones estándar de muestra que calculamos anteriormente. Comenzamos preguntando: es $b_2 - b_1$ diferente de 0? Use toda la información que hemos aprendido anteriormente para construir un intervalo de confianza del 95% para la diferencia $b_2$ y $b_1$.

19\. El intervalo de confianza nos dice que hay un efecto encuestador relativamente fuerte que resulta en una diferencia de aproximadamente 5%. La variabilidad aleatoria no parece explicarlo. Podemos calcular un valor p para transmitir el hecho de que el azar no lo explica. ¿Cuál es el valor p?


20\. La estadística formada dividiendo nuestra estimación de $b_2-b_1$ por su error estándar estimado:

$$
\frac{\bar{Y}_2 - \bar{Y}_1}{\sqrt{s_2^2/N_2 + s_1^2/N_1}}
$$

se llama la estadística t. Ahora observe que tenemos más de dos encuestadores. También podemos probar el efecto de encuestador utilizando todos los encuestadores, no solo dos. La idea es comparar la variabilidad entre encuestas con la variabilidad dentro de las encuestas. De hecho, podemos construir estadísticas para probar los efectos y aproximar su distribución. El área de estadísticas que hace esto se llama Análisis de varianza o ANOVA. No lo cubrimos aquí, pero ANOVA proporciona un conjunto muy útil de herramientas para responder preguntas como: ¿hay un efecto encuestador?

Para este ejercicio, cree una nueva tabla:

```{r, eval=FALSE}
polls <- polls_us_election_2016 %>%
filter(enddate >= "2016-10-15" &
state == "U.S.") %>%
group_by(pollster) %>%
filter(n() >= 5) %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
ungroup()
```

Calcule el promedio y la desviación estándar para cada encuestador y examine la variabilidad entre los promedios y cómo se compara con la variabilidad dentro de los encuestadores, resumida por la desviación estándar.


