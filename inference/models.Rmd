# Modelos estadísticos {#models}

>> "Todos los modelos están equivocados, pero algunos son útiles." --George E. P. Box

El día antes de las elecciones presidenciales del 2008, FiveThirtyEight de Nate Silver declaró que "Barack Obama parece estar listo para una victoria electoral decisiva". Fueron hasta más lejos y predijeron que Obama ganaría las elecciones con 349 votos electorales a 189, y el voto popular por un margen de 6.1%. FiveThirtyEight también añadió una declaración probabilística a su predicción declarando que Obama tenía una probablidad de 91% de ganar las elecciones. Las predicciones fueron bastante precisas y, en los resultados finales, Obama ganó el colegio electoral 365 a 173 y el voto popular por una diferencia de 7.2%. El desempeño de FiveThirtyEight en las elecciones del 2008 atrajo la atención de expertos políticos y personalidades de la televisión. Cuatro años después, la semana antes de las elecciones presidenciales del 2012, Nate Silver de FiveThirtyEight le estaba dando a Obama una probabilidad de 90% de ganar a pesar de que muchos de los expertos pensaban que los resultados finales estarían más cerca. El comentarista político Joe Scarborough dijo durante su show^[https://www.youtube.com/watch?v=TbKkjm-gheY]:

>> Cualquiera que piense que esta elección no está pareja en este momento es un tremendo ideólogo ... son un chiste.

A lo que Nate Silver respondió a través de Twitter:

>> Si cree que la elección está pareja, apostemos. Si Obama gana, usted dona $1,000 a la Cruz Roja Americana. Si Romney gana, yo lo hago. ¿De acuerdo?

En 2016, Silver no estaba tan seguro y le dio a Hillary Clinton solo una probabilidad de 71% de ganar. En contraste, la mayoría de los otros pronosticadores estaban casi seguros de que ella ganaría. Ella perdió. Pero 71% sigue siendo más de 50%, ¿se equivocó el Sr. Silver? Además, ¿qué significa la probabilidad en este contexto? ¿Alguien está tirando dados?

En este capítulo demostraremos cómo los _agregadores de encuestas_, como FiveThirtyEight, recopilaron y combinaron datos informados por diferentes expertos para producir mejores predicciones. Introduciremos las ideas detrás de los _modelos estadísticos_, también conocidos como _modelos de probabilidad_, que utilizaron los agregadores de encuestas para mejorar los pronósticos electorales en comparación a las encuestas individuales. En este capítulo, motivamos los modelos, construyendo sobre los conceptos de inferencia estadística que aprendimos en el Capítulo \@ref(inference). Comenzamos con modelos relativamente sencillos, tomando en cuenta que el ejercicio real de la ciencia de datos de pronosticar elecciones involucra algunos modelos bastante complejos, que presentamos al final del capítulo en la Sección \@ref(election-forecasting).

```{r, echo=FALSE, message=FALSE}
set.seed(2)
img_path <- "inference/img/"
```

## Agregadores de encuestas

Como describimos anteriormente, unas semanas antes de las elecciones del 2012, Nate Silver le estaba dando a Obama una probabilidad de 90% de ganar. ¿Por qué tenía tanta confianza el señor Silver? Utilizaremos una simulación Monte Carlo para ilustrar la idea que tuvo el Sr. Silver y que otros no vieron. Para hacer esto, generamos resultados para 12 encuestas realizadas la semana anterior a las elecciones. Imitaremos tamaños de muestra de encuestas reales y construiremos e informaremos intervalos de confianza de 95\% para cada una de las 12 encuestas. Guardaremos los resultados de esta simulación en un set de datos y agregaremos una columna de ID de encuesta.


```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
d <- 0.039
Ns <- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p <- (d + 1)/ 2

polls <- map_df(Ns, function(N) {
x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
x_hat <- mean(x)
se_hat <- sqrt(x_hat * (1 - x_hat)/ N)
list(estimate = 2 * x_hat - 1,
low = 2*(x_hat - 1.96*se_hat) - 1,
high = 2*(x_hat + 1.96*se_hat) - 1,
sample_size = N)
}) %>% mutate(poll = seq_along(Ns))

```

Aquí tenemos una visualización que muestra los intervalos que los encuestadores reportaron para la diferencia entre Obama y Romney:

```{r simulated-polls, message=FALSE, echo=FALSE}
ggplot(polls, aes(poll, estimate, ymin=low, ymax=high)) +
geom_hline(yintercept = 0) +
geom_point(col="#00B6EB")+
geom_errorbar(col="#00B6EB") +
coord_flip() +
scale_x_continuous(breaks=c(1,ncol(polls))) +
scale_y_continuous(limits = c(-0.17, 0.17)) +
geom_hline(yintercept = 2*p-1, lty=2)
```

No es sorprendente que las 12 encuestas informen intervalos de confianza que incluyen el resultado de la noche electoral (línea discontinua). Sin embargo, las 12 encuestas también incluyen 0 (línea negra sólida). Por lo tanto, si se les pide individualmente una predicción, los encuestadores tendrían que decir: las probabilidades están parejas. A continuación describimos una idea clave que no consideraron.

Los agregadores de encuestas, como Nate Silver, se dieron cuenta de que al combinar los resultados de diferentes encuestas, la precisión podría mejorar enormemente. Al hacer esto, estamos llevando a cabo una encuesta con un gran tamaño de muestra. Por lo tanto, podemos informar un intervalo de confianza menor de 95\% y una predicción más precisa.

Aunque como agregadores no tenemos acceso a los datos sin procesar de la encuesta, podemos usar las matemáticas para reconstruir lo que habríamos obtenido si hubiéramos hecho una encuesta grande con:

```{r}
sum(polls$sample_size)
```

participantes. Básicamente, construimos una estimación de la diferencia, llamémosla $d$, con un promedio ponderado de la siguiente manera:

```{r}
d_hat <- polls %>%
summarize(avg = sum(estimate*sample_size)/ sum(sample_size)) %>%
pull(avg)
```

Una vez que tengamos una estimación de $d$, podemos construir una estimación de la proporción votando por Obama, que luego podemos usar para estimar el error estándar. Tan pronto hacemos esto, vemos que nuestro margen de error es `r p_hat <- (1+d_hat)/2; moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size)); moe`.

Por lo tanto, podemos predecir que la diferencia será `r round(d_hat*100,1)` más o menos `r round(moe*100 ,1)`, que no solo incluye el resultado real que observamos en la noche de las elecciones, sino que está bastante lejos de incluir 0. Una vez que combinamos las 12 encuestas, estamos seguros de que Obama ganará el voto popular.

```{r confidence-coverage-2008-election, echo=FALSE}
p_hat <- (1+d_hat)/2
moe <- 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size))
new_row <- tibble(d_hat, d_hat-moe, d_hat+moe, sum(polls$sample_size),13)
names(new_row) <- names(polls)
polls2 <- bind_rows(polls, new_row)
polls2$poll<-as.character(polls2$poll);
polls2$poll[13] <- "Avg"
polls2$col <- as.character(c(rep(2,12),1))
ggplot(polls2, aes(poll, estimate, ymin=low, ymax=high, color=col)) +
geom_hline(yintercept = 0) +
geom_point(show.legend = FALSE)+
geom_errorbar(show.legend = FALSE) +
coord_flip() +
scale_y_continuous(limits = c(-0.17, 0.17)) +
geom_hline(yintercept = 2*p-1, lty=2)
```

Por supuesto, esto fue solo una simulación para ilustrar la idea. El ejercicio real de la ciencia de datos de pronosticar elecciones es mucho más complicado y requiere modelos estadísticos. A continuación explicamos cómo los encuestadores ajustan los modelos multinivel a los datos y los utilizan para pronosticar los resultados electorales. En las elecciones presidenciales estadounidenses del 2008 y 2012, Nate Silver utilizó este enfoque para hacer una predicción casi perfecta y callar a los expertos.

Desde las elecciones del 2008, otras organizaciones han establecido sus propios grupos de pronóstico de elecciones que, como el de Nate Silver, agrega datos de encuestas y utiliza modelos estadísticos para hacer predicciones. En 2016, los pronosticadores subestimaron por mucho las posibilidades de Trump de ganar. El día antes de las elecciones, el _New York Times_ informó^[https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html] las siguientes probabilidades de que Hillary Clinton ganara la presidencia:


```{r, echo=FALSE, out.width="100%"}
#knitr::include_graphics(file.path(img_path, "pollster-2016-predictions.png"))
tmp <- data.frame(NYT = " 85%", `538` = " 71%", HuffPost = " 98%", PW = " 89%", PEC = " >99%", DK = " 92%", Cook = " Lean Dem", Roth = " Lean Dem", check.names = FALSE, row.names = "Win Prob")
if(knitr::is_html_output()){
knitr::kable(tmp, "html") %>%
kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
knitr::kable(tmp, "latex", booktabs = TRUE) %>%
kableExtra::kable_styling(font_size = 8)
}
```

<!--(Source: [New York Times](https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html))-->

Por ejemplo, el Consorcio Electoral de Princeton (_Princeton Election Consortium_ en inglés) le dio a Trump menos de 1% de probabilidad de ganar, mientras que el _Huffington Post_ le dio una probabilidad de 2%. Por el contrario, FiveThirtyEight le daba a Trump una probabilidad de ganar de  29%, más que la probabilidad de lanzar dos monedas y obtener dos caras. De hecho, cuatro días antes de las elecciones, FiveThirtyEight publicó un artículo titulado _Trump is Just A Normal Polling Error Behind Clinton_^[https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind- Clinton/].
Al entender los modelos estadísticos y cómo los pronosticadores los usan, comenzaremos a entender cómo sucedió esto.

Aunque no tan interesante como predecir el colegio electoral, para fines ilustrativos comenzaremos analizando las predicciones para el voto popular. FiveThirtyEight predijo una ventaja de 3.6% para Clinton^[https://projects.fivethirtyeight.com/2016-election-forecast/] y su intervalo de confianza incluyó el resultado real de una diferencia de 2.1%  (48.2% a 46.1%). Además, FiveThirtyEight estuvo mucho más seguro sobre la posibilidad de que Clinton ganara el voto popular, dándole una probabilidad de 81.4%. Su predicción se resumió con un gráfico como este:


```{r fivethirtyeight-densities, echo=FALSE, out.width="80%", fig.height=2}
## knitr::include_graphics(file.path(img_path, "popular-vote-538.png"))

my_dgamma <- function(x, mean = 1, sd = 1){
shape = mean^2/sd^2
scale = sd^2/ mean
dgamma(x, shape = shape, scale = scale)
}

my_qgamma <- function(mean = 1, sd = 1){
shape = mean^2/sd^2
scale = sd^2/ mean
qgamma(c(0.1,0.9), shape = shape, scale = scale)
}

tmp <- tibble(candidate = c("Clinton", "Trump", "Johnson"), avg = c(48.5, 44.9, 5.0), avg_txt = c("48.5%", "44.9%", "5.0%"), sd = rep(2, 3), m = my_dgamma(avg, avg, sd)) %>%
mutate(candidate = reorder(candidate, -avg))

xx <- seq(0, 75, len = 300)
tmp_2 <- map_df(1:3, function(i){
tibble(candidate = tmp$candidate[i],
avg = tmp$avg[i],
sd = tmp$sd[i],
x = xx,
y = my_dgamma(xx, tmp$avg[i], tmp$sd[i]))
})

tmp_3 <- map_df(1:3, function(i){
qq <- my_qgamma(tmp$avg[i], tmp$sd[i])
xx <- seq(qq[1], qq[2], len = 200)
tibble(candidate = tmp$candidate[i],
avg = tmp$avg[i],
sd = tmp$sd[i],
x = xx,
y = my_dgamma(xx, tmp$avg[i], tmp$sd[i]))
})

tmp_2 %>%
ggplot(aes(x, ymax = y, ymin = 0)) +
geom_ribbon(fill = "grey") +
facet_grid(candidate~., switch="y") +
scale_x_continuous(breaks = seq(0, 75, 25), position = "top",
label= paste0(seq(0, 75, 25), "%")) +
geom_abline(intercept = 0, slope = 0) +
xlab("") + ylab("") +
theme_minimal() +
theme(panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
strip.text.y = element_text(angle = 180, size = 11, vjust = 0.2)) +
geom_ribbon(data = tmp_3, mapping = aes(x = x, ymax = y, ymin = 0, fill = candidate), inherit.aes = FALSE, show.legend = FALSE) +
scale_fill_manual(values = c("#3cace4", "#fc5c34", "#fccc2c")) +
geom_point(data = tmp, mapping = aes(x = avg, y = m), inherit.aes = FALSE) +
geom_text(data = tmp, mapping = aes(x = avg, y = m, label = avg_txt), inherit.aes = FALSE, hjust = 0, nudge_x = 1)


```

Las áreas coloreadas representan valores con una probabilidad de 80% de incluir el resultado real, según el modelo de FiveThirtyEight.
<!--(Source: [FiveThirtyEight](https://projects.fivethirtyeight.com/2016-election-forecast/))-->

Presentamos datos reales de las elecciones presidenciales de EE. UU. del 2016 para mostrar cómo se motivan y se contruyen los modelos para producir estas predicciones. Para comprender la declaración "81.4% de probabilidad", necesitamos describir las estadísticas bayesianas, lo que hacemos en las Secciones \@ref(bayesian-statistics) y \@ref(bayesian-approach).

### Datos de encuesta

Utilizamos datos de encuestas públicas organizados por FiveThirtyEight para las elecciones presidenciales del 2016. Los datos se incluyen como parte del paquete __dslabs__:

```{r}
data(polls_us_election_2016)
```

La tabla incluye los resultados de las encuestas nacionales, así como las encuestas estatales, tomadas durante el año anterior a la elección. Para este primer ejemplo, filtraremos los datos para incluir encuestas nacionales realizadas durante la semana previa a las elecciones. También eliminamos las encuestas que FiveThirtyEight ha determinado que no son confiables y calificaron con una "B" o menos. Algunas encuestas no han sido calificadas e incluimos aquellas:

```{r}
polls <- polls_us_election_2016 %>%
filter(state == "U.S." & enddate >= "2016-10-31" &
(grade %in% c("A+","A","A-","B+") | is.na(grade)))
```

Agregamos la estimación de la diferencia:

```{r}
polls <- polls %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Para este ejemplo, suponemos que solo hay dos partes y llamaremos $p$ a la proporción de votos para Clinton y $1-p$ a la proporción votando por Trump. Estamos interesados en la diferencia $2p-1$. Llamemos a la diferencia $d$.


Tenemos `r nrow(polls)` estimaciones de la diferencia. La teoría que aprendimos nos dice que estas estimaciones son una variable aleatoria con una distribución de probabilidad que es aproximadamente normal. El valor esperado es la diferencia de la noche electoral $d$ y el error estándar es $2\sqrt{p (1 - p)/ N}$. Suponiendo que el modelo de urna que describimos anteriormente es bueno, podemos usar esta información para construir un intervalo de confianza basado en los datos agregados. La diferencia estimada es:

```{r}
d_hat <- polls %>%
summarize(d_hat = sum(spread * samplesize)/ sum(samplesize)) %>%
pull(d_hat)
```

y el error estándar es:

```{r}
p_hat <- (d_hat+1)/2
moe <- 1.96 * 2 * sqrt(p_hat * (1 - p_hat)/ sum(polls$samplesize))
moe
```

Entonces informamos una variabilidad de `r round(d_hat*100,2)`\% con un margen de error de `r round(moe*100,2)`\%. En la noche de las elecciones, descubrimos que el porcentaje real era 2.1\%, que está fuera de un intervalo de confianza de 95\%. ¿Que pasó?

Un histograma de las variabilidades reportadas muestra un problema:
```{r polls-2016-spread-histogram}
polls %>%
ggplot(aes(spread)) +
geom_histogram(color="black", binwidth = .01)
```

Los datos no parecen estar distribuidos normalmente y el error estándar parece ser mayor que `r moe`. La teoría no está funcionando bien aquí.

### Sesgo de los encuestadores

Observen que varios encuestadores están involucrados y algunos toman varias encuestas por semana:

```{r}
polls %>% group_by(pollster) %>% summarize(n())
```

Visualicemos los datos de los encuestadores que sondean regularmente:

```{r pollster-bias, echo=FALSE}
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>%
ggplot(aes(pollster, spread)) +
geom_point() +
coord_flip()
```

Este gráfico revela un resultado inesperado. Primero, consideren que el error estándar predicho por la teoría para cada encuesta:

```{r}
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>%
summarize(se = 2 * sqrt(p_hat * (1-p_hat)/ median(samplesize)))
```

está entre 0.018 y 0.033, que concuerda con la variación de encuesta a encuesta que vemos para cada encuestador. Sin embargo, parece haber diferencias _entre las encuestadores_. Observen, por ejemplo, cómo el encuestador USC Dornsife/LA Times predice una ventaja de 4\% para Trump, mientras que Ipsos predice una ventaja mayor de 5\% para Clinton. La teoría que aprendimos no dice nada acerca de diferentes encuestadores que producen encuestas con diferentes valores esperados. Todas las encuestas deben tener el mismo valor esperado. FiveThirtyEight se refiere a estas diferencias como "house effects". También las llamamos _sesgo de encuestadores_.

En la siguiente sección, en lugar de utilizar la teoría del modelo de urna, desarrollaremos un modelo basado en datos.


## Modelos basados en datos {#data-driven-model}

Para cada encuestador, recopilemos el último resultado que informan antes de las elecciones:

```{r}
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
filter(enddate == max(enddate)) %>%
ungroup()
```

Aquí hay un histograma de los datos para estos `r nrow(one_poll_per_pollster)` encuestadores:

```{r pollster-bias-histogram}
qplot(spread, data = one_poll_per_pollster, binwidth = 0.01)
```

En la sección anterior, vimos que usar la teoría del modelo de urna para combinar estos resultados a veces no es apropiado debido al efecto de encuestador. En cambio, modelaremos estos datos de las diferencias directamente.

El nuevo modelo también puede considerarse como un modelo de urna, aunque la conexión no es tan directa. En lugar de 0s (republicanos) y 1s (demócratas), nuestra urna ahora contiene los resultados de las encuestas de todos los posibles encuestadores. Suponemos que el valor esperado de nuestra urna es la diferencia real $d=2p-1$.

Dado que en lugar de 0s y 1s, nuestra urna contiene números continuos entre -1 y 1, la desviación estándar de la urna ya no es $\sqrt{p(1-p)}$. En vez de la variabilidad del muestreo de votantes, el error estándar ahora incluye la variabilidad entre encuestadores. Nuestra nueva urna también incluye la variabilidad de muestreo del sondeo. De cualquier manera, esta desviación estándar ahora es un parámetro desconocido. En los libros de texto de estadística, el símbolo griego $\sigma$ se usa para representar este parámetro.

En resumen, tenemos dos parámetros desconocidos: el valor esperado $d$ y la desviación estándar $\sigma$.

Nuestra tarea es estimar $d$. Como modelamos los valores observados $X_1,\dots X_N$ como una muestra aleatoria de la urna, el CLT aún podría funcionar en esta situación porque es un promedio de variables aleatorias independientes. Para un tamaño de muestra suficientemente grande $N$, la distribución de probabilidad del promedio de la muestra $\bar{X}$ es aproximadamente normal con valor esperado $\mu$ y error estándar $\sigma/\sqrt{N}$. Si estamos dispuestos a considerar $N=15$ como suficientemente grande, podemos usar esto para construir intervalos de confianza.

Un problema es que no sabemos $\sigma$. Pero la teoría nos dice que podemos estimar el modelo de urna $\sigma$ con la _desviación estándar de la muestra_ definida como
$s = \sqrt{ \sum_{i=1}^N (X_i - \bar{X})^2/ (N-1)}$.

A diferencia de la definición de desviación estándar de la población, ahora dividimos por $N-1$. Esto hace $s$ una mejor estimación de $\sigma$. Hay una explicación matemática para esto, que se explica en la mayoría de los libros de texto de estadística, pero no la cubrimos aquí.

La función `sd` de R calcula la desviación estándar de la muestra:

```{r}
sd(one_poll_per_pollster$spread)
```

Ahora estamos listos para formar un nuevo intervalo de confianza basado en nuestro nuevo modelo y en datos:

```{r}
results <- one_poll_per_pollster %>%
summarize(avg = mean(spread),
se = sd(spread)/ sqrt(length(spread))) %>%
mutate(start = avg - 1.96 * se,
end = avg + 1.96 * se)
round(results * 100, 1)
```

Nuestro intervalo de confianza ahora es más amplio ya que incorpora la variabilidad de encuestador. Incluye el resultado de la noche electoral de 2.1%. Además, tengan en cuenta que era lo suficientemente pequeño como para no incluir 0, lo que significa que estábamos seguros de que Clinton ganaría el voto popular.

¿Estamos listos ahora para declarar una probabilidad de que Clinton gane el voto popular? Aún no. En nuestro modelo $d$ es un parámetro fijo, por lo que no podemos hablar de probabilidades. Para ofrecer probabilidades, necesitaremos aprender sobre las estadísticas bayesianas.

## Ejercicios

Hemos estado utilizando modelos de urna para motivar el uso de modelos de probabilidad. La mayoría de las aplicaciones de ciencia de datos no están relacionadas con datos obtenidos de urnas. Más comunes son los datos que provienen de individuos. La razón por la que la probabilidad importa aquí es porque los datos provienen de una muestra aleatoria. La muestra aleatoria se toma de una población y la urna sirve como analogía para la población.

Volvamos al conjunto de datos de alturas. Supongamos que consideramos a los varones de nuestra clase como la población.

```{r, eval=FALSE}
library(dslabs)
data(heights)
x <- heights %>% filter(sex == "Male") %>%
pull(height)
```

1\. Matemáticamente hablando, `x` es nuestra población. Usando la analogía de la urna, tenemos una urna con los valores de `x` dentro de ella. ¿Cuáles son el promedio y la desviación estándar de nuestra población?


2\. Llame al promedio de población calculado arriba $\mu$ y la desviación estándar $\sigma$. Ahora tome una muestra de tamaño 50, con reemplazo, y construya una estimación para $\mu$ y $\sigma$.

3\. ¿Qué nos dice la teoría sobre el promedio de la muestra $\bar{X}$ y como se relaciona con $\mu$?

a. Es prácticamente idéntico a $\mu$.
b. Es una variable aleatoria con valor esperado $\mu$ y error estándar $\sigma/\sqrt{N}$.
c. Es una variable aleatoria con valor esperado $\mu$ y error estándar $\sigma$.
d. No contiene información.


4\. Entonces, ¿cómo es esto útil? Vamos a utilizar un ejemplo simplificado pero ilustrativo. Supongamos que queremos saber la altura promedio de nuestros estudiantes varones, pero solo llegamos a medir 50 de los 708. Usaremos $\bar{X}$ como nuestra estimación. Sabemos por la respuesta al ejercicio 3 que la estimación estándar de nuestro error $\bar{X}-\mu$ es $\sigma/\sqrt{N}$. Queremos calcular esto, pero no sabemos $\sigma$. Según lo que se describe en esta sección, indique su estimación de $\sigma$.


5\. Ahora que tenemos una estimación de $\sigma$, llamemos a nuestra estimación $s$. Construya un intervalo de confianza de 95% para $\mu$.


6\. Ahora ejecute una simulación Monte Carlo en la que calcula 10,000 intervalos de confianza como acaba de hacer. ¿Qué proporción de estos intervalos incluye $\mu$?

7\. En esta sección, discutimos el sesgo de encuestador. Utilizamos la visualización para motivar la presencia de tal sesgo. Aquí le daremos un tratamiento más riguroso. Consideraremos dos encuestadores que realizaron encuestas diarias. Examinaremos las encuestas nacionales del mes anterior a las elecciones.

```{r, eval=FALSE}
data(polls_us_election_2016)
polls <- polls_us_election_2016 %>%
filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research",
"The Times-Picayune/Lucid") &
enddate >= "2016-10-15" &
state == "U.S.") %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
```

Queremos contestar la pregunta: ¿hay un sesgo en la encuesta? Haga un gráfico que muestre la diferencia para cada encuesta.


8\. Los datos parecen sugerir que hay una diferencia. Sin embargo, estos datos están sujetos a variabilidad. Quizás las diferencias que observamos se deben al azar.

La teoría del modelo de urna no dice nada sobre el efecto del encuestador. Bajo el modelo de urna, ambos encuestadores tienen el mismo valor esperado: la diferencia del día de las elecciones, que llamamos $d$.

Para responder a la pregunta "¿hay un modelo de urna?", modelaremos los datos observados $Y_{i,j}$ de la siguiente manera:

$$
Y_{i,j} = d + b_i + \varepsilon_{i,j}
$$

con $i=1,2$ indexando los dos encuestadores, $b_i$ el sesgo para el encuestador $i$ y $\varepsilon_{ij}$ representando la variabilidad aleatoria de las encuestas. Suponemos que los $\varepsilon$ son independientes entre sí, tienen valor esperado $0$ y desviación estándar $\sigma_i$ independientemente de $j$.

¿Cuál de las siguientes mejor representa nuestra pregunta?

a. ¿Es $\varepsilon_{i,j}$ = 0?
b. ¿Cuán cerca están los $Y_{i,j}$ a $d$?
c. ¿Es $b_1 \neq b_2$?
d. ¿Son $b_1 = 0$ y $b_2 = 0$?

9\. En el lado derecho de este modelo solo $\varepsilon_{i,j}$ es una variable aleatoria. Los otros dos son constantes. ¿Cuál es el valor esperado de $Y_{1,j}$?


10\. Supongamos que definimos $\bar{Y}_1$ como el promedio de los resultados de la encuesta del primer encuestador, $Y_{1,1},\dots,Y_{1,N_1}$ con $N_1$ el número de encuestas realizadas por el primer encuestador:

```{r, eval=FALSE}
polls %>%
filter(pollster=="Rasmussen Reports/Pulse Opinion Research") %>%
summarize(N_1 = n())
```

¿Cuál es el valor esperado de $\bar{Y}_1$?


11\. ¿Cuál es el error estándar de $\bar{Y}_1$ ?

12\. Supongamos que definimos $\bar{Y}_2$ como el promedio de los resultados de la encuesta de la primera encuesta, $Y_{2,1},\dots,Y_{2,N_2}$ con $N_2$ el número de encuestas realizadas por el primer encuestador. ¿Cuál es el valor esperado $\bar{Y}_2$?

13\. ¿Cuál es el error estándar de $\bar{Y}_2$ ?

14\. Usando lo que aprendimos al responder a las preguntas anteriores, ¿cuál es el valor esperado de $\bar{Y}_{2} - \bar{Y}_1$?

15\. Usando lo que aprendimos al responder a las preguntas anteriores, ¿cuál es el error estándar de $\bar{Y}_{2} - \bar{Y}_1$?

16\. La respuesta a la pregunta anterior depende de $\sigma_1$ y $\sigma_2$, que no sabemos. Aprendimos que podemos estimarlos con la desviación estándar de la muestra. Escriba un código que calcule estas dos estimaciones.


17\. ¿Qué nos dice el CLT sobre la distribución de $\bar{Y}_2 - \bar{Y}_1$?

a. Nada porque este no es el promedio de una muestra.
b. Como el $Y_{ij}$ son aproximadamente normales, también lo son los promedios.
c. Como $\bar{Y}_2$ y $\bar{Y}_1$ son promedios de muestras, si suponemos que $N_2$ y $N_1$ son lo suficientemente grandes, cada uno es aproximadamente normal. La diferencia de normales también es normal.
d. Los datos no son 0 o 1, por lo que el CLT no se aplica.

18\. Hemos construido una variable aleatoria que tiene un valor esperado $b_2 - b_1$, la diferencia de sesgo del encuestador. Si nuestro modelo funciona, entonces esta variable aleatoria tiene una distribución aproximadamente normal y sabemos su error estándar. El error estándar depende de $\sigma_1$ y $\sigma_2$, pero podemos usar las desviaciones estándar de muestra que calculamos anteriormente. Comenzamos preguntando: ¿$b_2 - b_1$ es diferente de 0? Use toda la información que hemos aprendido anteriormente para construir un intervalo de confianza de 95% para la diferencia $b_2$ y $b_1$.

19\. El intervalo de confianza nos dice que hay un efecto encuestador relativamente fuerte que resulta en una diferencia de aproximadamente 5%. La variabilidad aleatoria no parece explicarlo. Podemos calcular un valor p para explicar el hecho de que el azar no lo explica. ¿Cuál es el valor p?


20\. La estadística formada al dividir nuestra estimación de $b_2-b_1$ por su error estándar estimado:

$$
\frac{\bar{Y}_2 - \bar{Y}_1}{\sqrt{s_2^2/N_2 + s_1^2/N_1}}
$$

se llama la estadística t. Ahora observe que tenemos más de dos encuestadores. También podemos probar para el efecto de encuestador utilizando todos los encuestadores, no solo dos. La idea es comparar la variabilidad entre encuestas con la variabilidad dentro de las encuestas. De hecho, podemos construir estadísticas para probar los efectos y aproximar su distribución. El área de estadísticas que hace esto se llama Análisis de la varianza o ANOVA por sus siglas en inglés. No lo cubrimos aquí, pero ANOVA provee un set muy útil de herramientas para responder a preguntas como: ¿hay un efecto encuestador?

Para este ejercicio, cree una nueva tabla:

```{r, eval=FALSE}
polls <- polls_us_election_2016 %>%
filter(enddate >= "2016-10-15" &
state == "U.S.") %>%
group_by(pollster) %>%
filter(n() >= 5) %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
ungroup()
```

Calcule el promedio y la desviación estándar para cada encuestador y examine la variabilidad entre los promedios y cómo se compara con la variabilidad dentro de los encuestadores, resumida por la desviación estándar.


