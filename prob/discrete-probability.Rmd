## Probabilidad discreta

Comenzamos explorando algunos principios básicos relacionados con datos categóricos. Esta parte de la probabilidad se conoce como _probabilidad discreta_. Más tarde, tal nos ayudará a comprender la teoría de la probabilidad que luego presentaremos para datos numéricos y continuos, que es mucho más común en las aplicaciones de ciencia de datos. La probabilidad discreta es más útil en los juegos de cartas y, por ende, usamos estos como ejemplos.

### Frecuencia relativa

Si bien la palabra probabilidad se usa en el lenguaje cotidiano, responder a preguntas sobre la probabilidad es difícil, si no imposible. Aquí discutimos una definición matemática de _probabilidad_ que nos permite dar respuestas precisas a ciertas preguntas.

Por ejemplo, si tengo 2 canicas rojas y 3 canicas azules dentro de una bolsa^[https://en.wikipedia.org/wiki/Urn_problem] y escojo una al azar, ¿cuál es la probabilidad de elegir una roja? Nuestra intuición nos dice que la respuesta es 2/5 o 40%. Se puede dar una definición precisa al señalar que hay cinco resultados posibles de los cuales dos satisfacen la condición necesaria para el evento "escoger una canica roja". Dado que cada uno de los cinco resultados tiene la misma posibilidad de ocurrir, concluimos que la probabilidad es .4 para rojo y .6 para azul.

Una forma más tangible de pensar en la probabilidad de un evento es la proporción de veces que ocurre el evento cuando repetimos el experimento un número infinito de veces, independientemente y bajo las mismas condiciones.

### Notación

Usamos la notación $\mbox{Pr}(A)$ para denotar la probabilidad de que suceda evento $A$. Usamos el término general _evento_ para referirnos a cosas que pueden suceder cuando algo ocurre por casualidad. En nuestro ejemplo anterior, el evento fue "escoger una canica roja". En una encuesta política en la que llamamos al azar a 100 probables votantes estadounidenses, un ejemplo de un evento es "llamar a 48 demócratas y 52 republicanos".

En aplicaciones de ciencia de datos, frecuentemente trabajaremos con variables continuas. Estos eventos a menudo serán cosas como "es esta persona más alta que 6 pies". En ese caso, escribimos eventos en una forma más matemática: $X \geq 6$. Veremos más de estos ejemplos a continuación. Aquí nos enfocamos en datos categóricos.


### Distribuciones de probabilidad

Si conocemos la frecuencia relativa de las diferentes categorías, definir una distribución para resultados categóricos es relativamente sencillo. Simplemente asignamos una probabilidad a cada categoría. En los casos que pueden considerarse como canicas en una bolsa, para cada tipo de canica, su proporción define la distribución.

Si estamos llamando al azar a votantes probables de una población que es 44% demócratas, 44% republicanos, 10% indecisos y 2% del partido verde, estas proporciones definen la probabilidad para cada grupo. La distribución de probabilidad es:


| | | |
|-------------------------|---|----|
| Pr (elegir un republicano) | = | 0.44 |
| Pr (elegir un demócrata) | = | 0.44 |
| Pr (elegir un indeciso) | = | 0.10 |
| Pr (elegir un verde) | = | 0.02 |

## Simulaciones Monte Carlo para datos categóricos

Las computadoras ofrecen una forma de realizar el experimento aleatorio sencillo descrito anteriormente: elegir una canica al azar de una bolsa que contiene tres canicas azules y dos rojas. Los generadores de números aleatorios nos permiten imitar el proceso de escoger al azar.

Un ejemplo es la función `sample` en R. Demostramos su uso en el código a continuación. Primero, usamos la función `rep` para generar la bolsa:

```{r}
beads <- rep(c("red", "blue"), times = c(2,3))
beads
```

y luego usamos `sample` para elegir una canica al azar:

```{r}
sample(beads, 1)
```

Esta línea de código produce un resultado aleatorio. Queremos repetir este experimento un número infinito de veces, pero es imposible repetirlo para siempre. En cambio, repetimos el experimento un número suficientemente grande de veces para que los resultados sean prácticamente equivalentes a repetirlo para siempre. **Este es un ejemplo de una simulación _Monte Carlo_**.

Gran parte de lo que estudian los estadísticos matemáticos y teóricos, que no discutimos en este libro, se relaciona con proveer definiciones rigurosas de "prácticamente equivalente", así como estudiar cuán cerca nos llevan un gran número de experimentos a lo que sucede en el límite. Más adelante en esta sección, ofrecemeos un acercamiento práctico para decidir qué es "lo suficientemente grande".

Para realizar nuestra primera simulación Monte Carlo, utilizamos la función `replicate`, que nos permite repetir la misma tarea varias veces. Aquí, repetimos el evento aleatorio $B =$ 10,000 veces:

```{r}
B <- 10000
events <- replicate(B, sample(beads, 1))
```

Ahora podemos ver si nuestra definición realmente está de acuerdo con esta aproximación de simulación Monte Carlo. Nosotros podemos usar `table` para ver la distribución:

```{r}
tab <- table(events)
tab
```

y `prop.table` nos da las proporciones:

```{r}
prop.table(tab)
```

Los números anteriores son probabilidades estimadas proveídas por una simulación Monte Carlo. La teoría estadística, que no discutimos aquí, nos dice que en lo que $B$ se hace más grande, las estimaciones se acercan a 3/5 = .6 y 2/5 = .4.

Aunque este es un ejemplo sencillo y no muy útil, luego utilizaremos simulaciones Monte Carlo para estimar probabilidades en casos en los cuales es difícil calcular cantidades exactas. Antes de profundizar en ejemplos más complejos, usaremos algunos sencillos para demostrar las herramientas informáticas disponibles en R.

### Fijar la semilla aleatoria

Antes de continuar, explicaremos brevemente la importante línea de código a continuación:

```{r}
set.seed(1986)
```

A lo largo de este libro, utilizamos generadores de números aleatorios. Esto implica que muchos de los resultados que presentamos pueden cambiar por casualidad y una versión congelada del libro puede mostrar un resultado diferente al que obtienen cuando intenten codificar como observan en el libro. Esto no es un problema, ya que los resultados son aleatorios y pueden cambiar. Sin embargo, si quieren asegurarse de que los resultados son exactamente los mismos cada vez que los ejecuten, pueden fijar la semilla (_seed_ en inglés) de generación de números aleatorios de R en un número específico. Arriba la fijamos en 1986. Queremos evitar usar la misma semilla cada vez. Una forma popular de escoger la semilla es restando el mes y el día del año. Por ejemplo, para el 20 de diciembre de 2018 fijamos la semilla en 1986: $2018 - 12 - 20 = 1986$.

Pueden obtener más información sobre cómo fijar la semilla mirando la documentación:

```{r,eval=FALSE}
?set.seed
```

En los ejercicios, es posible que les pidamos que fijen la semilla para asegurar que sus resultados sean exactamente lo que esperan.

### Con y sin reemplazo

La función `sample` tiene un argumento que nos permite elegir más de un elemento de la bolsa. Sin embargo, por defecto, esta selección ocurre _sin reemplazo_: después de seleccionar una canica, no se vuelve a colocar en la bolsa. Observen lo que sucede cuando pedimos seleccionar cinco canicas al azar:

```{r}
sample(beads, 5)
sample(beads, 5)
sample(beads, 5)
```

Esto resulta en reordenamientos que siempre tienen tres canicas azules y dos rojas. Si pedimos que se seleccionen seis canicas, obtenemos un error:

```{r, eval=FALSE}
sample(beads, 6)
```

`Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE'`

Sin embargo, la función `sample` se puede usar directamente, sin el uso de `replicate`, para repetir el mismo experimento de elegir 1 de las 5 canicas, continuamente, en las mismas condiciones. Para hacer esto, muestreamos _con reemplazo_: se devuelve la canica a la bolsa después de seleccionarla.
Podemos decirle a `sample` que haga esto cambiando el argumento `replace`, que por defecto es `FALSE`, a `replace = TRUE`:

```{r}
events <- sample(beads, B, replace = TRUE)
prop.table(table(events))
```

No sorprende que obtengamos resultados muy similares a los obtenidos previamente con `replicate`.

## Independencia

Decimos que dos eventos son independientes si el resultado de uno no afecta al otro. El ejemplo clásico es el lanzamiento de monedas. Cada vez que lanzamos una moneda, la probabilidad de ver cara es 1/2, independientemente de los resultados de lanzamientos anteriores. Lo mismo es cierto cuando recogemos canicas de una bolsa con reemplazo. [fix] En el ejemplo anterior, la probabilidad de rojo es 0.40 independientemente de las elecciones anteriores.

Muchos ejemplos de eventos que no son independientes provienen de juegos de cartas. Cuando repartimos la primera carta, la probabilidad de obtener una K es 1/13 ya que hay trece posibilidades: Dos, Tres, $\dots$, Diez, J, Q, K y As. Pero si repartimos una K como la primera carta y no la reemplazamos en la baraja, las probabilidades de que una segunda carta sea K es menor porque solo quedan tres Ks: la probabilidad es 3 de 51. Estos eventos por lo tanto **no son independientes**: el primer resultado afecta al siguiente.

Para ver un caso extremo de eventos no independientes, consideren nuestro ejemplo de escoger cinco canicas al azar **sin** reemplazo:

```{r, echo=FALSE}
set.seed(1)
```

```{r}
x <- sample(beads, 5)
```

Si tienen que adivinar el color de la primera canica, predecirán el azul ya que el azul tiene un 60% de posibilidad. Pero si les mostramos el resultado de los últimos cuatro resultados:

```{r}
x[2:5]
```

¿aún adivinarían azul? Por supuesto que no. Ahora saben que la probabilidad de rojo es 1 ya que la única canica que queda es roja. Los eventos no son independientes, por lo que las probabilidades cambian.

## Probabilidades condicionales

Cuando los eventos no son independientes, las _probabilidades condicionales_ son útiles. Ya vimos un ejemplo de una probabilidad condicional: calculamos la probabilidad de que una segunda carta repartida sea K dado que la primera fue K. En la probabilidad, usamos la siguiente notación:

$$
\mbox{Pr}(\mbox{Card 2 is a king} \mid \mbox{Card 1 is a king}) = 3/51
$$

Utilizamos el $\mid$ como abreviatura de "dado eso" o "condicional en".

Cuando dos eventos, digamos $A$ y $B$, son independientes, tenemos:

$$
\mbox{Pr}(A \mid B) = \mbox{Pr}(A)
$$

Esta es la forma matemática de decir: el hecho de que $B$ sucedió no afecta la probabilidad de que $A$ suceda. De hecho, esto puede considerarse la definición matemática de independencia.

## Reglas de la adición y la multiplicación

### Regla de la multiplicación

Si queremos saber la probabilidad de que ocurran dos eventos, digamos $A$ y $B$, podemos usar la regla de la multiplicación:

$$
\mbox{Pr}(A \mbox{ and } B) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)
$$
Usemos el juego de cartas Blackjack como ejemplo. En Blackjack, se les asignan dos cartas al azar. Después de ver lo que tienen, pueden pedir más cartas. El objetivo es acercarse más a 21 que el croupier, sin pasar. Las _cartas con figuras_  o _figuras_ (_face cards_ en inglés) valen 10 puntos y las Ases valen 11 o 1 (uno elige).

Entonces, en un juego de Blackjack, para calcular las posibilidades de obtener un 21 sacando un As y luego una carta de figura, calculamos la probabilidad de que la primera carta sea un As y multiplicamos por la probabilidad de sacar una carta de figura o un 10 dado que la primera fue un As: $1/13 \times 16/51 \approx 0.025$.

La regla de la multiplicación también se aplica a más de dos eventos. Podemos usar la inducción para incluir más eventos:

$$
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)\mbox{Pr}(C \mid A \mbox{ and } B)
$$

### Regla de la multiplicación bajo independencia

Cuando tenemos eventos independientes, la regla de la multiplicación se hace más sencilla:

$$
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B)\mbox{Pr}(C)
$$

Pero debemos tener mucho cuidado antes de usar esto ya que suponer independencia puede dar como resultado cálculos de probabilidad muy diferentes e incorrectos cuando en realidad no tenemos independencia.

Como ejemplo, imaginen un caso judicial en el que se describió al sospechoso como teniendo bigote y barba. El acusado tiene bigote y barba y la fiscalía trae a un "experto" que testifica que 1/10 hombres tienen barba y 1/5 tienen bigote, así que usando la regla de la multiplicación concluimos que solo $1/10 \times 1/5$ o 0.02 tienen ambos.

¡Pero para multiplicar así necesitamos suponer independencia! Digamos que la probabilidad condicional de que un hombre tenga un bigote condicionado en que tenga barba es .95. Entonces el cálculo correcto de la probabilidad resulta en un número mucho mayor: $1/10 \times 95/100 = 0.095$.

La regla de la multiplicación también nos da una fórmula general para calcular probabilidades condicionales:


$$
\mbox{Pr}(B \mid A) = \frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
$$

Para ilustrar cómo usamos estas fórmulas y conceptos en la práctica, utilizaremos varios ejemplos relacionados con los juegos de cartas.

### Regla de la adición

La regla de la adición nos dice que:

$$
\mbox{Pr}(A \mbox{ or } B) = \mbox{Pr}(A) + \mbox{Pr}(B) - \mbox{Pr}(A \mbox{ and } B)
$$

Esta regla es intuitiv: piense en un diagrama de Venn. Si simplemente sumamos las probabilidades, contamos la intersección dos veces, por lo que debemos restar una instancia.

```{r venn-diagram-addition-rule, fig.height=7.5, fig.width=7.5, echo=FALSE, warning=FALSE, message=FALSE, out.width="35%"}
library(VennDiagram)
rafalib::mypar()
grid.newpage()
tmp <- draw.pairwise.venn(22, 20, 11, category = c("A", "B"),
lty = rep("blank", 2),
fill = c("light blue", "pink"),
alpha = rep(0.5, 2),
cat.dist = rep(0.025, 2), cex=0, cat.cex = rep(2.5,2))
```

## Combinaciones y permutaciones

En nuestro primer ejemplo, imaginamos una bolsa con cinco canicas. Recuerden que para calcular la distribución de probabilidad de una elección, simplemente enumeramos todas las posibilidades. Hubo 5 y para cada evento, contamos cuántas de estas posibilidades estaban asociadas con el evento. Entonces la probabilidad de elegir una canica azul es 3/5 porque de los cinco resultados posibles, tres fueron azules.

Para casos más complicados, los cálculos no son tan sencillos. Por ejemplo, ¿cuál es la probabilidad de que si cojo cinco cartas sin reemplazo, obtengo todas cartas del mismo palo (_suit_ en inglés), lo que se conoce como "flush" en el póker? En un curso de probabilidad discreta, se aprende la teoría sobre cómo hacer estos cálculos. Aquí nos enfocamos en cómo usar el código R para calcular las respuestas.

Primero, construyamos una baraja de cartas. Para esto, usaremos las funciones `expand.grid` y `paste`. Usamos `paste` para crear cadenas uniendo cadenas más pequeñas. Para hacer esto, tomamos el número y el tipo de una carta y creamos el nombre de la carta de esta manera:

```{r}
number <- "Three"
suit <- "Hearts"
paste(number, suit)
```

`paste` también funciona en pares de vectores que realizan la operación elemento por elemento:

```{r}
paste(letters[1:5], as.character(1:5))
```

La función `expand.grid` nos da todas las combinaciones de entradas de dos vectores. Por ejemplo, si tienen pantalones azules y negros y camisas blancas, grises y a cuadros (_plaid_ en inglés), todas sus combinaciones son:

```{r}
expand.grid(pants = c("blue", "black"), shirt = c("white", "grey", "plaid"))
```

Así es como generamos una baraja de cartas:
```{r}
suits <- c("Diamonds", "Clubs", "Hearts", "Spades")
numbers <- c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven",
"Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck <- expand.grid(number=numbers, suit=suits)
deck <- paste(deck$number, deck$suit)
```

Con la baraja construida, podemos verificar que la probabilidad de una K en la primera carta es 1/13 calculando la proporción de posibles resultados que satisfagan nuestra condición:

```{r}
kings <- paste("King", suits)
mean(deck %in% kings)
```

Ahora, ¿qué tal la probabilidad condicional de que la segunda carta sea una K dado que la primera era una K? Anteriormente, dedujimos que si una K ya está fuera de la baraja y quedan 51 cartas, entonces la probabilidad es 3/51. Confirmemos enumerando todos los resultados posibles.

Para hacer esto, podemos usar la función `permutations` del paquete __gtools__. Para cualquier lista de tamaño `n`, esta función calcula todas las diferentes combinaciones que podemos obtener cuando seleccionamos `r` artículos. Aquí están todas las formas en que podemos elegir dos números de una lista que consiste en `1,2,3`:

```{r, message=FALSE, warning=FALSE}
library(gtools)
permutations(3, 2)
```

Observen que el orden importa aquí: 3,1 es diferente de 1,3. Además, tengan en cuenta que (1,1), (2,2) y (3,3) no aparecen porque una vez que elegimos un número, no puede volver a aparecer.

Opcionalmente, podemos agregar un vector. Si desean ver cinco números de teléfono aleatorios de siete dígitos de todos los números de teléfono posibles (sin repeticiones), pueden escribir:

```{r}
all_phone_numbers <- permutations(10, 7, v = 0:9)
n <- nrow(all_phone_numbers)
index <- sample(n, 5)
all_phone_numbers[index,]
```

En lugar de usar los números del 1 al 10, el valor por defecto, R usa lo que proveemos a través de `v`: los dígitos de 0 a 9.

Para calcular todas las formas posibles en que podemos elegir dos cartas cuando el orden importa, escribimos:
```{r}
hands <- permutations(52, 2, v = deck)
```

Esta es una matriz con dos columnas y `r nrow(hands)` filas. Con una matriz podemos obtener la primera y segunda carta así:

```{r}
first_card <- hands[,1]
second_card <- hands[,2]
```

[fix how can a hand have one card] Ahora los casos para los cuales la primera mano era una K se pueden calcular así:

```{r}
kings <- paste("King", suits)
sum(first_card %in% kings)
```

Para obtener la probabilidad condicional, calculamos qué fracción de estos tiene una K como la segunda carta:

```{r}
sum(first_card%in%kings & second_card%in%kings)/ sum(first_card%in%kings)
```

que es exactamente 3/51, como ya habíamos deducido. Tengan en cuenta que el código anterior es equivalente a:

```{r}
mean(first_card%in%kings & second_card%in%kings)/ mean(first_card%in%kings)
```

que usa `mean` en lugar de `sum` y es una versión R de:

$$
\frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
$$


¿Y qué tal si el orden no importa? Por ejemplo, en Blackjack si le dan un As y una carta de figura como su primera mano, se llama un _Natural 21_ y ganan automáticamente. Si quisiéramos calcular la probabilidad de que esto suceda, enumeraríamos las _combinaciones_, no las permutaciones, ya que el orden no importa.

```{r}
combinations(3,2)
```

En la segunda línea, el resultado no incluye (2,1) porque (1,2) ya se enumeró. Lo mismo aplica a (3,1) y (3,2).

Entonces, para calcular la probabilidad de un _Natural 21_ en Blackjack, podemos hacer esto:

```{r}
aces <- paste("Ace", suits)

facecard <- c("King", "Queen", "Jack", "Ten")
facecard <- expand.grid(number = facecard, suit = suits)
facecard <- paste(facecard$number, facecard$suit)

hands <- combinations(52, 2, v = deck)
mean(hands[,1] %in% aces & hands[,2] %in% facecard)
```

[fix check para] En la última línea, suponemos que el As es la primera carta que nos dan. Esto es solo porque sabemos cómo `combination` enumera las posibilidades y enumerará este caso primero. Pero para estar seguros, podríamos haber producido la misma respuesta al escribir lo siguiente:

```{r}
mean((hands[,1] %in% aces & hands[,2] %in% facecard) |
(hands[,2] %in% aces & hands[,1] %in% facecard))
```


### Ejemplo Monte Carlo

En lugar de usar `combinations` para deducir la probabilidad exacta de un _Natural 21_, podemos usar un Monte Carlo para estimar esta probabilidad. En este caso, escogemos dos cartas una y otra vez y notamos cuántos 21s tenemos. Podemos usar la función `sample` para escoger dos cartas sin reemplazos:

```{r}
hand <- sample(deck, 2)
hand
```

Y luego verificar si una carta es un As y la otra una figura o un 10. De ahora en adelante, incluimos 10 cuando decimos carta de figura o figura. Ahora necesitamos verificar ambas posibilidades:

```{r}
(hands[1] %in% aces & hands[2] %in% facecard) |
(hands[2] %in% aces & hands[1] %in% facecard)
```

Si repetimos esto 10,000 veces, obtenemos una muy buena aproximación de la probabilidad de un _Natural 21_.

Comencemos escribiendo una función que escoja una mano y devuelva TRUE si obtenemos un 21. La función no necesita argumentos porque usa objetos definidos en el entorno global.
```{r}
blackjack <- function(){
hand <- sample(deck, 2)
(hand[1] %in% aces & hand[2] %in% facecard) |
(hand[2] %in% aces & hand[1] %in% facecard)
}
```

Aquí tenemos que verificar ambas posibilidades: As primero o As segundo porque no estamos usando la función `combinations`. La función devuelve `TRUE` si obtenemos un 21 y `FALSE` de otra manera:

```{r}
blackjack()
```

Ahora podemos jugar este juego, digamos, 10,000 veces:

```{r}
B <- 10000
results <- replicate(B, blackjack())
mean(results)
```

## Ejemplos

En esta sección, describimos dos ejemplos populares de probabilidad discreta: el problema Monty Hall y el problema del cumpleaños. Usamos R para ayudar a ilustrar los conceptos matemáticos.

### Problema Monty Hall

[fix check all] En la década de 1970 en EE.UU, hubo un juego televiso llamado "Let's Make a Deal" y Monty Hall era el anfitrión. En algún momento del juego, se le pedía al concursante que eligiera una de tres puertas. Detrás de una puerta había un premio, mientras detrás de las otras puertas tenían una cabra que señalaba que el concursante había perdido. Después de que el concursante eligiera una puerta y antes de revelar si esa puerta contenía un premio, Monty Hall abría una de las otras dos puertas y le mostraba al concursante que no había ningún premio detrás de esa puerta. Luego le preguntaba al concursante: "¿Quieres cambiar de puerta?" ¿Qué harían Uds.?

Podemos usar la probabilidad para mostrar que si se quedan con la opción de la puerta original, sus posibilidades de ganar un premio siguen siendo 1 en 3. [fix check] Sin embargo, si cambian a la otra puerta, ¡sus posibilidades de ganar duplican a 2 en 3! Esto parece contradictorio. Muchas personas piensan incorrectamente que ambas posibilidades son 1 en 2 ya que uno elige entre 2 opciones. Pueden ver una explicación matemática detallada en Khan Academy^[https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem] o leer una en Wikipedia^[https://en.wikipedia.org/wiki/Monty_Hall_problem]. A continuación, usamos una simulación Monte Carlo para ver qué estrategia es mejor. [fix] Tengan en cuenta que este código se escribe en más detalle de lo necesario para fines pedagógicos.

[fix] Comencemos con la estrategia del palo/"stick strategy"??:

```{r}
B <- 10000
monty_hall <- function(strategy){
doors <- as.character(1:3)
prize <- sample(c("car", "goat", "goat"))
prize_door <- doors[prize == "car"]
my_pick <- sample(doors, 1)
show <- sample(doors[!doors %in% c(my_pick, prize_door)],1)
stick <- my_pick
stick == prize_door
switch <- doors[!doors%in%c(my_pick, show)]
choice <- ifelse(strategy == "stick", stick, switch)
choice == prize_door
}
stick <- replicate(B, monty_hall("stick"))
mean(stick)
switch <- replicate(B, monty_hall("switch"))
mean(switch)
```

[fix check] Mientras escribimos el código, notamos que las líneas que comienzan con `my_pick` y `show` no afectan la última operación lógica cuando nos atenemos a nuestra elección original. De esto, debemos darnos cuenta de que la probabilidad es de 1 en 3, con lo que comenzamos. Cuando cambiamos, la estimación Monte Carlo confirma el cálculo de 2/3. Esto nos ayuda entender el problema mejor al mostrar que estamos quitando una puerta, `show`, que definitivamente no esconde un premio de nuestras opciones. También vemos que, a menos que lo hagamos bien cuando elegimos por primera vez, ustedes ganan: 1 - 1/3 = 2/3.

### Problema de cumpleaños

Supongamos que están en un salón de clases con 50 personas. Si suponemos que este es un grupo de 50 personas seleccionadas al azar, ¿cuál es la probabilidad de que al menos dos personas tengan el mismo cumpleaños? Aunque es algo avanzado, podemos deducir esto matemáticamente. Haremos esto más tarde, pero aquí usamos una simulación Monte Carlo. Para simplificar, suponemos que nadie nació el 29 de febrero. En realidad esto no cambia mucho la respuesta.

Primero, tengan en cuenta que los cumpleaños se pueden representar como números entre 1 y 365, por lo que se puede obtener una muestra de 50 cumpleaños de esta manera:

```{r,echo=FALSE}
set.seed(1)
```

```{r}
n <- 50
bdays <- sample(1:365, n, replace = TRUE)
```

Para verificar si en este set particular de 50 personas tenemos al menos dos con el mismo cumpleaños, podemos usar la función `duplicated`, que devuelve `TRUE` siempre que un elemento de un vector es un duplicado. Aquí hay un ejemplo:

```{r}
duplicated(c(1,2,3,1,4,3,5))
```

La segunda vez que aparecen 1 y 3, obtenemos un `TRUE`. Entonces, para verificar si dos cumpleaños fueron iguales, simplemente usamos las funciones `any` y `duplicated` así:

```{r}
any(duplicated(bdays))
```

En este caso, vemos que sucedió. Al menos dos personas tuvieron el mismo cumpleaños.

Para estimar la probabilidad de un cumpleaños compartido en el grupo, repetimos este experimento muestreando sets de 50 cumpleaños una y otra vez:

```{r birthday-problem}
B <- 10000
same_birthday <- function(n){
bdays <- sample(1:365, n, replace=TRUE)
any(duplicated(bdays))
}
results <- replicate(B, same_birthday(50))
mean(results)
```

¿Esperaban que la probabilidad fuera tan alta?

Las personas tienden a subestimar estas probabilidades. Para tener una idea de por qué es tan alto, piense en lo que sucede cuando el tamaño del grupo es cercano a 365. En esta etapa, se nos acaban los días y la probabilidad es una.

[fix check all paragraph] Digamos que queremos usar este conocimiento para apostar con amigos sobre si dos personas en un grupo de personas tienen el mismo cumpleaños. ¿Cuándo son las posibilidades superiores a 50%? ¿Superiores a 75%?

Empecemos creando una tabla de consulta. Rápidamente podemos crear una función para calcular esto para cualquier tamaño de grupo:

```{r}
compute_prob <- function(n, B=10000){
results <- replicate(B, same_birthday(n))
mean(results)
}
```


[check] Usando la función `sapply`, podemos realizar operaciones elemento por elemento en cualquier función:

```{r}
n <- seq(1,60)
prob <- sapply(n, compute_prob)
```

Ahora podemos hacer una gráfica de las probabilidades estimadas de dos personas tener el mismo cumpleaños en un grupo de tamaño $n$:

```{r birthday-problem-mc-probabilities, warning=FALSE, message=FALSE}
library(tidyverse)
prob <- sapply(n, compute_prob)
qplot(n, prob)
```

Ahora calculemos las probabilidades exactas en lugar de usar aproximaciones Monte Carlo. No solo obtenemos la respuesta exacta usando matemáticas, sino que los cálculos son mucho más rápidos ya que no tenemos que generar experimentos.


Para simplificar las matemáticas, en lugar de calcular la probabilidad de que ocurra, calcularemos la probabilidad de que no ocurra. Para esto, usamos la regla de la multiplicación.

[fix check all] Comencemos con la primera persona. La probabilidad de que persona 1 tenga un cumpleaños único es 1. La probabilidad de que persona 2 tenga un cumpleaños único, dado que persona 1 ya cumplió uno, es 364/365. Luego, dado que las dos primeras personas tienen cumpleaños únicos, persona 3 tiene 363 días para elegir. Continuamos de esta manera y encontramos que las posibilidades de que todas las 50 personas tengan un cumpleaños único son:

$$
1 \times \frac{364}{365}\times\frac{363}{365} \dots \frac{365-n + 1}{365}
$$

Podemos escribir una función que haga esto para cualquier número:

```{r birthday-problem-exact-probabilities}
exact_prob <- function(n){
prob_unique <- seq(365,365-n+1)/365
1 - prod( prob_unique)
}
eprob <- sapply(n, exact_prob)
qplot(n, prob) + geom_line(aes(n, eprob), col = "red")
```

Este gráfico muestra que la simulación Monte Carlo ofrece una muy buena estimación de la probabilidad exacta. Si no hubiera sido posible calcular las probabilidades exactas, aún habríamos podido estimar con precisión las probabilidades.


## [fix] Infinito en la práctica

La teoría descrita aquí requiere repetir experimentos una y otra vez para siempre. En la práctica no podemos hacer esto. En los ejemplos anteriores, utilizamos $B=10,000$ experimentos Monte Carlo y resultó que esto nos dio estimaciones precisas. Cuanto mayor sea este número, más precisa será la estimación hasta que la aproximación sea tan buena que sus computadoras no puedan notar la diferencia. Pero en cálculos más complejos, 10,000 puede ser insuficiente. Además, para algunos cálculos, 10,000 experimentos podrían ser computacionalmente infactibles. En la práctica, no sabremos cuál es la respuesta, por lo que no sabremos si nuestra estimación Monte Carlo es precisa. [fix] Sabemos que entre más grande sea $B$, mejor será la aproximación. ¿Pero cuán grande necesitamos que sea? Esta es realmente una pregunta desafiante y contestarla frecuentemente requiere una formación avanzada en estadística teórica.

Un enfoque práctico que describiremos aquí es verificar la estabilidad de la estimación. A continuación ofrecemos un ejemplo con el problema de cumpleaños para un grupo de 25 personas.

```{r monte-carlo-convergence}
B <- 10^seq(1, 5, len = 100)
compute_prob <- function(B, n=25){
same_day <- replicate(B, same_birthday(n))
mean(same_day)
}
prob <- sapply(B, compute_prob)
qplot(log10(B), prob, geom = "line")
```

En este gráfico, podemos ver que los valores comienzan a estabilizarse (es decir, varían menos de .01) alrededor de 1000. Noten que la probabilidad exacta, que en este caso sabemos, es `r eprob[25]`.



## Ejercicios


1\. Se escoge una canica al azar de una bolsa que contiene: 3 canicas cian, 5 canicas magenta y 7 canicas amarillas. ¿Cuál es la probabilidad de que la canica sea cian?


2\. ¿Cuál es la probabilidad de que la canica no sea cian?


3\. En lugar de escoger solo una vez, considere escoger dos canicas. Usted saca la primera canica sin devolverla a la bolsa. Este es un muestreo **sin** reemplazo. ¿Cuál es la probabilidad de que la primera canica sea cian y la segunda no sea cian?


4\. Ahora repita el experimento, pero esta vez, después de sacar la primera canica y regsitrar el color, devuélvala a la bolsa y agite la bolsa. Este es un muestreo **con** reemplazo. ¿Cuál es la probabilidad de que la primera canica sea cian y la segunda canica no sea cian?


5\. [fix] Dos eventos $A$ y $B$ son independientes si $\mbox{Pr}(A \mbox{ and } B) = \mbox{Pr}(A) P(B)$. ¿Bajo qué situación son independientes la seleccion?

a. No reemplazas el articulo seleccionado.
b. Reemplazas el articulo seleccionado.
c. Ninguno.
d. Ambos.

6\. Digamos que ha sacado 5 canicas de la bolsa, con reemplazo, y todas han sido amarillas. ¿Cuál es la probabilidad de que la próxima sea amarilla?

7\. Si lanzas un dado de 6 lados seis veces, ¿cuál es la probabilidad de no ver un 6?

8\. [fix] Dos equipos de baloncesto, digamos los Celtics y los Cavs, están jugando una serie de siete juegos. Los Cavs son un mejor equipo y tienen un 60% de posibilidad de ganar cada juego. ¿Cuál es la probabilidad de que los Celtics ganen **al menos** un juego?

9\. Cree una simulación Monte Carlo para confirmar su respuesta al problema anterior. Utilizar `B <- 10000` simulaciones. Sugerencia: use el siguiente código para generar los resultados de los primeros cuatro juegos:

```{r, eval=FALSE}
celtic_wins <- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))
```
Los Celtics deben ganar uno de estos 4 juegos.

10\. Dos equipos de baloncesto, digamos los Cavs y los Warriors, están jugando una serie de campeonato de siete juegos. El primero en ganar cuatro juegos, por ende, gana la serie. Los equipos son igualmente buenos, por lo que cada uno tiene una probabilidad de 50-50 de ganar cada juego. Si los Cavs pierden el primer juego, ¿cuál es la probabilidad de que ganen la serie?

11\. Confirme los resultados de la pregunta anterior con una simulación Monte Carlo.

12\. Dos equipos, $A$ y $B$, están jugando una serie de siete juegos. Equipo $A$ es mejor que equipo $B$ y tiene un $p>0.5$ posibilidad de ganar cada juego. Dado un valor $p$, la probabilidad de ganar la serie para el equipo de abajo $B$ se puede calcular con la siguiente función basada en una simulación Monte Carlo:

```{r, eval=FALSE}
prob_win <- function(p){
B <- 10000
result <- replicate(B, {
b_win <- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))
sum(b_win)>=4
})
mean(result)
}
```

Usa la función `sapply` para calcular la probabilidad, llámela `Pr`, de ganar para `p <- seq(0.5, 0.95, 0.025)`. Luego grafica el resultado.

13\. Repita el ejercicio anterior, pero ahora mantenga la probabilidad fija en `p <- 0.75` y calcule la probabilidad de diferentes longitudes de serie: lo mejor de 1 juego, 3 juegos, 5 juegos, ... Específicamente, `N <- seq(1, 25, 2)`. Sugerencia: use esta función:

```{r, eval = FALSE}
prob_win <- function(N, p=0.75){
B <- 10000
result <- replicate(B, {
b_win <- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))
sum(b_win)>=(N+1)/2
})
mean(result)
}
```

