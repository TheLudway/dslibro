# Minería de textos

Con la excepción de las etiquetas utilizadas para representar datos categóricos, nos hemos enfocado en los datos numéricos. Pero en muchas aplicaciones, los datos comienzan como texto. Ejemplos bien conocidos son el filtrado de spam, la prevención del delito cibernético, la lucha contra el terrorismo y el análisis de sentimiento (también conocido como minería de opinión). En todos estos casos, los datos sin procesar se componen de texto de forma libre. Nuestra tarea es extraer información de estos datos. En esta sección, aprendemos cómo generar resúmenes numéricos útiles a partir de datos de texto a los que podemos aplicar algunas de las poderosas técnicas de visualización y análisis de datos que hemos aprendido.


## Estudio de caso: tuits de Trump

Durante las elecciones presidenciales estadounidenses de 2016, el candidato Donald J. Trump usó su cuenta de Twitter como una manera de comunicarse con los posibles votantes. El 6 de agosto de 2016, Todd Vaziri tuiteó^[https://twitter.com/tvaziri/status/762005541388378112/photo/1] sobre Trump declarando que "Cada tweet no hiperbólico es de iPhone (su personal). Cada tweet hiperbólico es de Android (de él) ".
El científico de datos David Robinson realizó un análisis^[http://varianceexplained.org/r/trump-tweets/] para determinar si los datos respaldan esta afirmación. Aquí, revisamos el análisis de David para aprender algunos de los conceptos básicos de la minería de textos. Para obtener más información sobre la minería de textos en R, recomendamos el libro _Text Mining with R_^[https://www.tidytextmining.com/] de Julia Silge y David Robinson.

```{r,echo=FALSE}
set.seed(2002)
```

Utilizaremos los siguientes paquetes:
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(scales)
```

En general, podemos extraer datos directamente de Twitter usando el paquete __rtweet__. Sin embargo, en este caso, un grupo ya ha compilado datos para nosotros y los ha puesto a disposición en [http://www.trumptwitterarchive.com](http://www.trumptwitterarchive.com). Podemos obtener los datos de su API JSON usando un _script_ como este:

```{r, eval=FALSE}
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
trump_tweets <- map(2009:2017, ~sprintf(url, .x)) %>%
  map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
  filter(!is_retweet & !str_detect(text, '^"')) %>%
  mutate(created_at = parse_date_time(created_at,
                                      orders = "a b! d! H!:M!:S! z!* Y!",
                                      tz="EST"))
```

Para facilitar el análisis, incluimos el resultado del código anterior en el paquete __dslabs__:

```{r}
library(dslabs)
data("trump_tweets")
```

Pueden ver el _data frame_ con información sobre los tuits al escribir:

```{r, eval=FALSE}
head(trump_tweets)
```

con las siguientes variables incluidas:

```{r}
names(trump_tweets)
```

El archivo de ayuda `?trump_tweets` provee detalles sobre lo que representa cada variable. Los tuits están representados por el variable `text`:

```{r}
trump_tweets$text[16413] %>% str_wrap(width = options()$width) %>% cat
```

y la variable `source` nos dice qué dispositivo se usó para componer y cargar cada tuit:

```{r}
trump_tweets %>% count(source) %>% arrange(desc(n)) %>% head(5)
```

Estamos interesados en lo que sucedió durante la campaña, por lo que para este análisis nos enfocaremos en lo que se tuiteó entre el día en que Trump anunció su campaña y el día de las elecciones. Definimos la siguiente tabla que contiene solo los tuits de ese período de tiempo. Tengan en cuenta que usamos `extract` para eliminar la parte `Twitter for` de `source` y filtrar los _retweets_.

```{r}
campaign_tweets <- trump_tweets %>%
  extract(source, "source", "Twitter for (.*)") %>%
  filter(source %in% c("Android", "iPhone") &
           created_at >= ymd("2015-06-17") &
           created_at < ymd("2016-11-08")) %>%
  filter(!is_retweet) %>%
  arrange(created_at)  %>%
  as_tibble()
```

Ahora podemos usar la visualización de datos para explorar la posibilidad de que dos grupos diferentes hayan escrito los mensajes desde estos dispositivos. Para cada tuit, extraeremos la hora en que se publicó (hora de la costa este de EE.UU. o EST por sus siglas en inglés), y luego calcularemos la proporción de tuits tuiteada a cada hora para cada dispositivo:

```{r tweets-by-time-by-device}
campaign_tweets %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  count(source, hour) %>%
  group_by(source) %>%
  mutate(percent = n/ sum(n)) %>%
  ungroup %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
```


Notamos un gran pico para Android en las primeras horas de la mañana, entre las 6 y las 8 de la mañana. Parece haber una clara diferencia en estos patrones. Por lo tanto, suponemos que dos entidades diferentes están utilizando estos dos dispositivos.

Ahora estudiaremos cómo difieren los tuits cuando comparamos Android con iPhone. Para hacer esto, utilizaremos el paquete __tidytext__.


## Texto como datos

El paquete __tidytext__ nos ayuda a convertir texto de forma libre en una tabla ordenada. Tener los datos en este formato facilita enormemente la visualización de datos y el uso de técnicas estadísticas.

```{r}
library(tidytext)
```

La función principal necesaria para lograr esto es `unnest_tokens`. Un _token_ se refiere a una unidad que consideramos como un punto de datos. Los _tokens_ más comunes son las palabras, pero también pueden ser caracteres individuales, _ngrams_, oraciones, líneas o un patrón definido por una expresión regular. Las funciones tomarán un vector de cadenas y extraerán los _tokens_ para que cada uno obtenga una fila en la nueva tabla. Aquí hay un ejemplo sencillo:

```{r}
poem <- c("Roses are red,", "Violets are blue,",
          "Sugar is sweet,", "And so are you.")
example <- tibble(line = c(1, 2, 3, 4),
                  text = poem)
example
example %>% unnest_tokens(word, text)
```

Ahora consideremos un ejemplo de los tuits. Miren el tuit número 3008 porque luego nos permitirá ilustrar un par de puntos:

```{r}
i <- 3008
campaign_tweets$text[i] %>% str_wrap(width = 65) %>% cat()
campaign_tweets[i,] %>%
  unnest_tokens(word, text) %>%
  pull(word)
```

Noten que la función intenta convertir _tokens_ en palabras. Para hacer esto, sin embargo, elimina los caracteres que son importantes en el contexto de Twitter. Específicamente, la función elimina todos los `#` y `@`. Un _token_ en el contexto de Twitter no es lo mismo que en el contexto del inglés hablado o escrito. Por esta razón, en lugar de usar el valor predeterminado, `words`, usamos el _token_ `tweets` que incluye patrones que comienzan con `@` y `#`:


```{r, message=FALSE, warning=FALSE}
campaign_tweets[i,] %>%
  unnest_tokens(word, text, token = "tweets") %>%
  pull(word)
```

Otro ajuste menor que queremos hacer es eliminar los enlaces a las imágenes:

```{r, message=FALSE, warning=FALSE}
links <- "https://t.co/[A-Za-z\\d]+|&amp;"
campaign_tweets[i,] %>%
  mutate(text = str_replace_all(text, links, "")) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  pull(word)
```

Ya estamos listos para extraer las palabras de todos nuestros tuits.

```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets %>%
  mutate(text = str_replace_all(text, links, "")) %>%
  unnest_tokens(word, text, token = "tweets")
```


Y ahora podemos responder a preguntas como "¿cuáles son las palabras más utilizadas?":

```{r}
tweet_words %>%
  count(word) %>%
  arrange(desc(n)) %>%
  slice(1:10)
```

No es sorprendente que estas sean las palabras principales. Las palabras principales no son informativas. El paquete _tidytext_ tiene una base de datos de estas palabras de uso común, denominadas palabras _stop_, en la minería de textos:

```{r}
stop_words
```

Si filtramos las filas que representan las palabras _stop_ con `filter(!word %in% stop_words$word)`:

```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets %>%
  mutate(text = str_replace_all(text, links, "")) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word )
```

terminamos con un conjunto mucho más informativo de las 10 palabras más tuiteadas:

```{r}
tweet_words %>%
  count(word) %>%
  top_n(10, n) %>%
  mutate(word = reorder(word, n)) %>%
  arrange(desc(n))
```

Una exploración de las palabras resultantes (que no se muestran aquí) revela un par de características no deseadas en nuestros _tokens_. Primero, algunos de nuestros _tokens_ son solo números (años, por ejemplo). Queremos eliminarlos y podemos encontrarlos usando la expresión regular `^\d+$`. Segundo, algunos de nuestros _tokens_ provienen de una cita y comienzan con `'`. Queremos eliminar el `'` cuando está al comienzo de una palabra, así que simplemente usamos `str_replace`. Agregamos estas dos líneas al código anterior para generar nuestra tabla final:


```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets %>%
  mutate(text = str_replace_all(text, links, "")) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word &
           !str_detect(word, "^\\d+$")) %>%
  mutate(word = str_replace(word, "^'", ""))
```

Ahora que tenemos las palabras en una tabla e información sobre qué dispositivo se usó para componer el tuit, podemos comenzar a explorar qué palabras son más comunes al comparar Android con iPhone.

Para cada palabra, queremos saber si es más probable que provenga de un tuit de Android o un tuit de iPhone. En la sección \@ref(association-tests), introdujimos el _riesgo relativo_ (_odds ratio_ en inglés) como un resumen estadístico útil para cuantificar estas diferencias. Para cada dispositivo y una palabra dada, llamémosla `y`, calculamos el riesgo relativo. Aquí tendremos muchas proporciones que son 0, así que usamos la corrección 0.5 descrita en la Sección \@ref(association-tests).

```{r}
android_iphone_or <- tweet_words %>%
  count(word, source) %>%
  pivot_wider(names_from = "source", values_from = "n", values_fill = 0) %>%
  mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / 
           ( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
```

Aquí están los riesgos relativos más altos para Android:

```{r}
android_iphone_or %>% arrange(desc(or))
```

y los más altos para iPhone:

```{r}
android_iphone_or %>% arrange(or)
```

Dado que varias de estas palabras son palabras generales de baja frecuencia, podemos imponer un filtro basado en la frecuencia total así:

```{r}
android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(desc(or))

android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(or)
```

Ya vemos un patrón en los tipos de palabras que se tuitean más desde un dispositivo que desde otro. Sin embargo, no estamos interesados en palabras específicas sino en el tono. La afirmación de Vaziri es que los tuits de Android son más hiperbólicos. Entonces, ¿cómo podemos verificar esto con datos? _Hipérbole_ es un sentimiento difícil de extraer de las palabras, ya que se basa en la interpretación de frases. No obstante, las palabras pueden asociarse con sentimientos más básicos como la ira, el miedo, la alegría y la sorpresa. En la siguiente sección, demostramos el análisis básico de sentimientos.

## Análisis de sentimiento

En el análisis de sentimiento, asignamos una palabra a uno o más "sentimientos". Aunque este enfoque no siempre indentificará sentimientos que dependen del contexto, como el sarcasmo, cuando se realiza en grandes cantidades de palabras, los resúmenes pueden ofrecer información.

El primer paso en el análisis de sentimiento es asignar un sentimiento a cada palabra. Como demostramos, el paquete __tidytext__ incluye varios mapas o léxicos. También usaremos el paquete __textdata__.

```{r, message=FALSE, warning=FALSE}
library(tidytext)
library(textdata)
```

El léxico `bing` divide las palabras en sentimientos `positive` y `negative`. Podemos ver esto usando la función `get_sentiments` de __tidytext__:

```{r, eval=FALSE}
get_sentiments("bing")
```

El léxico `AFINN` asigna una puntuación entre -5 y 5, con -5 el más negativo y 5 el más positivo. Tengan en cuenta que este léxico debe descargarse la primera vez que llamen a la función `get_sentiment`:

```{r, eval=FALSE}
get_sentiments("afinn")
```

Los léxicos `loughran` y `nrc` ofrecen varios sentimientos diferentes. Noten que estos también deben descargarse la primera vez que los usen.

```{r}
get_sentiments("loughran") %>% count(sentiment)
```

```{r}
get_sentiments("nrc") %>% count(sentiment)
```

Para nuestro análisis, estamos interesados en explorar los diferentes sentimientos de cada tuit, por lo que utilizaremos el léxico `nrc`:

```{r}
nrc <- get_sentiments("nrc") %>%
  select(word, sentiment)
```

Podemos combinar las palabras y los sentimientos usando `inner_join`, que solo mantendrá palabras asociadas con un sentimiento. Aquí tenemos 10 palabras aleatorias extraídas de los tuits:


```{r}
tweet_words %>% inner_join(nrc, by = "word") %>%
  select(source, word, sentiment) %>%
  sample_n(5)
```

Ahora estamos listos para realizar un análisis cuantitativo comparando los sentimientos de los tuits publicados desde cada dispositivo. Podríamos realizar un análisis tuit por tuit, asignando un sentimiento a cada tuit. Sin embargo, esto sería un desafío ya que cada tuit tendrá varios sentimientos adjuntos, uno para cada palabra que aparezca en el léxico. Con fines ilustrativos, realizaremos un análisis mucho más sencillo: contaremos y compararemos las frecuencias de cada sentimiento que aparece en cada dispositivo.


```{r}
sentiment_counts <- tweet_words %>%
  left_join(nrc, by = "word") %>%
  count(source, sentiment) %>%
  pivot_wider(names_from = "source", values_from = "n") %>%
  mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
```

Para cada sentimiento, podemos calcular las probabilidades de estar en el dispositivo: proporción de palabras con sentimiento versus proporción de palabras sin. Entonces calculamos el riesgo relativo comparando los dos dispositivos.

```{r}
sentiment_counts %>%
  mutate(Android = Android/ (sum(Android) - Android) ,
         iPhone = iPhone/ (sum(iPhone) - iPhone),
         or = Android/iPhone) %>%
  arrange(desc(or))
```

Sí vemos algunas diferencias y el orden es particularmente interesante: ¡los tres sentimientos más grandes son el asco, la ira y lo negativo! ¿Pero estas diferencias son solo por casualidad? ¿Cómo se compara esto si solo estamos asignando sentimientos al azar? A fin de responder a esta pregunta, para cada sentimiento podemos calcular un riesgo relativo y un intervalo de confianza, como se definen en la Sección \@ref(association-tests). Agregaremos los dos valores que necesitamos para formar una tabla de dos por dos y el riesgo relativo:

```{r}
library(broom)
log_or <- sentiment_counts %>%
  mutate(log_or = log((Android/ (sum(Android) - Android))/
                        (iPhone/ (sum(iPhone) - iPhone))),
         se = sqrt(1/Android + 1/(sum(Android) - Android) +
                     1/iPhone + 1/(sum(iPhone) - iPhone)),
         conf.low = log_or - qnorm(0.975)*se,
         conf.high = log_or + qnorm(0.975)*se) %>%
  arrange(desc(log_or))

log_or
```

Una visualización gráfica muestra algunos sentimientos que están claramente sobrerrepresentados:

```{r tweets-log-odds-ratio}
log_or %>%
  mutate(sentiment = reorder(sentiment, log_or)) %>%
  ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point(aes(sentiment, log_or)) +
  ylab("Log odds ratio for association between Android and sentiment") +
  coord_flip()
```

Vemos que el disgusto, la ira, los sentimientos negativos, la tristeza y el miedo están asociados con el Android de una manera que es difícil de explicar solo por casualidad. Las palabras no asociadas con un sentimiento estaban fuertemente asociadas con el iPhone, que está de acuerdo con la afirmación original sobre los tuits hiperbólicos.

Si estamos interesados en explorar qué palabras específicas están impulsando estas diferencias, podemos referirnos a nuestro objeto `android_iphone_or`:

```{r}
android_iphone_or %>% inner_join(nrc) %>%
  filter(sentiment == "disgust" & Android + iPhone > 10) %>%
  arrange(desc(or))
```

y hacer un gráfico:

```{r log-odds-by-word, out.width="100%"}
android_iphone_or %>% inner_join(nrc, by = "word") %>%
  mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
  mutate(log_or = log(or)) %>%
  filter(Android + iPhone > 10 & abs(log_or)>1) %>%
  mutate(word = reorder(word, log_or)) %>%
  ggplot(aes(word, log_or, fill = log_or < 0)) +
  facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
  geom_bar(stat="identity", show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


Este es solo un ejemplo sencillo de los muchos análisis que uno puede realizar con __tidytext__.
Para obtener más información, nuevamente recomendamos el libro _Tidy Text Mining_^[https://www.tidytextmining.com/].


## Ejercicios


_Project Gutenberg_ es un archivo digital de libros de dominio público. El paquete __gutenbergr__ de R facilita la importación de estos textos en R. Puede instalar y cargarlo escribiendo:

```{r, eval=FALSE}
install.packages("gutenbergr")
library(gutenbergr)
```

Los libros disponibles se pueden ver así:

```{r, eval=FALSE}
gutenberg_metadata
```

1\. Utilice `str_detect` para encontrar la identificación de la novela _Pride and Prejudice_.


2\. Observe que hay varias versiones. La función `gutenberg_works()`filtra esta tabla para eliminar réplicas e incluye solo trabajos en inglés. Lea el archivo de ayuda y use esta función para encontrar la identificación de _Pride and Prejudice_.


3\. Utilice la función `gutenberg_download` para descargar el texto de _Pride and Prejudice_. Guárdelo en un objeto llamado `book`.


4\. Use el paquete __tidytext__ para crear una tabla ordenada con todas las palabras en el texto. Guarde la tabla en un objeto llamado `words`.


5\. Más adelante haremos un gráfico de sentimiento versus ubicación en el libro. Para esto, será útil agregar una columna a la tabla con el número de palabra. 


6\. Elimine las palabras _stop_ y los números del objeto `words`. Sugerencia: use `anti_join`.


7\. Ahora use el léxico `AFINN` para asignar un valor de sentimiento a cada palabra.


8\. Haga un gráfico de puntuación de sentimiento versus ubicación en el libro y agregue un suavizador.


9\. Suponga que hay 300 palabras por página. Convierta las ubicaciones en páginas y luego calcule el sentimiento promedio en cada página. Grafique ese puntaje promedio por página. Agregue un suavizador que pase por los datos.


