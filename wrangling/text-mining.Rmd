# Extracción de textos

Con la excepción de las etiquetas utilizadas para representar datos categóricos, nos hemos centrado en los datos numéricos. Pero en muchas aplicaciones, los datos comienzan como texto. Ejemplos bien conocidos son el filtrado de spam, la prevención del delito cibernético, la lucha contra el terrorismo y el análisis de sentimientos. En todos estos casos, los datos sin procesar se componen de texto de forma libre. Nuestra tarea es extraer información de estos datos. En esta sección, aprendemos cómo generar resúmenes numéricos útiles a partir de datos de texto a los que podemos aplicar algunas de las poderosas técnicas de visualización y análisis de datos que hemos aprendido.


## Estudio de caso: tuits de Trump

Durante las elecciones presidenciales estadounidenses de 2016, el candidato Donald J. Trump usó su cuenta de Twitter como una forma de comunicarse con los posibles votantes. El 6 de agosto de 2016, Todd Vaziri tuiteó^[https://twitter.com/tvaziri/status/762005541388378112/photo/1] sobre Trump diciendo que "Cada tweet no hiperbólico es de iPhone (su personal). Cada tweet hiperbólico es de Android (de él) ".
El científico de datos David Robinson realizó un análisis^[http://varianceexplained.org/r/trump-tweets/] para determinar si los datos respaldan esta afirmación. Aquí, revisamos el análisis de David para aprender algunos de los conceptos básicos de la minería de textos. Para obtener más información sobre la minería de texto en R, recomendamos el libro Minería de texto con R^[https://www.tidytextmining.com/] de Julia Silge y David Robinson.

```{r,echo=FALSE}
set.seed(2002)
```

Utilizaremos las siguientes bibliotecas:
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(scales)
```

En general, podemos extraer datos directamente de Twitter usando el paquete __rtweet__. Sin embargo, en este caso, un grupo ya ha compilado datos para nosotros y los ha puesto a disposición en [http://www.trumptwitterarchive.com] (http://www.trumptwitterarchive.com). Podemos obtener los datos de su API JSON usando un script como este:

```{r, eval=FALSE}
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
trump_tweets <- map(2009:2017, ~sprintf(url, .x)) %>%
map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
filter(!is_retweet & !str_detect(text, '^"')) %>%
mutate(created_at = parse_date_time(created_at,
orders = "a b! d! H!:M!:S! z!* Y!",
tz="EST"))
```

Para mayor comodidad, incluimos el resultado del código anterior en el paquete __dslabs__:

```{r}
library(dslabs)
data("trump_tweets")
```

Puede ver el marco de datos con información sobre los tweets escribiendo

```{r, eval=FALSE}
head(trump_tweets)
```

con las siguientes variables incluidas:

```{r}
names(trump_tweets)
```

El archivo de ayuda `?trump_tweets` proporciona detalles sobre lo que representa cada variable. Los tweets están representados por el `text` variable:

```{r}
trump_tweets$text[16413] %>% str_wrap(width = options()$width) %>% cat
```

y la variable fuente nos dice qué dispositivo se usó para componer y cargar cada tweet:

```{r}
trump_tweets %>% count(source) %>% arrange(desc(n)) %>% head(5)
```

Estamos interesados en lo que sucedió durante la campaña, por lo que para este análisis nos centraremos en lo que se tuiteó entre el día en que Trump anunció su campaña y el día de las elecciones. Definimos la siguiente tabla que contiene solo los tweets de ese período de tiempo. Tenga en cuenta que usamos `extract` para eliminar el `Twitter for` parte de la fuente y filtrar retweets.

```{r}
campaign_tweets <- trump_tweets %>%
extract(source, "source", "Twitter for (.*)") %>%
filter(source %in% c("Android", "iPhone") &
created_at >= ymd("2015-06-17") &
created_at < ymd("2016-11-08")) %>%
filter(!is_retweet) %>%
arrange(created_at)
```

Ahora podemos usar la visualización de datos para explorar la posibilidad de que dos grupos diferentes tuiteen desde estos dispositivos. Para cada tweet, extraeremos la hora, hora de la costa este (EST), se tuiteó y luego calcularemos la proporción de tweets tuiteados a cada hora para cada dispositivo:

```{r tweets-by-time-by-device}
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n/ sum(n)) %>%
ungroup %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
```


Notamos un gran pico para Android en las primeras horas de la mañana, entre las 6 y las 8 de la mañana. Parece haber una clara diferencia en estos patrones. Por lo tanto, asumiremos que dos entidades diferentes están utilizando estos dos dispositivos.

Ahora estudiaremos cómo difieren los tweets cuando comparamos Android con iPhone. Para hacer esto, presentamos el paquete __tidytext__.


## Texto como datos

El paquete __tidytext__ nos ayuda a convertir texto de forma libre en una tabla ordenada. Tener los datos en este formato facilita enormemente la visualización de datos y el uso de técnicas estadísticas.

```{r}
library(tidytext)
```

La función principal necesaria para lograr esto es `unnest_tokens`. Un token se refiere a una unidad que consideramos como un punto de datos. Los _token_ más comunes serán las palabras, pero también pueden ser caracteres individuales, ngrams, oraciones, líneas o un patrón definido por una expresión regular. Las funciones tomarán un vector de cadenas y extraerán los tokens para que cada uno obtenga una fila en la nueva tabla. Aquí hay un ejemplo simple:

```{r}
poem <- c("Roses are red,", "Violets are blue,",
"Sugar is sweet,", "And so are you.")
example <- tibble(line = c(1, 2, 3, 4),
text = poem)
example
example %>% unnest_tokens(word, text)
```

Ahora veamos un ejemplo de los tweets. Veremos el tweet número 3008 porque luego nos permitirá ilustrar un par de puntos:

```{r}
i <- 3008
campaign_tweets$text[i] %>% str_wrap(width = 65) %>% cat()
campaign_tweets[i,] %>%
unnest_tokens(word, text) %>%
pull(word)
```

Tenga en cuenta que la función intenta convertir tokens en palabras. Para hacer esto, sin embargo, elimina los caracteres que son importantes en el contexto de twitter. A saber, la función elimina todos los `#` y `@`. Un _token_ en el contexto de Twitter no es lo mismo que en el contexto del inglés hablado o escrito. Por esta razón, en lugar de usar las palabras predeterminadas, usamos el `tweets` el token incluye patrones que comienzan con @ y #:


```{r, message=FALSE, warning=FALSE}
campaign_tweets[i,] %>%
unnest_tokens(word, text, token = "tweets") %>%
pull(word)
```

Otro ajuste menor que queremos hacer es eliminar los enlaces a las imágenes:

```{r, message=FALSE, warning=FALSE}
links <- "https://t.co/[A-Za-z\\d]+|&amp;"
campaign_tweets[i,] %>%
mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(word, text, token = "tweets") %>%
pull(word)
```

Ahora estamos listos para extraer las palabras de todos nuestros tweets.

```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(word, text, token = "tweets")
```


Y ahora podemos responder preguntas como "¿cuáles son las palabras más utilizadas?":

```{r}
tweet_words %>%
count(word) %>%
arrange(desc(n))
```

No es sorprendente que estas sean las palabras principales. Las palabras principales no son informativas. El paquete _tidytext_ tiene una base de datos de estas palabras de uso común, denominadas _paras de palabras_, en la minería de texto:

```{r}
stop_words
```

Si filtramos las filas que representan palabras de detención con `filter(!word %in% stop_words$word)`:

```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word )
```

terminamos con un conjunto mucho más informativo de las 10 palabras más tuiteadas:

```{r}
tweet_words %>%
count(word) %>%
top_n(10, n) %>%
mutate(word = reorder(word, n)) %>%
arrange(desc(n))
```

Una exploración de las palabras resultantes (que no se muestran aquí) revela un par de características no deseadas en nuestros tokens. Primero, algunos de nuestros tokens son solo números (años, por ejemplo). Queremos eliminarlos y podemos encontrarlos usando la expresión regular `^\d+$`. Segundo, algunos de nuestros tokens provienen de una cita y comienzan con `'`. Queremos eliminar el `'` cuando está al comienzo de una palabra, así que simplemente `str_replace`. Agregamos estas dos líneas al código anterior para generar nuestra tabla final:


```{r, message=FALSE, warning=FALSE}
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, links, "")) %>%
unnest_tokens(word, text, token = "tweets") %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
```

Ahora que tenemos todas nuestras palabras en una tabla, junto con información sobre qué dispositivo se usó para componer el tweet del que provienen, podemos comenzar a explorar qué palabras son más comunes al comparar Android con iPhone.

Para cada palabra, queremos saber si es más probable que provenga de un tweet de Android o un tweet de iPhone. En la sección \@ref(association-tests) introdujimos el odds ratio como un resumen estadístico útil para cuantificar estas diferencias. Para cada dispositivo y una palabra dada, llamémosla `y`, calculamos las probabilidades o la razón entre la proporción de palabras que son `y` y no `y` y calcule la razón de esas probabilidades. Aquí tendremos muchas proporciones que son 0, así que usamos la corrección 0.5 descrita en la Sección NA.

```{r}
android_iphone_or <- tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0) %>%
mutate(or = (Android + 0.5)/ (sum(Android) - Android + 0.5)/
( (iPhone + 0.5)/ (sum(iPhone) - iPhone + 0.5)))
```

Aquí están las razones de probabilidades más altas para Android

```{r}
android_iphone_or %>% arrange(desc(or))
```

y la parte superior para iPhone:
```{r}
android_iphone_or %>% arrange(or)
```

Dado que varias de estas palabras son palabras generales de baja frecuencia, podemos imponer un filtro basado en la frecuencia total de esta manera:

```{r}
android_iphone_or %>% filter(Android+iPhone > 100) %>%
arrange(desc(or))

android_iphone_or %>% filter(Android+iPhone > 100) %>%
arrange(or)
```

Ya vemos un patrón en los tipos de palabras que se twittean más desde un dispositivo que desde el otro. Sin embargo, no estamos interesados en palabras específicas sino en el tono. La afirmación de Vaziri es que los tweets de Android son más hiperbólicos. Entonces, ¿cómo podemos verificar esto con datos? _Hyperbolic_ es un sentimiento difícil de extraer de las palabras, ya que se basa en la interpretación de frases. Sin embargo, las palabras pueden asociarse a sentimientos más básicos como la ira, el miedo, la alegría y la sorpresa. En la siguiente sección, demostramos el análisis básico de sentimientos.

## Análisis de los sentimientos

En el análisis de sentimientos, asignamos una palabra a uno o más "sentimientos". Aunque este enfoque perderá sentimientos dependientes del contexto, como el sarcasmo, cuando se realiza en grandes cantidades de palabras, los resúmenes pueden proporcionar información.

El primer paso en el análisis de sentimientos es asignar un sentimiento a cada palabra. Como demostramos, el paquete __tidytext__ incluye varios mapas o léxicos. También usaremos el paquete __textdata__.

```{r, message=FALSE, warning=FALSE}
library(tidytext)
library(textdata)
```

Los `bing` léxico divide las palabras en `positive` y `negative` sentimientos Podemos ver esto usando la función _tidytext_ `get_sentiments`:

```{r, eval=FALSE}
get_sentiments("bing")
```

Los `AFINN` léxico asigna un puntaje entre -5 y 5, con -5 el más negativo y 5 el más positivo. Tenga en cuenta que este léxico debe descargarse la primera vez que llame a la función `get_sentiment`:

```{r, eval=FALSE}
get_sentiments("afinn")
```

Los `loughran` y `nrc` los léxicos proporcionan varios sentimientos diferentes. Tenga en cuenta que estos también deben descargarse la primera vez que los use.

```{r}
get_sentiments("loughran") %>% count(sentiment)
```

```{r}
get_sentiments("nrc") %>% count(sentiment)
```

Para nuestro análisis, estamos interesados en explorar los diferentes sentimientos de cada tweet, por lo que utilizaremos el `nrc` léxico:

```{r}
nrc <- get_sentiments("nrc") %>%
select(word, sentiment)
```

Podemos combinar las palabras y sentimientos usando `inner_join`, que solo mantendrá palabras asociadas con un sentimiento. Aquí hay 10 palabras aleatorias extraídas de los tweets:


```{r}
tweet_words %>% inner_join(nrc, by = "word") %>%
select(source, word, sentiment) %>%
sample_n(5)
```

Ahora estamos listos para realizar un análisis cuantitativo comparando Android y iPhone comparando los sentimientos de los tweets publicados desde cada dispositivo. Aquí podríamos realizar un análisis de tweet por tweet, asignando un sentimiento a cada tweet. Sin embargo, esto será un desafío ya que cada tweet tendrá varios sentimientos adjuntos, uno por cada palabra que aparezca en el léxico. Con fines ilustrativos, realizaremos un análisis mucho más simple: contaremos y compararemos las frecuencias de cada sentimiento que aparece en cada dispositivo.


```{r}
sentiment_counts <- tweet_words %>%
left_join(nrc, by = "word") %>%
count(source, sentiment) %>%
spread(source, n) %>%
mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
```

Para cada sentimiento, podemos calcular las probabilidades de estar en el dispositivo: proporción de palabras con sentimiento versus proporción de palabras sin, y luego calcular la razón de probabilidades comparando los dos dispositivos.

```{r}
sentiment_counts %>%
mutate(Android = Android/ (sum(Android) - Android) ,
iPhone = iPhone/ (sum(iPhone) - iPhone),
or = Android/iPhone) %>%
arrange(desc(or))
```

Así que vemos algunas diferencias y el orden es interesante: ¡los tres sentimientos más grandes son el asco, la ira y lo negativo! ¿Pero son estas diferencias solo por casualidad? ¿Cómo se compara esto si solo estamos asignando sentimientos al azar? Para responder a esta pregunta, podemos calcular, para cada sentimiento, un cociente de probabilidades y un intervalo de confianza, como se define en la Sección \@ref(association-tests). Agregaremos los dos valores que necesitamos para formar una tabla de dos por dos y la razón de posibilidades:

```{r}
library(broom)
log_or <- sentiment_counts %>%
mutate(log_or = log((Android/ (sum(Android) - Android))/
(iPhone/ (sum(iPhone) - iPhone))),
se = sqrt(1/Android + 1/(sum(Android) - Android) +
1/iPhone + 1/(sum(iPhone) - iPhone)),
conf.low = log_or - qnorm(0.975)*se,
conf.high = log_or + qnorm(0.975)*se) %>%
arrange(desc(log_or))

log_or
```

Una visualización gráfica muestra algunos sentimientos que están claramente sobrerrepresentados:

```{r tweets-log-odds-ratio}
log_or %>%
mutate(sentiment = reorder(sentiment, log_or)) %>%
ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
geom_errorbar() +
geom_point(aes(sentiment, log_or)) +
ylab("Log odds ratio for association between Android and sentiment") +
coord_flip()
```

Vemos que el disgusto, la ira, los sentimientos negativos, la tristeza y el miedo están asociados con el Android de una manera que es difícil de explicar solo por casualidad. Las palabras no asociadas a un sentimiento estaban fuertemente asociadas con la fuente del iPhone, que está de acuerdo con la afirmación original sobre los tweets hiperbólicos.

Si estamos interesados en explorar qué palabras específicas están impulsando estas diferencias, podemos referirnos a nuestro `android_iphone_or` objeto:

```{r}
android_iphone_or %>% inner_join(nrc) %>%
filter(sentiment == "disgust" & Android + iPhone > 10) %>%
arrange(desc(or))
```

y podemos hacer un gráfico:

```{r log-odds-by-word, out.width="100%"}
android_iphone_or %>% inner_join(nrc, by = "word") %>%
mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
mutate(log_or = log(or)) %>%
filter(Android + iPhone > 10 & abs(log_or)>1) %>%
mutate(word = reorder(word, log_or)) %>%
ggplot(aes(word, log_or, fill = log_or < 0)) +
facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
geom_bar(stat="identity", show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


Este es solo un ejemplo simple de los muchos análisis que uno puede realizar con tidytext.
Para obtener más información, nuevamente recomendamos el libro Tidy Text Mining^[https://www.tidytextmining.com/].


## Ejercicios


Project Gutenberg es un archivo digital de libros de dominio público. El paquete R __gutenbergr__ facilita la importación de estos textos en R.

Puede instalar y cargar escribiendo:

```{r, eval=FALSE}
install.packages("gutenbergr")
library(gutenbergr)
```

Puedes ver los libros que están disponibles así:

```{r, eval=FALSE}
gutenberg_metadata
```

1\. Utilizar `str_detect` para encontrar la identificación de la novela Orgullo y prejuicio.


2\. Notamos que hay varias versiones. los `gutenberg_works()` la función filtra esta tabla para eliminar réplicas e incluye solo trabajos en inglés. Lea el archivo de ayuda y use esta función para encontrar la ID de _Pride and Prejudice_.



3\. Utilizar el `gutenberg_download` función para descargar el texto de Orgullo y prejuicio. Guárdelo en un objeto llamado `book`.


4\. Use el paquete __tidytext__ para crear una tabla ordenada con todas las palabras en el texto. Guarde la tabla en un objeto llamado `words`


5\. Más adelante haremos una trama de sentimiento versus ubicación en el libro. Para esto, será útil agregar una columna con el número de palabra a la tabla.


6\. Elimine las palabras de parada y los números del `words` objeto. Sugerencia: use el `anti_join`.


7\. Ahora usa el `AFINN` léxico para asignar un valor de sentimiento a cada palabra.


8\. Haga un diagrama de puntuación de sentimiento versus ubicación en el libro y agregue un suavizador.


9\. Suponga que hay 300 palabras por página. Convierta las ubicaciones en páginas y luego calcule el sentimiento promedio en cada página. Trace ese puntaje promedio por página. Agregue un suavizador que parece pasar por los datos.


