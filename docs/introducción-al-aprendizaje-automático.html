<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 27 Introducción al aprendizaje automático | Introducción a la Ciencia de Datos</title>
  <meta name="description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 27 Introducción al aprendizaje automático | Introducción a la Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 27 Introducción al aprendizaje automático | Introducción a la Ciencia de Datos" />
  
  <meta name="twitter:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2021-01-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="minería-de-textos.html"/>
<link rel="next" href="suavizado.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introducción a la Ciencia de Datos</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="" data-path="agradecimientos.html"><a href="agradecimientos.html"><i class="fa fa-check"></i>Agradecimientos</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i>Introducción</a><ul>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#los-casos-de-estudio"><i class="fa fa-check"></i>Los casos de estudio</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#quién-encontrará-útil-este-libro"><i class="fa fa-check"></i>¿Quién encontrará útil este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#que-cubre-este-libro"><i class="fa fa-check"></i>¿Que cubre este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#qué-no-cubre-este-libro"><i class="fa fa-check"></i>¿Qué no cubre este libro?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Comenzando con R y RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#por-qué-r"><i class="fa fa-check"></i><b>1.1</b> ¿Por qué R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#la-consola-r"><i class="fa fa-check"></i><b>1.2</b> La consola R</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> <em>Scripts</em></a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#paneles"><i class="fa fa-check"></i><b>1.4.1</b> Paneles</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> <em>Key bindings</em></a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#cómo-ejecutar-comandos-mientras-edita-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Cómo ejecutar comandos mientras edita <em>scripts</em></a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#cómo-cambiar-las-opciones-globales"><i class="fa fa-check"></i><b>1.4.4</b> Cómo cambiar las opciones globales</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#instalación-de-paquetes-de-r"><i class="fa fa-check"></i><b>1.5</b> Instalación de paquetes de R</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> Lo básico de R</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#caso-de-estudio-los-asesinatos-con-armas-en-ee.-uu."><i class="fa fa-check"></i><b>2.1</b> Caso de estudio: los asesinatos con armas en EE. UU.</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#lo-básico"><i class="fa fa-check"></i><b>2.2</b> Lo básico</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objetos"><i class="fa fa-check"></i><b>2.2.1</b> Objetos</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#el-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.2</b> El espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#funciones"><i class="fa fa-check"></i><b>2.2.3</b> Funciones</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#otros-objetos-predefinidos"><i class="fa fa-check"></i><b>2.2.4</b> Otros objetos predefinidos</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#nombres-de-variables"><i class="fa fa-check"></i><b>2.2.5</b> Nombres de variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#cómo-guardar-su-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.6</b> Cómo guardar su espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#scripts-motivantes"><i class="fa fa-check"></i><b>2.2.7</b> <em>Scripts</em> motivantes</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#cómo-comentar-su-código"><i class="fa fa-check"></i><b>2.2.8</b> Cómo comentar su código</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#ejercicios"><i class="fa fa-check"></i><b>2.3</b> Ejercicios</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#tipos-de-datos"><i class="fa fa-check"></i><b>2.4</b> Tipos de datos</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> <em>data frames</em></a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#cómo-examinar-un-objeto"><i class="fa fa-check"></i><b>2.4.2</b> Cómo examinar un objeto</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#el-operador-de-acceso"><i class="fa fa-check"></i><b>2.4.3</b> El operador de acceso: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectores-numéricos-de-caracteres-y-lógicos"><i class="fa fa-check"></i><b>2.4.4</b> Vectores: numéricos, de caracteres y lógicos</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factores</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#listas"><i class="fa fa-check"></i><b>2.4.6</b> Listas</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#ejercicios-1"><i class="fa fa-check"></i><b>2.5</b> Ejercicios</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectores</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-vectores"><i class="fa fa-check"></i><b>2.6.1</b> Cómo crear vectores</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#nombres"><i class="fa fa-check"></i><b>2.6.2</b> Nombres</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#secuencias"><i class="fa fa-check"></i><b>2.6.3</b> Secuencias</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-un-subconjunto"><i class="fa fa-check"></i><b>2.6.4</b> Cómo crear un subconjunto</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#la-conversión-forzada"><i class="fa fa-check"></i><b>2.7</b> La conversión forzada</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-available-na"><i class="fa fa-check"></i><b>2.7.1</b> <em>Not available</em> (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#ejercicios-2"><i class="fa fa-check"></i><b>2.8</b> Ejercicios</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> <em>Sorting</em></a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-y-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> y <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#cuidado-con-el-reciclaje"><i class="fa fa-check"></i><b>2.9.5</b> Cuidado con el reciclaje</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#ejercicios-3"><i class="fa fa-check"></i><b>2.10</b> Ejercicios</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#aritmética-de-vectores"><i class="fa fa-check"></i><b>2.11</b> Aritmética de vectores</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-un-vector"><i class="fa fa-check"></i><b>2.11.1</b> <em>Rescaling</em> un vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#dos-vectores"><i class="fa fa-check"></i><b>2.11.2</b> Dos vectores</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#ejercicios-4"><i class="fa fa-check"></i><b>2.12</b> Ejercicios</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexación"><i class="fa fa-check"></i><b>2.13</b> Indexación</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#crear-subconjuntos-con-lógicos"><i class="fa fa-check"></i><b>2.13.1</b> Crear subconjuntos con lógicos</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#operadores-lógicos"><i class="fa fa-check"></i><b>2.13.2</b> Operadores lógicos</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#ejercicios-5"><i class="fa fa-check"></i><b>2.14</b> Ejercicios</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#gráficos-básicos"><i class="fa fa-check"></i><b>2.15</b> Gráficos básicos</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#ejercicios-6"><i class="fa fa-check"></i><b>2.16</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html"><i class="fa fa-check"></i><b>3</b> Conceptos básicos de programación</a><ul>
<li class="chapter" data-level="3.1" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Expresiones condicionales</a></li>
<li class="chapter" data-level="3.2" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#cómo-definir-funciones"><i class="fa fa-check"></i><b>3.2</b> Cómo definir funciones</a></li>
<li class="chapter" data-level="3.3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> <em>Namespaces</em></a></li>
<li class="chapter" data-level="3.4" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#bucles-for"><i class="fa fa-check"></i><b>3.4</b> Bucles-for</a></li>
<li class="chapter" data-level="3.5" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorización y funcionales</a></li>
<li class="chapter" data-level="3.6" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#ejercicios-7"><i class="fa fa-check"></i><b>3.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Data <em>tidy</em></a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-8"><i class="fa fa-check"></i><b>4.2</b> Ejercicios</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-manipular-los-data-frames"><i class="fa fa-check"></i><b>4.3</b> Cómo manipular los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-añadir-una-columna-con-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Cómo añadir una columna con <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-subconjuntos-con-filter"><i class="fa fa-check"></i><b>4.3.2</b> Cómo crear subconjuntos con <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-seleccionar-columnas-con-select"><i class="fa fa-check"></i><b>4.3.3</b> Cómo seleccionar columnas con <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-9"><i class="fa fa-check"></i><b>4.4</b> Ejercicios</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#el-pipe"><i class="fa fa-check"></i><b>4.5</b> El <em>pipe</em>: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-10"><i class="fa fa-check"></i><b>4.6</b> Ejercicios</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#cómo-resumir-datos"><i class="fa fa-check"></i><b>4.7</b> Cómo resumir datos</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Cómo agrupar y luego resumir con <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-los-data-frames"><i class="fa fa-check"></i><b>4.8</b> Cómo ordenar los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-anidadamente"><i class="fa fa-check"></i><b>4.8.1</b> Cómo ordenar anidadamente</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#los-primeros-n"><i class="fa fa-check"></i><b>4.8.2</b> Los primeros <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-11"><i class="fa fa-check"></i><b>4.9</b> Ejercicios</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> <em>Tibbles</em></a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-ven-mejor"><i class="fa fa-check"></i><b>4.10.1</b> Los <em>tibbles</em> se ven mejor</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#los-subconjuntos-de-tibbles-son-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Los subconjuntos de <em>tibbles</em> son <em>tibbles</em></a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-pueden-tener-entradas-complejas"><i class="fa fa-check"></i><b>4.10.3</b> Los <em>tibbles</em> pueden tener entradas complejas</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-pueden-agrupar"><i class="fa fa-check"></i><b>4.10.4</b> Los <em>tibbles</em> se pueden agrupar</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-un-tibble-usando-tibble-en-lugar-de-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Cómo crear un <em>tibble</em> usando <code>tibble</code> en lugar de <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#el-operador-punto"><i class="fa fa-check"></i><b>4.11</b> El operador punto</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#el-paquete-purrr"><i class="fa fa-check"></i><b>4.13</b> El paquete <strong>purrr</strong></a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#los-condicionales-de-tidyverse"><i class="fa fa-check"></i><b>4.14</b> Los condicionales de <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-12"><i class="fa fa-check"></i><b>4.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importando datos</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#las-rutas-y-el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1</b> Las rutas y el directorio de trabajo</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#el-sistema-de-archivos"><i class="fa fa-check"></i><b>5.1.1</b> El sistema de archivos</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#las-rutas-relativas-y-completas"><i class="fa fa-check"></i><b>5.1.2</b> Las rutas relativas y completas</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1.3</b> El directorio de trabajo</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#cómo-generar-los-nombres-de-ruta"><i class="fa fa-check"></i><b>5.1.4</b> Cómo generar los nombres de ruta</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#cómo-copiar-los-archivos-usando-rutas"><i class="fa fa-check"></i><b>5.1.5</b> Cómo copiar los archivos usando rutas</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#los-paquetes-readr-y-readxl"><i class="fa fa-check"></i><b>5.2</b> Los paquetes readr y readxl</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#ejercicios-13"><i class="fa fa-check"></i><b>5.3</b> Ejercicios</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#cómo-descargar-archivos"><i class="fa fa-check"></i><b>5.4</b> Cómo descargar archivos</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#las-funciones-de-importación-de-base-r"><i class="fa fa-check"></i><b>5.5</b> Las funciones de importación de base R</a><ul>
<li class="chapter" data-level="5.5.1" data-path="importing-data.html"><a href="importing-data.html#scan"><i class="fa fa-check"></i><b>5.5.1</b> <code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#archivos-de-texto-versus-archivos-binarios"><i class="fa fa-check"></i><b>5.6</b> Archivos de texto versus archivos binarios</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#cómo-organizar-datos-con-hojas-de-cálculo"><i class="fa fa-check"></i><b>5.8</b> Cómo organizar datos con hojas de cálculo</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#ejercicios-14"><i class="fa fa-check"></i><b>5.9</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>II Visualización de datos</b></span></li>
<li class="chapter" data-level="6" data-path="introducción-a-la-visualización-de-datos.html"><a href="introducción-a-la-visualización-de-datos.html"><i class="fa fa-check"></i><b>6</b> Introducción a la visualización de datos</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#los-componentes-de-un-gráfico"><i class="fa fa-check"></i><b>7.1</b> Los componentes de un gráfico</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#objetos-ggplot"><i class="fa fa-check"></i><b>7.2</b> objetos <code>ggplot</code></a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometrías"><i class="fa fa-check"></i><b>7.3</b> Geometrías</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos"><i class="fa fa-check"></i><b>7.4</b> Mapeos estéticos</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#capas"><i class="fa fa-check"></i><b>7.5</b> Capas</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#cómo-probar-varios-argumentos"><i class="fa fa-check"></i><b>7.5.1</b> Cómo probar varios argumentos</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos-globales-versus-locales"><i class="fa fa-check"></i><b>7.6</b> Mapeos estéticos globales versus locales</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#escalas"><i class="fa fa-check"></i><b>7.7</b> Escalas</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#etiquetas-y-títulos"><i class="fa fa-check"></i><b>7.8</b> Etiquetas y títulos</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categorías-como-colores"><i class="fa fa-check"></i><b>7.9</b> Categorías como colores</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#anotación-formas-y-ajustes"><i class="fa fa-check"></i><b>7.10</b> Anotación, formas y ajustes</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Paquetes complementarios</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#cómo-combinarlo-todo"><i class="fa fa-check"></i><b>7.12</b> Cómo combinarlo todo</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Gráficos rápidos con <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#cuadrículas-de-gráficos"><i class="fa fa-check"></i><b>7.14</b> Cuadrículas de gráficos</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#ejercicios-15"><i class="fa fa-check"></i><b>7.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Cómo visualizar distribuciones de datos</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#tipos-de-variables"><i class="fa fa-check"></i><b>8.1</b> Tipos de variables</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#estudio-de-caso-describiendo-alturas-de-estudiantes"><i class="fa fa-check"></i><b>8.2</b> Estudio de caso: describiendo alturas de estudiantes</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#la-función-de-distribución"><i class="fa fa-check"></i><b>8.3</b> La función de distribución</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Funciones de distribución acumulada</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histogramas"><i class="fa fa-check"></i><b>8.5</b> Histogramas</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#densidad-suave"><i class="fa fa-check"></i><b>8.6</b> Densidad suave</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#cómo-interpretar-el-eje-y"><i class="fa fa-check"></i><b>8.6.1</b> Cómo interpretar el eje-y</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densidades-permiten-estratificación"><i class="fa fa-check"></i><b>8.6.2</b> Densidades permiten estratificación</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#ejercicios-16"><i class="fa fa-check"></i><b>8.7</b> Ejercicios</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> La distribución normal</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#unidades-estándar"><i class="fa fa-check"></i><b>8.9</b> Unidades estándar</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#gráficos-q-q"><i class="fa fa-check"></i><b>8.10</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja"><i class="fa fa-check"></i><b>8.12</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Estratificación</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Estudio de caso: descripción de alturas de estudiantes (continuación)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#ejercicios-17"><i class="fa fa-check"></i><b>8.15</b> Ejercicios</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> Geometrías ggplot2</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#diagramas-de-barras"><i class="fa fa-check"></i><b>8.16.1</b> Diagramas de barras</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histogramas-1"><i class="fa fa-check"></i><b>8.16.2</b> Histogramas</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#gráficos-de-densidad"><i class="fa fa-check"></i><b>8.16.3</b> Gráficos de densidad</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja-1"><i class="fa fa-check"></i><b>8.16.4</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#gráficos-q-q-1"><i class="fa fa-check"></i><b>8.16.5</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#imágenes"><i class="fa fa-check"></i><b>8.16.6</b> Imágenes</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#gráficos-rápidos"><i class="fa fa-check"></i><b>8.16.7</b> Gráficos rápidos</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#ejercicios-18"><i class="fa fa-check"></i><b>8.17</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Visualización de datos en la práctica</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#estudio-de-caso-nuevas-ideas-sobre-la-pobreza"><i class="fa fa-check"></i><b>9.1</b> Estudio de caso: nuevas ideas sobre la pobreza</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#la-prueba-de-hans-rosling"><i class="fa fa-check"></i><b>9.1.1</b> La prueba de Hans Rosling</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-dispersión"><i class="fa fa-check"></i><b>9.2</b> Diagrama de dispersión</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#separar-en-facetas"><i class="fa fa-check"></i><b>9.3</b> Separar en facetas</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#escalas-fijas-para-mejores-comparaciones"><i class="fa fa-check"></i><b>9.3.2</b> Escalas fijas para mejores comparaciones</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#gráficos-de-series-de-tiempo"><i class="fa fa-check"></i><b>9.4</b> Gráficos de series de tiempo</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#etiquetas-en-lugar-de-leyendas"><i class="fa fa-check"></i><b>9.4.1</b> Etiquetas en lugar de leyendas</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#transformaciones-de-datos"><i class="fa fa-check"></i><b>9.5</b> Transformaciones de datos</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#transformación-logarítmica"><i class="fa fa-check"></i><b>9.5.1</b> Transformación logarítmica</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#qué-base"><i class="fa fa-check"></i><b>9.5.2</b> ¿Qué base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transformar-los-valores-o-la-escala"><i class="fa fa-check"></i><b>9.5.3</b> ¿Transformar los valores o la escala?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#cómo-visualizar-distribuciones-multimodales"><i class="fa fa-check"></i><b>9.6</b> Cómo visualizar distribuciones multimodales</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#cómo-comparar-múltiples-distribuciones-con-diagramas-de-caja-y-gráficos-ridge"><i class="fa fa-check"></i><b>9.7</b> Cómo comparar múltiples distribuciones con diagramas de caja y gráficos <em>ridge</em></a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-caja"><i class="fa fa-check"></i><b>9.7.1</b> Diagrama de caja</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#gráficos-ridge"><i class="fa fa-check"></i><b>9.7.2</b> Gráficos <em>ridge</em></a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#ejemplo-distribuciones-de-ingresos-de-1970-versus-2010"><i class="fa fa-check"></i><b>9.7.3</b> Ejemplo: distribuciones de ingresos de 1970 versus 2010</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#cómo-obtener-acceso-a-variables-calculadas"><i class="fa fa-check"></i><b>9.7.4</b> Cómo obtener acceso a variables calculadas</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#densidades-ponderadas"><i class="fa fa-check"></i><b>9.7.5</b> Densidades ponderadas</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#la-falacia-ecológica-y-la-importancia-de-mostrar-los-datos"><i class="fa fa-check"></i><b>9.8</b> La falacia ecológica y la importancia de mostrar los datos</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Transformación logística</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#mostrar-los-datos"><i class="fa fa-check"></i><b>9.8.2</b> Mostrar los datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html"><i class="fa fa-check"></i><b>10</b> Principios de visualización de datos</a><ul>
<li class="chapter" data-level="10.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-datos-utilizando-señales-visuales"><i class="fa fa-check"></i><b>10.1</b> Cómo codificar datos utilizando señales visuales</a></li>
<li class="chapter" data-level="10.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#sepa-cuándo-incluir-0"><i class="fa fa-check"></i><b>10.2</b> Sepa cuándo incluir 0</a></li>
<li class="chapter" data-level="10.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#no-distorsionar-cantidades"><i class="fa fa-check"></i><b>10.3</b> No distorsionar cantidades</a></li>
<li class="chapter" data-level="10.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ordenar-categorías-por-un-valor-significativo"><i class="fa fa-check"></i><b>10.4</b> Ordenar categorías por un valor significativo</a></li>
<li class="chapter" data-level="10.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#mostrar-los-datos-1"><i class="fa fa-check"></i><b>10.5</b> Mostrar los datos</a></li>
<li class="chapter" data-level="10.6" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-facilitar-comparaciones"><i class="fa fa-check"></i><b>10.6</b> Cómo facilitar comparaciones</a><ul>
<li class="chapter" data-level="10.6.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-ejes-comunes"><i class="fa fa-check"></i><b>10.6.1</b> Use ejes comunes</a></li>
<li class="chapter" data-level="10.6.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#alinee-gráficos-verticalmente-para-ver-cambios-horizontales-y-horizontalmente-para-ver-cambios-verticales"><i class="fa fa-check"></i><b>10.6.2</b> Alinee gráficos verticalmente para ver cambios horizontales y horizontalmente para ver cambios verticales</a></li>
<li class="chapter" data-level="10.6.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#considere-transformaciones"><i class="fa fa-check"></i><b>10.6.3</b> Considere transformaciones</a></li>
<li class="chapter" data-level="10.6.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#señales-visuales-comparadas-deben-estar-adyacentes"><i class="fa fa-check"></i><b>10.6.4</b> Señales visuales comparadas deben estar adyacentes</a></li>
<li class="chapter" data-level="10.6.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#piense-en-los-daltónicos"><i class="fa fa-check"></i><b>10.7</b> Piense en los daltónicos</a></li>
<li class="chapter" data-level="10.8" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráficos-para-dos-variables"><i class="fa fa-check"></i><b>10.8</b> Gráficos para dos variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráfico-bland-altman"><i class="fa fa-check"></i><b>10.8.2</b> Gráfico Bland-Altman</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-una-tercera-variable"><i class="fa fa-check"></i><b>10.9</b> Cómo codificar una tercera variable</a></li>
<li class="chapter" data-level="10.10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-los-gráficos-pseudo-tridimensionales"><i class="fa fa-check"></i><b>10.10</b> Evite los gráficos pseudo-tridimensionales</a></li>
<li class="chapter" data-level="10.11" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-demasiados-dígitos-significativos"><i class="fa fa-check"></i><b>10.11</b> Evite demasiados dígitos significativos</a></li>
<li class="chapter" data-level="10.12" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#conozca-a-su-audiencia"><i class="fa fa-check"></i><b>10.12</b> Conozca a su audiencia</a></li>
<li class="chapter" data-level="10.13" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-19"><i class="fa fa-check"></i><b>10.13</b> Ejercicios</a></li>
<li class="chapter" data-level="10.14" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Estudio de caso: las vacunas y las enfermedades infecciosas</a></li>
<li class="chapter" data-level="10.15" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-20"><i class="fa fa-check"></i><b>10.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Resúmenes robustos</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#valores-atípicos"><i class="fa fa-check"></i><b>11.1</b> Valores atípicos</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#mediana"><i class="fa fa-check"></i><b>11.2</b> Mediana</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#el-rango-intercuartil-iqr"><i class="fa fa-check"></i><b>11.3</b> El rango intercuartil (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#la-definición-de-tukey-de-un-valor-atípico"><i class="fa fa-check"></i><b>11.4</b> La definición de Tukey de un valor atípico</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#desviación-absoluta-mediana"><i class="fa fa-check"></i><b>11.5</b> Desviación absoluta mediana</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#ejercicios-21"><i class="fa fa-check"></i><b>11.6</b> Ejercicios</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#estudio-de-caso-alturas-autoreportadas-de-estudiantes"><i class="fa fa-check"></i><b>11.7</b> Estudio de caso: alturas autoreportadas de estudiantes</a></li>
</ul></li>
<li class="part"><span><b>III Estadísticas con R</b></span></li>
<li class="chapter" data-level="12" data-path="introducción-a-las-estadísticas-con-r.html"><a href="introducción-a-las-estadísticas-con-r.html"><i class="fa fa-check"></i><b>12</b> Introducción a las estadísticas con R</a></li>
<li class="chapter" data-level="13" data-path="probabilidad.html"><a href="probabilidad.html"><i class="fa fa-check"></i><b>13</b> Probabilidad</a><ul>
<li class="chapter" data-level="13.1" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-discreta"><i class="fa fa-check"></i><b>13.1</b> Probabilidad discreta</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probabilidad.html"><a href="probabilidad.html#frecuencia-relativa"><i class="fa fa-check"></i><b>13.1.1</b> Frecuencia relativa</a></li>
<li class="chapter" data-level="13.1.2" data-path="probabilidad.html"><a href="probabilidad.html#notación"><i class="fa fa-check"></i><b>13.1.2</b> Notación</a></li>
<li class="chapter" data-level="13.1.3" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>13.1.3</b> Distribuciones de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-datos-categóricos"><i class="fa fa-check"></i><b>13.2</b> Simulaciones Monte Carlo para datos categóricos</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probabilidad.html"><a href="probabilidad.html#fijar-la-semilla-aleatoria"><i class="fa fa-check"></i><b>13.2.1</b> Fijar la semilla aleatoria</a></li>
<li class="chapter" data-level="13.2.2" data-path="probabilidad.html"><a href="probabilidad.html#con-y-sin-reemplazo"><i class="fa fa-check"></i><b>13.2.2</b> Con y sin reemplazo</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probabilidad.html"><a href="probabilidad.html#independencia"><i class="fa fa-check"></i><b>13.3</b> Independencia</a></li>
<li class="chapter" data-level="13.4" data-path="probabilidad.html"><a href="probabilidad.html#probabilidades-condicionales"><i class="fa fa-check"></i><b>13.4</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="13.5" data-path="probabilidad.html"><a href="probabilidad.html#reglas-de-la-adición-y-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5</b> Reglas de la adición y de la multiplicación</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5.1</b> Regla de la multiplicación</a></li>
<li class="chapter" data-level="13.5.2" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación-bajo-independencia"><i class="fa fa-check"></i><b>13.5.2</b> Regla de la multiplicación bajo independencia</a></li>
<li class="chapter" data-level="13.5.3" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-adición"><i class="fa fa-check"></i><b>13.5.3</b> Regla de la adición</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probabilidad.html"><a href="probabilidad.html#combinaciones-y-permutaciones"><i class="fa fa-check"></i><b>13.6</b> Combinaciones y permutaciones</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-monte-carlo"><i class="fa fa-check"></i><b>13.6.1</b> Ejemplo Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos"><i class="fa fa-check"></i><b>13.7</b> Ejemplos</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probabilidad.html"><a href="probabilidad.html#problema-monty-hall"><i class="fa fa-check"></i><b>13.7.1</b> Problema Monty Hall</a></li>
<li class="chapter" data-level="13.7.2" data-path="probabilidad.html"><a href="probabilidad.html#problema-de-cumpleaños"><i class="fa fa-check"></i><b>13.7.2</b> Problema de cumpleaños</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probabilidad.html"><a href="probabilidad.html#infinito-en-la-práctica"><i class="fa fa-check"></i><b>13.8</b> Infinito en la práctica</a></li>
<li class="chapter" data-level="13.9" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-22"><i class="fa fa-check"></i><b>13.9</b> Ejercicios</a></li>
<li class="chapter" data-level="13.10" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-continua"><i class="fa fa-check"></i><b>13.10</b> Probabilidad continua</a></li>
<li class="chapter" data-level="13.11" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-continuas"><i class="fa fa-check"></i><b>13.11</b> Distribuciones teóricas continuas</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-como-aproximaciones"><i class="fa fa-check"></i><b>13.11.1</b> Distribuciones teóricas como aproximaciones</a></li>
<li class="chapter" data-level="13.11.2" data-path="probabilidad.html"><a href="probabilidad.html#la-densidad-de-probabilidad"><i class="fa fa-check"></i><b>13.11.2</b> La densidad de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-variables-continuas"><i class="fa fa-check"></i><b>13.12</b> Simulaciones Monte Carlo para variables continuas</a></li>
<li class="chapter" data-level="13.13" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-continuas"><i class="fa fa-check"></i><b>13.13</b> Distribuciones continuas</a></li>
<li class="chapter" data-level="13.14" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-23"><i class="fa fa-check"></i><b>13.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html"><i class="fa fa-check"></i><b>14</b> Variables aleatorias</a><ul>
<li class="chapter" data-level="14.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#variables-aleatorias-1"><i class="fa fa-check"></i><b>14.1</b> Variables aleatorias</a></li>
<li class="chapter" data-level="14.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#modelos-de-muestreo"><i class="fa fa-check"></i><b>14.2</b> Modelos de muestreo</a></li>
<li class="chapter" data-level="14.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#la-distribución-de-probabilidad-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>14.3</b> La distribución de probabilidad de una variable aleatoria</a></li>
<li class="chapter" data-level="14.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#distribuciones-versus-distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>14.4</b> Distribuciones versus distribuciones de probabilidad</a></li>
<li class="chapter" data-level="14.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#notación-para-variables-aleatorias"><i class="fa fa-check"></i><b>14.5</b> Notación para variables aleatorias</a></li>
<li class="chapter" data-level="14.6" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#el-valor-esperado-y-el-error-estándar"><i class="fa fa-check"></i><b>14.6</b> El valor esperado y el error estándar</a><ul>
<li class="chapter" data-level="14.6.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#población-sd-versus-la-muestra-sd"><i class="fa fa-check"></i><b>14.6.1</b> Población SD versus la muestra SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7</b> Teorema del límite central</a><ul>
<li class="chapter" data-level="14.7.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#cuán-grande-es-grande-en-el-teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7.1</b> ¿Cuán grande es grande en el teorema del límite central?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#propiedades-estadísticas-de-promedios"><i class="fa fa-check"></i><b>14.8</b> Propiedades estadísticas de promedios</a></li>
<li class="chapter" data-level="14.9" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ley-de-los-grandes-números"><i class="fa fa-check"></i><b>14.9</b> Ley de los grandes números</a><ul>
<li class="chapter" data-level="14.9.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#malinterpretando-la-ley-de-promedios"><i class="fa fa-check"></i><b>14.9.1</b> Malinterpretando la ley de promedios</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-24"><i class="fa fa-check"></i><b>14.10</b> Ejercicios</a></li>
<li class="chapter" data-level="14.11" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#estudio-de-caso-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Estudio de caso: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#tasas-de-interés-explicadas-con-modelo-de-oportunidad"><i class="fa fa-check"></i><b>14.11.1</b> Tasas de interés explicadas con modelo de oportunidad</a></li>
<li class="chapter" data-level="14.11.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-25"><i class="fa fa-check"></i><b>14.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Inferencia estadística</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#encuestas"><i class="fa fa-check"></i><b>15.1</b> Encuestas</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#el-modelo-de-muestreo-para-encuestas"><i class="fa fa-check"></i><b>15.1.1</b> El modelo de muestreo para encuestas</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#poblaciones-muestras-parámetros-y-estimaciones"><i class="fa fa-check"></i><b>15.2</b> Poblaciones, muestras, parámetros y estimaciones</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#el-promedio-de-la-muestra"><i class="fa fa-check"></i><b>15.2.1</b> El promedio de la muestra</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parámetros"><i class="fa fa-check"></i><b>15.2.2</b> Parámetros</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#encuesta-versus-pronóstico"><i class="fa fa-check"></i><b>15.2.3</b> Encuesta versus pronóstico</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#propiedades-de-nuestra-estimación-valor-esperado-y-error-estándar"><i class="fa fa-check"></i><b>15.2.4</b> Propiedades de nuestra estimación: valor esperado y error estándar</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#ejercicios-26"><i class="fa fa-check"></i><b>15.3</b> Ejercicios</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Teorema del límite central en la práctica</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo"><i class="fa fa-check"></i><b>15.4.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#la-diferencia"><i class="fa fa-check"></i><b>15.4.2</b> La diferencia</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#sesgo-por-qué-no-realizar-una-encuesta-bien-grande"><i class="fa fa-check"></i><b>15.4.3</b> Sesgo: ¿por qué no realizar una encuesta bien grande?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#ejercicios-27"><i class="fa fa-check"></i><b>15.5</b> Ejercicios</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#intervalos-de-confianza"><i class="fa fa-check"></i><b>15.6</b> Intervalos de confianza</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo-1"><i class="fa fa-check"></i><b>15.6.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#el-idioma-correcto"><i class="fa fa-check"></i><b>15.6.2</b> El idioma correcto</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#ejercicios-28"><i class="fa fa-check"></i><b>15.7</b> Ejercicios</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#poder"><i class="fa fa-check"></i><b>15.8</b> Poder</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#valores-p"><i class="fa fa-check"></i><b>15.9</b> valores p</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Pruebas de asociación</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#tablas-2x2"><i class="fa fa-check"></i><b>15.10.2</b> Tablas 2x2</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#prueba-de-chi-cuadrado"><i class="fa fa-check"></i><b>15.10.3</b> Prueba de chi-cuadrado</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> Riesgo relativo</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#intervalos-de-confianza-para-el-riesgo-relativo"><i class="fa fa-check"></i><b>15.10.5</b> Intervalos de confianza para el riesgo relativo</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#corrección-de-recuento-pequeño"><i class="fa fa-check"></i><b>15.10.6</b> Corrección de recuento pequeño</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#muestras-grandes-valores-p-pequeños"><i class="fa fa-check"></i><b>15.10.7</b> Muestras grandes, valores p pequeños</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#ejercicios-29"><i class="fa fa-check"></i><b>15.11</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Modelos estadísticos</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#agregadores-de-encuestas"><i class="fa fa-check"></i><b>16.1</b> Agregadores de encuestas</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#datos-de-encuesta"><i class="fa fa-check"></i><b>16.1.1</b> Datos de encuesta</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#sesgo-de-los-encuestadores"><i class="fa fa-check"></i><b>16.1.2</b> Sesgo de los encuestadores</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Modelos basados en datos</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#ejercicios-30"><i class="fa fa-check"></i><b>16.3</b> Ejercicios</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Estadísticas bayesianas</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#teorema-de-bayes"><i class="fa fa-check"></i><b>16.4.1</b> Teorema de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#simulación-del-teorema-de-bayes"><i class="fa fa-check"></i><b>16.5</b> Simulación del teorema de Bayes</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-en-la-práctica"><i class="fa fa-check"></i><b>16.5.1</b> Bayes en la práctica</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#modelos-jerárquicos"><i class="fa fa-check"></i><b>16.6</b> Modelos jerárquicos</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#ejercicios-31"><i class="fa fa-check"></i><b>16.7</b> Ejercicios</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Estudio de caso: pronóstico de elecciones</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Enfoque bayesiano</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#el-sesgo-general"><i class="fa fa-check"></i><b>16.8.2</b> El sesgo general</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#representaciones-matemáticas-de-modelos"><i class="fa fa-check"></i><b>16.8.3</b> Representaciones matemáticas de modelos</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#prediciendo-el-colegio-electoral"><i class="fa fa-check"></i><b>16.8.4</b> Prediciendo el colegio electoral</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#pronósticos"><i class="fa fa-check"></i><b>16.8.5</b> Pronósticos</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#ejercicios-32"><i class="fa fa-check"></i><b>16.9</b> Ejercicios</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> La distribución t</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regresión</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#estudio-de-caso-la-altura-es-hereditaria"><i class="fa fa-check"></i><b>17.1</b> Estudio de caso: ¿la altura es hereditaria?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> El coeficiente de correlación</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#la-correlación-de-muestra-es-una-variable-aleatoria"><i class="fa fa-check"></i><b>17.2.1</b> La correlación de muestra es una variable aleatoria</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#la-correlación-no-siempre-es-un-resumen-útil"><i class="fa fa-check"></i><b>17.2.2</b> La correlación no siempre es un resumen útil</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Valor esperado condicional</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#la-línea-de-regresión"><i class="fa fa-check"></i><b>17.4</b> La línea de regresión</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regresión-mejora-precisión"><i class="fa fa-check"></i><b>17.4.1</b> Regresión mejora precisión</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#distribución-normal-de-dos-variables-avanzada"><i class="fa fa-check"></i><b>17.4.2</b> Distribución normal de dos variables (avanzada)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#varianza-explicada"><i class="fa fa-check"></i><b>17.4.3</b> Varianza explicada</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#advertencia-hay-dos-líneas-de-regresión"><i class="fa fa-check"></i><b>17.4.4</b> Advertencia: hay dos líneas de regresión</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#ejercicios-33"><i class="fa fa-check"></i><b>17.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>18</b> Modelos lineales</a><ul>
<li class="chapter" data-level="18.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball"><i class="fa fa-check"></i><b>18.1</b> Estudio de caso: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#sabermetrics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetrics</a></li>
<li class="chapter" data-level="18.1.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#conceptos-básicos-de-béisbol"><i class="fa fa-check"></i><b>18.1.2</b> Conceptos básicos de béisbol</a></li>
<li class="chapter" data-level="18.1.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#no-hay-premios-para-bb"><i class="fa fa-check"></i><b>18.1.3</b> No hay premios para BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#base-por-bolas-o-bases-robadas"><i class="fa fa-check"></i><b>18.1.4</b> ¿Base por bolas o bases robadas?</a></li>
<li class="chapter" data-level="18.1.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-aplicada-a-las-estadísticas-de-béisbol"><i class="fa fa-check"></i><b>18.1.5</b> Regresión aplicada a las estadísticas de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#confusión"><i class="fa fa-check"></i><b>18.2</b> Confusión</a><ul>
<li class="chapter" data-level="18.2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#entender-confusión-a-través-de-estratificación"><i class="fa fa-check"></i><b>18.2.1</b> Entender confusión a través de estratificación</a></li>
<li class="chapter" data-level="18.2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>18.2.2</b> Regresión lineal múltiple</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse"><i class="fa fa-check"></i><b>18.3</b> Estimaciones de mínimos cuadrados</a><ul>
<li class="chapter" data-level="18.3.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#interpretando-modelos-lineales"><i class="fa fa-check"></i><b>18.3.1</b> Interpretando modelos lineales</a></li>
<li class="chapter" data-level="18.3.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimaciones-de-mínimos-cuadrados-lse"><i class="fa fa-check"></i><b>18.3.2</b> Estimaciones de mínimos cuadrados (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-función-lm"><i class="fa fa-check"></i><b>18.3.3</b> La función <code>lm</code></a></li>
<li class="chapter" data-level="18.3.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.4</b> LSE son variables aleatorias</a></li>
<li class="chapter" data-level="18.3.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-pronosticados-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.5</b> Valores pronosticados son variables aleatorias</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-34"><i class="fa fa-check"></i><b>18.4</b> Ejercicios</a></li>
<li class="chapter" data-level="18.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-en-el-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Regresión lineal en el tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#el-paquete-broom"><i class="fa fa-check"></i><b>18.5.1</b> El paquete broom</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-35"><i class="fa fa-check"></i><b>18.6</b> Ejercicios</a></li>
<li class="chapter" data-level="18.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball-continuación"><i class="fa fa-check"></i><b>18.7</b> Estudio de caso: Moneyball (continuación)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#añadiendo-información-sobre-salario-y-posición"><i class="fa fa-check"></i><b>18.7.1</b> Añadiendo información sobre salario y posición</a></li>
<li class="chapter" data-level="18.7.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#escoger-nueve-jugadores"><i class="fa fa-check"></i><b>18.7.2</b> Escoger nueve jugadores</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-falacia-de-la-regresión"><i class="fa fa-check"></i><b>18.8</b> La falacia de la regresión</a></li>
<li class="chapter" data-level="18.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-de-error-de-medición"><i class="fa fa-check"></i><b>18.9</b> Modelos de error de medición</a></li>
<li class="chapter" data-level="18.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-36"><i class="fa fa-check"></i><b>18.10</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html"><i class="fa fa-check"></i><b>19</b> La asociación no implica causalidad</a><ul>
<li class="chapter" data-level="19.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#correlación-espuria"><i class="fa fa-check"></i><b>19.1</b> Correlación espuria</a></li>
<li class="chapter" data-level="19.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#valores-atípicos-1"><i class="fa fa-check"></i><b>19.2</b> Valores atípicos</a></li>
<li class="chapter" data-level="19.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#inversión-de-causa-y-efecto"><i class="fa fa-check"></i><b>19.3</b> Inversión de causa y efecto</a></li>
<li class="chapter" data-level="19.4" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#factores-de-confusión"><i class="fa fa-check"></i><b>19.4</b> Factores de confusión</a><ul>
<li class="chapter" data-level="19.4.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejemplo-admisiones-a-la-universidad-de-california-berkeley"><i class="fa fa-check"></i><b>19.4.1</b> Ejemplo: admisiones a la Universidad de California, Berkeley</a></li>
<li class="chapter" data-level="19.4.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#confusión-explicada-gráficamente"><i class="fa fa-check"></i><b>19.4.2</b> Confusión explicada gráficamente</a></li>
<li class="chapter" data-level="19.4.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#promedio-después-de-estratificar"><i class="fa fa-check"></i><b>19.4.3</b> Promedio después de estratificar</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#la-paradoja-de-simpson"><i class="fa fa-check"></i><b>19.5</b> La paradoja de Simpson</a></li>
<li class="chapter" data-level="19.6" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejercicios-37"><i class="fa fa-check"></i><b>19.6</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>IV <em>Wrangling</em> de datos</b></span></li>
<li class="chapter" data-level="20" data-path="introducción-al-wrangling-de-datos.html"><a href="introducción-al-wrangling-de-datos.html"><i class="fa fa-check"></i><b>20</b> Introducción al wrangling de datos</a></li>
<li class="chapter" data-level="21" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html"><i class="fa fa-check"></i><b>21</b> Cómo cambiar el formato de datos</a><ul>
<li class="chapter" data-level="21.1" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#ejercicios-38"><i class="fa fa-check"></i><b>21.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="unir-tablas.html"><a href="unir-tablas.html"><i class="fa fa-check"></i><b>22</b> Unir tablas</a><ul>
<li class="chapter" data-level="22.1" data-path="unir-tablas.html"><a href="unir-tablas.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="unir-tablas.html"><a href="unir-tablas.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="unir-tablas.html"><a href="unir-tablas.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="unir-tablas.html"><a href="unir-tablas.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="unir-tablas.html"><a href="unir-tablas.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="unir-tablas.html"><a href="unir-tablas.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="unir-tablas.html"><a href="unir-tablas.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="unir-tablas.html"><a href="unir-tablas.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-columnas"><i class="fa fa-check"></i><b>22.2.1</b> Pegando columnas</a></li>
<li class="chapter" data-level="22.2.2" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-filas"><i class="fa fa-check"></i><b>22.2.2</b> Pegando filas</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="unir-tablas.html"><a href="unir-tablas.html#operadores-de-sets"><i class="fa fa-check"></i><b>22.3</b> Operadores de sets</a><ul>
<li class="chapter" data-level="22.3.1" data-path="unir-tablas.html"><a href="unir-tablas.html#intersecar"><i class="fa fa-check"></i><b>22.3.1</b> Intersecar</a></li>
<li class="chapter" data-level="22.3.2" data-path="unir-tablas.html"><a href="unir-tablas.html#unión"><i class="fa fa-check"></i><b>22.3.2</b> Unión</a></li>
<li class="chapter" data-level="22.3.3" data-path="unir-tablas.html"><a href="unir-tablas.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="unir-tablas.html"><a href="unir-tablas.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="unir-tablas.html"><a href="unir-tablas.html#ejercicios-39"><i class="fa fa-check"></i><b>22.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html"><i class="fa fa-check"></i><b>23</b> Extracción de la web</a><ul>
<li class="chapter" data-level="23.1" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#el-paquete-rvest"><i class="fa fa-check"></i><b>23.2</b> El paquete rvest</a></li>
<li class="chapter" data-level="23.3" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> Selectores CSS</a></li>
<li class="chapter" data-level="23.4" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#ejercicios-40"><i class="fa fa-check"></i><b>23.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html"><i class="fa fa-check"></i><b>24</b> Procesamiento de cadenas</a><ul>
<li class="chapter" data-level="24.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#stringr"><i class="fa fa-check"></i><b>24.1</b> El paquete stringr</a></li>
<li class="chapter" data-level="24.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-1-datos-de-asesinatos-en-ee.-uu."><i class="fa fa-check"></i><b>24.2</b> Estudio de caso 1: datos de asesinatos en EE. UU.</a></li>
<li class="chapter" data-level="24.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas"><i class="fa fa-check"></i><b>24.3</b> Estudio de caso 2: alturas autoreportadas</a></li>
<li class="chapter" data-level="24.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cómo-escapar-al-definir-cadenas"><i class="fa fa-check"></i><b>24.4</b> Cómo <em>escapar</em> al definir cadenas</a></li>
<li class="chapter" data-level="24.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#expresiones-regulares"><i class="fa fa-check"></i><b>24.5</b> Expresiones regulares</a><ul>
<li class="chapter" data-level="24.5.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#las-cadenas-son-una-expresión-regular"><i class="fa fa-check"></i><b>24.5.1</b> Las cadenas son una expresión regular</a></li>
<li class="chapter" data-level="24.5.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#caracteres-especiales"><i class="fa fa-check"></i><b>24.5.2</b> Caracteres especiales</a></li>
<li class="chapter" data-level="24.5.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#clases-de-caracteres"><i class="fa fa-check"></i><b>24.5.3</b> Clases de caracteres</a></li>
<li class="chapter" data-level="24.5.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#anclas"><i class="fa fa-check"></i><b>24.5.4</b> Anclas</a></li>
<li class="chapter" data-level="24.5.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores"><i class="fa fa-check"></i><b>24.5.5</b> Cuantificadores</a></li>
<li class="chapter" data-level="24.5.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#espacio-en-blanco-s"><i class="fa fa-check"></i><b>24.5.6</b> Espacio en blanco <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores-1"><i class="fa fa-check"></i><b>24.5.7</b> Cuantificadores: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#no"><i class="fa fa-check"></i><b>24.5.8</b> No</a></li>
<li class="chapter" data-level="24.5.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-con-expresiones-regulares"><i class="fa fa-check"></i><b>24.6</b> Buscar y reemplazar con expresiones regulares</a><ul>
<li class="chapter" data-level="24.6.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-usando-grupos"><i class="fa fa-check"></i><b>24.6.1</b> Buscar y reemplazar usando grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#probar-y-mejorar"><i class="fa fa-check"></i><b>24.7</b> Probar y mejorar</a></li>
<li class="chapter" data-level="24.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#podar"><i class="fa fa-check"></i><b>24.8</b> Podar</a></li>
<li class="chapter" data-level="24.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cambio-de-mayúsculas-o-minúsculas"><i class="fa fa-check"></i><b>24.9</b> Cambio de mayúsculas o minúsculas</a></li>
<li class="chapter" data-level="24.10" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas-continuación"><i class="fa fa-check"></i><b>24.10</b> Estudio de caso 2: alturas autoreportadas (continuación)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#la-función-extract"><i class="fa fa-check"></i><b>24.10.1</b> La función <code>extract</code></a></li>
<li class="chapter" data-level="24.10.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#juntando-todas-la-piezas"><i class="fa fa-check"></i><b>24.10.2</b> Juntando todas la piezas</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#división-de-cadenas"><i class="fa fa-check"></i><b>24.11</b> División de cadenas</a></li>
<li class="chapter" data-level="24.12" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-3-extracción-de-tablas-de-un-pdf"><i class="fa fa-check"></i><b>24.12</b> Estudio de caso 3: extracción de tablas de un PDF</a></li>
<li class="chapter" data-level="24.13" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recodificación</a></li>
<li class="chapter" data-level="24.14" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#ejercicios-41"><i class="fa fa-check"></i><b>24.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html"><i class="fa fa-check"></i><b>25</b> Cómo leer y procesar fechas y horas</a><ul>
<li class="chapter" data-level="25.1" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#el-tipo-de-datos-de-fecha"><i class="fa fa-check"></i><b>25.1</b> El tipo de datos de fecha</a></li>
<li class="chapter" data-level="25.2" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> El paquete lubridate</a></li>
<li class="chapter" data-level="25.3" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#ejercicios-42"><i class="fa fa-check"></i><b>25.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="minería-de-textos.html"><a href="minería-de-textos.html"><i class="fa fa-check"></i><b>26</b> Minería de textos</a><ul>
<li class="chapter" data-level="26.1" data-path="minería-de-textos.html"><a href="minería-de-textos.html#estudio-de-caso-tuits-de-trump"><i class="fa fa-check"></i><b>26.1</b> Estudio de caso: tuits de Trump</a></li>
<li class="chapter" data-level="26.2" data-path="minería-de-textos.html"><a href="minería-de-textos.html#texto-como-datos"><i class="fa fa-check"></i><b>26.2</b> Texto como datos</a></li>
<li class="chapter" data-level="26.3" data-path="minería-de-textos.html"><a href="minería-de-textos.html#análisis-de-sentimiento"><i class="fa fa-check"></i><b>26.3</b> Análisis de sentimiento</a></li>
<li class="chapter" data-level="26.4" data-path="minería-de-textos.html"><a href="minería-de-textos.html#ejercicios-43"><i class="fa fa-check"></i><b>26.4</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html"><i class="fa fa-check"></i><b>27</b> Introducción al aprendizaje automático</a><ul>
<li class="chapter" data-level="27.1" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#notación-1"><i class="fa fa-check"></i><b>27.1</b> Notación</a></li>
<li class="chapter" data-level="27.2" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#un-ejemplo"><i class="fa fa-check"></i><b>27.2</b> Un ejemplo</a></li>
<li class="chapter" data-level="27.3" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#ejercicios-44"><i class="fa fa-check"></i><b>27.3</b> Ejercicios</a></li>
<li class="chapter" data-level="27.4" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#métricas-de-evaluación"><i class="fa fa-check"></i><b>27.4</b> Métricas de evaluación</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#conjuntos-de-entrenamiento-y-prueba"><i class="fa fa-check"></i><b>27.4.1</b> Conjuntos de entrenamiento y prueba</a></li>
<li class="chapter" data-level="27.4.2" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#precisión-general"><i class="fa fa-check"></i><b>27.4.2</b> Precisión general</a></li>
<li class="chapter" data-level="27.4.3" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#la-matriz-de-confusión"><i class="fa fa-check"></i><b>27.4.3</b> La matriz de confusión</a></li>
<li class="chapter" data-level="27.4.4" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#sensibilidad-y-especificidad"><i class="fa fa-check"></i><b>27.4.4</b> Sensibilidad y especificidad</a></li>
<li class="chapter" data-level="27.4.5" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#precisión-equilibrada-y-f_1-puntuación"><i class="fa fa-check"></i><b>27.4.5</b> Precisión equilibrada y <span class="math inline">\(F_1\)</span> puntuación</a></li>
<li class="chapter" data-level="27.4.6" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#la-prevalencia-importa-en-la-práctica"><i class="fa fa-check"></i><b>27.4.6</b> La prevalencia importa en la práctica</a></li>
<li class="chapter" data-level="27.4.7" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#roc-y-curvas-de-recuperación-de-precisión"><i class="fa fa-check"></i><b>27.4.7</b> ROC y curvas de recuperación de precisión</a></li>
<li class="chapter" data-level="27.4.8" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> La función de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#ejercicios-45"><i class="fa fa-check"></i><b>27.5</b> Ejercicios</a></li>
<li class="chapter" data-level="27.6" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#probabilidades-y-expectativas-condicionales"><i class="fa fa-check"></i><b>27.6</b> Probabilidades y expectativas condicionales</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#probabilidades-condicionales-1"><i class="fa fa-check"></i><b>27.6.1</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="27.6.2" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#expectativas-condicionales"><i class="fa fa-check"></i><b>27.6.2</b> Expectativas condicionales</a></li>
<li class="chapter" data-level="27.6.3" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#la-expectativa-condicional-minimiza-la-función-de-pérdida-al-cuadrado"><i class="fa fa-check"></i><b>27.6.3</b> La expectativa condicional minimiza la función de pérdida al cuadrado</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#ejercicios-46"><i class="fa fa-check"></i><b>27.7</b> Ejercicios</a></li>
<li class="chapter" data-level="27.8" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Estudio de caso: ¿es un 2 o un 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="suavizado.html"><a href="suavizado.html"><i class="fa fa-check"></i><b>28</b> Suavizado</a><ul>
<li class="chapter" data-level="28.1" data-path="suavizado.html"><a href="suavizado.html#alisado-de-bin"><i class="fa fa-check"></i><b>28.1</b> Alisado de bin</a></li>
<li class="chapter" data-level="28.2" data-path="suavizado.html"><a href="suavizado.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="suavizado.html"><a href="suavizado.html#regresión-ponderada-local-loess"><i class="fa fa-check"></i><b>28.3</b> Regresión ponderada local (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="suavizado.html"><a href="suavizado.html#parábolas-de-montaje"><i class="fa fa-check"></i><b>28.3.1</b> Parábolas de montaje</a></li>
<li class="chapter" data-level="28.3.2" data-path="suavizado.html"><a href="suavizado.html#cuidado-con-los-parámetros-de-suavizado-predeterminados"><i class="fa fa-check"></i><b>28.3.2</b> Cuidado con los parámetros de suavizado predeterminados</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="suavizado.html"><a href="suavizado.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Conectando el suavizado al aprendizaje automático</a></li>
<li class="chapter" data-level="28.5" data-path="suavizado.html"><a href="suavizado.html#ejercicios-47"><i class="fa fa-check"></i><b>28.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Validación cruzada</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivación con vecinos k-más cercanos</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#sobreentrenamiento"><i class="fa fa-check"></i><b>29.1.1</b> Sobreentrenamiento</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#alisado-excesivo"><i class="fa fa-check"></i><b>29.1.2</b> Alisado excesivo</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#escogiendo-el-k-en-knn"><i class="fa fa-check"></i><b>29.1.3</b> Escogiendo el <span class="math inline">\(k\)</span> en kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#descripción-matemática-de-validación-cruzada"><i class="fa fa-check"></i><b>29.2</b> Descripción matemática de validación cruzada</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#validación-cruzada-k-fold"><i class="fa fa-check"></i><b>29.3</b> Validación cruzada K-fold</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-48"><i class="fa fa-check"></i><b>29.4</b> Ejercicios</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-49"><i class="fa fa-check"></i><b>29.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> El paquete caret</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#el-caret-train-functon"><i class="fa fa-check"></i><b>30.1</b> El caret <code>train</code> functon</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Validación cruzada</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#ejemplo-ajuste-con-loess"><i class="fa fa-check"></i><b>30.3</b> Ejemplo: ajuste con loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html"><i class="fa fa-check"></i><b>31</b> Ejemplos de algoritmos</a><ul>
<li class="chapter" data-level="31.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-lineal"><i class="fa fa-check"></i><b>31.1</b> Regresión lineal</a><ul>
<li class="chapter" data-level="31.1.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#los-predict-función"><i class="fa fa-check"></i><b>31.1.1</b> Los <code>predict</code> función</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-50"><i class="fa fa-check"></i><b>31.2</b> Ejercicios</a></li>
<li class="chapter" data-level="31.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística"><i class="fa fa-check"></i><b>31.3</b> Regresión logística</a><ul>
<li class="chapter" data-level="31.3.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>31.3.1</b> Modelos lineales generalizados</a></li>
<li class="chapter" data-level="31.3.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística-con-más-de-un-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Regresión logística con más de un predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-51"><i class="fa fa-check"></i><b>31.4</b> Ejercicios</a></li>
<li class="chapter" data-level="31.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#k-vecinos-más-cercanos"><i class="fa fa-check"></i><b>31.5</b> k-vecinos más cercanos</a></li>
<li class="chapter" data-level="31.6" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-52"><i class="fa fa-check"></i><b>31.6</b> Ejercicios</a></li>
<li class="chapter" data-level="31.7" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-generativos"><i class="fa fa-check"></i><b>31.7</b> Modelos generativos</a><ul>
<li class="chapter" data-level="31.7.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#controlando-la-prevalencia"><i class="fa fa-check"></i><b>31.7.2</b> Controlando la prevalencia</a></li>
<li class="chapter" data-level="31.7.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-cuadrático"><i class="fa fa-check"></i><b>31.7.3</b> Análisis discriminante cuadrático</a></li>
<li class="chapter" data-level="31.7.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-lineal"><i class="fa fa-check"></i><b>31.7.4</b> Análisis discriminante lineal</a></li>
<li class="chapter" data-level="31.7.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#conexión-a-distancia"><i class="fa fa-check"></i><b>31.7.5</b> Conexión a distancia</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#estudio-de-caso-más-de-tres-clases"><i class="fa fa-check"></i><b>31.8</b> Estudio de caso: más de tres clases</a></li>
<li class="chapter" data-level="31.9" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-53"><i class="fa fa-check"></i><b>31.9</b> Ejercicios</a></li>
<li class="chapter" data-level="31.10" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-y-regresión-cart"><i class="fa fa-check"></i><b>31.10</b> Árboles de clasificación y regresión (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#la-maldición-de-la-dimensionalidad"><i class="fa fa-check"></i><b>31.10.1</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="31.10.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#carrera-motivación"><i class="fa fa-check"></i><b>31.10.2</b> CARRERA motivación</a></li>
<li class="chapter" data-level="31.10.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-regresión"><i class="fa fa-check"></i><b>31.10.3</b> Árboles de regresión</a></li>
<li class="chapter" data-level="31.10.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-decisión"><i class="fa fa-check"></i><b>31.10.4</b> Árboles de clasificación (decisión)</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#bosques-al-azar"><i class="fa fa-check"></i><b>31.11</b> Bosques al azar</a></li>
<li class="chapter" data-level="31.12" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-54"><i class="fa fa-check"></i><b>31.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html"><i class="fa fa-check"></i><b>32</b> Machine learning en la práctica</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#preprocesamiento"><i class="fa fa-check"></i><b>32.1</b> Preprocesamiento</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#k-vecino-más-cercano-y-bosque-aleatorio"><i class="fa fa-check"></i><b>32.2</b> k-vecino más cercano y bosque aleatorio</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#importancia-variable"><i class="fa fa-check"></i><b>32.3</b> Importancia variable</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#evaluaciones-visuales"><i class="fa fa-check"></i><b>32.4</b> Evaluaciones visuales</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html"><i class="fa fa-check"></i><b>33</b> Grandes conjuntos de datos</a><ul>
<li class="chapter" data-level="33.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Álgebra matricial</a><ul>
<li class="chapter" data-level="33.1.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#notación-2"><i class="fa fa-check"></i><b>33.1.1</b> Notación</a></li>
<li class="chapter" data-level="33.1.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#convertir-un-vector-en-una-matriz"><i class="fa fa-check"></i><b>33.1.2</b> Convertir un vector en una matriz</a></li>
<li class="chapter" data-level="33.1.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#resúmenes-de-filas-y-columnas"><i class="fa fa-check"></i><b>33.1.3</b> Resúmenes de filas y columnas</a></li>
<li class="chapter" data-level="33.1.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#filtrar-columnas-basadas-en-resúmenes"><i class="fa fa-check"></i><b>33.1.5</b> Filtrar columnas basadas en resúmenes</a></li>
<li class="chapter" data-level="33.1.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#indexación-con-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexación con matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#binarizar-los-datos"><i class="fa fa-check"></i><b>33.1.7</b> Binarizar los datos</a></li>
<li class="chapter" data-level="33.1.8" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#vectorización-para-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorización para matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#operaciones-de-álgebra-matricial"><i class="fa fa-check"></i><b>33.1.9</b> Operaciones de álgebra matricial</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-55"><i class="fa fa-check"></i><b>33.2</b> Ejercicios</a></li>
<li class="chapter" data-level="33.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia"><i class="fa fa-check"></i><b>33.3</b> Distancia</a><ul>
<li class="chapter" data-level="33.3.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.1</b> Distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia-en-dimensiones-superiores"><i class="fa fa-check"></i><b>33.3.2</b> Distancia en dimensiones superiores</a></li>
<li class="chapter" data-level="33.3.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejemplo-de-distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.3</b> Ejemplo de distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Espacio predictor</a></li>
<li class="chapter" data-level="33.3.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia-entre-predictores"><i class="fa fa-check"></i><b>33.3.5</b> Distancia entre predictores</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-56"><i class="fa fa-check"></i><b>33.4</b> Ejercicios</a></li>
<li class="chapter" data-level="33.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#reducción-de-dimensiones"><i class="fa fa-check"></i><b>33.5</b> Reducción de dimensiones</a><ul>
<li class="chapter" data-level="33.5.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#preservando-la-distancia"><i class="fa fa-check"></i><b>33.5.1</b> Preservando la distancia</a></li>
<li class="chapter" data-level="33.5.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#transformaciones-lineales-avanzado"><i class="fa fa-check"></i><b>33.5.2</b> Transformaciones lineales (avanzado)</a></li>
<li class="chapter" data-level="33.5.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#transformaciones-ortogonales-avanzado"><i class="fa fa-check"></i><b>33.5.3</b> Transformaciones ortogonales (avanzado)</a></li>
<li class="chapter" data-level="33.5.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#análisis-de-componentes-principales"><i class="fa fa-check"></i><b>33.5.4</b> Análisis de componentes principales</a></li>
<li class="chapter" data-level="33.5.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejemplo-de-iris"><i class="fa fa-check"></i><b>33.5.5</b> Ejemplo de Iris</a></li>
<li class="chapter" data-level="33.5.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejemplo-de-mnist"><i class="fa fa-check"></i><b>33.5.6</b> Ejemplo de MNIST</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-57"><i class="fa fa-check"></i><b>33.6</b> Ejercicios</a></li>
<li class="chapter" data-level="33.7" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#sistemas-de-recomendación"><i class="fa fa-check"></i><b>33.7</b> Sistemas de recomendación</a><ul>
<li class="chapter" data-level="33.7.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#datos-de-lente-de-película"><i class="fa fa-check"></i><b>33.7.1</b> Datos de lente de película</a></li>
<li class="chapter" data-level="33.7.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#sistemas-de-recomendación-como-un-desafío-de-aprendizaje-automático"><i class="fa fa-check"></i><b>33.7.2</b> Sistemas de recomendación como un desafío de aprendizaje automático</a></li>
<li class="chapter" data-level="33.7.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Función de pérdida</a></li>
<li class="chapter" data-level="33.7.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#un-primer-modelo"><i class="fa fa-check"></i><b>33.7.4</b> Un primer modelo</a></li>
<li class="chapter" data-level="33.7.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#modelado-de-efectos-de-películas"><i class="fa fa-check"></i><b>33.7.5</b> Modelado de efectos de películas</a></li>
<li class="chapter" data-level="33.7.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#efectos-de-usuario"><i class="fa fa-check"></i><b>33.7.6</b> Efectos de usuario</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-58"><i class="fa fa-check"></i><b>33.8</b> Ejercicios</a></li>
<li class="chapter" data-level="33.9" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#regularización"><i class="fa fa-check"></i><b>33.9</b> Regularización</a><ul>
<li class="chapter" data-level="33.9.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#motivación"><i class="fa fa-check"></i><b>33.9.1</b> Motivación</a></li>
<li class="chapter" data-level="33.9.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#mínimos-cuadrados-penalizados"><i class="fa fa-check"></i><b>33.9.2</b> Mínimos cuadrados penalizados</a></li>
<li class="chapter" data-level="33.9.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#elegir-los-términos-de-penalización"><i class="fa fa-check"></i><b>33.9.3</b> Elegir los términos de penalización</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-59"><i class="fa fa-check"></i><b>33.10</b> Ejercicios</a></li>
<li class="chapter" data-level="33.11" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#factorización-matricial"><i class="fa fa-check"></i><b>33.11</b> Factorización matricial</a><ul>
<li class="chapter" data-level="33.11.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#análisis-de-factores"><i class="fa fa-check"></i><b>33.11.1</b> Análisis de factores</a></li>
<li class="chapter" data-level="33.11.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#conexión-a-svd-y-pca"><i class="fa fa-check"></i><b>33.11.2</b> Conexión a SVD y PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-60"><i class="fa fa-check"></i><b>33.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Agrupación</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#agrupación-jerárquica"><i class="fa fa-check"></i><b>34.1</b> Agrupación jerárquica</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#mapas-de-calor"><i class="fa fa-check"></i><b>34.3</b> Mapas de calor</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#características-de-filtrado"><i class="fa fa-check"></i><b>34.4</b> Características de filtrado</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#ejercicios-61"><i class="fa fa-check"></i><b>34.5</b> Ejercicios</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción a la Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introducción-al-aprendizaje-automático" class="section level1">
<h1><span class="header-section-number">Capítulo 27</span> Introducción al aprendizaje automático</h1>
<p>Quizás las metodologías de ciencia de datos más populares provienen del campo del aprendizaje de máquinas. Las historias de éxito del aprendizaje automático incluyen lectores de códigos postales escritos a mano implementados por el servicio postal, tecnología de reconocimiento de voz como Siri de Apple, sistemas de recomendación de películas, detectores de spam y malware, predictores de precios de viviendas y automóviles sin conductor. Aunque hoy en día la Inteligencia Artificial y el aprendizaje automático se usan indistintamente, hacemos la siguiente distinción: mientras que los primeros algoritmos de inteligencia artificial, como los utilizados por las máquinas de ajedrez, implementaron la toma de decisiones basada en reglas programables derivadas de la teoría o los primeros principios, en la máquina las decisiones de aprendizaje se basan en algoritmos <strong>construidos con datos</strong>.</p>
<div id="notación-1" class="section level2">
<h2><span class="header-section-number">27.1</span> Notación</h2>
<p>En el aprendizaje automático, los datos se presentan en forma de:</p>
<ol style="list-style-type: decimal">
<li>el resultado que queremos predecir y</li>
<li>las <em>características</em> que usaremos para predecir el resultado</li>
</ol>
<p>Queremos construir un algoritmo que tome los valores de las características como entrada y devuelva una predicción para el resultado cuando no sabemos el resultado. El enfoque de aprendizaje automático consiste en entrenar un algoritmo utilizando un conjunto de datos para el que sí conocemos el resultado, y luego aplicar este algoritmo en el futuro para hacer una predicción cuando no sabemos el resultado.</p>
<p>Aquí usaremos <span class="math inline">\(Y\)</span> para denotar el resultado y <span class="math inline">\(X_1, \dots, X_p\)</span> para denotar características. Tenga en cuenta que las características a veces se denominan predictores o covariables. Consideramos que todos estos son sinónimos.</p>
<p>Los problemas de predicción se pueden dividir en resultados categóricos y continuos. Para resultados categóricos, <span class="math inline">\(Y\)</span> puede ser cualquiera de <span class="math inline">\(K\)</span> clases El número de clases puede variar mucho entre las aplicaciones.
Por ejemplo, en los datos del lector de dígitos, <span class="math inline">\(K=10\)</span> siendo las clases los dígitos 0, 1, 2, 3, 4, 5, 6, 7, 8 y 9. En el reconocimiento de voz, los resultados son todas las palabras o frases posibles que estamos tratando de detectar. La detección de spam tiene dos resultados: spam o no spam. En este libro, denotamos el <span class="math inline">\(K\)</span> categorías con índices <span class="math inline">\(k=1,\dots,K\)</span>. Sin embargo, para datos binarios usaremos <span class="math inline">\(k=0,1\)</span> para conveniencias matemáticas que demostraremos más adelante.</p>
<p>La configuración general es la siguiente. Tenemos una serie de características y un resultado desconocido que queremos predecir:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
outcome
</th>
<th style="text-align:center;">
feature 1
</th>
<th style="text-align:center;">
feature 2
</th>
<th style="text-align:center;">
feature 3
</th>
<th style="text-align:center;">
feature 4
</th>
<th style="text-align:center;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
?
</td>
<td style="text-align:center;">
<span class="math inline">\(X_1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_2\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_3\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_4\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_5\)</span>
</td>
</tr>
</tbody>
</table>
<p>Para <em>construir un modelo</em> que proporcione una predicción para cualquier conjunto de valores observados <span class="math inline">\(X_1=x_1, X_2=x_2, \dots X_5=x_5\)</span>, recopilamos datos para los cuales conocemos el resultado:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
outcome
</th>
<th style="text-align:left;">
feature 1
</th>
<th style="text-align:left;">
feature 2
</th>
<th style="text-align:left;">
feature 3
</th>
<th style="text-align:left;">
feature 4
</th>
<th style="text-align:left;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_n\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,5}\)</span>
</td>
</tr>
</tbody>
</table>
<p>Cuando el resultado es continuo, nos referimos a la tarea de aprendizaje automático como predicción, y el resultado principal del modelo es una función <span class="math inline">\(f\)</span> que produce automáticamente una predicción, denotada con <span class="math inline">\(\hat{y}\)</span>, para cualquier conjunto de predictores: <span class="math inline">\(\hat{y} = f(x_1, x_2, \dots, x_p)\)</span>. Usamos el término resultado real para denotar lo que terminamos observando. Entonces queremos la predicción <span class="math inline">\(\hat{y}\)</span> para que coincida con el resultado real <span class="math inline">\(y\)</span> tan bien como sea posible. Debido a que nuestro resultado es continuo, nuestras predicciones <span class="math inline">\(\hat{y}\)</span> no será exactamente correcto o incorrecto, sino que determinaremos un <em>error</em> definido como la diferencia entre la predicción y el resultado real <span class="math inline">\(y - \hat{y}\)</span>.</p>
<p>Cuando el resultado es categórico, nos referimos a la tarea de aprendizaje automático como <em>clasificación</em>, y el resultado principal del modelo será una <em>regla de decisión</em> que prescribe cuál de las <span class="math inline">\(K\)</span> clases que debemos predecir. En este escenario, la mayoría de los modelos proporcionan funciones de los predictores para cada clase. <span class="math inline">\(k\)</span>, <span class="math inline">\(f_k(x_1, x_2, \dots, x_p)\)</span>, que se utilizan para tomar esta decisión. Cuando los datos son binarios, las reglas de decisión típicas se ven así: si <span class="math inline">\(f_1(x_1, x_2, \dots, x_p) &gt; C\)</span>, pronostique la categoría 1, si no la otra categoría, con <span class="math inline">\(C\)</span> un límite predeterminado. Debido a que los resultados son categóricos, nuestras predicciones serán correctas o incorrectas.</p>
<p>Tenga en cuenta que estos términos varían entre cursos, libros de texto y otras publicaciones. A menudo, la predicción se usa tanto para resultados categóricos como continuos, y el término <em>regresión</em> puede usarse para el caso continuo. Aquí evitamos usar <em>regresión_para evitar confusiones con nuestro uso previo del término</em> regresión lineal_. En la mayoría de los casos, estará claro si nuestros resultados son categóricos o continuos, por lo que evitaremos usar estos términos cuando sea posible.</p>
</div>
<div id="un-ejemplo" class="section level2">
<h2><span class="header-section-number">27.2</span> Un ejemplo</h2>
<p>Consideremos el ejemplo del lector de código postal. El primer paso para manejar el correo recibido en la oficina de correos es ordenar las letras por código postal:</p>
<p><img src="ml/img/how-to-write-a-address-on-an-envelope-how-to-write-the-address-on-an-envelope-write-address-on-envelope-india-finishedenvelope-x69070.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Originalmente, los humanos tenían que clasificarlos a mano. Para hacer esto, tuvieron que leer los códigos postales en cada letra. Hoy, gracias a los algoritmos de aprendizaje automático, una computadora puede leer códigos postales y luego un robot clasifica las letras. En esta parte del libro, aprenderemos cómo construir algoritmos que puedan leer un dígito.</p>
<p>El primer paso para construir un algoritmo es entender
¿cuáles son los resultados y características? A continuación hay tres imágenes de dígitos escritos. Estos ya han sido leídos por un humano y se les ha asignado un resultado. <span class="math inline">\(Y\)</span>. Estos se consideran conocidos y sirven como conjunto de entrenamiento.</p>
<p><img src="libro_files/figure-html/digit-images-example-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Las imágenes se convierten en <span class="math inline">\(28 \times 28 = 784\)</span> píxeles y, para cada píxel, obtenemos una intensidad de escala de grises entre 0 (blanco) y 255 (negro), que consideramos continua por ahora. La siguiente gráfica muestra las características individuales de cada imagen:</p>
<p><img src="libro_files/figure-html/example-images-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Para cada imagen digitalizada <span class="math inline">\(i\)</span>, tenemos un resultado categórico <span class="math inline">\(Y_i\)</span> que puede ser uno de los 10 valores ( <span class="math inline">\(0,1,2,3,4,5,6,7,8,9\)</span>) y características <span class="math inline">\(X_{i,1}, \dots, X_{i,784}\)</span>. Usamos negrita <span class="math inline">\(\mathbf{X}_i = (X_{i,1}, \dots, X_{i,784})\)</span> para distinguir el vector de predictores de los predictores individuales. Cuando nos referimos a un conjunto arbitrario de características en lugar de una imagen específica en nuestro conjunto de datos, descartamos el índice <span class="math inline">\(i\)</span> y use <span class="math inline">\(Y\)</span> y <span class="math inline">\(\mathbf{X} = (X_{1}, \dots, X_{784})\)</span>. Utilizamos variables en mayúsculas porque, en general, pensamos en los predictores como variables aleatorias. Usamos minúsculas, por ejemplo <span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>, para denotar valores observados. Cuando codificamos nos quedamos en minúsculas.</p>
<p>La tarea de aprendizaje automático es construir un algoritmo que devuelva una predicción para cualquiera de los posibles valores de las características. Aquí, aprenderemos varios enfoques para construir estos algoritmos. Aunque en este punto puede parecer imposible lograr esto, comenzaremos con ejemplos simples y desarrollaremos nuestro conocimiento hasta que podamos atacar los más complejos. De hecho, comenzamos con un ejemplo artificialmente simple con un solo predictor y luego pasamos a un ejemplo un poco más realista con dos predictores. Una vez que comprendamos esto, atacaremos los desafíos de aprendizaje automático del mundo real que involucran muchos predictores.</p>
</div>
<div id="ejercicios-44" class="section level2">
<h2><span class="header-section-number">27.3</span> Ejercicios</h2>
<p>1. Para cada uno de los siguientes, determine si el resultado es continuo o categórico:</p>
<ol style="list-style-type: lower-alpha">
<li>Lector de dígitos
si. Recomendaciones de películas</li>
<li>Filtro de spam
re. Hospitalizaciones</li>
</ol>
<ol start="1001" style="list-style-type: lower-roman">
<li>Siri (reconocimiento de voz)</li>
</ol>
<p>2. ¿Cuántas funciones tenemos disponibles para la predicción en el conjunto de datos de dígitos?</p>
<p>3. En el ejemplo del lector de dígitos, los resultados se almacenan aquí:</p>
<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1000-1"><a href="introducción-al-aprendizaje-automático.html#cb1000-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1000-2"><a href="introducción-al-aprendizaje-automático.html#cb1000-2"></a>y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels</span></code></pre></div>
<p>¿Las siguientes operaciones tienen un significado práctico?</p>
<div class="sourceCode" id="cb1001"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1001-1"><a href="introducción-al-aprendizaje-automático.html#cb1001-1"></a>y[<span class="dv">5</span>] <span class="op">+</span><span class="st"> </span>y[<span class="dv">6</span>]</span>
<span id="cb1001-2"><a href="introducción-al-aprendizaje-automático.html#cb1001-2"></a>y[<span class="dv">5</span>] <span class="op">&gt;</span><span class="st"> </span>y[<span class="dv">6</span>]</span></code></pre></div>
<p>Elige la mejor respuesta:</p>
<ol style="list-style-type: lower-alpha">
<li>Sí porque <span class="math inline">\(9 + 2 = 11\)</span> y <span class="math inline">\(9 &gt; 2\)</span>.
si. No porque <code>y</code> no es un vector numérico.</li>
<li>No, porque 11 no es un dígito. Son dos dígitos.
re. No, porque estas son etiquetas que representan una categoría, no un número. UN <code>9</code> representa una clase, no el número 9.</li>
</ol>

</div>
<div id="métricas-de-evaluación" class="section level2">
<h2><span class="header-section-number">27.4</span> Métricas de evaluación</h2>
<p>Antes de comenzar a describir enfoques para optimizar la forma en que construimos algoritmos, primero debemos definir a qué nos referimos cuando decimos que un enfoque es mejor que otro. En esta sección, nos centramos en describir las formas en que se evalúan los algoritmos de aprendizaje automático. Específicamente, necesitamos cuantificar lo que queremos decir con “mejor”.</p>
<p>Para nuestra primera introducción a los conceptos de aprendizaje automático, comenzaremos con un ejemplo aburrido y simple: cómo predecir el sexo usando la altura. A medida que explicamos el aprendizaje automático paso a paso, este ejemplo nos permitirá establecer el primer bloque de construcción. Muy pronto, estaremos atacando desafíos más interesantes. Utilizamos el paquete <strong>caret</strong>, que tiene varias funciones útiles para construir y evaluar métodos de aprendizaje automático y presentamos con más detalle en la Sección <a href="caret.html#caret">30</a>.</p>
<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1002-1"><a href="introducción-al-aprendizaje-automático.html#cb1002-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1002-2"><a href="introducción-al-aprendizaje-automático.html#cb1002-2"></a><span class="kw">library</span>(caret)</span></code></pre></div>
<p>Para un primer ejemplo, usamos los datos de altura en dslabs:</p>
<div class="sourceCode" id="cb1003"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1003-1"><a href="introducción-al-aprendizaje-automático.html#cb1003-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1003-2"><a href="introducción-al-aprendizaje-automático.html#cb1003-2"></a><span class="kw">data</span>(heights)</span></code></pre></div>
<p>Comenzamos definiendo el resultado y los predictores.</p>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1004-1"><a href="introducción-al-aprendizaje-automático.html#cb1004-1"></a>y &lt;-<span class="st"> </span>heights<span class="op">$</span>sex</span>
<span id="cb1004-2"><a href="introducción-al-aprendizaje-automático.html#cb1004-2"></a>x &lt;-<span class="st"> </span>heights<span class="op">$</span>height</span></code></pre></div>
<p>En este caso, solo tenemos un predictor, altura y <code>y</code> es claramente un resultado categórico ya que los valores observados son <code>Male</code> o <code>Female</code>.
Sabemos que no podremos predecir <span class="math inline">\(Y\)</span> muy exactamente basado en <span class="math inline">\(X\)</span> porque las alturas promedio masculinas y femeninas no son tan diferentes en relación con la variabilidad dentro del grupo. ¿Pero podemos hacerlo mejor que adivinar? Para responder a esta pregunta, necesitamos una definición cuantitativa de mejor.</p>
<div id="conjuntos-de-entrenamiento-y-prueba" class="section level3">
<h3><span class="header-section-number">27.4.1</span> Conjuntos de entrenamiento y prueba</h3>
<p>En última instancia, se evalúa un algoritmo de aprendizaje automático sobre cómo funciona en el mundo real con conjuntos de datos completamente nuevos. Sin embargo, cuando desarrollamos un algoritmo, generalmente tenemos un conjunto de datos para el que conocemos los resultados, como lo hacemos con las alturas: sabemos el sexo de cada estudiante en nuestro conjunto de datos. Por lo tanto, para imitar el proceso de evaluación final, generalmente dividimos los datos en dos partes y actuamos como si no supiéramos el resultado de una de estas. Dejamos de fingir que no conocemos el resultado para evaluar el algoritmo, pero solo * después * de haber terminado de construirlo. Nos referimos al grupo para el que conocemos el resultado, y lo usamos para desarrollar el algoritmo, como el conjunto <em>training</em>. Nos referimos al grupo para el que pretendemos que no conocemos el resultado como el conjunto <em>test</em>.</p>
<p>Una forma estándar de generar los conjuntos de entrenamiento y prueba es dividiendo aleatoriamente los datos. El paquete <strong>caret</strong> incluye la función <code>createDataPartition</code> eso nos ayuda a generar índices para dividir aleatoriamente los datos en conjuntos de entrenamiento y prueba:</p>
<div class="sourceCode" id="cb1005"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1005-1"><a href="introducción-al-aprendizaje-automático.html#cb1005-1"></a><span class="kw">set.seed</span>(<span class="dv">2007</span>)</span>
<span id="cb1005-2"><a href="introducción-al-aprendizaje-automático.html#cb1005-2"></a>test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p>El argumento <code>times</code> se usa para definir cuántas muestras aleatorias de índices devolver, el argumento <code>p</code> se utiliza para definir qué proporción de los datos está representada por el índice y el argumento <code>list</code> se usa para decidir si queremos que los índices se devuelvan como una lista o no.
Podemos usar el resultado de la <code>createDataPartition</code> llamada a la función para definir los conjuntos de entrenamiento y prueba de esta manera:</p>
<div class="sourceCode" id="cb1006"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1006-1"><a href="introducción-al-aprendizaje-automático.html#cb1006-1"></a>test_set &lt;-<span class="st"> </span>heights[test_index, ]</span>
<span id="cb1006-2"><a href="introducción-al-aprendizaje-automático.html#cb1006-2"></a>train_set &lt;-<span class="st"> </span>heights[<span class="op">-</span>test_index, ]</span></code></pre></div>
<p>Ahora desarrollaremos un algoritmo usando <strong>solo</strong> el conjunto de entrenamiento. Una vez que hayamos terminado de desarrollar el algoritmo, lo congelaremos y lo evaluaremos utilizando el conjunto de prueba. La forma más sencilla de evaluar el algoritmo cuando los resultados son categóricos es simplemente informar la proporción de casos que se predijeron correctamente <strong>en el conjunto de pruebas</strong>. Esta métrica generalmente se conoce como <em>precisión general</em>.</p>
</div>
<div id="precisión-general" class="section level3">
<h3><span class="header-section-number">27.4.2</span> Precisión general</h3>
<p>Para demostrar el uso de la precisión general, crearemos dos algoritmos competitivos y los compararemos.</p>
<p>Comencemos desarrollando el algoritmo de máquina más simple posible: adivinar el resultado.</p>
<div class="sourceCode" id="cb1007"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1007-1"><a href="introducción-al-aprendizaje-automático.html#cb1007-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), <span class="kw">length</span>(test_index), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Tenga en cuenta que estamos ignorando completamente el predictor y simplemente adivinando el sexo.</p>
<p>En las aplicaciones de aprendizaje automático, es útil usar factores para representar los resultados categóricos porque las funciones R desarrolladas para el aprendizaje automático, como las del paquete <strong>caret</strong>, requieren o recomiendan que los resultados categóricos se codifiquen como factores. Entonces convertir <code>y_hat</code> a factores que utilizan el <code>factor</code> función:</p>
<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1008-1"><a href="introducción-al-aprendizaje-automático.html#cb1008-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), <span class="kw">length</span>(test_index), <span class="dt">replace =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1008-2"><a href="introducción-al-aprendizaje-automático.html#cb1008-2"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span></code></pre></div>
<p>La precisión general se define simplemente como la proporción general que se predice correctamente:</p>
<div class="sourceCode" id="cb1009"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1009-1"><a href="introducción-al-aprendizaje-automático.html#cb1009-1"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1009-2"><a href="introducción-al-aprendizaje-automático.html#cb1009-2"></a><span class="co">#&gt; [1] 0.51</span></span></code></pre></div>
<p>No es sorprendente que nuestra precisión sea del 50%. Estamos adivinando!</p>
<p>¿Podemos hacerlo mejor? El análisis de datos exploratorios sugiere que podemos porque, en promedio, los machos son ligeramente más altos que las hembras:</p>
<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1010-1"><a href="introducción-al-aprendizaje-automático.html#cb1010-1"></a>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="kw">mean</span>(height), <span class="kw">sd</span>(height))</span>
<span id="cb1010-2"><a href="introducción-al-aprendizaje-automático.html#cb1010-2"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1010-3"><a href="introducción-al-aprendizaje-automático.html#cb1010-3"></a><span class="co">#&gt; # A tibble: 2 x 3</span></span>
<span id="cb1010-4"><a href="introducción-al-aprendizaje-automático.html#cb1010-4"></a><span class="co">#&gt;   sex    `mean(height)` `sd(height)`</span></span>
<span id="cb1010-5"><a href="introducción-al-aprendizaje-automático.html#cb1010-5"></a><span class="co">#&gt;   &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;</span></span>
<span id="cb1010-6"><a href="introducción-al-aprendizaje-automático.html#cb1010-6"></a><span class="co">#&gt; 1 Female           64.9         3.76</span></span>
<span id="cb1010-7"><a href="introducción-al-aprendizaje-automático.html#cb1010-7"></a><span class="co">#&gt; 2 Male             69.3         3.61</span></span></code></pre></div>
<p>Pero, ¿cómo hacemos uso de esta idea? Probemos con otro enfoque simple: predecir <code>Male</code> si la altura está dentro de dos desviaciones estándar del hombre promedio:</p>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="introducción-al-aprendizaje-automático.html#cb1011-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">62</span>, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1011-2"><a href="introducción-al-aprendizaje-automático.html#cb1011-2"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span></code></pre></div>
<p>La precisión aumenta de 0,50 a aproximadamente 0,80:</p>
<div class="sourceCode" id="cb1012"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1012-1"><a href="introducción-al-aprendizaje-automático.html#cb1012-1"></a><span class="kw">mean</span>(y <span class="op">==</span><span class="st"> </span>y_hat)</span>
<span id="cb1012-2"><a href="introducción-al-aprendizaje-automático.html#cb1012-2"></a><span class="co">#&gt; [1] 0.793</span></span></code></pre></div>
<p>¿Pero podemos hacerlo aún mejor? En el ejemplo anterior, utilizamos un valor de corte de 62, pero podemos examinar la precisión obtenida para otros valores de corte y luego elegir el valor que proporcione los mejores resultados. Pero recuerde, <strong>es importante que optimicemos el límite utilizando solo el conjunto de entrenamiento</strong>: el conjunto de prueba es solo para evaluación. Aunque para este ejemplo simplista no es un gran problema, más adelante aprenderemos que evaluar un algoritmo en el conjunto de entrenamiento puede conducir a un sobreajuste, que a menudo resulta en evaluaciones peligrosamente sobre optimistas.</p>
<p>Aquí examinamos la precisión de 10 puntos de corte diferentes y elegimos el que produce el mejor resultado:</p>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="introducción-al-aprendizaje-automático.html#cb1013-1"></a>cutoff &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">61</span>, <span class="dv">70</span>)</span>
<span id="cb1013-2"><a href="introducción-al-aprendizaje-automático.html#cb1013-2"></a>accuracy &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(cutoff, <span class="cf">function</span>(x){</span>
<span id="cb1013-3"><a href="introducción-al-aprendizaje-automático.html#cb1013-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(train_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>x, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1013-4"><a href="introducción-al-aprendizaje-automático.html#cb1013-4"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1013-5"><a href="introducción-al-aprendizaje-automático.html#cb1013-5"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>train_set<span class="op">$</span>sex)</span>
<span id="cb1013-6"><a href="introducción-al-aprendizaje-automático.html#cb1013-6"></a>})</span></code></pre></div>
<p>Podemos hacer un gráfico que muestre la precisión obtenida en el conjunto de entrenamiento para hombres y mujeres:</p>
<p><img src="libro_files/figure-html/accuracy-vs-cutoff-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que el valor máximo es:</p>
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="introducción-al-aprendizaje-automático.html#cb1014-1"></a><span class="kw">max</span>(accuracy)</span>
<span id="cb1014-2"><a href="introducción-al-aprendizaje-automático.html#cb1014-2"></a><span class="co">#&gt; [1] 0.85</span></span></code></pre></div>
<p>que es mucho mayor que 0.5. El límite que resulta en esta precisión es:</p>
<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1015-1"><a href="introducción-al-aprendizaje-automático.html#cb1015-1"></a>best_cutoff &lt;-<span class="st"> </span>cutoff[<span class="kw">which.max</span>(accuracy)]</span>
<span id="cb1015-2"><a href="introducción-al-aprendizaje-automático.html#cb1015-2"></a>best_cutoff</span>
<span id="cb1015-3"><a href="introducción-al-aprendizaje-automático.html#cb1015-3"></a><span class="co">#&gt; [1] 64</span></span></code></pre></div>
<p>Ahora podemos probar este límite en nuestro conjunto de pruebas para asegurarnos de que nuestra precisión no sea demasiado optimista:</p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="introducción-al-aprendizaje-automático.html#cb1016-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(test_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>best_cutoff, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1016-2"><a href="introducción-al-aprendizaje-automático.html#cb1016-2"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1016-3"><a href="introducción-al-aprendizaje-automático.html#cb1016-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">factor</span>(y_hat)</span>
<span id="cb1016-4"><a href="introducción-al-aprendizaje-automático.html#cb1016-4"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1016-5"><a href="introducción-al-aprendizaje-automático.html#cb1016-5"></a><span class="co">#&gt; [1] 0.804</span></span></code></pre></div>
<p>Vemos que es un poco más bajo que la precisión observada para el conjunto de entrenamiento, pero aún es mejor que adivinar. Y al probar en un conjunto de datos en el que no entrenamos, sabemos que nuestro resultado no se debe a que se haya elegido un buen resultado.</p>
</div>
<div id="la-matriz-de-confusión" class="section level3">
<h3><span class="header-section-number">27.4.3</span> La matriz de confusión</h3>
<p>La regla de predicción que desarrollamos en la sección anterior predice <code>Male</code> si el alumno es más alto que 64 pulgadas. Dado que la mujer promedio es aproximadamente 64 pulgadas, esta regla de predicción parece incorrecta. ¿Que pasó? Si un estudiante es la altura de la mujer promedio, ¿no deberíamos predecir <code>Female</code>?</p>
<p>En términos generales, la precisión general puede ser una medida engañosa. Para ver esto, comenzaremos construyendo lo que se conoce como <em>matriz de confusión</em>, que básicamente tabula cada combinación de predicción y valor real. Podemos hacer esto en R usando la función <code>table</code>:</p>
<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1017-1"><a href="introducción-al-aprendizaje-automático.html#cb1017-1"></a><span class="kw">table</span>(<span class="dt">predicted =</span> y_hat, <span class="dt">actual =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1017-2"><a href="introducción-al-aprendizaje-automático.html#cb1017-2"></a><span class="co">#&gt;          actual</span></span>
<span id="cb1017-3"><a href="introducción-al-aprendizaje-automático.html#cb1017-3"></a><span class="co">#&gt; predicted Female Male</span></span>
<span id="cb1017-4"><a href="introducción-al-aprendizaje-automático.html#cb1017-4"></a><span class="co">#&gt;    Female     48   32</span></span>
<span id="cb1017-5"><a href="introducción-al-aprendizaje-automático.html#cb1017-5"></a><span class="co">#&gt;    Male       71  374</span></span></code></pre></div>
<p>Si estudiamos esta tabla detenidamente, revela un problema. Si calculamos la precisión por separado para cada sexo, obtenemos:</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="introducción-al-aprendizaje-automático.html#cb1018-1"></a>test_set <span class="op">%&gt;%</span></span>
<span id="cb1018-2"><a href="introducción-al-aprendizaje-automático.html#cb1018-2"></a><span class="kw">mutate</span>(<span class="dt">y_hat =</span> y_hat) <span class="op">%&gt;%</span></span>
<span id="cb1018-3"><a href="introducción-al-aprendizaje-automático.html#cb1018-3"></a><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span></span>
<span id="cb1018-4"><a href="introducción-al-aprendizaje-automático.html#cb1018-4"></a><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>sex))</span>
<span id="cb1018-5"><a href="introducción-al-aprendizaje-automático.html#cb1018-5"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1018-6"><a href="introducción-al-aprendizaje-automático.html#cb1018-6"></a><span class="co">#&gt; # A tibble: 2 x 2</span></span>
<span id="cb1018-7"><a href="introducción-al-aprendizaje-automático.html#cb1018-7"></a><span class="co">#&gt;   sex    accuracy</span></span>
<span id="cb1018-8"><a href="introducción-al-aprendizaje-automático.html#cb1018-8"></a><span class="co">#&gt;   &lt;fct&gt;     &lt;dbl&gt;</span></span>
<span id="cb1018-9"><a href="introducción-al-aprendizaje-automático.html#cb1018-9"></a><span class="co">#&gt; 1 Female    0.403</span></span>
<span id="cb1018-10"><a href="introducción-al-aprendizaje-automático.html#cb1018-10"></a><span class="co">#&gt; 2 Male      0.921</span></span></code></pre></div>
<p>Hay un desequilibrio en la precisión para hombres y mujeres: se predice que demasiadas mujeres son hombres. ¡Estamos llamando a casi la mitad de las mujeres varones! ¿Cómo puede nuestra precisión general ser tan alta entonces? Esto se debe a que la <em>prevalencia</em> de los hombres en este conjunto de datos es alta. Estas alturas se obtuvieron de tres cursos de ciencias de datos, dos de los cuales tenían más hombres matriculados:</p>
<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1019-1"><a href="introducción-al-aprendizaje-automático.html#cb1019-1"></a>prev &lt;-<span class="st"> </span><span class="kw">mean</span>(y <span class="op">==</span><span class="st"> &quot;Male&quot;</span>)</span>
<span id="cb1019-2"><a href="introducción-al-aprendizaje-automático.html#cb1019-2"></a>prev</span>
<span id="cb1019-3"><a href="introducción-al-aprendizaje-automático.html#cb1019-3"></a><span class="co">#&gt; [1] 0.773</span></span></code></pre></div>
<p>Entonces, al calcular la precisión general, el alto porcentaje de errores cometidos para las mujeres se ve superado por las ganancias en las llamadas correctas para los hombres. ** Esto en realidad puede ser un gran problema en el aprendizaje automático. ** Si sus datos de entrenamiento están sesgados de alguna manera, es probable que también desarrolle algoritmos sesgados. El hecho de que hayamos utilizado un conjunto de pruebas no importa porque también se deriva del conjunto de datos sesgado original. Esta es una de las razones por las que observamos métricas distintas de la precisión general al evaluar un algoritmo de aprendizaje automático.</p>
<p>Hay varias métricas que podemos usar para evaluar un algoritmo de manera que la prevalencia no empañe nuestra evaluación, y todos estos pueden derivarse de la matriz de confusión. Una mejora general del uso de la precisión general es estudiar <em>sensibilidad_y_especificidad</em> por separado.</p>
</div>
<div id="sensibilidad-y-especificidad" class="section level3">
<h3><span class="header-section-number">27.4.4</span> Sensibilidad y especificidad</h3>
<p>Para definir la sensibilidad y la especificidad, necesitamos un resultado binario. Cuando los resultados son categóricos, podemos definir estos términos para una categoría específica. En el ejemplo de dígitos, podemos pedir la especificidad en el caso de predecir correctamente 2 en lugar de algún otro dígito. Una vez que especificamos una categoría de interés, podemos hablar sobre resultados positivos, <span class="math inline">\(Y=1\)</span> y resultados negativos <span class="math inline">\(Y=0\)</span>.</p>
<p>En general, <em>sensibilidad</em> se define como la capacidad de un algoritmo para predecir un resultado positivo cuando el resultado real es positivo: <span class="math inline">\(\hat{Y}=1\)</span> cuando <span class="math inline">\(Y=1\)</span>. Porque un algoritmo que llama a todo positivo ( <span class="math inline">\(\hat{Y}=1\)</span> pase lo que pase) tiene una sensibilidad perfecta, esta métrica por sí sola no es suficiente para juzgar un algoritmo. Por esta razón, también examinamos <em>especificidad</em>, que generalmente se define como la capacidad de un algoritmo para no predecir un resultado positivo <span class="math inline">\(\hat{Y}=0\)</span> cuando el resultado real no es positivo <span class="math inline">\(Y=0\)</span>. Podemos resumir de la siguiente manera:</p>
<ul>
<li>Alta sensibilidad: <span class="math inline">\(Y=1 \implies \hat{Y}=1\)</span></li>
<li>Alta especificidad: <span class="math inline">\(Y=0 \implies \hat{Y} = 0\)</span></li>
</ul>
<p>Aunque lo anterior a menudo se considera la definición de especificidad, otra forma de pensar en la especificidad es por la proporción de llamadas positivas que en realidad son positivas:</p>
<ul>
<li>Alta especificidad: <span class="math inline">\(\hat{Y}=1 \implies Y=1\)</span>.</li>
</ul>
<p>Para proporcionar definiciones precisas, nombramos las cuatro entradas de la matriz de confusión:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Actually Positive
</th>
<th style="text-align:left;">
Actually Negative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Predicted positive
</td>
<td style="text-align:left;">
True positives (TP)
</td>
<td style="text-align:left;">
False positives (FP)
</td>
</tr>
<tr>
<td style="text-align:left;">
Predicted negative
</td>
<td style="text-align:left;">
False negatives (FN)
</td>
<td style="text-align:left;">
True negatives (TN)
</td>
</tr>
</tbody>
</table>
<p>La sensibilidad se cuantifica típicamente por <span class="math inline">\(TP/(TP+FN)\)</span>, la proporción de positivos reales (la primera columna = <span class="math inline">\(TP+FN\)</span>) que se llaman positivos ( <span class="math inline">\(TP\)</span>) Esta cantidad se conoce como <em>la tasa positiva verdadera</em> (TPR) o <em>recall</em>.</p>
<p>La especificidad se define como <span class="math inline">\(TN/(TN+FP)\)</span> o la proporción de negativos (la segunda columna = <span class="math inline">\(FP+TN\)</span>) que se llaman negativos ( <span class="math inline">\(TN\)</span>) Esta cantidad también se denomina tasa negativa verdadera (TNR). Hay otra forma de cuantificar la especificidad que es <span class="math inline">\(TP/(TP+FP)\)</span> o la proporción de resultados llamados positivos (la primera fila o <span class="math inline">\(TP+FP\)</span>) que en realidad son positivos ( <span class="math inline">\(TP\)</span>) Esta cantidad se conoce como _valor predictivo positivo (PPV) <em>y también como_precisión</em>. Tenga en cuenta que, a diferencia de TPR y TNR, la precisión depende de la prevalencia, ya que una mayor prevalencia implica que puede obtener una mayor precisión incluso al adivinar.</p>
<p>Los nombres múltiples pueden ser confusos, por lo que incluimos una tabla para ayudarnos a recordar los términos. La tabla incluye una columna que muestra la definición si pensamos en las proporciones como probabilidades.</p>
<table>
<colgroup>
<col width="18%" />
<col width="10%" />
<col width="20%" />
<col width="16%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>El</th>
<th>Medida de</th>
<th>Nombre 1</th>
<th>Nombre 2</th>
<th>Definición</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sensibilidad</td>
<td>TPR</td>
<td>Recordar</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=1 \mid Y=1)\)</span></td>
</tr>
<tr class="even">
<td>especificidad</td>
<td>TNR</td>
<td>1-FPR</td>
<td><span class="math inline">\(\frac{\mbox{TN}}{\mbox{TN}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=0 \mid Y=0)\)</span></td>
</tr>
<tr class="odd">
<td>especificidad</td>
<td>PPV</td>
<td>Precisión</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(Y=1 \mid \hat{Y}=1)\)</span></td>
</tr>
</tbody>
</table>
<p>Aquí, TPR es tasa positiva verdadera, FPR es tasa positiva falsa y PPV es valor predictivo positivo.
La función <strong>caret</strong> <code>confusionMatrix</code> calcula todas estas métricas para nosotros una vez que definimos qué categoría es “positiva”. La función espera factores como entrada, y el primer nivel se considera el resultado positivo o <span class="math inline">\(Y=1\)</span>. En nuestro ejemplo, NA es el primer nivel porque viene antes NA alfabéticamente. Si escribe esto en R, verá varias métricas que incluyen precisión, sensibilidad, especificidad y VPP.</p>
<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1020-1"><a href="introducción-al-aprendizaje-automático.html#cb1020-1"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span></code></pre></div>
<p>Puede acceder a estos directamente, por ejemplo, así:</p>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="introducción-al-aprendizaje-automático.html#cb1021-1"></a>cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1021-2"><a href="introducción-al-aprendizaje-automático.html#cb1021-2"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1021-3"><a href="introducción-al-aprendizaje-automático.html#cb1021-3"></a><span class="co">#&gt;    0.804</span></span>
<span id="cb1021-4"><a href="introducción-al-aprendizaje-automático.html#cb1021-4"></a>cm<span class="op">$</span>byClass[<span class="kw">c</span>(<span class="st">&quot;Sensitivity&quot;</span>,<span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Prevalence&quot;</span>)]</span>
<span id="cb1021-5"><a href="introducción-al-aprendizaje-automático.html#cb1021-5"></a><span class="co">#&gt; Sensitivity Specificity  Prevalence </span></span>
<span id="cb1021-6"><a href="introducción-al-aprendizaje-automático.html#cb1021-6"></a><span class="co">#&gt;       0.403       0.921       0.227</span></span></code></pre></div>
<p>Podemos ver que la alta precisión general es posible a pesar de la sensibilidad relativamente baja. Como sugerimos anteriormente, la razón por la que esto sucede es debido a la baja prevalencia (0.23): la proporción de mujeres es baja. Debido a que la prevalencia es baja, no predecir las mujeres reales como mujeres (baja sensibilidad) no disminuye la precisión tanto como no predecir hombres reales como hombres (baja especificidad). Este es un ejemplo de por qué es importante examinar la sensibilidad y la especificidad y no solo la precisión. Antes de aplicar este algoritmo a conjuntos de datos generales, debemos preguntarnos si la prevalencia será la misma.</p>
</div>
<div id="precisión-equilibrada-y-f_1-puntuación" class="section level3">
<h3><span class="header-section-number">27.4.5</span> Precisión equilibrada y <span class="math inline">\(F_1\)</span> puntuación</h3>
<p>Aunque generalmente recomendamos estudiar tanto la especificidad como la sensibilidad, a menudo es útil tener un resumen de un número, por ejemplo, para fines de optimización. Una medida que se prefiere sobre la precisión general es el promedio de especificidad y sensibilidad, conocida como precisión equilibrada. Debido a que la especificidad y la sensibilidad son tasas, es más apropiado calcular el promedio <em>armónico</em>. De hecho, el _ <span class="math inline">\(F_1\)</span>-score_, un resumen de un número ampliamente utilizado, es el promedio armónico de precisión y recuperación:</p>
<p><span class="math display">\[
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} +
\frac{1}{\mbox{precision}}\right) }
\]</span></p>
<p>Debido a que es más fácil de escribir, a menudo ve este promedio armónico reescrito como:</p>
<p><span class="math display">\[
2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
\]</span></p>
<p>al definir <span class="math inline">\(F_1\)</span>.</p>
<p>Recuerde que, según el contexto, algunos tipos de errores son más costosos que otros. Por ejemplo, en el caso de la seguridad del avión, es mucho más importante maximizar la sensibilidad sobre la especificidad: no predecir el mal funcionamiento de un avión antes de que se estrelle es un error mucho más costoso que aterrizar un avión cuando, de hecho, el avión está en Perfecta condicion. En un caso criminal de asesinato capital, lo contrario es cierto ya que un falso positivo puede llevar a ejecutar a una persona inocente. los <span class="math inline">\(F_1\)</span>-score se puede adaptar para pesar la especificidad y la sensibilidad de manera diferente. Para hacer esto, definimos <span class="math inline">\(\beta\)</span> para representar cuánto más importante se compara la sensibilidad con la especificidad y considerar un promedio armónico ponderado:</p>
<p><span class="math display">\[
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} +
\frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
\]</span></p>
<p>Los <code>F_meas</code> la función en el paquete <strong>caret</strong> calcula este resumen con <code>beta</code> por defecto a 1.</p>
<p>Reconstruyamos nuestro algoritmo de predicción, pero esta vez maximizando el puntaje F en lugar de la precisión general:</p>
<div class="sourceCode" id="cb1022"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1022-1"><a href="introducción-al-aprendizaje-automático.html#cb1022-1"></a>cutoff &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">61</span>, <span class="dv">70</span>)</span>
<span id="cb1022-2"><a href="introducción-al-aprendizaje-automático.html#cb1022-2"></a>F_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(cutoff, <span class="cf">function</span>(x){</span>
<span id="cb1022-3"><a href="introducción-al-aprendizaje-automático.html#cb1022-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(train_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>x, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1022-4"><a href="introducción-al-aprendizaje-automático.html#cb1022-4"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1022-5"><a href="introducción-al-aprendizaje-automático.html#cb1022-5"></a><span class="kw">F_meas</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> <span class="kw">factor</span>(train_set<span class="op">$</span>sex))</span>
<span id="cb1022-6"><a href="introducción-al-aprendizaje-automático.html#cb1022-6"></a>})</span></code></pre></div>
<p>Como antes, podemos trazar estos <span class="math inline">\(F_1\)</span> medidas versus los límites:</p>
<p><img src="libro_files/figure-html/f_1-vs-cutoff-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que se maximiza en <span class="math inline">\(F_1\)</span> valor de:</p>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="introducción-al-aprendizaje-automático.html#cb1023-1"></a><span class="kw">max</span>(F_<span class="dv">1</span>)</span>
<span id="cb1023-2"><a href="introducción-al-aprendizaje-automático.html#cb1023-2"></a><span class="co">#&gt; [1] 0.647</span></span></code></pre></div>
<p>Este máximo se logra cuando usamos el siguiente límite:</p>
<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1024-1"><a href="introducción-al-aprendizaje-automático.html#cb1024-1"></a>best_cutoff &lt;-<span class="st"> </span>cutoff[<span class="kw">which.max</span>(F_<span class="dv">1</span>)]</span>
<span id="cb1024-2"><a href="introducción-al-aprendizaje-automático.html#cb1024-2"></a>best_cutoff</span>
<span id="cb1024-3"><a href="introducción-al-aprendizaje-automático.html#cb1024-3"></a><span class="co">#&gt; [1] 66</span></span></code></pre></div>
<p>Un límite de 65 tiene más sentido que 64. Además, equilibra la especificidad y la sensibilidad de nuestra matriz de confusión:</p>
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="introducción-al-aprendizaje-automático.html#cb1025-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(test_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>best_cutoff, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1025-2"><a href="introducción-al-aprendizaje-automático.html#cb1025-2"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1025-3"><a href="introducción-al-aprendizaje-automático.html#cb1025-3"></a><span class="kw">sensitivity</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1025-4"><a href="introducción-al-aprendizaje-automático.html#cb1025-4"></a><span class="co">#&gt; [1] 0.63</span></span>
<span id="cb1025-5"><a href="introducción-al-aprendizaje-automático.html#cb1025-5"></a><span class="kw">specificity</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1025-6"><a href="introducción-al-aprendizaje-automático.html#cb1025-6"></a><span class="co">#&gt; [1] 0.833</span></span></code></pre></div>
<p>Ahora vemos que hacemos mucho mejor que adivinar, que tanto la sensibilidad como la especificidad son relativamente altas, y que hemos construido nuestro primer algoritmo de aprendizaje automático. Se necesita altura como predictor y predice mujeres si tienes 65 pulgadas o menos.</p>
</div>
<div id="la-prevalencia-importa-en-la-práctica" class="section level3">
<h3><span class="header-section-number">27.4.6</span> La prevalencia importa en la práctica</h3>
<p>Un algoritmo de aprendizaje automático con una sensibilidad y especificidad muy altas puede no ser útil en la práctica cuando la prevalencia es cercana a 0 o 1. Para ver esto, considere el caso de un médico que se especialice en una enfermedad rara y esté interesado en desarrollar un algoritmo para prediciendo quién tiene la enfermedad. El médico comparte datos con usted y luego desarrolla un algoritmo con una sensibilidad muy alta. Explica que esto significa que si un paciente tiene la enfermedad, es muy probable que el algoritmo prediga correctamente. También le dice al médico que también está preocupado porque, según el conjunto de datos que analizó, la mitad de los pacientes tienen la enfermedad: <span class="math inline">\(\mbox{Pr}(\hat{Y}=1)\)</span>. El médico no está preocupado ni impresionado y explica que lo importante es la precisión de la prueba: <span class="math inline">\(\mbox{Pr}(Y=1 | \hat{Y}=1)\)</span>. Usando el teorema de Bayes, podemos conectar las dos medidas:</p>
<p><span class="math display">\[ \mbox{Pr}(Y = 1\mid \hat{Y}=1) = \mbox{Pr}(\hat{Y}=1 \mid Y=1) \frac{\mbox{Pr}(Y=1)}{\mbox{Pr}(\hat{Y}=1)}\]</span></p>
<p>El médico sabe que la prevalencia de la enfermedad es de 5 en 1,000, lo que implica que <span class="math inline">\(\mbox{Pr}(Y=1) \,/ \,\mbox{Pr}(\hat{Y}=1) = 1/100\)</span> y, por lo tanto, la precisión de su algoritmo es inferior a 0.01. El médico no tiene mucho uso para su algoritmo.</p>
</div>
<div id="roc-y-curvas-de-recuperación-de-precisión" class="section level3">
<h3><span class="header-section-number">27.4.7</span> ROC y curvas de recuperación de precisión</h3>
<p>Al comparar los dos métodos (adivinar versus usar un límite de altura), observamos la precisión y <span class="math inline">\(F_1\)</span>. El segundo método claramente superó al primero. Sin embargo, si bien consideramos varios puntos de corte para el segundo método, para el primero solo consideramos un enfoque: adivinar con igual probabilidad. Tenga en cuenta que adivinar <code>Male</code> con mayor probabilidad nos daría una mayor precisión debido al sesgo en la muestra:</p>
<div class="sourceCode" id="cb1026"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1026-1"><a href="introducción-al-aprendizaje-automático.html#cb1026-1"></a>p &lt;-<span class="st"> </span><span class="fl">0.9</span></span>
<span id="cb1026-2"><a href="introducción-al-aprendizaje-automático.html#cb1026-2"></a>n &lt;-<span class="st"> </span><span class="kw">length</span>(test_index)</span>
<span id="cb1026-3"><a href="introducción-al-aprendizaje-automático.html#cb1026-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(p, <span class="dv">1</span><span class="op">-</span>p)) <span class="op">%&gt;%</span></span>
<span id="cb1026-4"><a href="introducción-al-aprendizaje-automático.html#cb1026-4"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1026-5"><a href="introducción-al-aprendizaje-automático.html#cb1026-5"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1026-6"><a href="introducción-al-aprendizaje-automático.html#cb1026-6"></a><span class="co">#&gt; [1] 0.739</span></span></code></pre></div>
<p>Pero, como se describió anteriormente, esto tendría el costo de una menor sensibilidad. Las curvas que describimos en esta sección nos ayudarán a ver esto.</p>
<p>Recuerde que para cada uno de estos parámetros, podemos obtener una sensibilidad y especificidad diferentes. Por esta razón, un enfoque muy común para evaluar métodos es compararlos gráficamente trazando ambos.</p>
<p>Una trama ampliamente utilizada que hace esto es la curva de características operativas del receptor (ROC). Si se pregunta de dónde viene este nombre, puede consultar el
Página de Wikipedia ROC<a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a>.</p>
<p>La curva ROC representa la sensibilidad (TPR) frente a la especificidad 1 o la tasa de falsos positivos (FPR). Aquí calculamos el TPR y el FPR necesarios para diferentes probabilidades de adivinar hombres:</p>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="introducción-al-aprendizaje-automático.html#cb1027-1"></a>probs &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb1027-2"><a href="introducción-al-aprendizaje-automático.html#cb1027-2"></a>guessing &lt;-<span class="st"> </span><span class="kw">map_df</span>(probs, <span class="cf">function</span>(p){</span>
<span id="cb1027-3"><a href="introducción-al-aprendizaje-automático.html#cb1027-3"></a>y_hat &lt;-</span>
<span id="cb1027-4"><a href="introducción-al-aprendizaje-automático.html#cb1027-4"></a><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(p, <span class="dv">1</span><span class="op">-</span>p)) <span class="op">%&gt;%</span></span>
<span id="cb1027-5"><a href="introducción-al-aprendizaje-automático.html#cb1027-5"></a><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>))</span>
<span id="cb1027-6"><a href="introducción-al-aprendizaje-automático.html#cb1027-6"></a><span class="kw">list</span>(<span class="dt">method =</span> <span class="st">&quot;Guessing&quot;</span>,</span>
<span id="cb1027-7"><a href="introducción-al-aprendizaje-automático.html#cb1027-7"></a><span class="dt">FPR =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">specificity</span>(y_hat, test_set<span class="op">$</span>sex),</span>
<span id="cb1027-8"><a href="introducción-al-aprendizaje-automático.html#cb1027-8"></a><span class="dt">TPR =</span> <span class="kw">sensitivity</span>(y_hat, test_set<span class="op">$</span>sex))</span>
<span id="cb1027-9"><a href="introducción-al-aprendizaje-automático.html#cb1027-9"></a>})</span></code></pre></div>
<p>Podemos usar un código similar para calcular estos valores para nuestro segundo enfoque. Al trazar ambas curvas juntas, podemos comparar la sensibilidad para diferentes valores de especificidad:</p>
<!--We can construct an ROC curve for the height-based approach:-->
<!--
<img src="libro_files/figure-html/roc-2-1.png" width="70%" style="display: block; margin: auto;" />
-->
<p><img src="libro_files/figure-html/roc-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Podemos ver que obtenemos una mayor sensibilidad con este enfoque para todos los valores de especificidad, lo que implica que, de hecho, es un método mejor. Tenga en cuenta que las curvas ROC para adivinar siempre caen en la línea de identificación. También tenga en cuenta que al hacer curvas ROC, a menudo es bueno agregar el límite asociado con cada punto.</p>
<p>Los paquetes __pROC__y<strong>plotROC</strong> son útiles para generar estos gráficos.</p>
<p>Las curvas ROC tienen una debilidad y es que ninguna de las medidas trazadas depende de la prevalencia. En los casos en que la prevalencia es importante, en su lugar podemos hacer un gráfico de recuerdo preciso. La idea es similar, pero en cambio graficamos la precisión con el recuerdo:</p>
<p><img src="libro_files/figure-html/precision-recall-1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>De esta trama vemos de inmediato que la precisión de adivinar no es alta. Esto se debe a que la prevalencia es baja. También vemos que si cambiamos los positivos para que signifiquen Masculino en lugar de Femenino, la curva ROC permanece igual, pero la gráfica de recuperación precisa cambia.</p>
</div>
<div id="loss-function" class="section level3">
<h3><span class="header-section-number">27.4.8</span> La función de pérdida</h3>
<p>Hasta ahora hemos descrito métricas de evaluación que se aplican exclusivamente a datos categóricos.
Específicamente, para los resultados binarios, hemos descrito cómo la sensibilidad, especificidad, precisión y <span class="math inline">\(F_1\)</span> se puede utilizar como cuantificación. Sin embargo, estas métricas no son útiles para resultados continuos. En esta sección, describimos cómo el enfoque general para definir “mejor” en el aprendizaje automático es definir una <em>función de pérdida</em>, que puede aplicarse tanto a datos categóricos como continuos.</p>
<p>La función de pérdida más utilizada es la función de pérdida al cuadrado. Si <span class="math inline">\(\hat{y}\)</span> es nuestro predictor y <span class="math inline">\(y\)</span> es el resultado observado, la función de pérdida al cuadrado es simplemente:</p>
<p><span class="math display">\[
(\hat{y} - y)^2
\]</span></p>
<p>Debido a que a menudo tenemos un conjunto de pruebas con muchas observaciones, digamos <span class="math inline">\(N\)</span>, usamos el error cuadrático medio (MSE):</p>
<p><span class="math display">\[
\mbox{MSE} = \frac{1}{N} \mbox{RSS} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span></p>
<p>En la práctica, a menudo informamos el error cuadrático medio (RMSE), que es <span class="math inline">\(\sqrt{\mbox{MSE}}\)</span>, porque está en las mismas unidades que los resultados. Pero hacer las matemáticas a menudo es más fácil con el MSE y, por lo tanto, se usa más comúnmente en los libros de texto, ya que estos generalmente describen las propiedades teóricas de los algoritmos.</p>
<p>Si los resultados son binarios, tanto RMSE como MSE son equivalentes a una precisión menos, ya que <span class="math inline">\((\hat{y} - y)^2\)</span> es 0 si la predicción fue correcta y 1 en caso contrario. En general, nuestro objetivo es construir un algoritmo que minimice la pérdida para que esté lo más cerca posible de 0.</p>
<p>Debido a que nuestros datos son generalmente una muestra aleatoria, podemos pensar en el MSE como una variable aleatoria y el MSE observado puede considerarse como una estimación del MSE esperado, que en notación matemática escribimos así:</p>
<p><span class="math display">\[
\mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
\]</span></p>
<p>Este es un concepto teórico porque en la práctica solo tenemos un conjunto de datos para trabajar. Pero en teoría, pensamos en tener una gran cantidad de muestras aleatorias (llámelo <span class="math inline">\(B\)</span>), aplique nuestro algoritmo a cada uno, obtenga un MSE para cada muestra aleatoria y piense en el MSE esperado como:</p>
<p><span class="math display">\[
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2
\]</span></p>
<p>con <span class="math inline">\(y_{i}^b\)</span> denotando el <span class="math inline">\(i\)</span> observación en el <span class="math inline">\(b\)</span> th muestra aleatoria y <span class="math inline">\(\hat{y}_i^b\)</span> la predicción resultante obtenida de aplicar exactamente el mismo algoritmo al <span class="math inline">\(b\)</span> muestra aleatoria. Nuevamente, en la práctica solo observamos una muestra aleatoria, por lo que el MSE esperado es solo teórico. Sin embargo, en el capítulo <a href="cross-validation.html#cross-validation">29</a> describimos un enfoque para estimar el MSE que trata de imitar esta cantidad teórica.</p>
<p>Tenga en cuenta que hay funciones de pérdida distintas de la pérdida al cuadrado. Por ejemplo, el <em>Mean Absolute Error</em> utiliza valores absolutos, <span class="math inline">\(|\hat{Y}_i - Y_i|\)</span> en lugar de cuadrar los errores
<span class="math inline">\((\hat{Y}_i - Y_i)^2\)</span>. Sin embargo, en este libro nos enfocamos en minimizar la pérdida cuadrada ya que es la más utilizada.</p>
</div>
</div>
<div id="ejercicios-45" class="section level2">
<h2><span class="header-section-number">27.5</span> Ejercicios</h2>
<p>Los <code>reported_height</code> y <code>height</code> se recopilaron conjuntos de datos de tres clases impartidas en los Departamentos de Informática y Bioestadística, así como de forma remota a través de la Escuela de Extensión. La clase de bioestadística se impartió en 2016 junto con una versión en línea ofrecida por la Escuela de Extensión. El 25 de enero de 2016 a las 8:15 a.m., durante una de las conferencias, los instructores pidieron a los estudiantes que completaran el cuestionario de sexo y altura que poblaba el <code>reported_height</code> conjunto de datos Los estudiantes en línea completaron la encuesta durante los próximos días, después de que la conferencia se publicara en línea. Podemos usar esta información para definir una variable, llamarla <code>type</code>, para denotar el tipo de estudiante: <code>inclass</code> o <code>online</code>:</p>
<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1028-1"><a href="introducción-al-aprendizaje-automático.html#cb1028-1"></a><span class="kw">library</span>(lubridate)</span>
<span id="cb1028-2"><a href="introducción-al-aprendizaje-automático.html#cb1028-2"></a><span class="kw">data</span>(<span class="st">&quot;reported_heights&quot;</span>)</span>
<span id="cb1028-3"><a href="introducción-al-aprendizaje-automático.html#cb1028-3"></a>dat &lt;-<span class="st"> </span><span class="kw">mutate</span>(reported_heights, <span class="dt">date_time =</span> <span class="kw">ymd_hms</span>(time_stamp)) <span class="op">%&gt;%</span></span>
<span id="cb1028-4"><a href="introducción-al-aprendizaje-automático.html#cb1028-4"></a><span class="kw">filter</span>(date_time <span class="op">&gt;=</span><span class="st"> </span><span class="kw">make_date</span>(<span class="dv">2016</span>, <span class="dv">01</span>, <span class="dv">25</span>) <span class="op">&amp;</span></span>
<span id="cb1028-5"><a href="introducción-al-aprendizaje-automático.html#cb1028-5"></a>date_time <span class="op">&lt;</span><span class="st"> </span><span class="kw">make_date</span>(<span class="dv">2016</span>, <span class="dv">02</span>, <span class="dv">1</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1028-6"><a href="introducción-al-aprendizaje-automático.html#cb1028-6"></a><span class="kw">mutate</span>(<span class="dt">type =</span> <span class="kw">ifelse</span>(<span class="kw">day</span>(date_time) <span class="op">==</span><span class="st"> </span><span class="dv">25</span> <span class="op">&amp;</span><span class="st"> </span><span class="kw">hour</span>(date_time) <span class="op">==</span><span class="st"> </span><span class="dv">8</span> <span class="op">&amp;</span></span>
<span id="cb1028-7"><a href="introducción-al-aprendizaje-automático.html#cb1028-7"></a><span class="kw">between</span>(<span class="kw">minute</span>(date_time), <span class="dv">15</span>, <span class="dv">30</span>),</span>
<span id="cb1028-8"><a href="introducción-al-aprendizaje-automático.html#cb1028-8"></a><span class="st">&quot;inclass&quot;</span>, <span class="st">&quot;online&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(sex, type)</span>
<span id="cb1028-9"><a href="introducción-al-aprendizaje-automático.html#cb1028-9"></a>x &lt;-<span class="st"> </span>dat<span class="op">$</span>type</span>
<span id="cb1028-10"><a href="introducción-al-aprendizaje-automático.html#cb1028-10"></a>y &lt;-<span class="st"> </span><span class="kw">factor</span>(dat<span class="op">$</span>sex, <span class="kw">c</span>(<span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>))</span></code></pre></div>
<p>1. Mostrar estadísticas de resumen que indican que el <code>type</code> es predictivo del sexo.</p>
<p>2. En lugar de usar la altura para predecir el sexo, use el <code>type</code> variable.</p>
<p>3. Mostrar la matriz de confusión.</p>
<p>4. Utilizar el <code>confusionMatrix</code> funcionen en el paquete <strong>caret</strong> para informar la precisión.</p>
<p>5. Ahora usa el <code>sensitivity</code> y <code>specificity</code> funciones para informar especificidad y sensibilidad.</p>
<p>6. ¿Cuál es la prevalencia (% de mujeres) en el <code>dat</code> conjunto de datos definido anteriormente?</p>

</div>
<div id="probabilidades-y-expectativas-condicionales" class="section level2">
<h2><span class="header-section-number">27.6</span> Probabilidades y expectativas condicionales</h2>
<p>En las aplicaciones de aprendizaje automático, rara vez podemos predecir los resultados perfectamente. Por ejemplo, los detectores de spam a menudo pierden correos electrónicos que son claramente spam, Siri a menudo no entiende las palabras que estamos diciendo, y su banco a veces piensa que su tarjeta fue robada cuando no fue así. La razón más común para no poder construir algoritmos perfectos es que es imposible. Para ver esto, tenga en cuenta que la mayoría de los conjuntos de datos incluirán grupos de observaciones con los mismos valores exactos observados para todos los predictores, pero con diferentes resultados. Debido a que nuestras reglas de predicción son funciones, las entradas iguales (los predictores) implican salidas iguales (las predicciones). Por lo tanto, para un desafío en el que los mismos predictores se asocian con diferentes resultados en diferentes observaciones individuales, es imposible predecir correctamente para todos estos casos. Vimos un ejemplo simple de esto en la sección anterior: para cualquier altura dada <span class="math inline">\(x\)</span>, tendrás hombres y mujeres que son <span class="math inline">\(x\)</span> pulgadas de alto.</p>
<p>Sin embargo, nada de esto significa que no podamos construir algoritmos útiles que sean mucho mejores que adivinar, y en algunos casos mejor que las opiniones de expertos. Para lograr esto de manera óptima, hacemos uso de representaciones probabilísticas del problema basadas en las ideas presentadas en la Sección <a href="regression.html#conditional-expectation">17.3</a>. Las observaciones con los mismos valores observados para los predictores pueden no ser todas iguales, pero podemos suponer que todas tienen la misma probabilidad de esta clase o de esa clase. Escribiremos esta idea matemáticamente para el caso de datos categóricos.</p>
<div id="probabilidades-condicionales-1" class="section level3">
<h3><span class="header-section-number">27.6.1</span> Probabilidades condicionales</h3>
<p>Usamos la notación <span class="math inline">\((X_1 = x_1,\dots,X_p=x_p)\)</span> para representar el hecho de que hemos observado valores <span class="math inline">\(x_1,\dots,x_p\)</span> para covariables <span class="math inline">\(X_1, \dots, X_p\)</span>. Esto no implica que el resultado <span class="math inline">\(Y\)</span> tomará un valor específico. En cambio, implica una probabilidad específica. En particular, denotamos las <em>probabilidades condicionales</em> para cada clase <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\mbox{Pr}(Y=k \mid X_1 = x_1,\dots,X_p=x_p), \, \mbox{for}\,k=1,\dots,K
\]</span></p>
<p>Para evitar escribir todos los predictores, usaremos las letras en negrita como esta: <span class="math inline">\(\mathbf{X} \equiv (X_1,\dots,X_p)\)</span> y <span class="math inline">\(\mathbf{x} \equiv (x_1,\dots,x_p)\)</span>. También usaremos la siguiente notación para la probabilidad condicional de ser clase <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
\]</span></p>
<p>Nota: utilizaremos el <span class="math inline">\(p(x)\)</span> notación para representar probabilidades condicionales como funciones de los predictores. No lo confundas con el <span class="math inline">\(p\)</span> eso representa el número de predictores.</p>
<p>Estas probabilidades guían la construcción de un algoritmo que hace la mejor predicción: para cualquier <span class="math inline">\(\mathbf{x}\)</span>, vamos a predecir la clase <span class="math inline">\(k\)</span> con la mayor probabilidad entre <span class="math inline">\(p_1(x), p_2(x), \dots p_K(x)\)</span>. En notación matemática, lo escribimos así: <span class="math inline">\(\hat{Y} = \max_k p_k(\mathbf{x})\)</span>.</p>
<p>En el aprendizaje automático, nos referimos a esto como <em>Bayes 'Rule</em>. Pero tenga en cuenta que esta es una regla teórica ya que en la práctica no sabemos <span class="math inline">\(p_k(\mathbf{x}), k=1,\dots,K\)</span>. De hecho, estimar estas probabilidades condicionales puede considerarse como el principal desafío del aprendizaje automático. Cuanto mejores sean nuestras estimaciones de probabilidad <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span>, mejor será nuestro predictor:</p>
<p><span class="math display">\[\hat{Y} = \max_k \hat{p}_k(\mathbf{x})\]</span></p>
<p>Entonces, lo que predeciremos depende de dos cosas: 1) qué tan cerca están las <span class="math inline">\(\max_k p_k(\mathbf{x})\)</span> a 1 o 0 (certeza perfecta)
y 2) qué tan cerca están nuestras estimaciones <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span> son a <span class="math inline">\(p_k(\mathbf{x})\)</span>. No podemos hacer nada con respecto a la primera restricción, ya que está determinada por la naturaleza del problema, por lo que nuestra energía busca encontrar formas de estimar mejor las probabilidades condicionales. La primera restricción implica que tenemos límites en cuanto a qué tan bien puede funcionar incluso el mejor algoritmo posible. Debería acostumbrarse a la idea de que, si bien en algunos desafíos podremos lograr una precisión casi perfecta, con lectores de dígitos, por ejemplo, en otros nuestro éxito está restringido por la aleatoriedad del proceso, con recomendaciones de películas, por ejemplo.</p>
<p>Antes de continuar, es importante recordar que definir nuestra predicción maximizando la probabilidad no siempre es óptimo en la práctica y depende del contexto. Como se discutió anteriormente, la sensibilidad y la especificidad pueden diferir en importancia. Pero incluso en estos casos, tener una buena estimación de la <span class="math inline">\(p_k(x), k=1,\dots,K\)</span> nos bastará para construir modelos de predicción óptimos, ya que podemos controlar el equilibrio entre especificidad y sensibilidad como lo deseemos. Por ejemplo, simplemente podemos cambiar los límites utilizados para predecir un resultado u otro. En el ejemplo del plano, podemos aterrizar el plano en cualquier momento en que la probabilidad de mal funcionamiento sea superior a 1 en un millón, en lugar del 1/2 predeterminado utilizado cuando los tipos de error son igualmente indeseados.</p>
</div>
<div id="expectativas-condicionales" class="section level3">
<h3><span class="header-section-number">27.6.2</span> Expectativas condicionales</h3>
<p>Para datos binarios, puedes pensar en la probabilidad <span class="math inline">\(\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})\)</span> como la proporción de 1s en el estrato de la población para la cual <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span>. Muchos de los algoritmos que aprenderemos se pueden aplicar tanto a datos categóricos como continuos debido a la conexión entre las probabilidades condicionales y las expectativas condicionales.</p>
<p>Porque la expectativa es el promedio de los valores <span class="math inline">\(y_1,\dots,y_n\)</span> en la población, en el caso en que el <span class="math inline">\(y\)</span> s son 0 o 1, la expectativa es equivalente a la probabilidad de elegir aleatoriamente uno ya que el promedio es simplemente la proporción de unos:</p>
<p><span class="math display">\[
\mbox{E}(Y \mid \mathbf{X}=\mathbf{x})=\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}).
\]</span></p>
<p>Como resultado, a menudo solo usamos la expectativa para denotar tanto la probabilidad condicional como la expectativa condicional.</p>
<p>Al igual que con los resultados categóricos, en la mayoría de las aplicaciones, los mismos predictores observados no garantizan los mismos resultados continuos. En cambio, suponemos que el resultado sigue la misma distribución condicional. Ahora explicaremos por qué usamos la expectativa condicional para definir nuestros predictores.</p>
</div>
<div id="la-expectativa-condicional-minimiza-la-función-de-pérdida-al-cuadrado" class="section level3">
<h3><span class="header-section-number">27.6.3</span> La expectativa condicional minimiza la función de pérdida al cuadrado</h3>
<p>¿Por qué nos importa la expectativa condicional en el aprendizaje automático? Esto se debe a que el valor esperado tiene una propiedad matemática atractiva: minimiza el MSE. Específicamente, de todas las predicciones posibles. <span class="math inline">\(\hat{Y}\)</span>,</p>
<p><span class="math display">\[
\hat{Y} = \mbox{E}(Y \mid \mathbf{X}=\mathbf{x}) \, \mbox{ minimizes } \, \mbox{E}\{ (\hat{Y} - Y)^2 \mid \mathbf{X}=\mathbf{x} \}
\]</span></p>
<p>Debido a esta propiedad, una descripción sucinta de la tarea principal del aprendizaje automático es que utilizamos datos para estimar:</p>
<p><span class="math display">\[
f(\mathbf{x}) \equiv \mbox{E}( Y \mid \mathbf{X}=\mathbf{x} )
\]</span></p>
<p>para cualquier conjunto de características <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_p)\)</span>. Por supuesto, esto es más fácil decirlo que hacerlo, ya que esta función puede tomar cualquier forma y <span class="math inline">\(p\)</span> puede ser muy grande. Considere un caso en el que solo tenemos un predictor <span class="math inline">\(x\)</span>. La expectativa <span class="math inline">\(\mbox{E}\{ Y \mid X=x \}\)</span> puede ser cualquier función de <span class="math inline">\(x\)</span>: una línea, una parábola, una onda sinusoidal, una función de paso, cualquier cosa. Se vuelve aún más complicado cuando consideramos instancias con grandes <span class="math inline">\(p\)</span>, en ese caso <span class="math inline">\(f(\mathbf{x})\)</span> es una función de un vector multidimensional <span class="math inline">\(\mathbf{x}\)</span>. Por ejemplo, en nuestro ejemplo de lector de dígitos <span class="math inline">\(p = 784\)</span>! ** La principal forma en que los algoritmos competitivos de aprendizaje automático difieren es en su enfoque para estimar esta expectativa. **</p>
</div>
</div>
<div id="ejercicios-46" class="section level2">
<h2><span class="header-section-number">27.7</span> Ejercicios</h2>
<p>1. Calcule las probabilidades condicionales de ser hombre para el <code>heights</code> conjunto de datos Redondea las alturas a la pulgada más cercana. Trazar la probabilidad condicional estimada <span class="math inline">\(P(x) = \mbox{Pr}(\mbox{Male} | \mbox{height}=x)\)</span> para cada <span class="math inline">\(x\)</span>.</p>
<p>2. En la gráfica que acabamos de hacer, vemos una gran variabilidad para valores bajos de altura. Esto se debe a que tenemos pocos puntos de datos en estos estratos. Esta vez usa el <code>quantile</code> función para cuantiles <span class="math inline">\(0.1,0.2,\dots,0.9\)</span> y el NA función para asegurar que cada grupo tenga el mismo número de puntos. Sugerencia: para cualquier vector numérico NA, puede crear grupos basados en cuantiles como este:</p>
<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1029-1"><a href="introducción-al-aprendizaje-automático.html#cb1029-1"></a><span class="kw">cut</span>(x, <span class="kw">quantile</span>(x, <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)), <span class="dt">include.lowest =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>3. Genere datos a partir de una distribución normal bivariada utilizando el paquete <strong>MASS</strong> como este:</p>
<div class="sourceCode" id="cb1030"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1030-1"><a href="introducción-al-aprendizaje-automático.html#cb1030-1"></a>Sigma &lt;-<span class="st"> </span><span class="dv">9</span><span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1030-2"><a href="introducción-al-aprendizaje-automático.html#cb1030-2"></a>dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma) <span class="op">%&gt;%</span></span>
<span id="cb1030-3"><a href="introducción-al-aprendizaje-automático.html#cb1030-3"></a><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>))</span></code></pre></div>
<p>Puede hacer un diagrama rápido de los datos usando <code>plot(dat)</code>. Use un enfoque similar al ejercicio anterior para estimar las expectativas condicionales y hacer un diagrama.</p>

</div>
<div id="two-or-seven" class="section level2">
<h2><span class="header-section-number">27.8</span> Estudio de caso: ¿es un 2 o un 7?</h2>
<p>En los dos ejemplos simples anteriores, solo teníamos un predictor. En realidad, no consideramos estos desafíos de aprendizaje automático, que se caracterizan por casos con muchos predictores. Volvamos al ejemplo de dígitos en el que teníamos 784 predictores. Para fines ilustrativos, comenzaremos simplificando este problema a uno con dos predictores y dos clases. Específicamente, definimos el desafío como construir un algoritmo que pueda determinar si un dígito es un 2 o 7 de los predictores. No estamos del todo listos para construir algoritmos con predictores 784, por lo que extraeremos dos predictores simples del 784: la proporción de píxeles oscuros que están en el cuadrante superior izquierdo ( <span class="math inline">\(X_1\)</span>) y el cuadrante inferior derecho ( <span class="math inline">\(X_2\)</span>).</p>
<p>Luego seleccionamos una muestra aleatoria de 1,000 dígitos, 500 en el conjunto de entrenamiento y 500 en el conjunto de prueba. Proporcionamos este conjunto de datos en el <code>dslabs</code> paquete:</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="introducción-al-aprendizaje-automático.html#cb1031-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1031-2"><a href="introducción-al-aprendizaje-automático.html#cb1031-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1031-3"><a href="introducción-al-aprendizaje-automático.html#cb1031-3"></a><span class="kw">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</span></code></pre></div>
<p>Podemos explorar los datos trazando los dos predictores y usando colores para denotar las etiquetas:</p>
<div class="sourceCode" id="cb1032"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1032-1"><a href="introducción-al-aprendizaje-automático.html#cb1032-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">color =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="libro_files/figure-html/two-or-seven-scatter-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Podemos ver de inmediato algunos patrones. Por ejemplo, si <span class="math inline">\(X_1\)</span> (el panel superior izquierdo) es muy grande, entonces el dígito es probablemente un 7. Además, para valores más pequeños de <span class="math inline">\(X_1\)</span>, los 2 parecen estar en los valores de rango medio de <span class="math inline">\(X_2\)</span>.</p>
<p>Estas son las imágenes de los dígitos con los valores más grandes y más pequeños para <span class="math inline">\(X_1\)</span>:
Y aquí están las imágenes originales correspondientes al mayor y menor valor de <span class="math inline">\(X_2\)</span>:</p>
<p><img src="libro_files/figure-html/two-or-seven-images-large-x1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Podemos comenzar a tener una idea de por qué estos predictores son útiles, pero también por qué el problema será algo desafiante.</p>
<p>Realmente no hemos aprendido ningún algoritmo todavía, así que intentemos construir un algoritmo usando regresión. El modelo es simplemente:</p>
<p><span class="math display">\[
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2) =
\beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>Lo ajustamos así:</p>
<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1033-1"><a href="introducción-al-aprendizaje-automático.html#cb1033-1"></a>fit &lt;-<span class="st"> </span>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span></span>
<span id="cb1033-2"><a href="introducción-al-aprendizaje-automático.html#cb1033-2"></a><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">ifelse</span>(y<span class="op">==</span><span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1033-3"><a href="introducción-al-aprendizaje-automático.html#cb1033-3"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x_<span class="dv">2</span>, <span class="dt">data =</span> .)</span></code></pre></div>
<p>Ahora podemos construir una regla de decisión basada en la estimación de <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>:</p>
<div class="sourceCode" id="cb1034"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1034-1"><a href="introducción-al-aprendizaje-automático.html#cb1034-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1034-2"><a href="introducción-al-aprendizaje-automático.html#cb1034-2"></a>p_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newdata =</span> mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb1034-3"><a href="introducción-al-aprendizaje-automático.html#cb1034-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(p_hat <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">7</span>, <span class="dv">2</span>))</span>
<span id="cb1034-4"><a href="introducción-al-aprendizaje-automático.html#cb1034-4"></a><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[[<span class="st">&quot;Accuracy&quot;</span>]]</span>
<span id="cb1034-5"><a href="introducción-al-aprendizaje-automático.html#cb1034-5"></a><span class="co">#&gt; [1] 0.75</span></span></code></pre></div>
<p>Obtenemos una precisión muy superior al 50%. No está mal para nuestro primer intento. ¿Pero podemos hacerlo mejor?</p>
<p>Porque construimos el <code>mnist_27</code> ejemplo y teníamos a nuestra disposición 60,000 dígitos solo en el conjunto de datos MNIST, lo usamos para construir la distribución condicional <em>verdadera</em> <span class="math inline">\(p(x_1, x_2)\)</span>. Tenga en cuenta que esto es algo a lo que no tenemos acceso en la práctica, pero lo incluimos en este ejemplo porque permite la comparación de <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> a la verdad <span class="math inline">\(p(x_1, x_2)\)</span>. Esta comparación nos enseña las limitaciones de diferentes algoritmos. Hagamos eso aquí. Hemos almacenado lo verdadero <span class="math inline">\(p(x_1,x_2)\)</span> en el NA objeto y puede trazar la imagen usando la función <strong>ggplot2</strong> NA.
Elegimos mejores colores y usamos el <code>stat_contour</code> función para dibujar una curva que separe pares <span class="math inline">\((x_1,x_2)\)</span> para cual <span class="math inline">\(p(x_1,x_2) &gt; 0.5\)</span> y pares para los cuales <span class="math inline">\(p(x_1,x_2) &lt; 0.5\)</span>:</p>
<div class="sourceCode" id="cb1035"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1035-1"><a href="introducción-al-aprendizaje-automático.html#cb1035-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>true_p <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">z =</span> p, <span class="dt">fill =</span> p)) <span class="op">+</span></span>
<span id="cb1035-2"><a href="introducción-al-aprendizaje-automático.html#cb1035-2"></a><span class="kw">geom_raster</span>() <span class="op">+</span></span>
<span id="cb1035-3"><a href="introducción-al-aprendizaje-automático.html#cb1035-3"></a><span class="kw">scale_fill_gradientn</span>(<span class="dt">colors=</span><span class="kw">c</span>(<span class="st">&quot;#F8766D&quot;</span>, <span class="st">&quot;white&quot;</span>, <span class="st">&quot;#00BFC4&quot;</span>)) <span class="op">+</span></span>
<span id="cb1035-4"><a href="introducción-al-aprendizaje-automático.html#cb1035-4"></a><span class="kw">stat_contour</span>(<span class="dt">breaks=</span><span class="kw">c</span>(<span class="fl">0.5</span>), <span class="dt">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/true-p-better-colors-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Arriba ves una trama de lo verdadero <span class="math inline">\(p(x,y)\)</span>. Para comenzar a comprender las limitaciones de la regresión logística aquí, primero tenga en cuenta que con la regresión logística <span class="math inline">\(\hat{p}(x,y)\)</span> tiene que ser un plano y, como resultado, el límite definido por la regla de decisión viene dado por:
<span class="math inline">\(\hat{p}(x,y) = 0.5\)</span>, lo que implica que el límite no puede ser otra cosa que una línea recta:</p>
<p><span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
x_2 = (0.5-\hat{\beta}_0)/\hat{\beta}_2 -\hat{\beta}_1/\hat{\beta}_2 x_1
\]</span>
Tenga en cuenta que para este límite, <span class="math inline">\(x_2\)</span> es una función lineal de <span class="math inline">\(x_1\)</span>. Esto implica que nuestro enfoque de regresión logística no tiene posibilidades de capturar la naturaleza no lineal de la verdadera <span class="math inline">\(p(x_1,x_2)\)</span>. A continuación se muestra una representación visual de <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>. Utilizamos el <code>squish</code> funcionan desde el paquete <strong>escalas</strong> para restringir las estimaciones entre 0 y 1. Podemos ver dónde se cometieron los errores al mostrar también los datos y el límite. Principalmente provienen de valores bajos <span class="math inline">\(x_1\)</span> que tienen un valor alto o bajo de <span class="math inline">\(x_2\)</span>. La regresión no puede atrapar esto.</p>
<p><img src="libro_files/figure-html/regression-p-hat-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Necesitamos algo más flexible: un método que permita estimaciones con formas distintas a un plano.</p>
<p>Vamos a aprender algunos algoritmos nuevos basados en diferentes ideas y conceptos. Pero lo que todos tienen en común es que permiten enfoques más flexibles. Comenzaremos describiendo el vecino más cercano y los enfoques del núcleo. Para introducir los conceptos detrás de estos enfoques, comenzaremos nuevamente con un ejemplo unidimensional simple y describiremos el concepto de <em>smoothing</em>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="101">
<li id="fn101"><p><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" class="uri">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a><a href="introducción-al-aprendizaje-automático.html#fnref101" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="minería-de-textos.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="suavizado.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dslibro/edit/master/ml/intro-ml.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
