<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 27 Introducción a machine learning | Introducción a la Ciencia de Datos</title>
  <meta name="description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 27 Introducción a machine learning | Introducción a la Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 27 Introducción a machine learning | Introducción a la Ciencia de Datos" />
  
  <meta name="twitter:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2021-03-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="minería-de-textos.html"/>
<link rel="next" href="suavización.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introducción a la Ciencia de Datos</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="" data-path="agradecimientos.html"><a href="agradecimientos.html"><i class="fa fa-check"></i>Agradecimientos</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i>Introducción</a><ul>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#los-casos-de-estudio"><i class="fa fa-check"></i>Los casos de estudio</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#quién-encontrará-útil-este-libro"><i class="fa fa-check"></i>¿Quién encontrará útil este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#que-cubre-este-libro"><i class="fa fa-check"></i>¿Que cubre este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#qué-no-cubre-este-libro"><i class="fa fa-check"></i>¿Qué no cubre este libro?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Comenzando con R y RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#por-qué-r"><i class="fa fa-check"></i><b>1.1</b> ¿Por qué R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#la-consola-r"><i class="fa fa-check"></i><b>1.2</b> La consola R</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> <em>Scripts</em></a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#paneles"><i class="fa fa-check"></i><b>1.4.1</b> Paneles</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> <em>Key bindings</em></a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#cómo-ejecutar-comandos-mientras-edita-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Cómo ejecutar comandos mientras edita <em>scripts</em></a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#cómo-cambiar-las-opciones-globales"><i class="fa fa-check"></i><b>1.4.4</b> Cómo cambiar las opciones globales</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#instalación-de-paquetes-de-r"><i class="fa fa-check"></i><b>1.5</b> Instalación de paquetes de R</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> Lo básico de R</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#caso-de-estudio-los-asesinatos-con-armas-en-ee.-uu."><i class="fa fa-check"></i><b>2.1</b> Caso de estudio: los asesinatos con armas en EE. UU.</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#lo-básico"><i class="fa fa-check"></i><b>2.2</b> Lo básico</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objetos"><i class="fa fa-check"></i><b>2.2.1</b> Objetos</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#el-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.2</b> El espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#funciones"><i class="fa fa-check"></i><b>2.2.3</b> Funciones</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#otros-objetos-predefinidos"><i class="fa fa-check"></i><b>2.2.4</b> Otros objetos predefinidos</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#nombres-de-variables"><i class="fa fa-check"></i><b>2.2.5</b> Nombres de variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#cómo-guardar-su-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.6</b> Cómo guardar su espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#scripts-motivantes"><i class="fa fa-check"></i><b>2.2.7</b> <em>Scripts</em> motivantes</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#cómo-comentar-su-código"><i class="fa fa-check"></i><b>2.2.8</b> Cómo comentar su código</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#ejercicios"><i class="fa fa-check"></i><b>2.3</b> Ejercicios</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#tipos-de-datos"><i class="fa fa-check"></i><b>2.4</b> Tipos de datos</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> <em>data frames</em></a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#cómo-examinar-un-objeto"><i class="fa fa-check"></i><b>2.4.2</b> Cómo examinar un objeto</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#el-operador-de-acceso"><i class="fa fa-check"></i><b>2.4.3</b> El operador de acceso: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectores-numéricos-de-caracteres-y-lógicos"><i class="fa fa-check"></i><b>2.4.4</b> Vectores: numéricos, de caracteres y lógicos</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factores</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#listas"><i class="fa fa-check"></i><b>2.4.6</b> Listas</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#ejercicios-1"><i class="fa fa-check"></i><b>2.5</b> Ejercicios</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectores</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-vectores"><i class="fa fa-check"></i><b>2.6.1</b> Cómo crear vectores</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#nombres"><i class="fa fa-check"></i><b>2.6.2</b> Nombres</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#secuencias"><i class="fa fa-check"></i><b>2.6.3</b> Secuencias</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-un-subconjunto"><i class="fa fa-check"></i><b>2.6.4</b> Cómo crear un subconjunto</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#la-conversión-forzada"><i class="fa fa-check"></i><b>2.7</b> La conversión forzada</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-available-na"><i class="fa fa-check"></i><b>2.7.1</b> <em>Not available</em> (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#ejercicios-2"><i class="fa fa-check"></i><b>2.8</b> Ejercicios</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> <em>Sorting</em></a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-y-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> y <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#cuidado-con-el-reciclaje"><i class="fa fa-check"></i><b>2.9.5</b> Cuidado con el reciclaje</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#ejercicios-3"><i class="fa fa-check"></i><b>2.10</b> Ejercicios</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#aritmética-de-vectores"><i class="fa fa-check"></i><b>2.11</b> Aritmética de vectores</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-un-vector"><i class="fa fa-check"></i><b>2.11.1</b> <em>Rescaling</em> un vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#dos-vectores"><i class="fa fa-check"></i><b>2.11.2</b> Dos vectores</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#ejercicios-4"><i class="fa fa-check"></i><b>2.12</b> Ejercicios</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexación"><i class="fa fa-check"></i><b>2.13</b> Indexación</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#crear-subconjuntos-con-lógicos"><i class="fa fa-check"></i><b>2.13.1</b> Crear subconjuntos con lógicos</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#operadores-lógicos"><i class="fa fa-check"></i><b>2.13.2</b> Operadores lógicos</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#ejercicios-5"><i class="fa fa-check"></i><b>2.14</b> Ejercicios</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#gráficos-básicos"><i class="fa fa-check"></i><b>2.15</b> Gráficos básicos</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#ejercicios-6"><i class="fa fa-check"></i><b>2.16</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html"><i class="fa fa-check"></i><b>3</b> Conceptos básicos de programación</a><ul>
<li class="chapter" data-level="3.1" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Expresiones condicionales</a></li>
<li class="chapter" data-level="3.2" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#cómo-definir-funciones"><i class="fa fa-check"></i><b>3.2</b> Cómo definir funciones</a></li>
<li class="chapter" data-level="3.3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> <em>Namespaces</em></a></li>
<li class="chapter" data-level="3.4" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#bucles-for"><i class="fa fa-check"></i><b>3.4</b> Bucles-for</a></li>
<li class="chapter" data-level="3.5" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorización y funcionales</a></li>
<li class="chapter" data-level="3.6" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#ejercicios-7"><i class="fa fa-check"></i><b>3.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Data <em>tidy</em></a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-8"><i class="fa fa-check"></i><b>4.2</b> Ejercicios</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-manipular-los-data-frames"><i class="fa fa-check"></i><b>4.3</b> Cómo manipular los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-añadir-una-columna-con-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Cómo añadir una columna con <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-subconjuntos-con-filter"><i class="fa fa-check"></i><b>4.3.2</b> Cómo crear subconjuntos con <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-seleccionar-columnas-con-select"><i class="fa fa-check"></i><b>4.3.3</b> Cómo seleccionar columnas con <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-9"><i class="fa fa-check"></i><b>4.4</b> Ejercicios</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#el-pipe"><i class="fa fa-check"></i><b>4.5</b> El <em>pipe</em>: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-10"><i class="fa fa-check"></i><b>4.6</b> Ejercicios</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#cómo-resumir-datos"><i class="fa fa-check"></i><b>4.7</b> Cómo resumir datos</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Cómo agrupar y luego resumir con <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-los-data-frames"><i class="fa fa-check"></i><b>4.8</b> Cómo ordenar los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-anidadamente"><i class="fa fa-check"></i><b>4.8.1</b> Cómo ordenar anidadamente</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#los-primeros-n"><i class="fa fa-check"></i><b>4.8.2</b> Los primeros <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-11"><i class="fa fa-check"></i><b>4.9</b> Ejercicios</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> <em>Tibbles</em></a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-ven-mejor"><i class="fa fa-check"></i><b>4.10.1</b> Los <em>tibbles</em> se ven mejor</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#los-subconjuntos-de-tibbles-son-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Los subconjuntos de <em>tibbles</em> son <em>tibbles</em></a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-pueden-tener-entradas-complejas"><i class="fa fa-check"></i><b>4.10.3</b> Los <em>tibbles</em> pueden tener entradas complejas</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-pueden-agrupar"><i class="fa fa-check"></i><b>4.10.4</b> Los <em>tibbles</em> se pueden agrupar</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-un-tibble-usando-tibble-en-lugar-de-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Cómo crear un <em>tibble</em> usando <code>tibble</code> en lugar de <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#el-operador-punto"><i class="fa fa-check"></i><b>4.11</b> El operador punto</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#el-paquete-purrr"><i class="fa fa-check"></i><b>4.13</b> El paquete <strong>purrr</strong></a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#los-condicionales-de-tidyverse"><i class="fa fa-check"></i><b>4.14</b> Los condicionales de <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-12"><i class="fa fa-check"></i><b>4.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importando datos</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#las-rutas-y-el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1</b> Las rutas y el directorio de trabajo</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#el-sistema-de-archivos"><i class="fa fa-check"></i><b>5.1.1</b> El sistema de archivos</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#las-rutas-relativas-y-completas"><i class="fa fa-check"></i><b>5.1.2</b> Las rutas relativas y completas</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1.3</b> El directorio de trabajo</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#cómo-generar-los-nombres-de-ruta"><i class="fa fa-check"></i><b>5.1.4</b> Cómo generar los nombres de ruta</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#cómo-copiar-los-archivos-usando-rutas"><i class="fa fa-check"></i><b>5.1.5</b> Cómo copiar los archivos usando rutas</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#los-paquetes-readr-y-readxl"><i class="fa fa-check"></i><b>5.2</b> Los paquetes readr y readxl</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#ejercicios-13"><i class="fa fa-check"></i><b>5.3</b> Ejercicios</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#cómo-descargar-archivos"><i class="fa fa-check"></i><b>5.4</b> Cómo descargar archivos</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#las-funciones-de-importación-de-base-r"><i class="fa fa-check"></i><b>5.5</b> Las funciones de importación de base R</a><ul>
<li class="chapter" data-level="5.5.1" data-path="importing-data.html"><a href="importing-data.html#scan"><i class="fa fa-check"></i><b>5.5.1</b> <code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#archivos-de-texto-versus-archivos-binarios"><i class="fa fa-check"></i><b>5.6</b> Archivos de texto versus archivos binarios</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#cómo-organizar-datos-con-hojas-de-cálculo"><i class="fa fa-check"></i><b>5.8</b> Cómo organizar datos con hojas de cálculo</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#ejercicios-14"><i class="fa fa-check"></i><b>5.9</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>II Visualización de datos</b></span></li>
<li class="chapter" data-level="6" data-path="introducción-a-la-visualización-de-datos.html"><a href="introducción-a-la-visualización-de-datos.html"><i class="fa fa-check"></i><b>6</b> Introducción a la visualización de datos</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#los-componentes-de-un-gráfico"><i class="fa fa-check"></i><b>7.1</b> Los componentes de un gráfico</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#objetos-ggplot"><i class="fa fa-check"></i><b>7.2</b> objetos <code>ggplot</code></a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometrías"><i class="fa fa-check"></i><b>7.3</b> Geometrías</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos"><i class="fa fa-check"></i><b>7.4</b> Mapeos estéticos</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#capas"><i class="fa fa-check"></i><b>7.5</b> Capas</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#cómo-probar-varios-argumentos"><i class="fa fa-check"></i><b>7.5.1</b> Cómo probar varios argumentos</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos-globales-versus-locales"><i class="fa fa-check"></i><b>7.6</b> Mapeos estéticos globales versus locales</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#escalas"><i class="fa fa-check"></i><b>7.7</b> Escalas</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#etiquetas-y-títulos"><i class="fa fa-check"></i><b>7.8</b> Etiquetas y títulos</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categorías-como-colores"><i class="fa fa-check"></i><b>7.9</b> Categorías como colores</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#anotación-formas-y-ajustes"><i class="fa fa-check"></i><b>7.10</b> Anotación, formas y ajustes</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Paquetes complementarios</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#cómo-combinarlo-todo"><i class="fa fa-check"></i><b>7.12</b> Cómo combinarlo todo</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Gráficos rápidos con <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#cuadrículas-de-gráficos"><i class="fa fa-check"></i><b>7.14</b> Cuadrículas de gráficos</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#ejercicios-15"><i class="fa fa-check"></i><b>7.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Cómo visualizar distribuciones de datos</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#tipos-de-variables"><i class="fa fa-check"></i><b>8.1</b> Tipos de variables</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#estudio-de-caso-describiendo-alturas-de-estudiantes"><i class="fa fa-check"></i><b>8.2</b> Estudio de caso: describiendo alturas de estudiantes</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#la-función-de-distribución"><i class="fa fa-check"></i><b>8.3</b> La función de distribución</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Funciones de distribución acumulada</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histogramas"><i class="fa fa-check"></i><b>8.5</b> Histogramas</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#densidad-suave"><i class="fa fa-check"></i><b>8.6</b> Densidad suave</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#cómo-interpretar-el-eje-y"><i class="fa fa-check"></i><b>8.6.1</b> Cómo interpretar el eje-y</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densidades-permiten-estratificación"><i class="fa fa-check"></i><b>8.6.2</b> Densidades permiten estratificación</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#ejercicios-16"><i class="fa fa-check"></i><b>8.7</b> Ejercicios</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> La distribución normal</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#unidades-estándar"><i class="fa fa-check"></i><b>8.9</b> Unidades estándar</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#gráficos-q-q"><i class="fa fa-check"></i><b>8.10</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja"><i class="fa fa-check"></i><b>8.12</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Estratificación</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Estudio de caso: descripción de alturas de estudiantes (continuación)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#ejercicios-17"><i class="fa fa-check"></i><b>8.15</b> Ejercicios</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> Geometrías ggplot2</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#diagramas-de-barras"><i class="fa fa-check"></i><b>8.16.1</b> Diagramas de barras</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histogramas-1"><i class="fa fa-check"></i><b>8.16.2</b> Histogramas</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#gráficos-de-densidad"><i class="fa fa-check"></i><b>8.16.3</b> Gráficos de densidad</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja-1"><i class="fa fa-check"></i><b>8.16.4</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#gráficos-q-q-1"><i class="fa fa-check"></i><b>8.16.5</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#imágenes"><i class="fa fa-check"></i><b>8.16.6</b> Imágenes</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#gráficos-rápidos"><i class="fa fa-check"></i><b>8.16.7</b> Gráficos rápidos</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#ejercicios-18"><i class="fa fa-check"></i><b>8.17</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Visualización de datos en la práctica</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#estudio-de-caso-nuevas-ideas-sobre-la-pobreza"><i class="fa fa-check"></i><b>9.1</b> Estudio de caso: nuevas ideas sobre la pobreza</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#la-prueba-de-hans-rosling"><i class="fa fa-check"></i><b>9.1.1</b> La prueba de Hans Rosling</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-dispersión"><i class="fa fa-check"></i><b>9.2</b> Diagrama de dispersión</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#separar-en-facetas"><i class="fa fa-check"></i><b>9.3</b> Separar en facetas</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#escalas-fijas-para-mejores-comparaciones"><i class="fa fa-check"></i><b>9.3.2</b> Escalas fijas para mejores comparaciones</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#gráficos-de-series-de-tiempo"><i class="fa fa-check"></i><b>9.4</b> Gráficos de series de tiempo</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#etiquetas-en-lugar-de-leyendas"><i class="fa fa-check"></i><b>9.4.1</b> Etiquetas en lugar de leyendas</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#transformaciones-de-datos"><i class="fa fa-check"></i><b>9.5</b> Transformaciones de datos</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#transformación-logarítmica"><i class="fa fa-check"></i><b>9.5.1</b> Transformación logarítmica</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#qué-base"><i class="fa fa-check"></i><b>9.5.2</b> ¿Qué base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transformar-los-valores-o-la-escala"><i class="fa fa-check"></i><b>9.5.3</b> ¿Transformar los valores o la escala?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#cómo-visualizar-distribuciones-multimodales"><i class="fa fa-check"></i><b>9.6</b> Cómo visualizar distribuciones multimodales</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#cómo-comparar-múltiples-distribuciones-con-diagramas-de-caja-y-gráficos-ridge"><i class="fa fa-check"></i><b>9.7</b> Cómo comparar múltiples distribuciones con diagramas de caja y gráficos <em>ridge</em></a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-caja"><i class="fa fa-check"></i><b>9.7.1</b> Diagrama de caja</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#gráficos-ridge"><i class="fa fa-check"></i><b>9.7.2</b> Gráficos <em>ridge</em></a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#ejemplo-distribuciones-de-ingresos-de-1970-versus-2010"><i class="fa fa-check"></i><b>9.7.3</b> Ejemplo: distribuciones de ingresos de 1970 versus 2010</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#cómo-obtener-acceso-a-variables-calculadas"><i class="fa fa-check"></i><b>9.7.4</b> Cómo obtener acceso a variables calculadas</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#densidades-ponderadas"><i class="fa fa-check"></i><b>9.7.5</b> Densidades ponderadas</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#la-falacia-ecológica-y-la-importancia-de-mostrar-los-datos"><i class="fa fa-check"></i><b>9.8</b> La falacia ecológica y la importancia de mostrar los datos</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Transformación logística</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#mostrar-los-datos"><i class="fa fa-check"></i><b>9.8.2</b> Mostrar los datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html"><i class="fa fa-check"></i><b>10</b> Principios de visualización de datos</a><ul>
<li class="chapter" data-level="10.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-datos-utilizando-señales-visuales"><i class="fa fa-check"></i><b>10.1</b> Cómo codificar datos utilizando señales visuales</a></li>
<li class="chapter" data-level="10.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#sepa-cuándo-incluir-0"><i class="fa fa-check"></i><b>10.2</b> Sepa cuándo incluir 0</a></li>
<li class="chapter" data-level="10.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#no-distorsionar-cantidades"><i class="fa fa-check"></i><b>10.3</b> No distorsionar cantidades</a></li>
<li class="chapter" data-level="10.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ordenar-categorías-por-un-valor-significativo"><i class="fa fa-check"></i><b>10.4</b> Ordenar categorías por un valor significativo</a></li>
<li class="chapter" data-level="10.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#mostrar-los-datos-1"><i class="fa fa-check"></i><b>10.5</b> Mostrar los datos</a></li>
<li class="chapter" data-level="10.6" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-facilitar-comparaciones"><i class="fa fa-check"></i><b>10.6</b> Cómo facilitar comparaciones</a><ul>
<li class="chapter" data-level="10.6.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-ejes-comunes"><i class="fa fa-check"></i><b>10.6.1</b> Use ejes comunes</a></li>
<li class="chapter" data-level="10.6.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#alinee-gráficos-verticalmente-para-ver-cambios-horizontales-y-horizontalmente-para-ver-cambios-verticales"><i class="fa fa-check"></i><b>10.6.2</b> Alinee gráficos verticalmente para ver cambios horizontales y horizontalmente para ver cambios verticales</a></li>
<li class="chapter" data-level="10.6.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#considere-transformaciones"><i class="fa fa-check"></i><b>10.6.3</b> Considere transformaciones</a></li>
<li class="chapter" data-level="10.6.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#señales-visuales-comparadas-deben-estar-adyacentes"><i class="fa fa-check"></i><b>10.6.4</b> Señales visuales comparadas deben estar adyacentes</a></li>
<li class="chapter" data-level="10.6.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#piense-en-los-daltónicos"><i class="fa fa-check"></i><b>10.7</b> Piense en los daltónicos</a></li>
<li class="chapter" data-level="10.8" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráficos-para-dos-variables"><i class="fa fa-check"></i><b>10.8</b> Gráficos para dos variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráfico-bland-altman"><i class="fa fa-check"></i><b>10.8.2</b> Gráfico Bland-Altman</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-una-tercera-variable"><i class="fa fa-check"></i><b>10.9</b> Cómo codificar una tercera variable</a></li>
<li class="chapter" data-level="10.10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-los-gráficos-pseudo-tridimensionales"><i class="fa fa-check"></i><b>10.10</b> Evite los gráficos pseudo-tridimensionales</a></li>
<li class="chapter" data-level="10.11" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-demasiados-dígitos-significativos"><i class="fa fa-check"></i><b>10.11</b> Evite demasiados dígitos significativos</a></li>
<li class="chapter" data-level="10.12" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#conozca-a-su-audiencia"><i class="fa fa-check"></i><b>10.12</b> Conozca a su audiencia</a></li>
<li class="chapter" data-level="10.13" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-19"><i class="fa fa-check"></i><b>10.13</b> Ejercicios</a></li>
<li class="chapter" data-level="10.14" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Estudio de caso: las vacunas y las enfermedades infecciosas</a></li>
<li class="chapter" data-level="10.15" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-20"><i class="fa fa-check"></i><b>10.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Resúmenes robustos</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#valores-atípicos"><i class="fa fa-check"></i><b>11.1</b> Valores atípicos</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#mediana"><i class="fa fa-check"></i><b>11.2</b> Mediana</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#el-rango-intercuartil-iqr"><i class="fa fa-check"></i><b>11.3</b> El rango intercuartil (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#la-definición-de-tukey-de-un-valor-atípico"><i class="fa fa-check"></i><b>11.4</b> La definición de Tukey de un valor atípico</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#desviación-absoluta-mediana"><i class="fa fa-check"></i><b>11.5</b> Desviación absoluta mediana</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#ejercicios-21"><i class="fa fa-check"></i><b>11.6</b> Ejercicios</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#estudio-de-caso-alturas-autoreportadas-de-estudiantes"><i class="fa fa-check"></i><b>11.7</b> Estudio de caso: alturas autoreportadas de estudiantes</a></li>
</ul></li>
<li class="part"><span><b>III Estadísticas con R</b></span></li>
<li class="chapter" data-level="12" data-path="introducción-a-las-estadísticas-con-r.html"><a href="introducción-a-las-estadísticas-con-r.html"><i class="fa fa-check"></i><b>12</b> Introducción a las estadísticas con R</a></li>
<li class="chapter" data-level="13" data-path="probabilidad.html"><a href="probabilidad.html"><i class="fa fa-check"></i><b>13</b> Probabilidad</a><ul>
<li class="chapter" data-level="13.1" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-discreta"><i class="fa fa-check"></i><b>13.1</b> Probabilidad discreta</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probabilidad.html"><a href="probabilidad.html#frecuencia-relativa"><i class="fa fa-check"></i><b>13.1.1</b> Frecuencia relativa</a></li>
<li class="chapter" data-level="13.1.2" data-path="probabilidad.html"><a href="probabilidad.html#notación"><i class="fa fa-check"></i><b>13.1.2</b> Notación</a></li>
<li class="chapter" data-level="13.1.3" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>13.1.3</b> Distribuciones de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-datos-categóricos"><i class="fa fa-check"></i><b>13.2</b> Simulaciones Monte Carlo para datos categóricos</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probabilidad.html"><a href="probabilidad.html#fijar-la-semilla-aleatoria"><i class="fa fa-check"></i><b>13.2.1</b> Fijar la semilla aleatoria</a></li>
<li class="chapter" data-level="13.2.2" data-path="probabilidad.html"><a href="probabilidad.html#con-y-sin-reemplazo"><i class="fa fa-check"></i><b>13.2.2</b> Con y sin reemplazo</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probabilidad.html"><a href="probabilidad.html#independencia"><i class="fa fa-check"></i><b>13.3</b> Independencia</a></li>
<li class="chapter" data-level="13.4" data-path="probabilidad.html"><a href="probabilidad.html#probabilidades-condicionales"><i class="fa fa-check"></i><b>13.4</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="13.5" data-path="probabilidad.html"><a href="probabilidad.html#reglas-de-la-adición-y-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5</b> Reglas de la adición y de la multiplicación</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5.1</b> Regla de la multiplicación</a></li>
<li class="chapter" data-level="13.5.2" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación-bajo-independencia"><i class="fa fa-check"></i><b>13.5.2</b> Regla de la multiplicación bajo independencia</a></li>
<li class="chapter" data-level="13.5.3" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-adición"><i class="fa fa-check"></i><b>13.5.3</b> Regla de la adición</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probabilidad.html"><a href="probabilidad.html#combinaciones-y-permutaciones"><i class="fa fa-check"></i><b>13.6</b> Combinaciones y permutaciones</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-monte-carlo"><i class="fa fa-check"></i><b>13.6.1</b> Ejemplo Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos"><i class="fa fa-check"></i><b>13.7</b> Ejemplos</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probabilidad.html"><a href="probabilidad.html#problema-monty-hall"><i class="fa fa-check"></i><b>13.7.1</b> Problema Monty Hall</a></li>
<li class="chapter" data-level="13.7.2" data-path="probabilidad.html"><a href="probabilidad.html#problema-de-cumpleaños"><i class="fa fa-check"></i><b>13.7.2</b> Problema de cumpleaños</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probabilidad.html"><a href="probabilidad.html#infinito-en-la-práctica"><i class="fa fa-check"></i><b>13.8</b> Infinito en la práctica</a></li>
<li class="chapter" data-level="13.9" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-22"><i class="fa fa-check"></i><b>13.9</b> Ejercicios</a></li>
<li class="chapter" data-level="13.10" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-continua"><i class="fa fa-check"></i><b>13.10</b> Probabilidad continua</a></li>
<li class="chapter" data-level="13.11" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-continuas"><i class="fa fa-check"></i><b>13.11</b> Distribuciones teóricas continuas</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-como-aproximaciones"><i class="fa fa-check"></i><b>13.11.1</b> Distribuciones teóricas como aproximaciones</a></li>
<li class="chapter" data-level="13.11.2" data-path="probabilidad.html"><a href="probabilidad.html#la-densidad-de-probabilidad"><i class="fa fa-check"></i><b>13.11.2</b> La densidad de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-variables-continuas"><i class="fa fa-check"></i><b>13.12</b> Simulaciones Monte Carlo para variables continuas</a></li>
<li class="chapter" data-level="13.13" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-continuas"><i class="fa fa-check"></i><b>13.13</b> Distribuciones continuas</a></li>
<li class="chapter" data-level="13.14" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-23"><i class="fa fa-check"></i><b>13.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html"><i class="fa fa-check"></i><b>14</b> Variables aleatorias</a><ul>
<li class="chapter" data-level="14.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#variables-aleatorias-1"><i class="fa fa-check"></i><b>14.1</b> Variables aleatorias</a></li>
<li class="chapter" data-level="14.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#modelos-de-muestreo"><i class="fa fa-check"></i><b>14.2</b> Modelos de muestreo</a></li>
<li class="chapter" data-level="14.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#la-distribución-de-probabilidad-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>14.3</b> La distribución de probabilidad de una variable aleatoria</a></li>
<li class="chapter" data-level="14.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#distribuciones-versus-distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>14.4</b> Distribuciones versus distribuciones de probabilidad</a></li>
<li class="chapter" data-level="14.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#notación-para-variables-aleatorias"><i class="fa fa-check"></i><b>14.5</b> Notación para variables aleatorias</a></li>
<li class="chapter" data-level="14.6" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#el-valor-esperado-y-el-error-estándar"><i class="fa fa-check"></i><b>14.6</b> El valor esperado y el error estándar</a><ul>
<li class="chapter" data-level="14.6.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#población-sd-versus-la-muestra-sd"><i class="fa fa-check"></i><b>14.6.1</b> Población SD versus la muestra SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7</b> Teorema del límite central</a><ul>
<li class="chapter" data-level="14.7.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#cuán-grande-es-grande-en-el-teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7.1</b> ¿Cuán grande es grande en el teorema del límite central?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#propiedades-estadísticas-de-promedios"><i class="fa fa-check"></i><b>14.8</b> Propiedades estadísticas de promedios</a></li>
<li class="chapter" data-level="14.9" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ley-de-los-grandes-números"><i class="fa fa-check"></i><b>14.9</b> Ley de los grandes números</a><ul>
<li class="chapter" data-level="14.9.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#malinterpretando-la-ley-de-promedios"><i class="fa fa-check"></i><b>14.9.1</b> Malinterpretando la ley de promedios</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-24"><i class="fa fa-check"></i><b>14.10</b> Ejercicios</a></li>
<li class="chapter" data-level="14.11" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#estudio-de-caso-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Estudio de caso: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#tasas-de-interés-explicadas-con-modelo-de-oportunidad"><i class="fa fa-check"></i><b>14.11.1</b> Tasas de interés explicadas con modelo de oportunidad</a></li>
<li class="chapter" data-level="14.11.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-25"><i class="fa fa-check"></i><b>14.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Inferencia estadística</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#encuestas"><i class="fa fa-check"></i><b>15.1</b> Encuestas</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#el-modelo-de-muestreo-para-encuestas"><i class="fa fa-check"></i><b>15.1.1</b> El modelo de muestreo para encuestas</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#poblaciones-muestras-parámetros-y-estimaciones"><i class="fa fa-check"></i><b>15.2</b> Poblaciones, muestras, parámetros y estimaciones</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#el-promedio-de-la-muestra"><i class="fa fa-check"></i><b>15.2.1</b> El promedio de la muestra</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parámetros"><i class="fa fa-check"></i><b>15.2.2</b> Parámetros</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#encuesta-versus-pronóstico"><i class="fa fa-check"></i><b>15.2.3</b> Encuesta versus pronóstico</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#propiedades-de-nuestra-estimación-valor-esperado-y-error-estándar"><i class="fa fa-check"></i><b>15.2.4</b> Propiedades de nuestra estimación: valor esperado y error estándar</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#ejercicios-26"><i class="fa fa-check"></i><b>15.3</b> Ejercicios</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Teorema del límite central en la práctica</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo"><i class="fa fa-check"></i><b>15.4.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#la-diferencia"><i class="fa fa-check"></i><b>15.4.2</b> La diferencia</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#sesgo-por-qué-no-realizar-una-encuesta-bien-grande"><i class="fa fa-check"></i><b>15.4.3</b> Sesgo: ¿por qué no realizar una encuesta bien grande?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#ejercicios-27"><i class="fa fa-check"></i><b>15.5</b> Ejercicios</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#intervalos-de-confianza"><i class="fa fa-check"></i><b>15.6</b> Intervalos de confianza</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo-1"><i class="fa fa-check"></i><b>15.6.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#el-idioma-correcto"><i class="fa fa-check"></i><b>15.6.2</b> El idioma correcto</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#ejercicios-28"><i class="fa fa-check"></i><b>15.7</b> Ejercicios</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#poder"><i class="fa fa-check"></i><b>15.8</b> Poder</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#valores-p"><i class="fa fa-check"></i><b>15.9</b> valores p</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Pruebas de asociación</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#tablas-2x2"><i class="fa fa-check"></i><b>15.10.2</b> Tablas 2x2</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#prueba-de-chi-cuadrado"><i class="fa fa-check"></i><b>15.10.3</b> Prueba de chi-cuadrado</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> Riesgo relativo</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#intervalos-de-confianza-para-el-riesgo-relativo"><i class="fa fa-check"></i><b>15.10.5</b> Intervalos de confianza para el riesgo relativo</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#corrección-de-recuento-pequeño"><i class="fa fa-check"></i><b>15.10.6</b> Corrección de recuento pequeño</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#muestras-grandes-valores-p-pequeños"><i class="fa fa-check"></i><b>15.10.7</b> Muestras grandes, valores p pequeños</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#ejercicios-29"><i class="fa fa-check"></i><b>15.11</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Modelos estadísticos</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#agregadores-de-encuestas"><i class="fa fa-check"></i><b>16.1</b> Agregadores de encuestas</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#datos-de-encuesta"><i class="fa fa-check"></i><b>16.1.1</b> Datos de encuesta</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#sesgo-de-los-encuestadores"><i class="fa fa-check"></i><b>16.1.2</b> Sesgo de los encuestadores</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Modelos basados en datos</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#ejercicios-30"><i class="fa fa-check"></i><b>16.3</b> Ejercicios</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Estadísticas bayesianas</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#teorema-de-bayes"><i class="fa fa-check"></i><b>16.4.1</b> Teorema de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#simulación-del-teorema-de-bayes"><i class="fa fa-check"></i><b>16.5</b> Simulación del teorema de Bayes</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-en-la-práctica"><i class="fa fa-check"></i><b>16.5.1</b> Bayes en la práctica</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#modelos-jerárquicos"><i class="fa fa-check"></i><b>16.6</b> Modelos jerárquicos</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#ejercicios-31"><i class="fa fa-check"></i><b>16.7</b> Ejercicios</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Estudio de caso: pronóstico de elecciones</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Enfoque bayesiano</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#el-sesgo-general"><i class="fa fa-check"></i><b>16.8.2</b> El sesgo general</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#representaciones-matemáticas-de-modelos"><i class="fa fa-check"></i><b>16.8.3</b> Representaciones matemáticas de modelos</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#prediciendo-el-colegio-electoral"><i class="fa fa-check"></i><b>16.8.4</b> Prediciendo el colegio electoral</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#pronósticos"><i class="fa fa-check"></i><b>16.8.5</b> Pronósticos</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#ejercicios-32"><i class="fa fa-check"></i><b>16.9</b> Ejercicios</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> La distribución t</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regresión</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#estudio-de-caso-la-altura-es-hereditaria"><i class="fa fa-check"></i><b>17.1</b> Estudio de caso: ¿la altura es hereditaria?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> El coeficiente de correlación</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#la-correlación-de-muestra-es-una-variable-aleatoria"><i class="fa fa-check"></i><b>17.2.1</b> La correlación de muestra es una variable aleatoria</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#la-correlación-no-siempre-es-un-resumen-útil"><i class="fa fa-check"></i><b>17.2.2</b> La correlación no siempre es un resumen útil</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Valor esperado condicional</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#la-línea-de-regresión"><i class="fa fa-check"></i><b>17.4</b> La línea de regresión</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regresión-mejora-precisión"><i class="fa fa-check"></i><b>17.4.1</b> Regresión mejora precisión</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#distribución-normal-de-dos-variables-avanzada"><i class="fa fa-check"></i><b>17.4.2</b> Distribución normal de dos variables (avanzada)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#varianza-explicada"><i class="fa fa-check"></i><b>17.4.3</b> Varianza explicada</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#advertencia-hay-dos-líneas-de-regresión"><i class="fa fa-check"></i><b>17.4.4</b> Advertencia: hay dos líneas de regresión</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#ejercicios-33"><i class="fa fa-check"></i><b>17.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>18</b> Modelos lineales</a><ul>
<li class="chapter" data-level="18.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball"><i class="fa fa-check"></i><b>18.1</b> Estudio de caso: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#sabermetrics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetrics</a></li>
<li class="chapter" data-level="18.1.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#conceptos-básicos-de-béisbol"><i class="fa fa-check"></i><b>18.1.2</b> Conceptos básicos de béisbol</a></li>
<li class="chapter" data-level="18.1.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#no-hay-premios-para-bb"><i class="fa fa-check"></i><b>18.1.3</b> No hay premios para BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#base-por-bolas-o-bases-robadas"><i class="fa fa-check"></i><b>18.1.4</b> ¿Base por bolas o bases robadas?</a></li>
<li class="chapter" data-level="18.1.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-aplicada-a-las-estadísticas-de-béisbol"><i class="fa fa-check"></i><b>18.1.5</b> Regresión aplicada a las estadísticas de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#confusión"><i class="fa fa-check"></i><b>18.2</b> Confusión</a><ul>
<li class="chapter" data-level="18.2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#entender-confusión-a-través-de-estratificación"><i class="fa fa-check"></i><b>18.2.1</b> Entender confusión a través de estratificación</a></li>
<li class="chapter" data-level="18.2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>18.2.2</b> Regresión lineal múltiple</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse"><i class="fa fa-check"></i><b>18.3</b> Estimaciones de mínimos cuadrados</a><ul>
<li class="chapter" data-level="18.3.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#interpretando-modelos-lineales"><i class="fa fa-check"></i><b>18.3.1</b> Interpretando modelos lineales</a></li>
<li class="chapter" data-level="18.3.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimaciones-de-mínimos-cuadrados-lse"><i class="fa fa-check"></i><b>18.3.2</b> Estimaciones de mínimos cuadrados (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-función-lm"><i class="fa fa-check"></i><b>18.3.3</b> La función <code>lm</code></a></li>
<li class="chapter" data-level="18.3.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.4</b> LSE son variables aleatorias</a></li>
<li class="chapter" data-level="18.3.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-pronosticados-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.5</b> Valores pronosticados son variables aleatorias</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-34"><i class="fa fa-check"></i><b>18.4</b> Ejercicios</a></li>
<li class="chapter" data-level="18.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-en-el-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Regresión lineal en el tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#el-paquete-broom"><i class="fa fa-check"></i><b>18.5.1</b> El paquete broom</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-35"><i class="fa fa-check"></i><b>18.6</b> Ejercicios</a></li>
<li class="chapter" data-level="18.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball-continuación"><i class="fa fa-check"></i><b>18.7</b> Estudio de caso: Moneyball (continuación)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#añadiendo-información-sobre-salario-y-posición"><i class="fa fa-check"></i><b>18.7.1</b> Añadiendo información sobre salario y posición</a></li>
<li class="chapter" data-level="18.7.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#escoger-nueve-jugadores"><i class="fa fa-check"></i><b>18.7.2</b> Escoger nueve jugadores</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-falacia-de-la-regresión"><i class="fa fa-check"></i><b>18.8</b> La falacia de la regresión</a></li>
<li class="chapter" data-level="18.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-de-error-de-medición"><i class="fa fa-check"></i><b>18.9</b> Modelos de error de medición</a></li>
<li class="chapter" data-level="18.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-36"><i class="fa fa-check"></i><b>18.10</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html"><i class="fa fa-check"></i><b>19</b> La asociación no implica causalidad</a><ul>
<li class="chapter" data-level="19.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#correlación-espuria"><i class="fa fa-check"></i><b>19.1</b> Correlación espuria</a></li>
<li class="chapter" data-level="19.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#valores-atípicos-1"><i class="fa fa-check"></i><b>19.2</b> Valores atípicos</a></li>
<li class="chapter" data-level="19.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#inversión-de-causa-y-efecto"><i class="fa fa-check"></i><b>19.3</b> Inversión de causa y efecto</a></li>
<li class="chapter" data-level="19.4" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#factores-de-confusión"><i class="fa fa-check"></i><b>19.4</b> Factores de confusión</a><ul>
<li class="chapter" data-level="19.4.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejemplo-admisiones-a-la-universidad-de-california-berkeley"><i class="fa fa-check"></i><b>19.4.1</b> Ejemplo: admisiones a la Universidad de California, Berkeley</a></li>
<li class="chapter" data-level="19.4.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#confusión-explicada-gráficamente"><i class="fa fa-check"></i><b>19.4.2</b> Confusión explicada gráficamente</a></li>
<li class="chapter" data-level="19.4.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#promedio-después-de-estratificar"><i class="fa fa-check"></i><b>19.4.3</b> Promedio después de estratificar</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#la-paradoja-de-simpson"><i class="fa fa-check"></i><b>19.5</b> La paradoja de Simpson</a></li>
<li class="chapter" data-level="19.6" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejercicios-37"><i class="fa fa-check"></i><b>19.6</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>IV <em>Wrangling</em> de datos</b></span></li>
<li class="chapter" data-level="20" data-path="introducción-al-wrangling-de-datos.html"><a href="introducción-al-wrangling-de-datos.html"><i class="fa fa-check"></i><b>20</b> Introducción al wrangling de datos</a></li>
<li class="chapter" data-level="21" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html"><i class="fa fa-check"></i><b>21</b> Cómo cambiar el formato de datos</a><ul>
<li class="chapter" data-level="21.1" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#ejercicios-38"><i class="fa fa-check"></i><b>21.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="unir-tablas.html"><a href="unir-tablas.html"><i class="fa fa-check"></i><b>22</b> Unir tablas</a><ul>
<li class="chapter" data-level="22.1" data-path="unir-tablas.html"><a href="unir-tablas.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="unir-tablas.html"><a href="unir-tablas.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="unir-tablas.html"><a href="unir-tablas.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="unir-tablas.html"><a href="unir-tablas.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="unir-tablas.html"><a href="unir-tablas.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="unir-tablas.html"><a href="unir-tablas.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="unir-tablas.html"><a href="unir-tablas.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="unir-tablas.html"><a href="unir-tablas.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-columnas"><i class="fa fa-check"></i><b>22.2.1</b> Pegando columnas</a></li>
<li class="chapter" data-level="22.2.2" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-filas"><i class="fa fa-check"></i><b>22.2.2</b> Pegando filas</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="unir-tablas.html"><a href="unir-tablas.html#operadores-de-sets"><i class="fa fa-check"></i><b>22.3</b> Operadores de sets</a><ul>
<li class="chapter" data-level="22.3.1" data-path="unir-tablas.html"><a href="unir-tablas.html#intersecar"><i class="fa fa-check"></i><b>22.3.1</b> Intersecar</a></li>
<li class="chapter" data-level="22.3.2" data-path="unir-tablas.html"><a href="unir-tablas.html#unión"><i class="fa fa-check"></i><b>22.3.2</b> Unión</a></li>
<li class="chapter" data-level="22.3.3" data-path="unir-tablas.html"><a href="unir-tablas.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="unir-tablas.html"><a href="unir-tablas.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="unir-tablas.html"><a href="unir-tablas.html#ejercicios-39"><i class="fa fa-check"></i><b>22.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html"><i class="fa fa-check"></i><b>23</b> Extracción de la web</a><ul>
<li class="chapter" data-level="23.1" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#el-paquete-rvest"><i class="fa fa-check"></i><b>23.2</b> El paquete rvest</a></li>
<li class="chapter" data-level="23.3" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> Selectores CSS</a></li>
<li class="chapter" data-level="23.4" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#ejercicios-40"><i class="fa fa-check"></i><b>23.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html"><i class="fa fa-check"></i><b>24</b> Procesamiento de cadenas</a><ul>
<li class="chapter" data-level="24.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#stringr"><i class="fa fa-check"></i><b>24.1</b> El paquete stringr</a></li>
<li class="chapter" data-level="24.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-1-datos-de-asesinatos-en-ee.-uu."><i class="fa fa-check"></i><b>24.2</b> Estudio de caso 1: datos de asesinatos en EE. UU.</a></li>
<li class="chapter" data-level="24.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas"><i class="fa fa-check"></i><b>24.3</b> Estudio de caso 2: alturas autoreportadas</a></li>
<li class="chapter" data-level="24.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cómo-escapar-al-definir-cadenas"><i class="fa fa-check"></i><b>24.4</b> Cómo <em>escapar</em> al definir cadenas</a></li>
<li class="chapter" data-level="24.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#expresiones-regulares"><i class="fa fa-check"></i><b>24.5</b> Expresiones regulares</a><ul>
<li class="chapter" data-level="24.5.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#las-cadenas-son-una-expresión-regular"><i class="fa fa-check"></i><b>24.5.1</b> Las cadenas son una expresión regular</a></li>
<li class="chapter" data-level="24.5.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#caracteres-especiales"><i class="fa fa-check"></i><b>24.5.2</b> Caracteres especiales</a></li>
<li class="chapter" data-level="24.5.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#clases-de-caracteres"><i class="fa fa-check"></i><b>24.5.3</b> Clases de caracteres</a></li>
<li class="chapter" data-level="24.5.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#anclas"><i class="fa fa-check"></i><b>24.5.4</b> Anclas</a></li>
<li class="chapter" data-level="24.5.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores"><i class="fa fa-check"></i><b>24.5.5</b> Cuantificadores</a></li>
<li class="chapter" data-level="24.5.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#espacio-en-blanco-s"><i class="fa fa-check"></i><b>24.5.6</b> Espacio en blanco <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores-1"><i class="fa fa-check"></i><b>24.5.7</b> Cuantificadores: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#no"><i class="fa fa-check"></i><b>24.5.8</b> No</a></li>
<li class="chapter" data-level="24.5.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-con-expresiones-regulares"><i class="fa fa-check"></i><b>24.6</b> Buscar y reemplazar con expresiones regulares</a><ul>
<li class="chapter" data-level="24.6.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-usando-grupos"><i class="fa fa-check"></i><b>24.6.1</b> Buscar y reemplazar usando grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#probar-y-mejorar"><i class="fa fa-check"></i><b>24.7</b> Probar y mejorar</a></li>
<li class="chapter" data-level="24.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#podar"><i class="fa fa-check"></i><b>24.8</b> Podar</a></li>
<li class="chapter" data-level="24.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cambio-de-mayúsculas-o-minúsculas"><i class="fa fa-check"></i><b>24.9</b> Cambio de mayúsculas o minúsculas</a></li>
<li class="chapter" data-level="24.10" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas-continuación"><i class="fa fa-check"></i><b>24.10</b> Estudio de caso 2: alturas autoreportadas (continuación)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#la-función-extract"><i class="fa fa-check"></i><b>24.10.1</b> La función <code>extract</code></a></li>
<li class="chapter" data-level="24.10.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#juntando-todas-la-piezas"><i class="fa fa-check"></i><b>24.10.2</b> Juntando todas la piezas</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#división-de-cadenas"><i class="fa fa-check"></i><b>24.11</b> División de cadenas</a></li>
<li class="chapter" data-level="24.12" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-3-extracción-de-tablas-de-un-pdf"><i class="fa fa-check"></i><b>24.12</b> Estudio de caso 3: extracción de tablas de un PDF</a></li>
<li class="chapter" data-level="24.13" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recodificación</a></li>
<li class="chapter" data-level="24.14" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#ejercicios-41"><i class="fa fa-check"></i><b>24.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html"><i class="fa fa-check"></i><b>25</b> Cómo leer y procesar fechas y horas</a><ul>
<li class="chapter" data-level="25.1" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#el-tipo-de-datos-de-fecha"><i class="fa fa-check"></i><b>25.1</b> El tipo de datos de fecha</a></li>
<li class="chapter" data-level="25.2" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> El paquete lubridate</a></li>
<li class="chapter" data-level="25.3" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#ejercicios-42"><i class="fa fa-check"></i><b>25.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="minería-de-textos.html"><a href="minería-de-textos.html"><i class="fa fa-check"></i><b>26</b> Minería de textos</a><ul>
<li class="chapter" data-level="26.1" data-path="minería-de-textos.html"><a href="minería-de-textos.html#estudio-de-caso-tuits-de-trump"><i class="fa fa-check"></i><b>26.1</b> Estudio de caso: tuits de Trump</a></li>
<li class="chapter" data-level="26.2" data-path="minería-de-textos.html"><a href="minería-de-textos.html#texto-como-datos"><i class="fa fa-check"></i><b>26.2</b> Texto como datos</a></li>
<li class="chapter" data-level="26.3" data-path="minería-de-textos.html"><a href="minería-de-textos.html#análisis-de-sentimiento"><i class="fa fa-check"></i><b>26.3</b> Análisis de sentimiento</a></li>
<li class="chapter" data-level="26.4" data-path="minería-de-textos.html"><a href="minería-de-textos.html#ejercicios-43"><i class="fa fa-check"></i><b>26.4</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html"><i class="fa fa-check"></i><b>27</b> Introducción a <em>machine learning</em></a><ul>
<li class="chapter" data-level="27.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#notación-1"><i class="fa fa-check"></i><b>27.1</b> Notación</a></li>
<li class="chapter" data-level="27.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#un-ejemplo"><i class="fa fa-check"></i><b>27.2</b> Un ejemplo</a></li>
<li class="chapter" data-level="27.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-44"><i class="fa fa-check"></i><b>27.3</b> Ejercicios</a></li>
<li class="chapter" data-level="27.4" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#métricas-de-evaluación"><i class="fa fa-check"></i><b>27.4</b> Métricas de evaluación</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#sets-de-entrenamiento-y-de-evaluación"><i class="fa fa-check"></i><b>27.4.1</b> Sets de entrenamiento y de evaluación</a></li>
<li class="chapter" data-level="27.4.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#exactitud-general"><i class="fa fa-check"></i><b>27.4.2</b> Exactitud general</a></li>
<li class="chapter" data-level="27.4.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-matriz-de-confusión"><i class="fa fa-check"></i><b>27.4.3</b> La matriz de confusión</a></li>
<li class="chapter" data-level="27.4.4" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#sensibilidad-y-especificidad"><i class="fa fa-check"></i><b>27.4.4</b> Sensibilidad y especificidad</a></li>
<li class="chapter" data-level="27.4.5" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#exactitud-equilibrada-y-medida-f_1"><i class="fa fa-check"></i><b>27.4.5</b> Exactitud equilibrada y medida <span class="math inline">\(F_1\)</span></a></li>
<li class="chapter" data-level="27.4.6" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-prevalencia-importa-en-la-práctica"><i class="fa fa-check"></i><b>27.4.6</b> La prevalencia importa en la práctica</a></li>
<li class="chapter" data-level="27.4.7" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#curvas-roc-y-precision-recall"><i class="fa fa-check"></i><b>27.4.7</b> Curvas ROC y precision-recall</a></li>
<li class="chapter" data-level="27.4.8" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> La función de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-45"><i class="fa fa-check"></i><b>27.5</b> Ejercicios</a></li>
<li class="chapter" data-level="27.6" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#probabilidades-y-expectativas-condicionales"><i class="fa fa-check"></i><b>27.6</b> Probabilidades y expectativas condicionales</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#probabilidades-condicionales-1"><i class="fa fa-check"></i><b>27.6.1</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="27.6.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#expectativas-condicionales"><i class="fa fa-check"></i><b>27.6.2</b> Expectativas condicionales</a></li>
<li class="chapter" data-level="27.6.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-expectativa-condicional-minimiza-la-función-de-pérdida-cuadrática"><i class="fa fa-check"></i><b>27.6.3</b> La expectativa condicional minimiza la función de pérdida cuadrática</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-46"><i class="fa fa-check"></i><b>27.7</b> Ejercicios</a></li>
<li class="chapter" data-level="27.8" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Estudio de caso: ¿es un 2 o un 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="suavización.html"><a href="suavización.html"><i class="fa fa-check"></i><b>28</b> Suavización</a><ul>
<li class="chapter" data-level="28.1" data-path="suavización.html"><a href="suavización.html#suavización-de-compartimientos"><i class="fa fa-check"></i><b>28.1</b> Suavización de compartimientos</a></li>
<li class="chapter" data-level="28.2" data-path="suavización.html"><a href="suavización.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="suavización.html"><a href="suavización.html#regresión-ponderada-local-loess"><i class="fa fa-check"></i><b>28.3</b> Regresión ponderada local (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="suavización.html"><a href="suavización.html#ajustando-con-parábolas"><i class="fa fa-check"></i><b>28.3.1</b> Ajustando con parábolas</a></li>
<li class="chapter" data-level="28.3.2" data-path="suavización.html"><a href="suavización.html#cuidado-con-los-parámetros-de-suavización-predeterminados"><i class="fa fa-check"></i><b>28.3.2</b> Cuidado con los parámetros de suavización predeterminados</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="suavización.html"><a href="suavización.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Conectando la suavización al <em>machine learning</em></a></li>
<li class="chapter" data-level="28.5" data-path="suavización.html"><a href="suavización.html#ejercicios-47"><i class="fa fa-check"></i><b>28.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Validación cruzada</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivación con k vecinos más cercanos</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#sobreentrenamiento"><i class="fa fa-check"></i><b>29.1.1</b> Sobreentrenamiento</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#sobre-suavización"><i class="fa fa-check"></i><b>29.1.2</b> Sobre suavización</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#escogiendo-la-k-en-knn"><i class="fa fa-check"></i><b>29.1.3</b> Escogiendo la <span class="math inline">\(k\)</span> en kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#descripción-matemática-de-validación-cruzada"><i class="fa fa-check"></i><b>29.2</b> Descripción matemática de validación cruzada</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#validación-cruzada-k-fold"><i class="fa fa-check"></i><b>29.3</b> Validación cruzada K-fold</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-48"><i class="fa fa-check"></i><b>29.4</b> Ejercicios</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-49"><i class="fa fa-check"></i><b>29.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> El paquete caret</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#la-función-train-de-caret"><i class="fa fa-check"></i><b>30.1</b> La función <code>train</code> de caret</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Validación cruzada</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#ejemplo-ajuste-con-loess"><i class="fa fa-check"></i><b>30.3</b> Ejemplo: ajuste con loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html"><i class="fa fa-check"></i><b>31</b> Ejemplos de algoritmos</a><ul>
<li class="chapter" data-level="31.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-lineal"><i class="fa fa-check"></i><b>31.1</b> Regresión lineal</a><ul>
<li class="chapter" data-level="31.1.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#la-función-predict"><i class="fa fa-check"></i><b>31.1.1</b> La función <code>predict</code></a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-50"><i class="fa fa-check"></i><b>31.2</b> Ejercicios</a></li>
<li class="chapter" data-level="31.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística"><i class="fa fa-check"></i><b>31.3</b> Regresión logística</a><ul>
<li class="chapter" data-level="31.3.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>31.3.1</b> Modelos lineales generalizados</a></li>
<li class="chapter" data-level="31.3.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística-con-más-de-un-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Regresión logística con más de un predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-51"><i class="fa fa-check"></i><b>31.4</b> Ejercicios</a></li>
<li class="chapter" data-level="31.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#k-vecinos-más-cercanos-knn"><i class="fa fa-check"></i><b>31.5</b> k vecinos más cercanos (kNN)</a></li>
<li class="chapter" data-level="31.6" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-52"><i class="fa fa-check"></i><b>31.6</b> Ejercicios</a></li>
<li class="chapter" data-level="31.7" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-generativos"><i class="fa fa-check"></i><b>31.7</b> Modelos generativos</a><ul>
<li class="chapter" data-level="31.7.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#controlando-la-prevalencia"><i class="fa fa-check"></i><b>31.7.2</b> Controlando la prevalencia</a></li>
<li class="chapter" data-level="31.7.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-cuadrático"><i class="fa fa-check"></i><b>31.7.3</b> Análisis discriminante cuadrático</a></li>
<li class="chapter" data-level="31.7.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-lineal"><i class="fa fa-check"></i><b>31.7.4</b> Análisis discriminante lineal</a></li>
<li class="chapter" data-level="31.7.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#conexión-a-distancia"><i class="fa fa-check"></i><b>31.7.5</b> Conexión a distancia</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#estudio-de-caso-más-de-tres-clases"><i class="fa fa-check"></i><b>31.8</b> Estudio de caso: más de tres clases</a></li>
<li class="chapter" data-level="31.9" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-53"><i class="fa fa-check"></i><b>31.9</b> Ejercicios</a></li>
<li class="chapter" data-level="31.10" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-y-regresión-cart"><i class="fa fa-check"></i><b>31.10</b> Árboles de clasificación y regresión (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#la-maldición-de-la-dimensionalidad"><i class="fa fa-check"></i><b>31.10.1</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="31.10.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#motivación-cart"><i class="fa fa-check"></i><b>31.10.2</b> Motivación CART</a></li>
<li class="chapter" data-level="31.10.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-regresión"><i class="fa fa-check"></i><b>31.10.3</b> Árboles de regresión</a></li>
<li class="chapter" data-level="31.10.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-decisión"><i class="fa fa-check"></i><b>31.10.4</b> Árboles de clasificación (decisión)</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#bosques-aleatorios"><i class="fa fa-check"></i><b>31.11</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="31.12" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-54"><i class="fa fa-check"></i><b>31.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html"><i class="fa fa-check"></i><b>32</b> Machine learning en la práctica</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#preprocesamiento"><i class="fa fa-check"></i><b>32.1</b> Preprocesamiento</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#k-vecino-más-cercano-y-bosque-aleatorio"><i class="fa fa-check"></i><b>32.2</b> k-vecino más cercano y bosque aleatorio</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#importancia-variable"><i class="fa fa-check"></i><b>32.3</b> Importancia variable</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#evaluaciones-visuales"><i class="fa fa-check"></i><b>32.4</b> Evaluaciones visuales</a></li>
<li class="chapter" data-level="32.5" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#conjuntos"><i class="fa fa-check"></i><b>32.5</b> Conjuntos</a></li>
<li class="chapter" data-level="32.6" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#ejercicios-55"><i class="fa fa-check"></i><b>32.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html"><i class="fa fa-check"></i><b>33</b> Grandes sets de datos</a><ul>
<li class="chapter" data-level="33.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Álgebra matricial</a><ul>
<li class="chapter" data-level="33.1.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#notación-2"><i class="fa fa-check"></i><b>33.1.1</b> Notación</a></li>
<li class="chapter" data-level="33.1.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#convertir-un-vector-en-una-matriz"><i class="fa fa-check"></i><b>33.1.2</b> Convertir un vector en una matriz</a></li>
<li class="chapter" data-level="33.1.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#resúmenes-de-filas-y-columnas"><i class="fa fa-check"></i><b>33.1.3</b> Resúmenes de filas y columnas</a></li>
<li class="chapter" data-level="33.1.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#filtrar-columnas-basadas-en-resúmenes"><i class="fa fa-check"></i><b>33.1.5</b> Filtrar columnas basadas en resúmenes</a></li>
<li class="chapter" data-level="33.1.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#indexación-con-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexación con matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#binarizar-los-datos"><i class="fa fa-check"></i><b>33.1.7</b> Binarizar los datos</a></li>
<li class="chapter" data-level="33.1.8" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#vectorización-para-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorización para matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#operaciones-de-álgebra-matricial"><i class="fa fa-check"></i><b>33.1.9</b> Operaciones de álgebra matricial</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-56"><i class="fa fa-check"></i><b>33.2</b> Ejercicios</a></li>
<li class="chapter" data-level="33.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia"><i class="fa fa-check"></i><b>33.3</b> Distancia</a><ul>
<li class="chapter" data-level="33.3.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.1</b> Distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia-en-dimensiones-superiores"><i class="fa fa-check"></i><b>33.3.2</b> Distancia en dimensiones superiores</a></li>
<li class="chapter" data-level="33.3.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejemplo-de-distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.3</b> Ejemplo de distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Espacio predictor</a></li>
<li class="chapter" data-level="33.3.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia-entre-predictores"><i class="fa fa-check"></i><b>33.3.5</b> Distancia entre predictores</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-57"><i class="fa fa-check"></i><b>33.4</b> Ejercicios</a></li>
<li class="chapter" data-level="33.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#reducción-de-dimensiones"><i class="fa fa-check"></i><b>33.5</b> Reducción de dimensiones</a><ul>
<li class="chapter" data-level="33.5.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#preservando-la-distancia"><i class="fa fa-check"></i><b>33.5.1</b> Preservando la distancia</a></li>
<li class="chapter" data-level="33.5.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#transformaciones-lineales-avanzado"><i class="fa fa-check"></i><b>33.5.2</b> Transformaciones lineales (avanzado)</a></li>
<li class="chapter" data-level="33.5.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#transformaciones-ortogonales-avanzado"><i class="fa fa-check"></i><b>33.5.3</b> Transformaciones ortogonales (avanzado)</a></li>
<li class="chapter" data-level="33.5.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#análisis-de-componentes-principales"><i class="fa fa-check"></i><b>33.5.4</b> Análisis de componentes principales</a></li>
<li class="chapter" data-level="33.5.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejemplo-de-iris"><i class="fa fa-check"></i><b>33.5.5</b> Ejemplo de Iris</a></li>
<li class="chapter" data-level="33.5.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejemplo-de-mnist"><i class="fa fa-check"></i><b>33.5.6</b> Ejemplo de MNIST</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-58"><i class="fa fa-check"></i><b>33.6</b> Ejercicios</a></li>
<li class="chapter" data-level="33.7" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#sistemas-de-recomendación"><i class="fa fa-check"></i><b>33.7</b> Sistemas de recomendación</a><ul>
<li class="chapter" data-level="33.7.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#datos-de-lente-de-película"><i class="fa fa-check"></i><b>33.7.1</b> Datos de lente de película</a></li>
<li class="chapter" data-level="33.7.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#sistemas-de-recomendación-como-un-desafío-de-aprendizaje-automático"><i class="fa fa-check"></i><b>33.7.2</b> Sistemas de recomendación como un desafío de aprendizaje automático</a></li>
<li class="chapter" data-level="33.7.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Función de pérdida</a></li>
<li class="chapter" data-level="33.7.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#un-primer-modelo"><i class="fa fa-check"></i><b>33.7.4</b> Un primer modelo</a></li>
<li class="chapter" data-level="33.7.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#modelado-de-efectos-de-películas"><i class="fa fa-check"></i><b>33.7.5</b> Modelado de efectos de películas</a></li>
<li class="chapter" data-level="33.7.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#efectos-de-usuario"><i class="fa fa-check"></i><b>33.7.6</b> Efectos de usuario</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-59"><i class="fa fa-check"></i><b>33.8</b> Ejercicios</a></li>
<li class="chapter" data-level="33.9" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#regularización"><i class="fa fa-check"></i><b>33.9</b> Regularización</a><ul>
<li class="chapter" data-level="33.9.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#motivación"><i class="fa fa-check"></i><b>33.9.1</b> Motivación</a></li>
<li class="chapter" data-level="33.9.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#mínimos-cuadrados-penalizados"><i class="fa fa-check"></i><b>33.9.2</b> Mínimos cuadrados penalizados</a></li>
<li class="chapter" data-level="33.9.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#elegir-los-términos-de-penalización"><i class="fa fa-check"></i><b>33.9.3</b> Elegir los términos de penalización</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-60"><i class="fa fa-check"></i><b>33.10</b> Ejercicios</a></li>
<li class="chapter" data-level="33.11" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#factorización-matricial"><i class="fa fa-check"></i><b>33.11</b> Factorización matricial</a><ul>
<li class="chapter" data-level="33.11.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#análisis-de-factores"><i class="fa fa-check"></i><b>33.11.1</b> Análisis de factores</a></li>
<li class="chapter" data-level="33.11.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#conexión-a-svd-y-pca"><i class="fa fa-check"></i><b>33.11.2</b> Conexión a SVD y PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-61"><i class="fa fa-check"></i><b>33.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Agrupación</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#agrupación-jerárquica"><i class="fa fa-check"></i><b>34.1</b> Agrupación jerárquica</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#mapas-de-calor"><i class="fa fa-check"></i><b>34.3</b> Mapas de calor</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#características-de-filtrado"><i class="fa fa-check"></i><b>34.4</b> Características de filtrado</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#ejercicios-62"><i class="fa fa-check"></i><b>34.5</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>VI Herramientas de productividad</b></span></li>
<li class="chapter" data-level="35" data-path="introducción-a-las-herramientas-de-productividad.html"><a href="introducción-a-las-herramientas-de-productividad.html"><i class="fa fa-check"></i><b>35</b> Introducción a las herramientas de productividad</a></li>
<li class="chapter" data-level="36" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>36</b> Instalación de R y RStudio</a><ul>
<li class="chapter" data-level="36.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#instalando-r"><i class="fa fa-check"></i><b>36.1</b> Instalando R</a></li>
<li class="chapter" data-level="36.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#instalación-de-rstudio"><i class="fa fa-check"></i><b>36.2</b> Instalación de RStudio</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html"><i class="fa fa-check"></i><b>37</b> Accediendo al terminal e instalando Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accediendo al terminal en una Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#instalando-git-en-la-mac"><i class="fa fa-check"></i><b>37.2</b> Instalando Git en la Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#instalación-de-git-y-git-bash-en-windows"><i class="fa fa-check"></i><b>37.3</b> Instalación de Git y Git Bash en Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accediendo a la terminal en Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizando con Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#convenio-de-denominación"><i class="fa fa-check"></i><b>38.1</b> Convenio de denominación</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> La terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> El sistema de archivos</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directorios-y-subdirectorios"><i class="fa fa-check"></i><b>38.3.1</b> Directorios y subdirectorios</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#el-directorio-de-inicio"><i class="fa fa-check"></i><b>38.3.2</b> El directorio de inicio</a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Directorio de trabajo</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Rutas</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#comandos-de-unix"><i class="fa fa-check"></i><b>38.4</b> Comandos de Unix</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listado-de-contenido-del-directorio"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listado de contenido del directorio</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-y-rmdir-crear-y-eliminar-un-directorio"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> y <code>rmdir</code>: crear y eliminar un directorio</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navegando-por-el-sistema-de-archivos-cambiando-directorios"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: navegando por el sistema de archivos cambiando directorios</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#algunos-ejemplos"><i class="fa fa-check"></i><b>38.5</b> Algunos ejemplos</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#más-comandos-de-unix"><i class="fa fa-check"></i><b>38.6</b> Más comandos de Unix</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-mover-archivos"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: mover archivos</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copiando-documentos"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copiando documentos</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-eliminar-archivos"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: eliminar archivos</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-mirando-un-archivo"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: mirando un archivo</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparación para un proyecto de ciencia de datos</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#unix-avanzado"><i class="fa fa-check"></i><b>38.8</b> Unix avanzado</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#argumentos"><i class="fa fa-check"></i><b>38.8.1</b> Argumentos</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#obteniendo-ayuda"><i class="fa fa-check"></i><b>38.8.2</b> Obteniendo ayuda</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#tuberías"><i class="fa fa-check"></i><b>38.8.3</b> Tuberías</a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#comodines"><i class="fa fa-check"></i><b>38.8.4</b> Comodines</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#variables-de-entorno"><i class="fa fa-check"></i><b>38.8.5</b> Variables de entorno</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#conchas"><i class="fa fa-check"></i><b>38.8.6</b> Conchas</a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#ejecutables"><i class="fa fa-check"></i><b>38.8.7</b> Ejecutables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permisos-y-tipos-de-archivo"><i class="fa fa-check"></i><b>38.8.8</b> Permisos y tipos de archivo</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#comandos-que-debes-aprender"><i class="fa fa-check"></i><b>38.8.9</b> Comandos que debes aprender</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#manipulación-de-archivos-en-r"><i class="fa fa-check"></i><b>38.8.10</b> Manipulación de archivos en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git y GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#por-qué-usar-git-y-github"><i class="fa fa-check"></i><b>39.1</b> ¿Por qué usar Git y GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#cuentas-github"><i class="fa fa-check"></i><b>39.2</b> Cuentas GitHub</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> Repositorios de GitHub</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Descripción general de Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clonar"><i class="fa fa-check"></i><b>39.4.1</b> Clonar</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Inicializando un directorio Git</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Usando Git y GitHub en RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Proyectos reproducibles con RStudio y R markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#proyectos-de-rstudio"><i class="fa fa-check"></i><b>40.1</b> Proyectos de RStudio</a></li>
<li class="chapter" data-level="40.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#r-descuento"><i class="fa fa-check"></i><b>40.2</b> R descuento</a><ul>
<li class="chapter" data-level="40.2.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#el-encabezado"><i class="fa fa-check"></i><b>40.2.1</b> El encabezado</a></li>
<li class="chapter" data-level="40.2.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#trozos-de-código-r"><i class="fa fa-check"></i><b>40.2.2</b> Trozos de código R</a></li>
<li class="chapter" data-level="40.2.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#opciones-globales"><i class="fa fa-check"></i><b>40.2.3</b> Opciones globales</a></li>
<li class="chapter" data-level="40.2.4" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#más-sobre-markdown-r"><i class="fa fa-check"></i><b>40.2.5</b> Más sobre Markdown R</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizando un proyecto de ciencia de datos</a><ul>
<li class="chapter" data-level="40.3.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#crear-directorios-en-unix"><i class="fa fa-check"></i><b>40.3.1</b> Crear directorios en Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#crear-un-proyecto-rstudio"><i class="fa fa-check"></i><b>40.3.2</b> Crear un proyecto RStudio</a></li>
<li class="chapter" data-level="40.3.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#editar-algunos-scripts-r"><i class="fa fa-check"></i><b>40.3.3</b> Editar algunos scripts R</a></li>
<li class="chapter" data-level="40.3.4" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#cree-más-directorios-usando-unix"><i class="fa fa-check"></i><b>40.3.4</b> Cree más directorios usando Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#agregar-un-archivo-readme"><i class="fa fa-check"></i><b>40.3.5</b> Agregar un archivo README</a></li>
<li class="chapter" data-level="40.3.6" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#inicializando-un-directorio-git"><i class="fa fa-check"></i><b>40.3.6</b> Inicializando un directorio Git</a></li>
<li class="chapter" data-level="40.3.7" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#agregue-confirme-y-envíe-archivos-con-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> Agregue, confirme y envíe archivos con RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción a la Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introducción-a-machine-learning" class="section level1">
<h1><span class="header-section-number">Capítulo 27</span> Introducción a <em>machine learning</em></h1>
<p>Quizás las metodologías de ciencia de datos más populares provienen del campo de <em>machine learning</em>. Las historias de éxito de <em>machine learning</em> incluyen lectores de códigos postales escritos a mano implementados por el servicio postal, tecnología de reconocimiento de voz como Siri de Apple, sistemas de recomendación de películas, detectores de spam y <em>malware</em>, automóviles sin conductor y predictores de precios de viviendas. Aunque hoy en día los términos Inteligencia Artificial y <em>machine learning</em> se usan indistintamente, hacemos la siguiente distinción: mientras que los primeros algoritmos de inteligencia artificial, como esos utilizados por las máquinas de ajedrez, implementaron la toma de decisiones según reglas programables derivadas de la teoría o de los primeros principios, en <em>machine learning</em> las decisiones de aprendizaje se basan en algoritmos <strong>que se construyen con datos</strong>.</p>
<div id="notación-1" class="section level2">
<h2><span class="header-section-number">27.1</span> Notación</h2>
<p>En <em>machine learning</em>, los datos se presentan en forma de:</p>
<ol style="list-style-type: decimal">
<li>el <em>resultado</em> (<em>outcome</em> en inglés) que queremos predecir y</li>
<li>las <em>características</em> (<em>features</em> en inglés) que usaremos para predecir el resultado.</li>
</ol>
<p>Queremos construir un algoritmo que tome los valores de las características como entrada y devuelva una predicción para el resultado cuando no sabemos el resultado. El enfoque de <em>machine learning</em> consiste en entrenar un algoritmo utilizando un set de datos para el cual conocemos el resultado, y luego usa este algoritmo en el futuro para hacer una predicción cuando no sabemos el resultado.</p>
<p>Aquí usaremos <span class="math inline">\(Y\)</span> para denotar el resultado y <span class="math inline">\(X_1, \dots, X_p\)</span> para denotar características. Tengan en cuenta que las características a veces se denominan predictores o covariables. Consideramos estos sinónimos.</p>
<p>Los problemas de predicción se pueden dividir en resultados categóricos y continuos. Para resultados categóricos, <span class="math inline">\(Y\)</span> puede ser cualquiera de <span class="math inline">\(K\)</span> clases. El número de clases puede variar mucho entre distintas aplicaciones.
Por ejemplo, en los datos del lector de dígitos, <span class="math inline">\(K=10\)</span> con las clases representando los dígitos 0, 1, 2, 3, 4, 5, 6, 7, 8 y 9. En el reconocimiento de voz, los resultados son todas las palabras o frases posibles que estamos tratando de detectar. La detección de spam tiene dos resultados: spam o no spam. En este libro, denotamos las categorías <span class="math inline">\(K\)</span> con índices <span class="math inline">\(k=1,\dots,K\)</span>. Sin embargo, para datos binarios usaremos <span class="math inline">\(k=0,1\)</span> para conveniencias matemáticas que demostraremos más adelante.</p>
<p>La configuración general es la siguiente. Tenemos una serie de características y un resultado desconocido que queremos predecir:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
outcome
</th>
<th style="text-align:center;">
feature 1
</th>
<th style="text-align:center;">
feature 2
</th>
<th style="text-align:center;">
feature 3
</th>
<th style="text-align:center;">
feature 4
</th>
<th style="text-align:center;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
?
</td>
<td style="text-align:center;">
<span class="math inline">\(X_1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_2\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_3\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_4\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(X_5\)</span>
</td>
</tr>
</tbody>
</table>
<p>Para <em>construir un modelo</em> que provee una predicción para cualquier conjunto de valores observados <span class="math inline">\(X_1=x_1, X_2=x_2, \dots X_5=x_5\)</span>, recolectamos datos para los cuales conocemos el resultado:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
outcome
</th>
<th style="text-align:left;">
feature 1
</th>
<th style="text-align:left;">
feature 2
</th>
<th style="text-align:left;">
feature 3
</th>
<th style="text-align:left;">
feature 4
</th>
<th style="text-align:left;">
feature 5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{1,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_{2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{2,5}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\vdots\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(y_n\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,1}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,3}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(x_{n,5}\)</span>
</td>
</tr>
</tbody>
</table>
<p>Cuando el resultado es continuo, nos referimos a la tarea de <em>machine learning</em> como <em>predicción</em>, y el resultado principal del modelo es una función <span class="math inline">\(f\)</span> que produce automáticamente una predicción, denotada con <span class="math inline">\(\hat{y}\)</span>, para cualquier conjunto de predictores: <span class="math inline">\(\hat{y} = f(x_1, x_2, \dots, x_p)\)</span>. Usamos el término <em>resultado real</em> (<em>actual outcome</em> en inglés) para denotar lo que acabamos observando. Entonces queremos que la predicción <span class="math inline">\(\hat{y}\)</span> coincida con resultado real <span class="math inline">\(y\)</span> lo mejor posible. Debido a que nuestro resultado es continuo, nuestras predicciones <span class="math inline">\(\hat{y}\)</span> no serán exactamente correctas o incorrectas, sino que determinaremos un <em>error</em> definido como la diferencia entre la predicción y el resultado real <span class="math inline">\(y - \hat{y}\)</span>.</p>
<p>Cuando el resultado es categórico, nos referimos a la tarea de <em>machine learning</em> como <em>clasificación</em>, y el resultado principal del modelo será una <em>regla de decisión</em> (<em>decision rule</em> en inglés) que determina cuál de las <span class="math inline">\(K\)</span> clases debemos predecir. En este escenario, la mayoría de los modelos proveen funciones de los predictores para cada clase. <span class="math inline">\(k\)</span>, <span class="math inline">\(f_k(x_1, x_2, \dots, x_p)\)</span>, que se utilizan para tomar esta decisión. Cuando los datos son binarios, una regla de decisión típica se vería así: si <span class="math inline">\(f_1(x_1, x_2, \dots, x_p) &gt; C\)</span>, pronostique la categoría 1, si no, pronostique la otra, con <span class="math inline">\(C\)</span> un umbral predeterminado. Debido a que los resultados son categóricos, nuestras predicciones serán correctas o incorrectas.</p>
<p>Tengan en cuenta que estos términos varían entre cursos, libros de texto y otras publicaciones. A menudo, el término <em>predicción</em> se usa tanto para resultados categóricos como continuos, y el término <em>regresión</em> puede usarse para el caso continuo. Aquí evitamos usar <em>regresión</em> para evitar confusión con nuestro uso previo del término <em>regresión lineal</em>. En la mayoría de los casos, estará claro si nuestros resultados son categóricos o continuos, por lo que evitaremos usar estos términos cuando sea posible.</p>
</div>
<div id="un-ejemplo" class="section level2">
<h2><span class="header-section-number">27.2</span> Un ejemplo</h2>
<p>Consideremos el ejemplo del lector de código postal. El primer paso para manejar el correo en la oficina de correos es ordenar las letras por código postal:</p>
<p><img src="ml/img/how-to-write-a-address-on-an-envelope-how-to-write-the-address-on-an-envelope-write-address-on-envelope-india-finishedenvelope-x69070.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Originalmente, los humanos tenían que clasificarlos a mano. Para hacer esto, tuvieron que leer los códigos postales de cada sobre. Hoy, gracias a los algoritmos de <em>machine learning</em>, una computadora puede leer códigos postales y luego un robot clasifica las letras. En esta parte del libro, aprenderemos cómo construir algoritmos que puedan leer un dígito.</p>
<p>El primer paso para construir un algoritmo es entender
¿cuáles son los resultados y las características? A continuación hay tres imágenes de dígitos escritos. Estos ya han sido leídos por un humano y se les ha asignado un resultado <span class="math inline">\(Y\)</span>. Por lo tanto, se consideran conocidos y sirven como set de entrenamiento.</p>
<p><img src="libro_files/figure-html/digit-images-example-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Las imágenes se convierten en <span class="math inline">\(28 \times 28 = 784\)</span> píxeles y, para cada píxel, obtenemos una intensidad de escala de grises entre 0 (blanco) y 255 (negro), que consideramos continua por ahora. El siguiente gráfico muestra las características individuales de cada imagen:</p>
<p><img src="libro_files/figure-html/example-images-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Para cada imagen digitalizada <span class="math inline">\(i\)</span>, tenemos un resultado categórico <span class="math inline">\(Y_i\)</span> que puede ser uno de los 10 valores (<span class="math inline">\(0,1,2,3,4,5,6,7,8,9\)</span>) y características <span class="math inline">\(X_{i,1}, \dots, X_{i,784}\)</span>. Usamos fuente negra <span class="math inline">\(\mathbf{X}_i = (X_{i,1}, \dots, X_{i,784})\)</span> para distinguir el vector de predictores de los predictores individuales. Cuando nos referimos a un conjunto arbitrario de características en lugar de una imagen específica en nuestro set de datos, descartamos el índice <span class="math inline">\(i\)</span> y usamos <span class="math inline">\(Y\)</span> y <span class="math inline">\(\mathbf{X} = (X_{1}, \dots, X_{784})\)</span>. Utilizamos variables en mayúsculas porque, en general, pensamos en los predictores como variables aleatorias. Usamos minúsculas, por ejemplo <span class="math inline">\(\mathbf{X} = \mathbf{x}\)</span>, para denotar valores observados. Cuando codificamos usamos minúsculas.</p>
<p>La tarea de <em>machine learning</em> es construir un algoritmo que devuelva una predicción para cualquiera de los posibles valores de las características. Aquí, aprenderemos varios enfoques para construir estos algoritmos. Aunque en este punto puede parecer imposible lograr esto, comenzaremos con ejemplos sencillos y desarrollaremos nuestro conocimiento hasta que podamos atacar los más complejos. De hecho, comenzamos con un ejemplo artificialmente simple con un solo predictor y luego pasamos a un ejemplo un poco más realista con dos predictores. Una vez que comprendamos estos, atacaremos los retos de <em>machine learning</em> del mundo real que involucran muchos predictores.</p>
</div>
<div id="ejercicios-44" class="section level2">
<h2><span class="header-section-number">27.3</span> Ejercicios</h2>
<p>1. Para cada uno de los siguientes, determine si el resultado es continuo o categórico:</p>
<ol style="list-style-type: lower-alpha">
<li>Lector de dígitos</li>
<li>Recomendaciones de películas</li>
<li>Filtro de spam</li>
<li>Hospitalizaciones</li>
<li>Siri (reconocimiento de voz)</li>
</ol>
<p>2. ¿Cuántas funciones tenemos disponibles para la predicción en el set de datos de dígitos?</p>
<p>3. En el ejemplo del lector de dígitos, los resultados se almacenan aquí:</p>
<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1000-1"><a href="introducción-a-machine-learning.html#cb1000-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1000-2"><a href="introducción-a-machine-learning.html#cb1000-2"></a>y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels</span></code></pre></div>
<p>¿Las siguientes operaciones tienen un significado práctico?</p>
<div class="sourceCode" id="cb1001"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1001-1"><a href="introducción-a-machine-learning.html#cb1001-1"></a>y[<span class="dv">5</span>] <span class="op">+</span><span class="st"> </span>y[<span class="dv">6</span>]</span>
<span id="cb1001-2"><a href="introducción-a-machine-learning.html#cb1001-2"></a>y[<span class="dv">5</span>] <span class="op">&gt;</span><span class="st"> </span>y[<span class="dv">6</span>]</span></code></pre></div>
<p>Eliga la mejor respuesta:</p>
<ol style="list-style-type: lower-alpha">
<li>Sí porque <span class="math inline">\(9 + 2 = 11\)</span> y <span class="math inline">\(9 &gt; 2\)</span>.</li>
<li>No, porque <code>y</code> no es un vector numérico.</li>
<li>No, porque 11 no es un dígito. Son dos dígitos.</li>
<li>No, porque estas son etiquetas que representan una categoría, no un número. Un <code>9</code> representa una clase, no el número 9.</li>
</ol>

</div>
<div id="métricas-de-evaluación" class="section level2">
<h2><span class="header-section-number">27.4</span> Métricas de evaluación</h2>
<p>Antes de comenzar a describir enfoques para optimizar la forma en que construimos algoritmos, primero debemos definir a qué nos referimos cuando decimos que un enfoque es mejor que otro. En esta sección, nos centramos en describir las formas en que se evalúan los algoritmos de <em>machine learning</em>. Específicamente, necesitamos cuantificar lo que queremos decir con “mejor”.</p>
<p>Para nuestra primera introducción a los conceptos de <em>machine learning</em>, comenzaremos con un ejemplo aburrido y sencillo: cómo predecir sexo basado en altura. A medida que explicamos <em>machine learning</em> paso a paso, este ejemplo nos permitirá establecer el primer componente básico. Muy pronto, estaremos atacando desafíos más interesantes. Utilizamos el paquete <strong>caret</strong>, que tiene varias funciones útiles para construir y evaluar métodos de <em>machine learning</em>. Presentamos los detalles de este paquete en el Capítulo <a href="caret.html#caret">30</a>.</p>
<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1002-1"><a href="introducción-a-machine-learning.html#cb1002-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1002-2"><a href="introducción-a-machine-learning.html#cb1002-2"></a><span class="kw">library</span>(caret)</span></code></pre></div>
<p>Como primer ejemplo, usamos los datos de altura en <strong>dslabs</strong>:</p>
<div class="sourceCode" id="cb1003"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1003-1"><a href="introducción-a-machine-learning.html#cb1003-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1003-2"><a href="introducción-a-machine-learning.html#cb1003-2"></a><span class="kw">data</span>(heights)</span></code></pre></div>
<p>Comenzamos definiendo el resultado y los predictores.</p>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1004-1"><a href="introducción-a-machine-learning.html#cb1004-1"></a>y &lt;-<span class="st"> </span>heights<span class="op">$</span>sex</span>
<span id="cb1004-2"><a href="introducción-a-machine-learning.html#cb1004-2"></a>x &lt;-<span class="st"> </span>heights<span class="op">$</span>height</span></code></pre></div>
<p>En este caso, solo tenemos un predictor, altura, mientras que <code>y</code> es claramente un resultado categórico ya que los valores observados son <code>Male</code> o <code>Female</code>. Sabemos que no podremos predecir <span class="math inline">\(Y\)</span> de forma precisa basado en <span class="math inline">\(X\)</span> porque las alturas promedio masculinas y femeninas no son tan diferentes en relación con la variabilidad dentro del grupo. ¿Pero podemos hacerlo mejor que simplemente adivinar? Para responder a esta pregunta, necesitamos una definición cuantitativa de “mejor”.</p>
<div id="sets-de-entrenamiento-y-de-evaluación" class="section level3">
<h3><span class="header-section-number">27.4.1</span> Sets de entrenamiento y de evaluación</h3>
<p>En última instancia, un algoritmo de <em>machine learning</em> se evalúa basado en cómo funciona en el mundo real con sets de datos completamente nuevos. Sin embargo, cuando desarrollamos un algoritmo, generalmente tenemos un set de datos para el cual conocemos los resultados, como lo hacemos con las alturas: sabemos el sexo de cada estudiante en nuestro set de datos. Por lo tanto, para imitar el proceso de evaluación final, generalmente dividimos los datos en dos partes y actuamos como si no supiéramos el resultado de una de estas. Dejamos de fingir que no conocemos el resultado para evaluar el algoritmo, pero solo <em>después</em> de haber terminado de construirlo. Nos referimos al grupo para el que conocemos el resultado y que usamos para desarrollar el algoritmo como el <em>set de entrenamiento</em> (<em>training set</em> en inglés). Nos referimos al grupo para el que pretendemos que no conocer el resultado como el <em>set de evaluación</em> (<em>test set</em> en inglés).</p>
<p>Una forma estándar de generar los sets de entrenamiento y de evaluación es dividiendo aleatoriamente los datos. El paquete <strong>caret</strong> incluye la función <code>createDataPartition</code> que nos ayuda a generar índices para dividir aleatoriamente los datos en sets de entrenamiento y de evaluación:</p>
<div class="sourceCode" id="cb1005"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1005-1"><a href="introducción-a-machine-learning.html#cb1005-1"></a><span class="kw">set.seed</span>(<span class="dv">2007</span>)</span>
<span id="cb1005-2"><a href="introducción-a-machine-learning.html#cb1005-2"></a>test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p>El argumento <code>times</code> se usa para definir cuántas muestras aleatorias de índices devolver, el argumento <code>p</code> se utiliza para definir qué proporción de los datos está representada por el índice y el argumento <code>list</code> se usa para decidir si queremos que los índices se devuelvan como una lista o no. Podemos usar el resultado de la llamada a la función <code>createDataPartition</code> para definir los sets de entrenamiento y de evaluación de esta manera:</p>
<div class="sourceCode" id="cb1006"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1006-1"><a href="introducción-a-machine-learning.html#cb1006-1"></a>test_set &lt;-<span class="st"> </span>heights[test_index, ]</span>
<span id="cb1006-2"><a href="introducción-a-machine-learning.html#cb1006-2"></a>train_set &lt;-<span class="st"> </span>heights[<span class="op">-</span>test_index, ]</span></code></pre></div>
<p>Ahora desarrollaremos un algoritmo usando <strong>solo</strong> el set de entrenamiento. Una vez que hayamos terminado de desarrollar el algoritmo, lo <em>congelaremos</em> y lo evaluaremos utilizando el set de evaluación. La forma más sencilla de evaluar el algoritmo cuando los resultados son categóricos es simplemente informar la proporción de casos que se predijeron correctamente <strong>en el set de evaluación</strong>. Esta métrica generalmente se conoce como <em>exactitud general</em> (<em>overall accuracy</em> en inglés).</p>
</div>
<div id="exactitud-general" class="section level3">
<h3><span class="header-section-number">27.4.2</span> Exactitud general</h3>
<p>Para demostrar el uso de la exactidud general, crearemos dos algoritmos diferentes y los compararemos.</p>
<p>Comencemos desarrollando el algoritmo más sencillo posible: adivinar el resultado.</p>
<div class="sourceCode" id="cb1007"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1007-1"><a href="introducción-a-machine-learning.html#cb1007-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), <span class="kw">length</span>(test_index), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Tengan en cuenta que estamos ignorando completamente el predictor y simplemente adivinando el sexo.</p>
<p>En las aplicaciones de <em>machine learning</em>, es útil usar factores para representar los resultados categóricos porque las funciones de R desarrolladas para <em>machine learning</em>, como las del paquete <strong>caret</strong>, requieren o recomiendan que los resultados categóricos se codifiquen como factores. Para convertir <code>y_hat</code> a factores podemos usar la función <code>factor</code>:</p>
<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1008-1"><a href="introducción-a-machine-learning.html#cb1008-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), <span class="kw">length</span>(test_index), <span class="dt">replace =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></span>
<span id="cb1008-2"><a href="introducción-a-machine-learning.html#cb1008-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span></code></pre></div>
<p>La <em>exactidud general</em> se define simplemente como la proporción general que se predice correctamente:</p>
<div class="sourceCode" id="cb1009"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1009-1"><a href="introducción-a-machine-learning.html#cb1009-1"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1009-2"><a href="introducción-a-machine-learning.html#cb1009-2"></a><span class="co">#&gt; [1] 0.51</span></span></code></pre></div>
<p>No es sorprendente que nuestra exactidud sea 50%. ¡Estamos adivinando!</p>
<p>¿Podemos mejorarla? El análisis de datos exploratorios sugiere que sí porque, en promedio, los hombres son un poco más altos que las mujeres:</p>
<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1010-1"><a href="introducción-a-machine-learning.html#cb1010-1"></a>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="kw">mean</span>(height), <span class="kw">sd</span>(height))</span>
<span id="cb1010-2"><a href="introducción-a-machine-learning.html#cb1010-2"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1010-3"><a href="introducción-a-machine-learning.html#cb1010-3"></a><span class="co">#&gt; # A tibble: 2 x 3</span></span>
<span id="cb1010-4"><a href="introducción-a-machine-learning.html#cb1010-4"></a><span class="co">#&gt;   sex    `mean(height)` `sd(height)`</span></span>
<span id="cb1010-5"><a href="introducción-a-machine-learning.html#cb1010-5"></a><span class="co">#&gt;   &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;</span></span>
<span id="cb1010-6"><a href="introducción-a-machine-learning.html#cb1010-6"></a><span class="co">#&gt; 1 Female           64.9         3.76</span></span>
<span id="cb1010-7"><a href="introducción-a-machine-learning.html#cb1010-7"></a><span class="co">#&gt; 2 Male             69.3         3.61</span></span></code></pre></div>
<p>Pero, ¿cómo usamos esta información? Probemos con otro enfoque sencillo: predecir <code>Male</code> si la altura está dentro de dos desviaciones estándar del hombre promedio.</p>
<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1011-1"><a href="introducción-a-machine-learning.html#cb1011-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">62</span>, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1011-2"><a href="introducción-a-machine-learning.html#cb1011-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span></code></pre></div>
<p>La exactidud aumenta de 0.50 a aproximadamente 0.80:</p>
<div class="sourceCode" id="cb1012"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1012-1"><a href="introducción-a-machine-learning.html#cb1012-1"></a><span class="kw">mean</span>(y <span class="op">==</span><span class="st"> </span>y_hat)</span>
<span id="cb1012-2"><a href="introducción-a-machine-learning.html#cb1012-2"></a><span class="co">#&gt; [1] 0.793</span></span></code></pre></div>
<p>¿Pero podemos mejorarla aún más? En el ejemplo anterior, utilizamos un umbral de 62, pero podemos examinar la exactidud obtenida para otros umbrales y luego elegir el valor que provee los mejores resultados. Sin embargo, recuerden que <strong>es importante que optimicemos el umbral utilizando solo el set de entrenamiento</strong>: el set de evaluación es solo para evaluación. Aunque para este ejemplo sencillo no es un problema, más adelante aprenderemos que evaluar un algoritmo en el set de entrenamiento puede conducir a un <em>sobreajuste</em> (<em>overfitting</em> en inglés), que a menudo resulta en evaluaciones peligrosamente sobre optimistas.</p>
<p>Aquí examinamos la exactidud de 10 umbrales diferentes y elegimos el que produce el mejor resultado:</p>
<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1013-1"><a href="introducción-a-machine-learning.html#cb1013-1"></a>cutoff &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">61</span>, <span class="dv">70</span>)</span>
<span id="cb1013-2"><a href="introducción-a-machine-learning.html#cb1013-2"></a>accuracy &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(cutoff, <span class="cf">function</span>(x){</span>
<span id="cb1013-3"><a href="introducción-a-machine-learning.html#cb1013-3"></a>  y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(train_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>x, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1013-4"><a href="introducción-a-machine-learning.html#cb1013-4"></a><span class="st">    </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1013-5"><a href="introducción-a-machine-learning.html#cb1013-5"></a>  <span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>train_set<span class="op">$</span>sex)</span>
<span id="cb1013-6"><a href="introducción-a-machine-learning.html#cb1013-6"></a>})</span></code></pre></div>
<p>Podemos hacer un gráfico que muestra la exactitud obtenida en el set de entrenamiento para hombres y mujeres:</p>
<p><img src="libro_files/figure-html/accuracy-vs-cutoff-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que el valor máximo es:</p>
<div class="sourceCode" id="cb1014"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1014-1"><a href="introducción-a-machine-learning.html#cb1014-1"></a><span class="kw">max</span>(accuracy)</span>
<span id="cb1014-2"><a href="introducción-a-machine-learning.html#cb1014-2"></a><span class="co">#&gt; [1] 0.85</span></span></code></pre></div>
<p>que es mucho más grande que 0.5. El umbral que resulta en esta exactitud es:</p>
<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1015-1"><a href="introducción-a-machine-learning.html#cb1015-1"></a>best_cutoff &lt;-<span class="st"> </span>cutoff[<span class="kw">which.max</span>(accuracy)]</span>
<span id="cb1015-2"><a href="introducción-a-machine-learning.html#cb1015-2"></a>best_cutoff</span>
<span id="cb1015-3"><a href="introducción-a-machine-learning.html#cb1015-3"></a><span class="co">#&gt; [1] 64</span></span></code></pre></div>
<p>Ahora podemos evaluar el uso de este umbral en nuestro set de evaluaciones para asegurarnos de que nuestra exactitud no sea demasiado optimista:</p>
<div class="sourceCode" id="cb1016"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1016-1"><a href="introducción-a-machine-learning.html#cb1016-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(test_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>best_cutoff, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1016-2"><a href="introducción-a-machine-learning.html#cb1016-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1016-3"><a href="introducción-a-machine-learning.html#cb1016-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">factor</span>(y_hat)</span>
<span id="cb1016-4"><a href="introducción-a-machine-learning.html#cb1016-4"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1016-5"><a href="introducción-a-machine-learning.html#cb1016-5"></a><span class="co">#&gt; [1] 0.804</span></span></code></pre></div>
<p>Vemos que es un poco más baja que la exactitud observada para el set de entrenamiento, pero aún es mejor que adivinar. Y al probar en un set de datos en el que no entrenamos, sabemos que nuestro resultado no se debe a que se haya elegido para dar un buen resultado en el set de evaluación.</p>
</div>
<div id="la-matriz-de-confusión" class="section level3">
<h3><span class="header-section-number">27.4.3</span> La matriz de confusión</h3>
<p>La regla de predicción que desarrollamos en la sección anterior predice <code>Male</code> si el alumno es más alto que 64 pulgadas. Dado que la mujer promedio es aproximadamente 64 pulgadas, esta regla de predicción parece incorrecta. ¿Que pasó? Si la altura de un estudiante es la de la mujer promedio, ¿no deberíamos predecir <code>Female</code>?</p>
<p>En términos generales, la exactitud general puede ser una medida engañosa. Para ver esto, comenzaremos construyendo lo que se conoce como <em>matriz de confusión</em> (<em>confusion matrix</em> en inglés), que básicamente tabula cada combinación de predicción y valor real. Podemos hacer esto en R usando la función <code>table</code>:</p>
<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1017-1"><a href="introducción-a-machine-learning.html#cb1017-1"></a><span class="kw">table</span>(<span class="dt">predicted =</span> y_hat, <span class="dt">actual =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1017-2"><a href="introducción-a-machine-learning.html#cb1017-2"></a><span class="co">#&gt;          actual</span></span>
<span id="cb1017-3"><a href="introducción-a-machine-learning.html#cb1017-3"></a><span class="co">#&gt; predicted Female Male</span></span>
<span id="cb1017-4"><a href="introducción-a-machine-learning.html#cb1017-4"></a><span class="co">#&gt;    Female     48   32</span></span>
<span id="cb1017-5"><a href="introducción-a-machine-learning.html#cb1017-5"></a><span class="co">#&gt;    Male       71  374</span></span></code></pre></div>
<p>Si estudiamos esta tabla detenidamente, revela un problema. Si calculamos la exactitud por separado para cada sexo, obtenemos:</p>
<div class="sourceCode" id="cb1018"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1018-1"><a href="introducción-a-machine-learning.html#cb1018-1"></a>test_set <span class="op">%&gt;%</span></span>
<span id="cb1018-2"><a href="introducción-a-machine-learning.html#cb1018-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> y_hat) <span class="op">%&gt;%</span></span>
<span id="cb1018-3"><a href="introducción-a-machine-learning.html#cb1018-3"></a><span class="st">  </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span></span>
<span id="cb1018-4"><a href="introducción-a-machine-learning.html#cb1018-4"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>sex))</span>
<span id="cb1018-5"><a href="introducción-a-machine-learning.html#cb1018-5"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1018-6"><a href="introducción-a-machine-learning.html#cb1018-6"></a><span class="co">#&gt; # A tibble: 2 x 2</span></span>
<span id="cb1018-7"><a href="introducción-a-machine-learning.html#cb1018-7"></a><span class="co">#&gt;   sex    accuracy</span></span>
<span id="cb1018-8"><a href="introducción-a-machine-learning.html#cb1018-8"></a><span class="co">#&gt;   &lt;fct&gt;     &lt;dbl&gt;</span></span>
<span id="cb1018-9"><a href="introducción-a-machine-learning.html#cb1018-9"></a><span class="co">#&gt; 1 Female    0.403</span></span>
<span id="cb1018-10"><a href="introducción-a-machine-learning.html#cb1018-10"></a><span class="co">#&gt; 2 Male      0.921</span></span></code></pre></div>
<p>Hay un desequilibrio en la exactitud para hombres y mujeres: se predice que demasiadas mujeres son hombres. ¡Estamos prediciendo que casi la mitad de las mujeres son hombres! ¿Cómo es que nuestra exactitud general sea tan alta? Esto se debe a que la <em>prevalencia</em> de los hombres en este set de datos es alta. Estas alturas se obtuvieron de tres cursos de ciencias de datos, dos de los cuales tenían más hombres matriculados:</p>
<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1019-1"><a href="introducción-a-machine-learning.html#cb1019-1"></a>prev &lt;-<span class="st"> </span><span class="kw">mean</span>(y <span class="op">==</span><span class="st"> &quot;Male&quot;</span>)</span>
<span id="cb1019-2"><a href="introducción-a-machine-learning.html#cb1019-2"></a>prev</span>
<span id="cb1019-3"><a href="introducción-a-machine-learning.html#cb1019-3"></a><span class="co">#&gt; [1] 0.773</span></span></code></pre></div>
<p>Entonces, al calcular la exactitud general, el alto porcentaje de errores cometidos para las mujeres se ve superado por las ganancias en las predicciones acertadas para los hombres. <strong>Esto puede ser un gran problema en <em>machine learning</em>.</strong> Si sus datos de entrenamiento están sesgados de alguna manera, es probable que también desarrolle algoritmos sesgados. El hecho de que hayamos utilizado un set de evaluación no importa porque también se deriva del original set de datos sesgado. Esta es una de las razones por las que observamos métricas distintas de la exactitud general al evaluar un algoritmo de <em>machine learning</em>.</p>
<p>Hay varias métricas que podemos usar para evaluar un algoritmo de manera que la prevalencia no afecte nuestra evaluación y todas estas pueden derivarse de la matriz de confusión. Una forma general de mejorar el uso de la exactitud general es estudiar <em>sensibilidad</em> y <em>especificidad</em> por separado.</p>
</div>
<div id="sensibilidad-y-especificidad" class="section level3">
<h3><span class="header-section-number">27.4.4</span> Sensibilidad y especificidad</h3>
<p>Para definir la sensibilidad y especificidad, necesitamos un resultado binario. Cuando los resultados son categóricos, podemos definir estos términos para una categoría específica. En el ejemplo de dígitos, podemos pedir la especificidad en el caso de predecir correctamente 2 en lugar de algún otro dígito. Una vez que especifiquemos una categoría de interés, podemos hablar sobre resultados positivos, <span class="math inline">\(Y=1\)</span> y resultados negativos, <span class="math inline">\(Y=0\)</span>.</p>
<p>En general, la <em>sensibilidad</em> se define como la capacidad de un algoritmo para predecir un resultado positivo cuando el resultado real es positivo: <span class="math inline">\(\hat{Y}=1\)</span> cuando <span class="math inline">\(Y=1\)</span>. Un algoritmo que predice que todo es positivo ( <span class="math inline">\(\hat{Y}=1\)</span> pase lo que pase) tiene una sensibilidad perfecta, pero esta métrica por sí sola no es suficiente para evaluar un algoritmo. Por esta razón, también examinamos la <em>especificidad</em>, que generalmente se define como la capacidad de un algoritmo para no predecir un resultado positivo <span class="math inline">\(\hat{Y}=0\)</span> cuando el resultado real no es positivo <span class="math inline">\(Y=0\)</span>. Podemos resumir de la siguiente manera:</p>
<ul>
<li>Alta sensibilidad: <span class="math inline">\(Y=1 \implies \hat{Y}=1\)</span></li>
<li>Alta especificidad: <span class="math inline">\(Y=0 \implies \hat{Y} = 0\)</span></li>
</ul>
<p>Aunque lo anterior a menudo se considera la definición de especificidad, otra forma de pensar en la especificidad es por la proporción de predicciones positivas que realmente son positivas:</p>
<ul>
<li>Alta especificidad: <span class="math inline">\(\hat{Y}=1 \implies Y=1\)</span>.</li>
</ul>
<p>Para ofrecer definiciones precisas, nombramos las cuatro entradas de la matriz de confusión:</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Actually Positive
</th>
<th style="text-align:left;">
Actually Negative
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Predicted positive
</td>
<td style="text-align:left;">
True positives (TP)
</td>
<td style="text-align:left;">
False positives (FP)
</td>
</tr>
<tr>
<td style="text-align:left;">
Predicted negative
</td>
<td style="text-align:left;">
False negatives (FN)
</td>
<td style="text-align:left;">
True negatives (TN)
</td>
</tr>
</tbody>
</table>
<p>La sensibilidad se cuantifica típicamente por <span class="math inline">\(TP/(TP+FN)\)</span>, la proporción de positivos verdaderos (la primera columna = <span class="math inline">\(TP+FN\)</span>) que se predicen ser positivos (<span class="math inline">\(TP\)</span>). Esta cantidad se conoce como la <em>tasa de positivos verdaderos</em> (<em>true positive rate</em> o TPR por sus siglas en inglés) o <em>recall</em>.</p>
<p>La especificidad se define como <span class="math inline">\(TN/(TN+FP)\)</span> o la proporción de negativos (la segunda columna = <span class="math inline">\(FP+TN\)</span>) que se llaman negativos (<span class="math inline">\(TN\)</span>). Esta cantidad también se denomina la <em>tasa de falsos positivos</em> (<em>true negative rate</em> o TNR por sus siglas en inglés). Hay otra forma de cuantificar la especificidad que es <span class="math inline">\(TP/(TP+FP)\)</span> o la proporción de resultados que se llaman positivos (la primera fila o <span class="math inline">\(TP+FP\)</span>) que realmente son positivos (<span class="math inline">\(TP\)</span>). Esta cantidad se conoce como <em>valor predictivo positivo</em> (<em>positive predictive value</em> o PPV por sus siglas en inglés) y también como <em>precisión</em>. Tengan en cuenta que, a diferencia de TPR y TNR, la precisión depende de la prevalencia, ya que una mayor prevalencia implica que pueden obtener una mayor precisión incluso cuando están adivinando.</p>
<p>Los diferentes nombres pueden ser confusos, por lo que incluimos una tabla para ayudarnos a recordar los términos. La tabla incluye una columna que muestra la definición si pensamos en las proporciones como probabilidades.</p>
<table>
<colgroup>
<col width="18%" />
<col width="10%" />
<col width="20%" />
<col width="16%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>Medida de</th>
<th>Nombre 1</th>
<th>Nombre 2</th>
<th>Definición</th>
<th>Representación de probabilidad</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sensibilidad</td>
<td>TPR</td>
<td>Recall</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP} + \mbox{FN}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=1 \mid Y=1)\)</span></td>
</tr>
<tr class="even">
<td>especificidad</td>
<td>TNR</td>
<td>1-FPR</td>
<td><span class="math inline">\(\frac{\mbox{TN}}{\mbox{TN}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(\hat{Y}=0 \mid Y=0)\)</span></td>
</tr>
<tr class="odd">
<td>especificidad</td>
<td>PPV</td>
<td>Precisión</td>
<td><span class="math inline">\(\frac{\mbox{TP}}{\mbox{TP}+\mbox{FP}}\)</span></td>
<td><span class="math inline">\(\mbox{Pr}(Y=1 \mid \hat{Y}=1)\)</span></td>
</tr>
</tbody>
</table>
<p>Aquí, TPR es tasa de positivos verdaderos, FPR es tasa de falsos positivos y PPV es valor predictivo positivo. La función <code>confusionMatrix</code> del paquete <strong>caret</strong> calcula todas estas métricas para nosotros una vez que definamos qué categoría es “positiva”. La función espera factores como entrada, y el primer nivel se considera el resultado positivo o <span class="math inline">\(Y=1\)</span>. En nuestro ejemplo, <code>Female</code> es el primer nivel porque viene antes de <code>Male</code> alfabéticamente. Si escriben esto en R, verán varias métricas que incluyen exactitud, sensibilidad, especificidad y PPV.</p>
<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1020-1"><a href="introducción-a-machine-learning.html#cb1020-1"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span></code></pre></div>
<p>Pueden acceder a estos directamente, por ejemplo, así:</p>
<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1021-1"><a href="introducción-a-machine-learning.html#cb1021-1"></a>cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1021-2"><a href="introducción-a-machine-learning.html#cb1021-2"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1021-3"><a href="introducción-a-machine-learning.html#cb1021-3"></a><span class="co">#&gt;    0.804</span></span>
<span id="cb1021-4"><a href="introducción-a-machine-learning.html#cb1021-4"></a>cm<span class="op">$</span>byClass[<span class="kw">c</span>(<span class="st">&quot;Sensitivity&quot;</span>,<span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Prevalence&quot;</span>)]</span>
<span id="cb1021-5"><a href="introducción-a-machine-learning.html#cb1021-5"></a><span class="co">#&gt; Sensitivity Specificity  Prevalence </span></span>
<span id="cb1021-6"><a href="introducción-a-machine-learning.html#cb1021-6"></a><span class="co">#&gt;       0.403       0.921       0.227</span></span></code></pre></div>
<p>Podemos ver que la alta exactitud general es posible a pesar de la sensibilidad relativamente baja. Como sugerimos anteriormente, la razón por la que esto sucede es debido a la baja prevalencia (0.23): la proporción de mujeres es baja. Como la prevalencia es baja, no predecir mujeres reales como mujeres (baja sensibilidad) no disminuye la exactitud tanto como no predecir hombres reales como hombres (baja especificidad). Este es un ejemplo de por qué es importante examinar la sensibilidad y la especificidad y no solo la exactitud. Antes de aplicar este algoritmo a sets de datos generales, debemos preguntarnos si la prevalencia será la misma.</p>
</div>
<div id="exactitud-equilibrada-y-medida-f_1" class="section level3">
<h3><span class="header-section-number">27.4.5</span> Exactitud equilibrada y medida <span class="math inline">\(F_1\)</span></h3>
<p>Aunque generalmente recomendamos estudiar tanto la especificidad como la sensibilidad, a menudo es útil tener un resumen de un número, por ejemplo, para fines de optimización. Una medida que se prefiere sobre la exactitud general es el promedio de especificidad y sensibilidad, conocida como <em>exactitud equilibrada</em> (<em>balanced accuracy</em> en inglés). Debido a que la especificidad y la sensibilidad son tasas, es más apropiado calcular la <em>media armónica</em> (<em>harmonic average</em> en inglés). De hecho, la <em>medida <span class="math inline">\(F_1\)</span></em> (<em><span class="math inline">\(F_1\)</span>-score</em> en inglés), un resumen de un número ampliamente utilizado, es la media armónica de precisión y <em>recall</em>:</p>
<p><span class="math display">\[
\frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} +
\frac{1}{\mbox{precision}}\right) }
\]</span></p>
<p>Debido a que es más fácil de escribir, a menudo se ve esta media armónica reescrita como:</p>
<p><span class="math display">\[
2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
\]</span></p>
<p>al definir <span class="math inline">\(F_1\)</span>.</p>
<p>Recuerden que, según el contexto, algunos tipos de errores son más costosos que otros. Por ejemplo, en el caso de la seguridad de los aviones, es mucho más importante maximizar la sensibilidad sobre la especificidad: no predecir el mal funcionamiento de un avión antes de que se estrelle es un error mucho más costoso que aterrizar un avión cuando, de hecho, el avión está en perfectas condiciones. En un caso criminal de asesinato, lo contrario es cierto ya que un falso positivo puede llevar a ejecutar a una persona inocente. La medida <span class="math inline">\(F_1\)</span> se puede adaptar para pesar la especificidad y la sensibilidad de manera diferente. Para hacer esto, definimos <span class="math inline">\(\beta\)</span> para representar cuánto más importante es la sensibilidad en comparación con la especificidad y consideramos una media armónica ponderada:</p>
<p><span class="math display">\[
\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} +
\frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
\]</span></p>
<p>La función <code>F_meas</code> en el paquete <strong>caret</strong> calcula este resumen con un valor de <code>beta</code> igual a 1 por defecto.</p>
<p>Reconstruyamos nuestro algoritmo de predicción, pero esta vez maximizando la medida F en lugar de la exactitud general:</p>
<div class="sourceCode" id="cb1022"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1022-1"><a href="introducción-a-machine-learning.html#cb1022-1"></a>cutoff &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">61</span>, <span class="dv">70</span>)</span>
<span id="cb1022-2"><a href="introducción-a-machine-learning.html#cb1022-2"></a>F_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(cutoff, <span class="cf">function</span>(x){</span>
<span id="cb1022-3"><a href="introducción-a-machine-learning.html#cb1022-3"></a>  y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(train_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>x, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1022-4"><a href="introducción-a-machine-learning.html#cb1022-4"></a><span class="st">    </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1022-5"><a href="introducción-a-machine-learning.html#cb1022-5"></a>  <span class="kw">F_meas</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> <span class="kw">factor</span>(train_set<span class="op">$</span>sex))</span>
<span id="cb1022-6"><a href="introducción-a-machine-learning.html#cb1022-6"></a>})</span></code></pre></div>
<p>Como antes, podemos trazar estas medidas <span class="math inline">\(F_1\)</span> versus los umbrales:</p>
<p><img src="libro_files/figure-html/f_1-vs-cutoff-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que el maximo de la medida <span class="math inline">\(F_1\)</span> es:</p>
<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1023-1"><a href="introducción-a-machine-learning.html#cb1023-1"></a><span class="kw">max</span>(F_<span class="dv">1</span>)</span>
<span id="cb1023-2"><a href="introducción-a-machine-learning.html#cb1023-2"></a><span class="co">#&gt; [1] 0.647</span></span></code></pre></div>
<p>Este máximo se logra cuando usamos el siguiente umbral:</p>
<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1024-1"><a href="introducción-a-machine-learning.html#cb1024-1"></a>best_cutoff &lt;-<span class="st"> </span>cutoff[<span class="kw">which.max</span>(F_<span class="dv">1</span>)]</span>
<span id="cb1024-2"><a href="introducción-a-machine-learning.html#cb1024-2"></a>best_cutoff</span>
<span id="cb1024-3"><a href="introducción-a-machine-learning.html#cb1024-3"></a><span class="co">#&gt; [1] 66</span></span></code></pre></div>
<p>Un umbral de 65 tiene más sentido que de 64. Además, equilibra la especificidad y la sensibilidad de nuestra matriz de confusión:</p>
<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1025-1"><a href="introducción-a-machine-learning.html#cb1025-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(test_set<span class="op">$</span>height <span class="op">&gt;</span><span class="st"> </span>best_cutoff, <span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1025-2"><a href="introducción-a-machine-learning.html#cb1025-2"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1025-3"><a href="introducción-a-machine-learning.html#cb1025-3"></a><span class="kw">sensitivity</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1025-4"><a href="introducción-a-machine-learning.html#cb1025-4"></a><span class="co">#&gt; [1] 0.63</span></span>
<span id="cb1025-5"><a href="introducción-a-machine-learning.html#cb1025-5"></a><span class="kw">specificity</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> test_set<span class="op">$</span>sex)</span>
<span id="cb1025-6"><a href="introducción-a-machine-learning.html#cb1025-6"></a><span class="co">#&gt; [1] 0.833</span></span></code></pre></div>
<p>Ahora vemos que obtenemos mejores resultados que adivinando, que tanto la sensibilidad como la especificidad son relativamente altas y que hemos construido nuestro primer algoritmo de <em>machine learning</em>. Este toma altura como predictor y predice mujeres si la persona mide 65 pulgadas o menos.</p>
</div>
<div id="la-prevalencia-importa-en-la-práctica" class="section level3">
<h3><span class="header-section-number">27.4.6</span> La prevalencia importa en la práctica</h3>
<p>Un algoritmo de <em>machine learning</em> con sensibilidad y especificidad muy altas puede ser inútil en la práctica cuando la prevalencia se acerca a 0 o 1. Para ver esto, consideren el caso de una doctora que se especializa en una enfermedad rara y que está interesada en desarrollar un algoritmo para predecir quién tiene la enfermedad. La doctora comparte los datos con ustedes, que entonces desarrollan un algoritmo con una sensibilidad muy alta. Explican que esto significa que si un paciente tiene la enfermedad, es muy probable que el algoritmo prediga correctamente. También le dicen a la doctora que están preocupados porque, según el set de datos que analizaron, la mitad de los pacientes tienen la enfermedad: <span class="math inline">\(\mbox{Pr}(\hat{Y}=1)\)</span>. La doctora no está preocupada ni impresionada y explica que lo importante es la precisión de la evaluación: <span class="math inline">\(\mbox{Pr}(Y=1 | \hat{Y}=1)\)</span>. Usando el teorema de Bayes, podemos conectar las dos medidas:</p>
<p><span class="math display">\[ \mbox{Pr}(Y = 1\mid \hat{Y}=1) = \mbox{Pr}(\hat{Y}=1 \mid Y=1) \frac{\mbox{Pr}(Y=1)}{\mbox{Pr}(\hat{Y}=1)}\]</span></p>
<p>La doctora sabe que la prevalencia de la enfermedad es de 5 en 1,000, lo que implica que <span class="math inline">\(\mbox{Pr}(Y=1) \,/ \,\mbox{Pr}(\hat{Y}=1) = 1/100\)</span> y, por lo tanto, la precisión de su algoritmo es inferior a 0.01. La doctora no tiene mucho uso para su algoritmo.</p>
</div>
<div id="curvas-roc-y-precision-recall" class="section level3">
<h3><span class="header-section-number">27.4.7</span> Curvas ROC y precision-recall</h3>
<p>Al comparar los dos métodos (adivinar versus usar un umbral de altura), comparamos la exactitud y <span class="math inline">\(F_1\)</span>. El segundo método claramente superó al primero. Sin embargo, si bien consideramos varios umbrales para el segundo método, para el primero solo consideramos un enfoque: adivinar con igual probabilidad. Noten que adivinar <code>Male</code> con mayor probabilidad nos daría una mayor exactitud debido al sesgo en la muestra:</p>
<div class="sourceCode" id="cb1026"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1026-1"><a href="introducción-a-machine-learning.html#cb1026-1"></a>p &lt;-<span class="st"> </span><span class="fl">0.9</span></span>
<span id="cb1026-2"><a href="introducción-a-machine-learning.html#cb1026-2"></a>n &lt;-<span class="st"> </span><span class="kw">length</span>(test_index)</span>
<span id="cb1026-3"><a href="introducción-a-machine-learning.html#cb1026-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(p, <span class="dv">1</span><span class="op">-</span>p)) <span class="op">%&gt;%</span></span>
<span id="cb1026-4"><a href="introducción-a-machine-learning.html#cb1026-4"></a><span class="st">  </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">levels</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1026-5"><a href="introducción-a-machine-learning.html#cb1026-5"></a><span class="kw">mean</span>(y_hat <span class="op">==</span><span class="st"> </span>test_set<span class="op">$</span>sex)</span>
<span id="cb1026-6"><a href="introducción-a-machine-learning.html#cb1026-6"></a><span class="co">#&gt; [1] 0.739</span></span></code></pre></div>
<p>Pero, como se describió anteriormente, esto tendría el costo de una menor sensibilidad. Las curvas que describimos en esta sección nos ayudarán a ver esto.</p>
<p>Recuerden que para cada uno de estos parámetros, podemos obtener una sensibilidad y especificidad diferente. Por esta razón, un enfoque muy común para evaluar métodos es compararlos gráficamente trazando ambos.</p>
<p>Un gráfico ampliamente utilizado que hace esto es la curva <em>característica operativa del receptor</em> (<em>Receiver Operating Characteristic</em> o ROC por sus siglas en inglés). Para aprender más sobre el origen del nombre, pueden consultar la página de Wikipedia Curva ROC<a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a>.</p>
<p>La curva ROC representa la sensibilidad (TPR) frente a la especificidad 1 o la tasa de falsos positivos (FPR). Aquí calculamos el TPR y el FPR necesarios para diferentes probabilidades de adivinar <code>Male</code>:</p>
<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1027-1"><a href="introducción-a-machine-learning.html#cb1027-1"></a>probs &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb1027-2"><a href="introducción-a-machine-learning.html#cb1027-2"></a>guessing &lt;-<span class="st"> </span><span class="kw">map_df</span>(probs, <span class="cf">function</span>(p){</span>
<span id="cb1027-3"><a href="introducción-a-machine-learning.html#cb1027-3"></a>  y_hat &lt;-</span>
<span id="cb1027-4"><a href="introducción-a-machine-learning.html#cb1027-4"></a><span class="st">    </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>, <span class="st">&quot;Female&quot;</span>), n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob=</span><span class="kw">c</span>(p, <span class="dv">1</span><span class="op">-</span>p)) <span class="op">%&gt;%</span></span>
<span id="cb1027-5"><a href="introducción-a-machine-learning.html#cb1027-5"></a><span class="st">    </span><span class="kw">factor</span>(<span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>))</span>
<span id="cb1027-6"><a href="introducción-a-machine-learning.html#cb1027-6"></a>  <span class="kw">list</span>(<span class="dt">method =</span> <span class="st">&quot;Guessing&quot;</span>,</span>
<span id="cb1027-7"><a href="introducción-a-machine-learning.html#cb1027-7"></a>       <span class="dt">FPR =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">specificity</span>(y_hat, test_set<span class="op">$</span>sex),</span>
<span id="cb1027-8"><a href="introducción-a-machine-learning.html#cb1027-8"></a>       <span class="dt">TPR =</span> <span class="kw">sensitivity</span>(y_hat, test_set<span class="op">$</span>sex))</span>
<span id="cb1027-9"><a href="introducción-a-machine-learning.html#cb1027-9"></a>})</span></code></pre></div>
<p>Podemos usar un código similar para calcular estos valores para nuestro segundo enfoque. Al graficar ambas curvas juntas, podemos comparar la sensibilidad para diferentes valores de especificidad:</p>
<!--We can construct an ROC curve for the height-based approach:-->
<!--
<img src="libro_files/figure-html/roc-2-1.png" width="70%" style="display: block; margin: auto;" />
-->
<p><img src="libro_files/figure-html/roc-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que obtenemos una mayor sensibilidad con este enfoque para todos los valores de especificidad, lo que implica que es un método mejor. Tengan en cuenta que si simplemente adivinamos, las curvas ROC caen en la línea de identidad. También noten que cuando hacemos curvas ROC, a veces ayuda agregar el umbral asociado con cada punto a la gráfica.</p>
<p>Los paquetes <strong>pROC</strong> y <strong>plotROC</strong> son útiles para generar estos gráficos.</p>
<p>Las curvas ROC tienen una debilidad y es que ninguna de las medidas graficadas depende de la prevalencia. En los casos en que la prevalencia es importante, en su lugar podemos hacer un gráfico <em>precision-recall</em>. La idea es similar, pero en cambio graficamos la precisión versus el <em>recall</em>:</p>
<p><img src="libro_files/figure-html/precision-recall-1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>En este gráfico inmediatamente vemos que la precisión de adivinar no es alta. Esto se debe a que la prevalencia es baja. También vemos que si cambiamos los positivos para que representen “Male” en lugar de “Female”, la curva ROC permanece igual, pero el gráfico <em>precision-recall</em> cambia.</p>
</div>
<div id="loss-function" class="section level3">
<h3><span class="header-section-number">27.4.8</span> La función de pérdida</h3>
<p>Hasta ahora hemos descrito métricas de evaluación que se aplican exclusivamente a datos categóricos. Específicamente, para los resultados binarios, hemos descrito cómo la sensibilidad, especificidad, precisión y <span class="math inline">\(F_1\)</span> se pueden utilizar como cuantificación. Sin embargo, estas métricas no son útiles para resultados continuos. En esta sección, describimos cómo el enfoque general para definir “mejor” en <em>machine learning</em> es definir una <em>función de pérdida</em> (<em>loss function</em> en inglés), que puede aplicarse tanto a datos categóricos como continuos.</p>
<p>La función de pérdida más utilizada es la función de pérdida al cuadrado. Si <span class="math inline">\(\hat{y}\)</span> es nuestro predictor e <span class="math inline">\(y\)</span> es el resultado observado, la función de pérdida al cuadrado es simplemente:</p>
<p><span class="math display">\[
(\hat{y} - y)^2
\]</span></p>
<p>Debido a que frecuentemente tenemos un set de evaluaciones con muchas observaciones, digamos <span class="math inline">\(N\)</span>, usamos el error cuadrático medio (<em>mean squared error</em> o MSE por sus siglas en inglés):</p>
<p><span class="math display">\[
\mbox{MSE} = \frac{1}{N} \mbox{RSS} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
\]</span></p>
<p>En la práctica, a menudo indicamos la raiz cudadrada de la desviación cuadrática media (<em>root mean squared error</em> o RMSE por sus siglas en inglés), que es <span class="math inline">\(\sqrt{\mbox{MSE}}\)</span>, porque está en las mismas unidades que los resultados. Pero hacer las matemáticas muchas veces es más fácil con el MSE y, por lo tanto, se usa más en los libros de texto, ya que estos generalmente describen las propiedades teóricas de los algoritmos.</p>
<p>Si los resultados son binarios, tanto RMSE como MSE son equivalentes a la exactitud menos uno, ya que <span class="math inline">\((\hat{y} - y)^2\)</span> es 0 si la predicción fue correcta y 1 en caso contrario. En general, nuestro objetivo es construir un algoritmo que minimice la pérdida para que esté lo más cerca posible de 0.</p>
<p>Debido a que nuestros datos son generalmente una muestra aleatoria, podemos pensar en el MSE como una variable aleatoria y el MSE observado puede considerarse como una estimación del MSE esperado, que en notación matemática escribimos así:</p>
<p><span class="math display">\[
\mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
\]</span></p>
<p>Este es un concepto teórico porque en la práctica solo tenemos un set de datos con el cual trabajar. Una forma de pensar en esta expectativa teórica es que es el MSE promedio para una gran número de muestras aleatorias (llámelo <span class="math inline">\(B\)</span>): aplicamos nuestro algoritmo a cada muestra aleatoria, calculamos el MSE observado y tomamos el promedio:</p>
<p><span class="math display">\[
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2
\]</span></p>
<p>con <span class="math inline">\(y_{i}^b\)</span> denotando la observación <span class="math inline">\(i\)</span> en la muestra aleatoria <span class="math inline">\(b\)</span> e <span class="math inline">\(\hat{y}_i^b\)</span> denotando la predicción resultante obtenida de aplicar exactamente el mismo algoritmo a la muestra aleatoria <span class="math inline">\(b\)</span>. Nuevamente, en la práctica solo observamos una muestra aleatoria, por lo que el MSE esperado es solo teórico. Sin embargo, en el Capítulo <a href="cross-validation.html#cross-validation">29</a> describimos un enfoque para estimar el MSE que trata de imitar esta cantidad teórica.</p>
<p>Tengan en cuenta que hay funciones de pérdida distintas de la función de pérdida cuadrática. Por ejemplo, el <em>Error Medio Absoluto</em> (<em>Mean Absolute Error</em> en inglés) utiliza valores absolutos, <span class="math inline">\(|\hat{Y}_i - Y_i|\)</span> en lugar de cuadrar los errores
<span class="math inline">\((\hat{Y}_i - Y_i)^2\)</span>. Sin embargo, en este libro nos enfocamos en minimizar la función de pérdida cuadrática ya que es la más utilizada.</p>
</div>
</div>
<div id="ejercicios-45" class="section level2">
<h2><span class="header-section-number">27.5</span> Ejercicios</h2>
<p>Los sets de datos <code>reported_height</code> y <code>height</code> se recopilaron de tres clases impartidas en los Departamentos de Ciencias Computacionales y Bioestadística, así como de forma remota a través de la Escuela de Extensión. La clase de bioestadística se impartió en 2016 junto con una versión en línea ofrecida por la Escuela de Extensión. El 25 de enero de 2016 a las 8:15 a.m., durante una de las clases, los instructores le pidieron a los estudiantes que completaran el cuestionario de sexo y altura que poblaba el set de datos <code>reported_height</code>. Los estudiantes en línea completaron la encuesta durante los próximos días, después de que la conferencia se publicara en línea. Podemos usar esta información para definir una variable, llamarla <code>type</code>, para denotar el tipo de estudiante, <code>inclass</code> (presenciales) o <code>online</code> (en línea):</p>
<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1028-1"><a href="introducción-a-machine-learning.html#cb1028-1"></a><span class="kw">library</span>(lubridate)</span>
<span id="cb1028-2"><a href="introducción-a-machine-learning.html#cb1028-2"></a><span class="kw">data</span>(<span class="st">&quot;reported_heights&quot;</span>)</span>
<span id="cb1028-3"><a href="introducción-a-machine-learning.html#cb1028-3"></a>dat &lt;-<span class="st"> </span><span class="kw">mutate</span>(reported_heights, <span class="dt">date_time =</span> <span class="kw">ymd_hms</span>(time_stamp)) <span class="op">%&gt;%</span></span>
<span id="cb1028-4"><a href="introducción-a-machine-learning.html#cb1028-4"></a><span class="st">  </span><span class="kw">filter</span>(date_time <span class="op">&gt;=</span><span class="st"> </span><span class="kw">make_date</span>(<span class="dv">2016</span>, <span class="dv">01</span>, <span class="dv">25</span>) <span class="op">&amp;</span></span>
<span id="cb1028-5"><a href="introducción-a-machine-learning.html#cb1028-5"></a><span class="st">           </span>date_time <span class="op">&lt;</span><span class="st"> </span><span class="kw">make_date</span>(<span class="dv">2016</span>, <span class="dv">02</span>, <span class="dv">1</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1028-6"><a href="introducción-a-machine-learning.html#cb1028-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">type =</span> <span class="kw">ifelse</span>(<span class="kw">day</span>(date_time) <span class="op">==</span><span class="st"> </span><span class="dv">25</span> <span class="op">&amp;</span><span class="st"> </span><span class="kw">hour</span>(date_time) <span class="op">==</span><span class="st"> </span><span class="dv">8</span> <span class="op">&amp;</span></span>
<span id="cb1028-7"><a href="introducción-a-machine-learning.html#cb1028-7"></a><span class="st">                         </span><span class="kw">between</span>(<span class="kw">minute</span>(date_time), <span class="dv">15</span>, <span class="dv">30</span>),</span>
<span id="cb1028-8"><a href="introducción-a-machine-learning.html#cb1028-8"></a>                       <span class="st">&quot;inclass&quot;</span>, <span class="st">&quot;online&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(sex, type)</span>
<span id="cb1028-9"><a href="introducción-a-machine-learning.html#cb1028-9"></a>x &lt;-<span class="st"> </span>dat<span class="op">$</span>type</span>
<span id="cb1028-10"><a href="introducción-a-machine-learning.html#cb1028-10"></a>y &lt;-<span class="st"> </span><span class="kw">factor</span>(dat<span class="op">$</span>sex, <span class="kw">c</span>(<span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>))</span></code></pre></div>
<p>1. Muestre estadísticas de resumen que indican que el <code>type</code> es predictivo del sexo.</p>
<p>2. En lugar de usar la altura para predecir el sexo, use el variable <code>type</code>.</p>
<p>3. Muestre la matriz de confusión.</p>
<p>4. Utilice la función <code>confusionMatrix</code> en el paquete <strong>caret</strong> para indicar la exactitud.</p>
<p>5. Ahora use las funciones <code>sensitivity</code> y <code>specificity</code> para indicar especificidad y sensibilidad.</p>
<p>6. ¿Cuál es la prevalencia (% de mujeres) en el set de datos <code>dat</code> definido anteriormente?</p>

</div>
<div id="probabilidades-y-expectativas-condicionales" class="section level2">
<h2><span class="header-section-number">27.6</span> Probabilidades y expectativas condicionales</h2>
<p>En las aplicaciones de <em>machine learning</em>, rara vez podemos predecir los resultados perfectamente. Por ejemplo, los detectores de spam a menudo no detectan correos electrónicos que son claramente spam, Siri no siempre entiende las palabras que estamos diciendo y su banco a veces piensa que su tarjeta fue robada cuando no fue así. La razón más común para no poder construir algoritmos perfectos es que es imposible. Para entender esto, noten que la mayoría de los sets de datos incluirán grupos de observaciones con los mismos valores exactos observados para todos los predictores, pero con diferentes resultados. Debido a que nuestras reglas de predicción son funciones, entradas iguales (los predictores) implican que los resultados (las predicciones) tienen que ser iguales. Por lo tanto, para un set de datos en el que los mismos predictores se asocian con diferentes resultados en diferentes observaciones individuales, es imposible predecir correctamente para todos estos casos. Vimos un ejemplo sencillo de esto en la sección anterior: para cualquier altura dada <span class="math inline">\(x\)</span>, tendrán hombres y mujeres que son <span class="math inline">\(x\)</span> pulgadas de alto.</p>
<p>Sin embargo, nada de esto significa que no podamos construir algoritmos útiles que sean mucho mejores que adivinar, y que en algunos casos sean mejores que las opiniones de expertos. Para lograr esto de manera óptima, hacemos uso de representaciones probabilísticas del problema basadas en las ideas presentadas en la Sección <a href="regression.html#conditional-expectation">17.3</a>. Las observaciones con los mismos valores observados para los predictores pueden ser desiguales, pero podemos suponer que todas tienen la misma probabilidad de esta clase o de esa clase. Escribiremos esta idea matemáticamente para el caso de datos categóricos.</p>
<div id="probabilidades-condicionales-1" class="section level3">
<h3><span class="header-section-number">27.6.1</span> Probabilidades condicionales</h3>
<p>Usamos la notación <span class="math inline">\((X_1 = x_1,\dots,X_p=x_p)\)</span> para representar el hecho de que hemos observado valores <span class="math inline">\(x_1, \dots ,x_p\)</span> para covariables <span class="math inline">\(X_1, \dots, X_p\)</span>. Esto no implica que el resultado <span class="math inline">\(Y\)</span> tomará un valor específico. En cambio, implica una probabilidad específica. En particular, denotamos las <em>probabilidades condicionales</em> para cada clase <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\mbox{Pr}(Y=k \mid X_1 = x_1,\dots,X_p=x_p), \, \mbox{for}\,k=1,\dots,K
\]</span></p>
<p>Para evitar escribir todos los predictores, usaremos fuente negra así: <span class="math inline">\(\mathbf{X} \equiv (X_1,\dots,X_p)\)</span> y <span class="math inline">\(\mathbf{x} \equiv (x_1,\dots,x_p)\)</span>. También usaremos la siguiente notación para la probabilidad condicional de ser clase <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
\]</span></p>
<p>Ojo: Utilizaremos la notación <span class="math inline">\(p(x)\)</span> para representar probabilidades condicionales como funciones de los predictores. No lo confundan con el <span class="math inline">\(p\)</span> que representa el número de predictores.</p>
<p>Estas probabilidades guían la construcción de un algoritmo que mejora la predicción: para cualquier <span class="math inline">\(\mathbf{x}\)</span>, vamos a predecir la clase <span class="math inline">\(k\)</span> con la mayor probabilidad entre <span class="math inline">\(p_1(x), p_2(x), \dots p_K(x)\)</span>. En notación matemática, lo escribimos así: <span class="math inline">\(\hat{Y} = \max_k p_k(\mathbf{x})\)</span>.</p>
<p>En <em>machine learning</em>, nos referimos a esto como la <em>Regla de Bayes</em>. Pero tengan en cuenta que esta es una regla teórica ya que en la práctica no sabemos <span class="math inline">\(p_k(\mathbf{x}), k=1,\dots,K\)</span>. De hecho, estimar estas probabilidades condicionales puede considerarse como el principal desafío de <em>machine learning</em>. Cuanto mejores sean nuestros estimados de la probabilidad <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span>, mejor será nuestro predictor:</p>
<p><span class="math display">\[\hat{Y} = \max_k \hat{p}_k(\mathbf{x})\]</span></p>
<p>Entonces, lo que predeciremos depende de dos cosas: 1) cuán cerca están las <span class="math inline">\(\max_k p_k(\mathbf{x})\)</span> a 1 o 0 (certeza perfecta)
y 2) cuán cerca están nuestros estimados de <span class="math inline">\(\hat{p}_k(\mathbf{x})\)</span> a <span class="math inline">\(p_k(\mathbf{x})\)</span>. No podemos hacer nada con respecto a la primera restricción, ya que está determinada por la naturaleza del problema y, por lo tanto, nos dedicaremos a encontrar buenas formas de estimar las probabilidades condicionales. La primera restricción implica que tenemos límites en cuanto a cuán bien puede funcionar hasta el mejor algoritmo posible. Deberían acostumbrarse a la idea de que, si bien en algunos retos podremos lograr una exactitud casi perfecta, por ejemplo con lectores de dígitos, en otros nuestro éxito está restringido por la aleatoriedad del proceso, como con recomendaciones de películas.</p>
<p>Antes de continuar, es importante recordar que definir nuestra predicción maximizando la probabilidad no siempre es óptimo en la práctica y depende del contexto. Como se discutió anteriormente, la sensibilidad y la especificidad pueden diferir en importancia. Pero incluso en estos casos, tener un buen estimado de la <span class="math inline">\(p_k(x), k=1,\dots,K\)</span> nos bastará para construir modelos de predicción óptimos, ya que podemos controlar el equilibrio entre especificidad y sensibilidad como queramos. Por ejemplo, simplemente podemos cambiar los umbrales utilizados para predecir un resultado u otro. En el ejemplo del avión, podemos aterrizar el avión en cualquier momento en que la probabilidad de mal funcionamiento sea superior a 1 en un millón, en lugar del 1/2 predeterminado que se usa cuando los tipos de error son igualmente indeseados.</p>
</div>
<div id="expectativas-condicionales" class="section level3">
<h3><span class="header-section-number">27.6.2</span> Expectativas condicionales</h3>
<p>Para datos binarios, pueden pensar en la probabilidad <span class="math inline">\(\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})\)</span> como la proporción de 1s en el estrato de la población para la cual <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span>. Muchos de los algoritmos que aprenderemos se pueden aplicar tanto a datos categóricos como continuos debido a la conexión entre las <em>probabilidades condicionales</em> y las <em>expectativas condicionales</em>.</p>
<p>Porque la expectativa es el promedio de los valores <span class="math inline">\(y_1,\dots,y_n\)</span> en la población, en el caso en que las <span class="math inline">\(y\)</span>s son 0 o 1, la expectativa es equivalente a la probabilidad de elegir aleatoriamente un uno ya que el promedio es simplemente la proporción de unos:</p>
<p><span class="math display">\[
\mbox{E}(Y \mid \mathbf{X}=\mathbf{x})=\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}).
\]</span></p>
<p>Como resultado, a menudo solo usamos la expectativa para denotar tanto la probabilidad condicional como la expectativa condicional.</p>
<p>Al igual que con los resultados categóricos, en la mayoría de las aplicaciones, los mismos predictores observados no garantizan los mismos resultados continuos. En cambio, suponemos que el resultado sigue la misma distribución condicional. Ahora explicaremos por qué usamos la expectativa condicional para definir nuestros predictores.</p>
</div>
<div id="la-expectativa-condicional-minimiza-la-función-de-pérdida-cuadrática" class="section level3">
<h3><span class="header-section-number">27.6.3</span> La expectativa condicional minimiza la función de pérdida cuadrática</h3>
<p>¿Por qué nos importa la expectativa condicional en <em>machine learning</em>? Se debe a que el valor esperado tiene una propiedad matemática atractiva: minimiza el MSE. Específicamente, de todas las predicciones posibles <span class="math inline">\(\hat{Y}\)</span>,</p>
<p><span class="math display">\[
\hat{Y} = \mbox{E}(Y \mid \mathbf{X}=\mathbf{x}) \, \mbox{ minimizes } \, \mbox{E}\{ (\hat{Y} - Y)^2 \mid \mathbf{X}=\mathbf{x} \}
\]</span></p>
<p>Debido a esta propiedad, una descripción sucinta de la tarea principal de <em>machine learning</em> es que utilizamos datos para estimar:</p>
<p><span class="math display">\[
f(\mathbf{x}) \equiv \mbox{E}( Y \mid \mathbf{X}=\mathbf{x} )
\]</span></p>
<p>para cualquier conjunto de características <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_p)\)</span>. Por supuesto, esto es más fácil decirlo que hacerlo, ya que esta función puede tomar cualquier forma y <span class="math inline">\(p\)</span> puede ser muy grande. Consideren un caso en el que solo tenemos un predictor <span class="math inline">\(x\)</span>. La expectativa <span class="math inline">\(\mbox{E}\{ Y \mid X=x \}\)</span> puede ser cualquier función de <span class="math inline">\(x\)</span>: una línea, una parábola, una onda sinusoidal, una función escalón, etc.. Se vuelve aún más complicado cuando consideramos instancias con grandes <span class="math inline">\(p\)</span>, en cual caso <span class="math inline">\(f(\mathbf{x})\)</span> es una función de un vector multidimensional <span class="math inline">\(\mathbf{x}\)</span>. Por ejemplo, en nuestro ejemplo de lector de dígitos <span class="math inline">\(p = 784\)</span>! <strong>La principal forma en que los algoritmos competitivos de <em>machine learning</em> difieren es en su enfoque para estimar esta expectativa. </strong></p>
</div>
</div>
<div id="ejercicios-46" class="section level2">
<h2><span class="header-section-number">27.7</span> Ejercicios</h2>
<p>1. Calcule las probabilidades condicionales de ser hombre para el set the datos <code>heights</code>. Redondee las alturas a la pulgada más cercana. Grafique la probabilidad condicional estimada <span class="math inline">\(P(x) = \mbox{Pr}(\mbox{Male} | \mbox{height}=x)\)</span> para cada <span class="math inline">\(x\)</span>.</p>
<p>2. En el gráfico que acabamos de hacer, vemos una gran variabilidad para valores bajos de altura. Esto se debe a que tenemos pocos puntos de datos en estos estratos. Use la función <code>quantile</code> para cuantiles <span class="math inline">\(0.1,0.2,\dots,0.9\)</span> y la función <code>cut</code> para asegurar que cada grupo tenga el mismo número de puntos. Sugerencia: para cualquier vector numérico <code>x</code>, puede crear grupos basados en cuantiles como este:</p>
<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1029-1"><a href="introducción-a-machine-learning.html#cb1029-1"></a><span class="kw">cut</span>(x, <span class="kw">quantile</span>(x, <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)), <span class="dt">include.lowest =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>3. Genere datos a partir de una distribución normal de dos variables utilizando el paquete <strong>MASS</strong> como este:</p>
<div class="sourceCode" id="cb1030"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1030-1"><a href="introducción-a-machine-learning.html#cb1030-1"></a>Sigma &lt;-<span class="st"> </span><span class="dv">9</span><span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1030-2"><a href="introducción-a-machine-learning.html#cb1030-2"></a>dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma) <span class="op">%&gt;%</span></span>
<span id="cb1030-3"><a href="introducción-a-machine-learning.html#cb1030-3"></a><span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>))</span></code></pre></div>
<p>Puede hacer un gráfico rápido de los datos usando <code>plot(dat)</code>. Use un enfoque similar al ejercicio anterior para estimar las expectativas condicionales y haga un gráfico.</p>

</div>
<div id="two-or-seven" class="section level2">
<h2><span class="header-section-number">27.8</span> Estudio de caso: ¿es un 2 o un 7?</h2>
<p>En los dos ejemplos anteriores, solo teníamos un predictor. Realmente no consideramos estos retos de <em>machine learning</em>, que se caracterizan por casos con muchos predictores. Volvamos al ejemplo de dígitos en el que teníamos 784 predictores. Para fines ilustrativos, comenzaremos simplificando este problema a uno con dos predictores y dos clases. Específicamente, definimos el desafío como construir un algoritmo que pueda determinar si un dígito es un 2 o 7 de los predictores. No estamos del todo listos para construir algoritmos con 784 predictores, por lo que extraeremos dos predictores sencillos de los 784: la proporción de píxeles oscuros que están en el cuadrante superior izquierdo (<span class="math inline">\(X_1\)</span>) y el cuadrante inferior derecho (<span class="math inline">\(X_2\)</span>).</p>
<p>Entonces seleccionamos una muestra aleatoria de 1,000 dígitos, 500 en el set de entrenamiento y 500 en el set de evaluación. Proveemos este set de datos en el paquete <code>dslabs</code>:</p>
<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1031-1"><a href="introducción-a-machine-learning.html#cb1031-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1031-2"><a href="introducción-a-machine-learning.html#cb1031-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1031-3"><a href="introducción-a-machine-learning.html#cb1031-3"></a><span class="kw">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</span></code></pre></div>
<p>Podemos explorar los datos graficando los dos predictores y usando colores para denotar las etiquetas:</p>
<div class="sourceCode" id="cb1032"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1032-1"><a href="introducción-a-machine-learning.html#cb1032-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">color =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="libro_files/figure-html/two-or-seven-scatter-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Inmediatamente podemos ver algunos patrones. Por ejemplo, si <span class="math inline">\(X_1\)</span> (el panel superior izquierdo) es muy grande, entonces el dígito es probablemente un 7. Además, para valores más pequeños de <span class="math inline">\(X_1\)</span>, los 2s parecen estar en los valores de rango medio de <span class="math inline">\(X_2\)</span>.</p>
<p>Para ilustrar como interpretar <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span>, incluimos cuatro imágenes como ejemplo. A la izquierda están las imágenes originales de los dos dígitos con los valores más grandes y más pequeños para <span class="math inline">\(X_1\)</span> y a la derecha tenemos las imágenes correspondientes a los valores más grandes y más pequeños de <span class="math inline">\(X_2\)</span>:</p>
<p><img src="libro_files/figure-html/two-or-seven-images-large-x1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Comenzamos a tener una idea de por qué estos predictores son útiles, pero también por qué el problema será algo desafiante.</p>
<p>Realmente no hemos aprendido ningún algoritmo todavía, así que intentemos construir un algoritmo usando regresión. El modelo es simplemente:</p>
<p><span class="math display">\[
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2) =
\beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>Lo ajustamos así:</p>
<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1033-1"><a href="introducción-a-machine-learning.html#cb1033-1"></a>fit &lt;-<span class="st"> </span>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span></span>
<span id="cb1033-2"><a href="introducción-a-machine-learning.html#cb1033-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">ifelse</span>(y<span class="op">==</span><span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1033-3"><a href="introducción-a-machine-learning.html#cb1033-3"></a><span class="st">  </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x_<span class="dv">2</span>, <span class="dt">data =</span> .)</span></code></pre></div>
<p>Ahora podemos construir una regla de decisión basada en el estimado de <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>:</p>
<div class="sourceCode" id="cb1034"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1034-1"><a href="introducción-a-machine-learning.html#cb1034-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1034-2"><a href="introducción-a-machine-learning.html#cb1034-2"></a>p_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newdata =</span> mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb1034-3"><a href="introducción-a-machine-learning.html#cb1034-3"></a>y_hat &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(p_hat <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">7</span>, <span class="dv">2</span>))</span>
<span id="cb1034-4"><a href="introducción-a-machine-learning.html#cb1034-4"></a><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[[<span class="st">&quot;Accuracy&quot;</span>]]</span>
<span id="cb1034-5"><a href="introducción-a-machine-learning.html#cb1034-5"></a><span class="co">#&gt; [1] 0.75</span></span></code></pre></div>
<p>Obtenemos una exactidud muy superior al 50%. No está mal para nuestro primer intento. ¿Pero podemos mejorar?</p>
<p>Como construimos el ejemplo <code>mnist_27</code> y teníamos a nuestra disposición 60,000 dígitos solo en el set de datos MNIST, lo usamos para construir la distribución condicional <em>verdadera</em> <span class="math inline">\(p(x_1, x_2)\)</span>. Recuerden que esto es algo a lo que no tenemos acceso en la práctica, pero lo incluimos en este ejemplo porque permite comparar <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> con la verdadera <span class="math inline">\(p(x_1, x_2)\)</span>. Esta comparación nos enseña las limitaciones de diferentes algoritmos. Hagamos eso aquí. Hemos almacenado el verdadero <span class="math inline">\(p(x_1,x_2)\)</span> en el objeto <code>mnist_27</code> y podemos graficar la imagen usando la función <code>geom_raster()</code> de <strong>ggplot2</strong> . Elegimos mejores colores y usamos la función <code>stat_contour</code> para dibujar una curva que separe pares <span class="math inline">\((x_1,x_2)\)</span> para cual <span class="math inline">\(p(x_1,x_2) &gt; 0.5\)</span> y pares para los cuales <span class="math inline">\(p(x_1,x_2) &lt; 0.5\)</span>:</p>
<div class="sourceCode" id="cb1035"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1035-1"><a href="introducción-a-machine-learning.html#cb1035-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>true_p <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">z =</span> p, <span class="dt">fill =</span> p)) <span class="op">+</span></span>
<span id="cb1035-2"><a href="introducción-a-machine-learning.html#cb1035-2"></a><span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span></span>
<span id="cb1035-3"><a href="introducción-a-machine-learning.html#cb1035-3"></a><span class="st">  </span><span class="kw">scale_fill_gradientn</span>(<span class="dt">colors=</span><span class="kw">c</span>(<span class="st">&quot;#F8766D&quot;</span>, <span class="st">&quot;white&quot;</span>, <span class="st">&quot;#00BFC4&quot;</span>)) <span class="op">+</span></span>
<span id="cb1035-4"><a href="introducción-a-machine-learning.html#cb1035-4"></a><span class="st">  </span><span class="kw">stat_contour</span>(<span class="dt">breaks=</span><span class="kw">c</span>(<span class="fl">0.5</span>), <span class="dt">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/true-p-better-colors-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Arriba ven un gráfico del verdadero <span class="math inline">\(p(x,y)\)</span>. Para comenzar a comprender las limitaciones de la regresión logística aquí, primero tengan en cuenta que con la regresión logística <span class="math inline">\(\hat{p}(x,y)\)</span> tiene que ser un plano y, como resultado, el límite definido por la regla de decisión viene dado por:
<span class="math inline">\(\hat{p}(x,y) = 0.5\)</span>, lo que implica que el límite no puede ser otra cosa que una línea recta:</p>
<p><span class="math display">\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies
x_2 = (0.5-\hat{\beta}_0)/\hat{\beta}_2 -\hat{\beta}_1/\hat{\beta}_2 x_1
\]</span>
Tengan en cuenta que para este límite, <span class="math inline">\(x_2\)</span> es una función lineal de <span class="math inline">\(x_1\)</span>. Esto implica que nuestro enfoque de regresión logística no tiene posibilidades de capturar la naturaleza no lineal de la verdadera <span class="math inline">\(p(x_1,x_2)\)</span>. A continuación se muestra una representación visual de <span class="math inline">\(\hat{p}(x_1, x_2)\)</span>. Utilizamos la función <code>squish</code> del paquete <strong>scales</strong> para restringir los estimados entre 0 y 1. Podemos ver dónde se cometieron los errores al mostrar también los datos y el límite. Principalmente provienen de valores bajos <span class="math inline">\(x_1\)</span> que tienen un valor alto o bajo de <span class="math inline">\(x_2\)</span>. La regresión no puede detectar esto.</p>
<p><img src="libro_files/figure-html/regression-p-hat-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Necesitamos algo más flexible: un método que permita estimados con formas distintas a un plano.</p>
<p>Vamos a aprender algunos algoritmos nuevos basados en diferentes ideas y conceptos. Pero lo que todos tienen en común es que permiten enfoques más flexibles. Comenzaremos describiendo alogoritmos basados en <em>nearest neighbor</em> o <em>kernels</em>. Para introducir los conceptos detrás de estos enfoques, comenzaremos nuevamente con un ejemplo unidimensional sencillo y describiremos el concepto de <em>suavización</em> (<em>smoothing</em> en inglés).</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="101">
<li id="fn101"><p><a href="https://es.wikipedia.org/wiki/Curva_ROC" class="uri">https://es.wikipedia.org/wiki/Curva_ROC</a><a href="introducción-a-machine-learning.html#fnref101" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="minería-de-textos.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="suavización.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dslibro/edit/master/ml/intro-ml.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
