<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 31 Ejemplos de algoritmos | Introducción a la Ciencia de Datos</title>
  <meta name="description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 31 Ejemplos de algoritmos | Introducción a la Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 31 Ejemplos de algoritmos | Introducción a la Ciencia de Datos" />
  
  <meta name="twitter:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2021-04-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="caret.html"/>
<link rel="next" href="machine-learning-en-la-práctica.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introducción a la Ciencia de Datos</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="" data-path="agradecimientos.html"><a href="agradecimientos.html"><i class="fa fa-check"></i>Agradecimientos</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i>Introducción</a><ul>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#los-casos-de-estudio"><i class="fa fa-check"></i>Los casos de estudio</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#quién-encontrará-útil-este-libro"><i class="fa fa-check"></i>¿Quién encontrará útil este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#que-cubre-este-libro"><i class="fa fa-check"></i>¿Que cubre este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#qué-no-cubre-este-libro"><i class="fa fa-check"></i>¿Qué no cubre este libro?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Comenzando con R y RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#por-qué-r"><i class="fa fa-check"></i><b>1.1</b> ¿Por qué R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#la-consola-r"><i class="fa fa-check"></i><b>1.2</b> La consola R</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> <em>Scripts</em></a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#paneles"><i class="fa fa-check"></i><b>1.4.1</b> Paneles</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> <em>Key bindings</em></a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#cómo-ejecutar-comandos-mientras-edita-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Cómo ejecutar comandos mientras edita <em>scripts</em></a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#cómo-cambiar-las-opciones-globales"><i class="fa fa-check"></i><b>1.4.4</b> Cómo cambiar las opciones globales</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#instalación-de-paquetes-de-r"><i class="fa fa-check"></i><b>1.5</b> Instalación de paquetes de R</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> Lo básico de R</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#caso-de-estudio-los-asesinatos-con-armas-en-ee.-uu."><i class="fa fa-check"></i><b>2.1</b> Caso de estudio: los asesinatos con armas en EE. UU.</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#lo-básico"><i class="fa fa-check"></i><b>2.2</b> Lo básico</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objetos"><i class="fa fa-check"></i><b>2.2.1</b> Objetos</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#el-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.2</b> El espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#funciones"><i class="fa fa-check"></i><b>2.2.3</b> Funciones</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#otros-objetos-predefinidos"><i class="fa fa-check"></i><b>2.2.4</b> Otros objetos predefinidos</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#nombres-de-variables"><i class="fa fa-check"></i><b>2.2.5</b> Nombres de variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#cómo-guardar-su-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.6</b> Cómo guardar su espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#scripts-motivantes"><i class="fa fa-check"></i><b>2.2.7</b> <em>Scripts</em> motivantes</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#cómo-comentar-su-código"><i class="fa fa-check"></i><b>2.2.8</b> Cómo comentar su código</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#ejercicios"><i class="fa fa-check"></i><b>2.3</b> Ejercicios</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#tipos-de-datos"><i class="fa fa-check"></i><b>2.4</b> Tipos de datos</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> <em>data frames</em></a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#cómo-examinar-un-objeto"><i class="fa fa-check"></i><b>2.4.2</b> Cómo examinar un objeto</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#el-operador-de-acceso"><i class="fa fa-check"></i><b>2.4.3</b> El operador de acceso: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectores-numéricos-de-caracteres-y-lógicos"><i class="fa fa-check"></i><b>2.4.4</b> Vectores: numéricos, de caracteres y lógicos</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factores</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#listas"><i class="fa fa-check"></i><b>2.4.6</b> Listas</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#ejercicios-1"><i class="fa fa-check"></i><b>2.5</b> Ejercicios</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectores</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-vectores"><i class="fa fa-check"></i><b>2.6.1</b> Cómo crear vectores</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#nombres"><i class="fa fa-check"></i><b>2.6.2</b> Nombres</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#secuencias"><i class="fa fa-check"></i><b>2.6.3</b> Secuencias</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-un-subconjunto"><i class="fa fa-check"></i><b>2.6.4</b> Cómo crear un subconjunto</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#la-conversión-forzada"><i class="fa fa-check"></i><b>2.7</b> La conversión forzada</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-available-na"><i class="fa fa-check"></i><b>2.7.1</b> <em>Not available</em> (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#ejercicios-2"><i class="fa fa-check"></i><b>2.8</b> Ejercicios</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> <em>Sorting</em></a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-y-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> y <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#cuidado-con-el-reciclaje"><i class="fa fa-check"></i><b>2.9.5</b> Cuidado con el reciclaje</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#ejercicios-3"><i class="fa fa-check"></i><b>2.10</b> Ejercicios</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#aritmética-de-vectores"><i class="fa fa-check"></i><b>2.11</b> Aritmética de vectores</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-un-vector"><i class="fa fa-check"></i><b>2.11.1</b> <em>Rescaling</em> un vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#dos-vectores"><i class="fa fa-check"></i><b>2.11.2</b> Dos vectores</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#ejercicios-4"><i class="fa fa-check"></i><b>2.12</b> Ejercicios</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexación"><i class="fa fa-check"></i><b>2.13</b> Indexación</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#crear-subconjuntos-con-lógicos"><i class="fa fa-check"></i><b>2.13.1</b> Crear subconjuntos con lógicos</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#operadores-lógicos"><i class="fa fa-check"></i><b>2.13.2</b> Operadores lógicos</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#ejercicios-5"><i class="fa fa-check"></i><b>2.14</b> Ejercicios</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#gráficos-básicos"><i class="fa fa-check"></i><b>2.15</b> Gráficos básicos</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#ejercicios-6"><i class="fa fa-check"></i><b>2.16</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html"><i class="fa fa-check"></i><b>3</b> Conceptos básicos de programación</a><ul>
<li class="chapter" data-level="3.1" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Expresiones condicionales</a></li>
<li class="chapter" data-level="3.2" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#cómo-definir-funciones"><i class="fa fa-check"></i><b>3.2</b> Cómo definir funciones</a></li>
<li class="chapter" data-level="3.3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> <em>Namespaces</em></a></li>
<li class="chapter" data-level="3.4" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#bucles-for"><i class="fa fa-check"></i><b>3.4</b> Bucles-for</a></li>
<li class="chapter" data-level="3.5" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorización y funcionales</a></li>
<li class="chapter" data-level="3.6" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#ejercicios-7"><i class="fa fa-check"></i><b>3.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Data <em>tidy</em></a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-8"><i class="fa fa-check"></i><b>4.2</b> Ejercicios</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-manipular-los-data-frames"><i class="fa fa-check"></i><b>4.3</b> Cómo manipular los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-añadir-una-columna-con-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Cómo añadir una columna con <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-subconjuntos-con-filter"><i class="fa fa-check"></i><b>4.3.2</b> Cómo crear subconjuntos con <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-seleccionar-columnas-con-select"><i class="fa fa-check"></i><b>4.3.3</b> Cómo seleccionar columnas con <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-9"><i class="fa fa-check"></i><b>4.4</b> Ejercicios</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#el-pipe"><i class="fa fa-check"></i><b>4.5</b> El <em>pipe</em>: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-10"><i class="fa fa-check"></i><b>4.6</b> Ejercicios</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#cómo-resumir-datos"><i class="fa fa-check"></i><b>4.7</b> Cómo resumir datos</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Cómo agrupar y luego resumir con <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-los-data-frames"><i class="fa fa-check"></i><b>4.8</b> Cómo ordenar los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-anidadamente"><i class="fa fa-check"></i><b>4.8.1</b> Cómo ordenar anidadamente</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#los-primeros-n"><i class="fa fa-check"></i><b>4.8.2</b> Los primeros <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-11"><i class="fa fa-check"></i><b>4.9</b> Ejercicios</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> <em>Tibbles</em></a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-ven-mejor"><i class="fa fa-check"></i><b>4.10.1</b> Los <em>tibbles</em> se ven mejor</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#los-subconjuntos-de-tibbles-son-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Los subconjuntos de <em>tibbles</em> son <em>tibbles</em></a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-pueden-tener-entradas-complejas"><i class="fa fa-check"></i><b>4.10.3</b> Los <em>tibbles</em> pueden tener entradas complejas</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-pueden-agrupar"><i class="fa fa-check"></i><b>4.10.4</b> Los <em>tibbles</em> se pueden agrupar</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-un-tibble-usando-tibble-en-lugar-de-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Cómo crear un <em>tibble</em> usando <code>tibble</code> en lugar de <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#el-operador-punto"><i class="fa fa-check"></i><b>4.11</b> El operador punto</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#el-paquete-purrr"><i class="fa fa-check"></i><b>4.13</b> El paquete <strong>purrr</strong></a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#los-condicionales-de-tidyverse"><i class="fa fa-check"></i><b>4.14</b> Los condicionales de <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-12"><i class="fa fa-check"></i><b>4.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importando datos</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#las-rutas-y-el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1</b> Las rutas y el directorio de trabajo</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#el-sistema-de-archivos"><i class="fa fa-check"></i><b>5.1.1</b> El sistema de archivos</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#las-rutas-relativas-y-completas"><i class="fa fa-check"></i><b>5.1.2</b> Las rutas relativas y completas</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1.3</b> El directorio de trabajo</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#cómo-generar-los-nombres-de-ruta"><i class="fa fa-check"></i><b>5.1.4</b> Cómo generar los nombres de ruta</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#cómo-copiar-los-archivos-usando-rutas"><i class="fa fa-check"></i><b>5.1.5</b> Cómo copiar los archivos usando rutas</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#los-paquetes-readr-y-readxl"><i class="fa fa-check"></i><b>5.2</b> Los paquetes readr y readxl</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#ejercicios-13"><i class="fa fa-check"></i><b>5.3</b> Ejercicios</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#cómo-descargar-archivos"><i class="fa fa-check"></i><b>5.4</b> Cómo descargar archivos</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#las-funciones-de-importación-de-base-r"><i class="fa fa-check"></i><b>5.5</b> Las funciones de importación de base R</a><ul>
<li class="chapter" data-level="5.5.1" data-path="importing-data.html"><a href="importing-data.html#scan"><i class="fa fa-check"></i><b>5.5.1</b> <code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#archivos-de-texto-versus-archivos-binarios"><i class="fa fa-check"></i><b>5.6</b> Archivos de texto versus archivos binarios</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#cómo-organizar-datos-con-hojas-de-cálculo"><i class="fa fa-check"></i><b>5.8</b> Cómo organizar datos con hojas de cálculo</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#ejercicios-14"><i class="fa fa-check"></i><b>5.9</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>II Visualización de datos</b></span></li>
<li class="chapter" data-level="6" data-path="introducción-a-la-visualización-de-datos.html"><a href="introducción-a-la-visualización-de-datos.html"><i class="fa fa-check"></i><b>6</b> Introducción a la visualización de datos</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#los-componentes-de-un-gráfico"><i class="fa fa-check"></i><b>7.1</b> Los componentes de un gráfico</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#objetos-ggplot"><i class="fa fa-check"></i><b>7.2</b> objetos <code>ggplot</code></a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometrías"><i class="fa fa-check"></i><b>7.3</b> Geometrías</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos"><i class="fa fa-check"></i><b>7.4</b> Mapeos estéticos</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#capas"><i class="fa fa-check"></i><b>7.5</b> Capas</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#cómo-probar-varios-argumentos"><i class="fa fa-check"></i><b>7.5.1</b> Cómo probar varios argumentos</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos-globales-versus-locales"><i class="fa fa-check"></i><b>7.6</b> Mapeos estéticos globales versus locales</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#escalas"><i class="fa fa-check"></i><b>7.7</b> Escalas</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#etiquetas-y-títulos"><i class="fa fa-check"></i><b>7.8</b> Etiquetas y títulos</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categorías-como-colores"><i class="fa fa-check"></i><b>7.9</b> Categorías como colores</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#anotación-formas-y-ajustes"><i class="fa fa-check"></i><b>7.10</b> Anotación, formas y ajustes</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Paquetes complementarios</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#cómo-combinarlo-todo"><i class="fa fa-check"></i><b>7.12</b> Cómo combinarlo todo</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Gráficos rápidos con <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#cuadrículas-de-gráficos"><i class="fa fa-check"></i><b>7.14</b> Cuadrículas de gráficos</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#ejercicios-15"><i class="fa fa-check"></i><b>7.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Cómo visualizar distribuciones de datos</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#tipos-de-variables"><i class="fa fa-check"></i><b>8.1</b> Tipos de variables</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#estudio-de-caso-describiendo-alturas-de-estudiantes"><i class="fa fa-check"></i><b>8.2</b> Estudio de caso: describiendo alturas de estudiantes</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#la-función-de-distribución"><i class="fa fa-check"></i><b>8.3</b> La función de distribución</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Funciones de distribución acumulada</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histogramas"><i class="fa fa-check"></i><b>8.5</b> Histogramas</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#densidad-suave"><i class="fa fa-check"></i><b>8.6</b> Densidad suave</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#cómo-interpretar-el-eje-y"><i class="fa fa-check"></i><b>8.6.1</b> Cómo interpretar el eje-y</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densidades-permiten-estratificación"><i class="fa fa-check"></i><b>8.6.2</b> Densidades permiten estratificación</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#ejercicios-16"><i class="fa fa-check"></i><b>8.7</b> Ejercicios</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> La distribución normal</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#unidades-estándar"><i class="fa fa-check"></i><b>8.9</b> Unidades estándar</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#gráficos-q-q"><i class="fa fa-check"></i><b>8.10</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja"><i class="fa fa-check"></i><b>8.12</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Estratificación</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Estudio de caso: descripción de alturas de estudiantes (continuación)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#ejercicios-17"><i class="fa fa-check"></i><b>8.15</b> Ejercicios</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> Geometrías ggplot2</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#diagramas-de-barras"><i class="fa fa-check"></i><b>8.16.1</b> Diagramas de barras</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histogramas-1"><i class="fa fa-check"></i><b>8.16.2</b> Histogramas</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#gráficos-de-densidad"><i class="fa fa-check"></i><b>8.16.3</b> Gráficos de densidad</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja-1"><i class="fa fa-check"></i><b>8.16.4</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#gráficos-q-q-1"><i class="fa fa-check"></i><b>8.16.5</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#imágenes"><i class="fa fa-check"></i><b>8.16.6</b> Imágenes</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#gráficos-rápidos"><i class="fa fa-check"></i><b>8.16.7</b> Gráficos rápidos</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#ejercicios-18"><i class="fa fa-check"></i><b>8.17</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Visualización de datos en la práctica</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#estudio-de-caso-nuevas-ideas-sobre-la-pobreza"><i class="fa fa-check"></i><b>9.1</b> Estudio de caso: nuevas ideas sobre la pobreza</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#la-prueba-de-hans-rosling"><i class="fa fa-check"></i><b>9.1.1</b> La prueba de Hans Rosling</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-dispersión"><i class="fa fa-check"></i><b>9.2</b> Diagrama de dispersión</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#separar-en-facetas"><i class="fa fa-check"></i><b>9.3</b> Separar en facetas</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#escalas-fijas-para-mejores-comparaciones"><i class="fa fa-check"></i><b>9.3.2</b> Escalas fijas para mejores comparaciones</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#gráficos-de-series-de-tiempo"><i class="fa fa-check"></i><b>9.4</b> Gráficos de series de tiempo</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#etiquetas-en-lugar-de-leyendas"><i class="fa fa-check"></i><b>9.4.1</b> Etiquetas en lugar de leyendas</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#transformaciones-de-datos"><i class="fa fa-check"></i><b>9.5</b> Transformaciones de datos</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#transformación-logarítmica"><i class="fa fa-check"></i><b>9.5.1</b> Transformación logarítmica</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#qué-base"><i class="fa fa-check"></i><b>9.5.2</b> ¿Qué base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transformar-los-valores-o-la-escala"><i class="fa fa-check"></i><b>9.5.3</b> ¿Transformar los valores o la escala?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#cómo-visualizar-distribuciones-multimodales"><i class="fa fa-check"></i><b>9.6</b> Cómo visualizar distribuciones multimodales</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#cómo-comparar-múltiples-distribuciones-con-diagramas-de-caja-y-gráficos-ridge"><i class="fa fa-check"></i><b>9.7</b> Cómo comparar múltiples distribuciones con diagramas de caja y gráficos <em>ridge</em></a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-caja"><i class="fa fa-check"></i><b>9.7.1</b> Diagrama de caja</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#gráficos-ridge"><i class="fa fa-check"></i><b>9.7.2</b> Gráficos <em>ridge</em></a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#ejemplo-distribuciones-de-ingresos-de-1970-versus-2010"><i class="fa fa-check"></i><b>9.7.3</b> Ejemplo: distribuciones de ingresos de 1970 versus 2010</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#cómo-obtener-acceso-a-variables-calculadas"><i class="fa fa-check"></i><b>9.7.4</b> Cómo obtener acceso a variables calculadas</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#densidades-ponderadas"><i class="fa fa-check"></i><b>9.7.5</b> Densidades ponderadas</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#la-falacia-ecológica-y-la-importancia-de-mostrar-los-datos"><i class="fa fa-check"></i><b>9.8</b> La falacia ecológica y la importancia de mostrar los datos</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Transformación logística</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#mostrar-los-datos"><i class="fa fa-check"></i><b>9.8.2</b> Mostrar los datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html"><i class="fa fa-check"></i><b>10</b> Principios de visualización de datos</a><ul>
<li class="chapter" data-level="10.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-datos-utilizando-señales-visuales"><i class="fa fa-check"></i><b>10.1</b> Cómo codificar datos utilizando señales visuales</a></li>
<li class="chapter" data-level="10.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#sepa-cuándo-incluir-0"><i class="fa fa-check"></i><b>10.2</b> Sepa cuándo incluir 0</a></li>
<li class="chapter" data-level="10.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#no-distorsionar-cantidades"><i class="fa fa-check"></i><b>10.3</b> No distorsionar cantidades</a></li>
<li class="chapter" data-level="10.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ordenar-categorías-por-un-valor-significativo"><i class="fa fa-check"></i><b>10.4</b> Ordenar categorías por un valor significativo</a></li>
<li class="chapter" data-level="10.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#mostrar-los-datos-1"><i class="fa fa-check"></i><b>10.5</b> Mostrar los datos</a></li>
<li class="chapter" data-level="10.6" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-facilitar-comparaciones"><i class="fa fa-check"></i><b>10.6</b> Cómo facilitar comparaciones</a><ul>
<li class="chapter" data-level="10.6.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-ejes-comunes"><i class="fa fa-check"></i><b>10.6.1</b> Use ejes comunes</a></li>
<li class="chapter" data-level="10.6.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#alinee-gráficos-verticalmente-para-ver-cambios-horizontales-y-horizontalmente-para-ver-cambios-verticales"><i class="fa fa-check"></i><b>10.6.2</b> Alinee gráficos verticalmente para ver cambios horizontales y horizontalmente para ver cambios verticales</a></li>
<li class="chapter" data-level="10.6.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#considere-transformaciones"><i class="fa fa-check"></i><b>10.6.3</b> Considere transformaciones</a></li>
<li class="chapter" data-level="10.6.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#señales-visuales-comparadas-deben-estar-adyacentes"><i class="fa fa-check"></i><b>10.6.4</b> Señales visuales comparadas deben estar adyacentes</a></li>
<li class="chapter" data-level="10.6.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#piense-en-los-daltónicos"><i class="fa fa-check"></i><b>10.7</b> Piense en los daltónicos</a></li>
<li class="chapter" data-level="10.8" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráficos-para-dos-variables"><i class="fa fa-check"></i><b>10.8</b> Gráficos para dos variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráfico-bland-altman"><i class="fa fa-check"></i><b>10.8.2</b> Gráfico Bland-Altman</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-una-tercera-variable"><i class="fa fa-check"></i><b>10.9</b> Cómo codificar una tercera variable</a></li>
<li class="chapter" data-level="10.10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-los-gráficos-pseudo-tridimensionales"><i class="fa fa-check"></i><b>10.10</b> Evite los gráficos pseudo-tridimensionales</a></li>
<li class="chapter" data-level="10.11" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-demasiados-dígitos-significativos"><i class="fa fa-check"></i><b>10.11</b> Evite demasiados dígitos significativos</a></li>
<li class="chapter" data-level="10.12" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#conozca-a-su-audiencia"><i class="fa fa-check"></i><b>10.12</b> Conozca a su audiencia</a></li>
<li class="chapter" data-level="10.13" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-19"><i class="fa fa-check"></i><b>10.13</b> Ejercicios</a></li>
<li class="chapter" data-level="10.14" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Estudio de caso: las vacunas y las enfermedades infecciosas</a></li>
<li class="chapter" data-level="10.15" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-20"><i class="fa fa-check"></i><b>10.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Resúmenes robustos</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#valores-atípicos"><i class="fa fa-check"></i><b>11.1</b> Valores atípicos</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#mediana"><i class="fa fa-check"></i><b>11.2</b> Mediana</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#el-rango-intercuartil-iqr"><i class="fa fa-check"></i><b>11.3</b> El rango intercuartil (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#la-definición-de-tukey-de-un-valor-atípico"><i class="fa fa-check"></i><b>11.4</b> La definición de Tukey de un valor atípico</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#desviación-absoluta-mediana"><i class="fa fa-check"></i><b>11.5</b> Desviación absoluta mediana</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#ejercicios-21"><i class="fa fa-check"></i><b>11.6</b> Ejercicios</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#estudio-de-caso-alturas-autoreportadas-de-estudiantes"><i class="fa fa-check"></i><b>11.7</b> Estudio de caso: alturas autoreportadas de estudiantes</a></li>
</ul></li>
<li class="part"><span><b>III Estadísticas con R</b></span></li>
<li class="chapter" data-level="12" data-path="introducción-a-las-estadísticas-con-r.html"><a href="introducción-a-las-estadísticas-con-r.html"><i class="fa fa-check"></i><b>12</b> Introducción a las estadísticas con R</a></li>
<li class="chapter" data-level="13" data-path="probabilidad.html"><a href="probabilidad.html"><i class="fa fa-check"></i><b>13</b> Probabilidad</a><ul>
<li class="chapter" data-level="13.1" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-discreta"><i class="fa fa-check"></i><b>13.1</b> Probabilidad discreta</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probabilidad.html"><a href="probabilidad.html#frecuencia-relativa"><i class="fa fa-check"></i><b>13.1.1</b> Frecuencia relativa</a></li>
<li class="chapter" data-level="13.1.2" data-path="probabilidad.html"><a href="probabilidad.html#notación"><i class="fa fa-check"></i><b>13.1.2</b> Notación</a></li>
<li class="chapter" data-level="13.1.3" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>13.1.3</b> Distribuciones de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-datos-categóricos"><i class="fa fa-check"></i><b>13.2</b> Simulaciones Monte Carlo para datos categóricos</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probabilidad.html"><a href="probabilidad.html#fijar-la-semilla-aleatoria"><i class="fa fa-check"></i><b>13.2.1</b> Fijar la semilla aleatoria</a></li>
<li class="chapter" data-level="13.2.2" data-path="probabilidad.html"><a href="probabilidad.html#con-y-sin-reemplazo"><i class="fa fa-check"></i><b>13.2.2</b> Con y sin reemplazo</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probabilidad.html"><a href="probabilidad.html#independencia"><i class="fa fa-check"></i><b>13.3</b> Independencia</a></li>
<li class="chapter" data-level="13.4" data-path="probabilidad.html"><a href="probabilidad.html#probabilidades-condicionales"><i class="fa fa-check"></i><b>13.4</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="13.5" data-path="probabilidad.html"><a href="probabilidad.html#reglas-de-la-adición-y-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5</b> Reglas de la adición y de la multiplicación</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5.1</b> Regla de la multiplicación</a></li>
<li class="chapter" data-level="13.5.2" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación-bajo-independencia"><i class="fa fa-check"></i><b>13.5.2</b> Regla de la multiplicación bajo independencia</a></li>
<li class="chapter" data-level="13.5.3" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-adición"><i class="fa fa-check"></i><b>13.5.3</b> Regla de la adición</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probabilidad.html"><a href="probabilidad.html#combinaciones-y-permutaciones"><i class="fa fa-check"></i><b>13.6</b> Combinaciones y permutaciones</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-monte-carlo"><i class="fa fa-check"></i><b>13.6.1</b> Ejemplo Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos"><i class="fa fa-check"></i><b>13.7</b> Ejemplos</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probabilidad.html"><a href="probabilidad.html#problema-monty-hall"><i class="fa fa-check"></i><b>13.7.1</b> Problema Monty Hall</a></li>
<li class="chapter" data-level="13.7.2" data-path="probabilidad.html"><a href="probabilidad.html#problema-de-cumpleaños"><i class="fa fa-check"></i><b>13.7.2</b> Problema de cumpleaños</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probabilidad.html"><a href="probabilidad.html#infinito-en-la-práctica"><i class="fa fa-check"></i><b>13.8</b> Infinito en la práctica</a></li>
<li class="chapter" data-level="13.9" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-22"><i class="fa fa-check"></i><b>13.9</b> Ejercicios</a></li>
<li class="chapter" data-level="13.10" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-continua"><i class="fa fa-check"></i><b>13.10</b> Probabilidad continua</a></li>
<li class="chapter" data-level="13.11" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-continuas"><i class="fa fa-check"></i><b>13.11</b> Distribuciones teóricas continuas</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-como-aproximaciones"><i class="fa fa-check"></i><b>13.11.1</b> Distribuciones teóricas como aproximaciones</a></li>
<li class="chapter" data-level="13.11.2" data-path="probabilidad.html"><a href="probabilidad.html#la-densidad-de-probabilidad"><i class="fa fa-check"></i><b>13.11.2</b> La densidad de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-variables-continuas"><i class="fa fa-check"></i><b>13.12</b> Simulaciones Monte Carlo para variables continuas</a></li>
<li class="chapter" data-level="13.13" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-continuas"><i class="fa fa-check"></i><b>13.13</b> Distribuciones continuas</a></li>
<li class="chapter" data-level="13.14" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-23"><i class="fa fa-check"></i><b>13.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html"><i class="fa fa-check"></i><b>14</b> Variables aleatorias</a><ul>
<li class="chapter" data-level="14.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#variables-aleatorias-1"><i class="fa fa-check"></i><b>14.1</b> Variables aleatorias</a></li>
<li class="chapter" data-level="14.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#modelos-de-muestreo"><i class="fa fa-check"></i><b>14.2</b> Modelos de muestreo</a></li>
<li class="chapter" data-level="14.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#la-distribución-de-probabilidad-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>14.3</b> La distribución de probabilidad de una variable aleatoria</a></li>
<li class="chapter" data-level="14.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#distribuciones-versus-distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>14.4</b> Distribuciones versus distribuciones de probabilidad</a></li>
<li class="chapter" data-level="14.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#notación-para-variables-aleatorias"><i class="fa fa-check"></i><b>14.5</b> Notación para variables aleatorias</a></li>
<li class="chapter" data-level="14.6" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#el-valor-esperado-y-el-error-estándar"><i class="fa fa-check"></i><b>14.6</b> El valor esperado y el error estándar</a><ul>
<li class="chapter" data-level="14.6.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#población-sd-versus-la-muestra-sd"><i class="fa fa-check"></i><b>14.6.1</b> Población SD versus la muestra SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7</b> Teorema del límite central</a><ul>
<li class="chapter" data-level="14.7.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#cuán-grande-es-grande-en-el-teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7.1</b> ¿Cuán grande es grande en el teorema del límite central?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#propiedades-estadísticas-de-promedios"><i class="fa fa-check"></i><b>14.8</b> Propiedades estadísticas de promedios</a></li>
<li class="chapter" data-level="14.9" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ley-de-los-grandes-números"><i class="fa fa-check"></i><b>14.9</b> Ley de los grandes números</a><ul>
<li class="chapter" data-level="14.9.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#malinterpretando-la-ley-de-promedios"><i class="fa fa-check"></i><b>14.9.1</b> Malinterpretando la ley de promedios</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-24"><i class="fa fa-check"></i><b>14.10</b> Ejercicios</a></li>
<li class="chapter" data-level="14.11" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#estudio-de-caso-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Estudio de caso: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#tasas-de-interés-explicadas-con-modelo-de-oportunidad"><i class="fa fa-check"></i><b>14.11.1</b> Tasas de interés explicadas con modelo de oportunidad</a></li>
<li class="chapter" data-level="14.11.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-25"><i class="fa fa-check"></i><b>14.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Inferencia estadística</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#encuestas"><i class="fa fa-check"></i><b>15.1</b> Encuestas</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#el-modelo-de-muestreo-para-encuestas"><i class="fa fa-check"></i><b>15.1.1</b> El modelo de muestreo para encuestas</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#poblaciones-muestras-parámetros-y-estimaciones"><i class="fa fa-check"></i><b>15.2</b> Poblaciones, muestras, parámetros y estimaciones</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#el-promedio-de-la-muestra"><i class="fa fa-check"></i><b>15.2.1</b> El promedio de la muestra</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parámetros"><i class="fa fa-check"></i><b>15.2.2</b> Parámetros</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#encuesta-versus-pronóstico"><i class="fa fa-check"></i><b>15.2.3</b> Encuesta versus pronóstico</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#propiedades-de-nuestra-estimación-valor-esperado-y-error-estándar"><i class="fa fa-check"></i><b>15.2.4</b> Propiedades de nuestra estimación: valor esperado y error estándar</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#ejercicios-26"><i class="fa fa-check"></i><b>15.3</b> Ejercicios</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Teorema del límite central en la práctica</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo"><i class="fa fa-check"></i><b>15.4.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#la-diferencia"><i class="fa fa-check"></i><b>15.4.2</b> La diferencia</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#sesgo-por-qué-no-realizar-una-encuesta-bien-grande"><i class="fa fa-check"></i><b>15.4.3</b> Sesgo: ¿por qué no realizar una encuesta bien grande?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#ejercicios-27"><i class="fa fa-check"></i><b>15.5</b> Ejercicios</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#intervalos-de-confianza"><i class="fa fa-check"></i><b>15.6</b> Intervalos de confianza</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo-1"><i class="fa fa-check"></i><b>15.6.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#el-idioma-correcto"><i class="fa fa-check"></i><b>15.6.2</b> El idioma correcto</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#ejercicios-28"><i class="fa fa-check"></i><b>15.7</b> Ejercicios</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#poder"><i class="fa fa-check"></i><b>15.8</b> Poder</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#valores-p"><i class="fa fa-check"></i><b>15.9</b> valores p</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Pruebas de asociación</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#tablas-2x2"><i class="fa fa-check"></i><b>15.10.2</b> Tablas 2x2</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#prueba-de-chi-cuadrado"><i class="fa fa-check"></i><b>15.10.3</b> Prueba de chi-cuadrado</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> Riesgo relativo</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#intervalos-de-confianza-para-el-riesgo-relativo"><i class="fa fa-check"></i><b>15.10.5</b> Intervalos de confianza para el riesgo relativo</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#corrección-de-recuento-pequeño"><i class="fa fa-check"></i><b>15.10.6</b> Corrección de recuento pequeño</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#muestras-grandes-valores-p-pequeños"><i class="fa fa-check"></i><b>15.10.7</b> Muestras grandes, valores p pequeños</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#ejercicios-29"><i class="fa fa-check"></i><b>15.11</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Modelos estadísticos</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#agregadores-de-encuestas"><i class="fa fa-check"></i><b>16.1</b> Agregadores de encuestas</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#datos-de-encuesta"><i class="fa fa-check"></i><b>16.1.1</b> Datos de encuesta</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#sesgo-de-los-encuestadores"><i class="fa fa-check"></i><b>16.1.2</b> Sesgo de los encuestadores</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Modelos basados en datos</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#ejercicios-30"><i class="fa fa-check"></i><b>16.3</b> Ejercicios</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Estadísticas bayesianas</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#teorema-de-bayes"><i class="fa fa-check"></i><b>16.4.1</b> Teorema de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#simulación-del-teorema-de-bayes"><i class="fa fa-check"></i><b>16.5</b> Simulación del teorema de Bayes</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-en-la-práctica"><i class="fa fa-check"></i><b>16.5.1</b> Bayes en la práctica</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#modelos-jerárquicos"><i class="fa fa-check"></i><b>16.6</b> Modelos jerárquicos</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#ejercicios-31"><i class="fa fa-check"></i><b>16.7</b> Ejercicios</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Estudio de caso: pronóstico de elecciones</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Enfoque bayesiano</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#el-sesgo-general"><i class="fa fa-check"></i><b>16.8.2</b> El sesgo general</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#representaciones-matemáticas-de-modelos"><i class="fa fa-check"></i><b>16.8.3</b> Representaciones matemáticas de modelos</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#prediciendo-el-colegio-electoral"><i class="fa fa-check"></i><b>16.8.4</b> Prediciendo el colegio electoral</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#pronósticos"><i class="fa fa-check"></i><b>16.8.5</b> Pronósticos</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#ejercicios-32"><i class="fa fa-check"></i><b>16.9</b> Ejercicios</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> La distribución t</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regresión</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#estudio-de-caso-la-altura-es-hereditaria"><i class="fa fa-check"></i><b>17.1</b> Estudio de caso: ¿la altura es hereditaria?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> El coeficiente de correlación</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#la-correlación-de-muestra-es-una-variable-aleatoria"><i class="fa fa-check"></i><b>17.2.1</b> La correlación de muestra es una variable aleatoria</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#la-correlación-no-siempre-es-un-resumen-útil"><i class="fa fa-check"></i><b>17.2.2</b> La correlación no siempre es un resumen útil</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Valor esperado condicional</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#la-línea-de-regresión"><i class="fa fa-check"></i><b>17.4</b> La línea de regresión</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regresión-mejora-precisión"><i class="fa fa-check"></i><b>17.4.1</b> Regresión mejora precisión</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#distribución-normal-de-dos-variables-avanzada"><i class="fa fa-check"></i><b>17.4.2</b> Distribución normal de dos variables (avanzada)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#varianza-explicada"><i class="fa fa-check"></i><b>17.4.3</b> Varianza explicada</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#advertencia-hay-dos-líneas-de-regresión"><i class="fa fa-check"></i><b>17.4.4</b> Advertencia: hay dos líneas de regresión</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#ejercicios-33"><i class="fa fa-check"></i><b>17.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>18</b> Modelos lineales</a><ul>
<li class="chapter" data-level="18.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball"><i class="fa fa-check"></i><b>18.1</b> Estudio de caso: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#sabermetrics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetrics</a></li>
<li class="chapter" data-level="18.1.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#conceptos-básicos-de-béisbol"><i class="fa fa-check"></i><b>18.1.2</b> Conceptos básicos de béisbol</a></li>
<li class="chapter" data-level="18.1.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#no-hay-premios-para-bb"><i class="fa fa-check"></i><b>18.1.3</b> No hay premios para BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#base-por-bolas-o-bases-robadas"><i class="fa fa-check"></i><b>18.1.4</b> ¿Base por bolas o bases robadas?</a></li>
<li class="chapter" data-level="18.1.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-aplicada-a-las-estadísticas-de-béisbol"><i class="fa fa-check"></i><b>18.1.5</b> Regresión aplicada a las estadísticas de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#confusión"><i class="fa fa-check"></i><b>18.2</b> Confusión</a><ul>
<li class="chapter" data-level="18.2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#entender-confusión-a-través-de-estratificación"><i class="fa fa-check"></i><b>18.2.1</b> Entender confusión a través de estratificación</a></li>
<li class="chapter" data-level="18.2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>18.2.2</b> Regresión lineal múltiple</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse"><i class="fa fa-check"></i><b>18.3</b> Estimaciones de mínimos cuadrados</a><ul>
<li class="chapter" data-level="18.3.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#interpretando-modelos-lineales"><i class="fa fa-check"></i><b>18.3.1</b> Interpretando modelos lineales</a></li>
<li class="chapter" data-level="18.3.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimaciones-de-mínimos-cuadrados-lse"><i class="fa fa-check"></i><b>18.3.2</b> Estimaciones de mínimos cuadrados (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-función-lm"><i class="fa fa-check"></i><b>18.3.3</b> La función <code>lm</code></a></li>
<li class="chapter" data-level="18.3.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.4</b> LSE son variables aleatorias</a></li>
<li class="chapter" data-level="18.3.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-pronosticados-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.5</b> Valores pronosticados son variables aleatorias</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-34"><i class="fa fa-check"></i><b>18.4</b> Ejercicios</a></li>
<li class="chapter" data-level="18.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-en-el-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Regresión lineal en el tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#el-paquete-broom"><i class="fa fa-check"></i><b>18.5.1</b> El paquete broom</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-35"><i class="fa fa-check"></i><b>18.6</b> Ejercicios</a></li>
<li class="chapter" data-level="18.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball-continuación"><i class="fa fa-check"></i><b>18.7</b> Estudio de caso: Moneyball (continuación)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#añadiendo-información-sobre-salario-y-posición"><i class="fa fa-check"></i><b>18.7.1</b> Añadiendo información sobre salario y posición</a></li>
<li class="chapter" data-level="18.7.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#escoger-nueve-jugadores"><i class="fa fa-check"></i><b>18.7.2</b> Escoger nueve jugadores</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-falacia-de-la-regresión"><i class="fa fa-check"></i><b>18.8</b> La falacia de la regresión</a></li>
<li class="chapter" data-level="18.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-de-error-de-medición"><i class="fa fa-check"></i><b>18.9</b> Modelos de error de medición</a></li>
<li class="chapter" data-level="18.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-36"><i class="fa fa-check"></i><b>18.10</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html"><i class="fa fa-check"></i><b>19</b> La asociación no implica causalidad</a><ul>
<li class="chapter" data-level="19.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#correlación-espuria"><i class="fa fa-check"></i><b>19.1</b> Correlación espuria</a></li>
<li class="chapter" data-level="19.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#valores-atípicos-1"><i class="fa fa-check"></i><b>19.2</b> Valores atípicos</a></li>
<li class="chapter" data-level="19.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#inversión-de-causa-y-efecto"><i class="fa fa-check"></i><b>19.3</b> Inversión de causa y efecto</a></li>
<li class="chapter" data-level="19.4" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#factores-de-confusión"><i class="fa fa-check"></i><b>19.4</b> Factores de confusión</a><ul>
<li class="chapter" data-level="19.4.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejemplo-admisiones-a-la-universidad-de-california-berkeley"><i class="fa fa-check"></i><b>19.4.1</b> Ejemplo: admisiones a la Universidad de California, Berkeley</a></li>
<li class="chapter" data-level="19.4.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#confusión-explicada-gráficamente"><i class="fa fa-check"></i><b>19.4.2</b> Confusión explicada gráficamente</a></li>
<li class="chapter" data-level="19.4.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#promedio-después-de-estratificar"><i class="fa fa-check"></i><b>19.4.3</b> Promedio después de estratificar</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#la-paradoja-de-simpson"><i class="fa fa-check"></i><b>19.5</b> La paradoja de Simpson</a></li>
<li class="chapter" data-level="19.6" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejercicios-37"><i class="fa fa-check"></i><b>19.6</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>IV <em>Wrangling</em> de datos</b></span></li>
<li class="chapter" data-level="20" data-path="introducción-al-wrangling-de-datos.html"><a href="introducción-al-wrangling-de-datos.html"><i class="fa fa-check"></i><b>20</b> Introducción al wrangling de datos</a></li>
<li class="chapter" data-level="21" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html"><i class="fa fa-check"></i><b>21</b> Cómo cambiar el formato de datos</a><ul>
<li class="chapter" data-level="21.1" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#ejercicios-38"><i class="fa fa-check"></i><b>21.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="unir-tablas.html"><a href="unir-tablas.html"><i class="fa fa-check"></i><b>22</b> Unir tablas</a><ul>
<li class="chapter" data-level="22.1" data-path="unir-tablas.html"><a href="unir-tablas.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="unir-tablas.html"><a href="unir-tablas.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="unir-tablas.html"><a href="unir-tablas.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="unir-tablas.html"><a href="unir-tablas.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="unir-tablas.html"><a href="unir-tablas.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="unir-tablas.html"><a href="unir-tablas.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="unir-tablas.html"><a href="unir-tablas.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="unir-tablas.html"><a href="unir-tablas.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-columnas"><i class="fa fa-check"></i><b>22.2.1</b> Pegando columnas</a></li>
<li class="chapter" data-level="22.2.2" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-filas"><i class="fa fa-check"></i><b>22.2.2</b> Pegando filas</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="unir-tablas.html"><a href="unir-tablas.html#operadores-de-sets"><i class="fa fa-check"></i><b>22.3</b> Operadores de sets</a><ul>
<li class="chapter" data-level="22.3.1" data-path="unir-tablas.html"><a href="unir-tablas.html#intersecar"><i class="fa fa-check"></i><b>22.3.1</b> Intersecar</a></li>
<li class="chapter" data-level="22.3.2" data-path="unir-tablas.html"><a href="unir-tablas.html#unión"><i class="fa fa-check"></i><b>22.3.2</b> Unión</a></li>
<li class="chapter" data-level="22.3.3" data-path="unir-tablas.html"><a href="unir-tablas.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="unir-tablas.html"><a href="unir-tablas.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="unir-tablas.html"><a href="unir-tablas.html#ejercicios-39"><i class="fa fa-check"></i><b>22.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html"><i class="fa fa-check"></i><b>23</b> Extracción de la web</a><ul>
<li class="chapter" data-level="23.1" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#el-paquete-rvest"><i class="fa fa-check"></i><b>23.2</b> El paquete rvest</a></li>
<li class="chapter" data-level="23.3" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> Selectores CSS</a></li>
<li class="chapter" data-level="23.4" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#ejercicios-40"><i class="fa fa-check"></i><b>23.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html"><i class="fa fa-check"></i><b>24</b> Procesamiento de cadenas</a><ul>
<li class="chapter" data-level="24.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#stringr"><i class="fa fa-check"></i><b>24.1</b> El paquete stringr</a></li>
<li class="chapter" data-level="24.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-1-datos-de-asesinatos-en-ee.-uu."><i class="fa fa-check"></i><b>24.2</b> Estudio de caso 1: datos de asesinatos en EE. UU.</a></li>
<li class="chapter" data-level="24.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas"><i class="fa fa-check"></i><b>24.3</b> Estudio de caso 2: alturas autoreportadas</a></li>
<li class="chapter" data-level="24.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cómo-escapar-al-definir-cadenas"><i class="fa fa-check"></i><b>24.4</b> Cómo <em>escapar</em> al definir cadenas</a></li>
<li class="chapter" data-level="24.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#expresiones-regulares"><i class="fa fa-check"></i><b>24.5</b> Expresiones regulares</a><ul>
<li class="chapter" data-level="24.5.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#las-cadenas-son-una-expresión-regular"><i class="fa fa-check"></i><b>24.5.1</b> Las cadenas son una expresión regular</a></li>
<li class="chapter" data-level="24.5.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#caracteres-especiales"><i class="fa fa-check"></i><b>24.5.2</b> Caracteres especiales</a></li>
<li class="chapter" data-level="24.5.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#clases-de-caracteres"><i class="fa fa-check"></i><b>24.5.3</b> Clases de caracteres</a></li>
<li class="chapter" data-level="24.5.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#anclas"><i class="fa fa-check"></i><b>24.5.4</b> Anclas</a></li>
<li class="chapter" data-level="24.5.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores"><i class="fa fa-check"></i><b>24.5.5</b> Cuantificadores</a></li>
<li class="chapter" data-level="24.5.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#espacio-en-blanco-s"><i class="fa fa-check"></i><b>24.5.6</b> Espacio en blanco <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores-1"><i class="fa fa-check"></i><b>24.5.7</b> Cuantificadores: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#no"><i class="fa fa-check"></i><b>24.5.8</b> No</a></li>
<li class="chapter" data-level="24.5.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-con-expresiones-regulares"><i class="fa fa-check"></i><b>24.6</b> Buscar y reemplazar con expresiones regulares</a><ul>
<li class="chapter" data-level="24.6.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-usando-grupos"><i class="fa fa-check"></i><b>24.6.1</b> Buscar y reemplazar usando grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#probar-y-mejorar"><i class="fa fa-check"></i><b>24.7</b> Probar y mejorar</a></li>
<li class="chapter" data-level="24.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#podar"><i class="fa fa-check"></i><b>24.8</b> Podar</a></li>
<li class="chapter" data-level="24.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cambio-de-mayúsculas-o-minúsculas"><i class="fa fa-check"></i><b>24.9</b> Cambio de mayúsculas o minúsculas</a></li>
<li class="chapter" data-level="24.10" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas-continuación"><i class="fa fa-check"></i><b>24.10</b> Estudio de caso 2: alturas autoreportadas (continuación)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#la-función-extract"><i class="fa fa-check"></i><b>24.10.1</b> La función <code>extract</code></a></li>
<li class="chapter" data-level="24.10.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#juntando-todas-la-piezas"><i class="fa fa-check"></i><b>24.10.2</b> Juntando todas la piezas</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#división-de-cadenas"><i class="fa fa-check"></i><b>24.11</b> División de cadenas</a></li>
<li class="chapter" data-level="24.12" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-3-extracción-de-tablas-de-un-pdf"><i class="fa fa-check"></i><b>24.12</b> Estudio de caso 3: extracción de tablas de un PDF</a></li>
<li class="chapter" data-level="24.13" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recodificación</a></li>
<li class="chapter" data-level="24.14" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#ejercicios-41"><i class="fa fa-check"></i><b>24.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html"><i class="fa fa-check"></i><b>25</b> Cómo leer y procesar fechas y horas</a><ul>
<li class="chapter" data-level="25.1" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#el-tipo-de-datos-de-fecha"><i class="fa fa-check"></i><b>25.1</b> El tipo de datos de fecha</a></li>
<li class="chapter" data-level="25.2" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> El paquete lubridate</a></li>
<li class="chapter" data-level="25.3" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#ejercicios-42"><i class="fa fa-check"></i><b>25.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="minería-de-textos.html"><a href="minería-de-textos.html"><i class="fa fa-check"></i><b>26</b> Minería de textos</a><ul>
<li class="chapter" data-level="26.1" data-path="minería-de-textos.html"><a href="minería-de-textos.html#estudio-de-caso-tuits-de-trump"><i class="fa fa-check"></i><b>26.1</b> Estudio de caso: tuits de Trump</a></li>
<li class="chapter" data-level="26.2" data-path="minería-de-textos.html"><a href="minería-de-textos.html#texto-como-datos"><i class="fa fa-check"></i><b>26.2</b> Texto como datos</a></li>
<li class="chapter" data-level="26.3" data-path="minería-de-textos.html"><a href="minería-de-textos.html#análisis-de-sentimiento"><i class="fa fa-check"></i><b>26.3</b> Análisis de sentimiento</a></li>
<li class="chapter" data-level="26.4" data-path="minería-de-textos.html"><a href="minería-de-textos.html#ejercicios-43"><i class="fa fa-check"></i><b>26.4</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html"><i class="fa fa-check"></i><b>27</b> Introducción a <em>machine learning</em></a><ul>
<li class="chapter" data-level="27.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#notación-1"><i class="fa fa-check"></i><b>27.1</b> Notación</a></li>
<li class="chapter" data-level="27.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#un-ejemplo"><i class="fa fa-check"></i><b>27.2</b> Un ejemplo</a></li>
<li class="chapter" data-level="27.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-44"><i class="fa fa-check"></i><b>27.3</b> Ejercicios</a></li>
<li class="chapter" data-level="27.4" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#métricas-de-evaluación"><i class="fa fa-check"></i><b>27.4</b> Métricas de evaluación</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#training-test"><i class="fa fa-check"></i><b>27.4.1</b> Sets de entrenamiento y de evaluación</a></li>
<li class="chapter" data-level="27.4.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#exactitud-general"><i class="fa fa-check"></i><b>27.4.2</b> Exactitud general</a></li>
<li class="chapter" data-level="27.4.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-matriz-de-confusión"><i class="fa fa-check"></i><b>27.4.3</b> La matriz de confusión</a></li>
<li class="chapter" data-level="27.4.4" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#sensibilidad-y-especificidad"><i class="fa fa-check"></i><b>27.4.4</b> Sensibilidad y especificidad</a></li>
<li class="chapter" data-level="27.4.5" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#exactitud-equilibrada-y-medida-f_1"><i class="fa fa-check"></i><b>27.4.5</b> Exactitud equilibrada y medida <span class="math inline">\(F_1\)</span></a></li>
<li class="chapter" data-level="27.4.6" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-prevalencia-importa-en-la-práctica"><i class="fa fa-check"></i><b>27.4.6</b> La prevalencia importa en la práctica</a></li>
<li class="chapter" data-level="27.4.7" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#curvas-roc-y-precision-recall"><i class="fa fa-check"></i><b>27.4.7</b> Curvas ROC y precision-recall</a></li>
<li class="chapter" data-level="27.4.8" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> La función de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-45"><i class="fa fa-check"></i><b>27.5</b> Ejercicios</a></li>
<li class="chapter" data-level="27.6" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#probabilidades-y-expectativas-condicionales"><i class="fa fa-check"></i><b>27.6</b> Probabilidades y expectativas condicionales</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#probabilidades-condicionales-1"><i class="fa fa-check"></i><b>27.6.1</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="27.6.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#expectativas-condicionales"><i class="fa fa-check"></i><b>27.6.2</b> Expectativas condicionales</a></li>
<li class="chapter" data-level="27.6.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-expectativa-condicional-minimiza-la-función-de-pérdida-cuadrática"><i class="fa fa-check"></i><b>27.6.3</b> La expectativa condicional minimiza la función de pérdida cuadrática</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-46"><i class="fa fa-check"></i><b>27.7</b> Ejercicios</a></li>
<li class="chapter" data-level="27.8" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Estudio de caso: ¿es un 2 o un 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="suavización.html"><a href="suavización.html"><i class="fa fa-check"></i><b>28</b> Suavización</a><ul>
<li class="chapter" data-level="28.1" data-path="suavización.html"><a href="suavización.html#suavización-de-compartimientos"><i class="fa fa-check"></i><b>28.1</b> Suavización de compartimientos</a></li>
<li class="chapter" data-level="28.2" data-path="suavización.html"><a href="suavización.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="suavización.html"><a href="suavización.html#regresión-ponderada-local-loess"><i class="fa fa-check"></i><b>28.3</b> Regresión ponderada local (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="suavización.html"><a href="suavización.html#ajustando-con-parábolas"><i class="fa fa-check"></i><b>28.3.1</b> Ajustando con parábolas</a></li>
<li class="chapter" data-level="28.3.2" data-path="suavización.html"><a href="suavización.html#cuidado-con-los-parámetros-de-suavización-predeterminados"><i class="fa fa-check"></i><b>28.3.2</b> Cuidado con los parámetros de suavización predeterminados</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="suavización.html"><a href="suavización.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Conectando la suavización al <em>machine learning</em></a></li>
<li class="chapter" data-level="28.5" data-path="suavización.html"><a href="suavización.html#ejercicios-47"><i class="fa fa-check"></i><b>28.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Validación cruzada</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivación con k vecinos más cercanos</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#sobreentrenamiento"><i class="fa fa-check"></i><b>29.1.1</b> Sobreentrenamiento</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#sobre-suavización"><i class="fa fa-check"></i><b>29.1.2</b> Sobre suavización</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#escogiendo-la-k-en-knn"><i class="fa fa-check"></i><b>29.1.3</b> Escogiendo la <span class="math inline">\(k\)</span> en kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#descripción-matemática-de-validación-cruzada"><i class="fa fa-check"></i><b>29.2</b> Descripción matemática de validación cruzada</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#validación-cruzada-k-fold"><i class="fa fa-check"></i><b>29.3</b> Validación cruzada K-fold</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-48"><i class="fa fa-check"></i><b>29.4</b> Ejercicios</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-49"><i class="fa fa-check"></i><b>29.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> El paquete caret</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#la-función-train-de-caret"><i class="fa fa-check"></i><b>30.1</b> La función <code>train</code> de caret</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Validación cruzada</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#ejemplo-ajuste-con-loess"><i class="fa fa-check"></i><b>30.3</b> Ejemplo: ajuste con loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html"><i class="fa fa-check"></i><b>31</b> Ejemplos de algoritmos</a><ul>
<li class="chapter" data-level="31.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-lineal"><i class="fa fa-check"></i><b>31.1</b> Regresión lineal</a><ul>
<li class="chapter" data-level="31.1.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#la-función-predict"><i class="fa fa-check"></i><b>31.1.1</b> La función <code>predict</code></a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-50"><i class="fa fa-check"></i><b>31.2</b> Ejercicios</a></li>
<li class="chapter" data-level="31.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística"><i class="fa fa-check"></i><b>31.3</b> Regresión logística</a><ul>
<li class="chapter" data-level="31.3.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>31.3.1</b> Modelos lineales generalizados</a></li>
<li class="chapter" data-level="31.3.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística-con-más-de-un-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Regresión logística con más de un predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-51"><i class="fa fa-check"></i><b>31.4</b> Ejercicios</a></li>
<li class="chapter" data-level="31.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#k-vecinos-más-cercanos-knn"><i class="fa fa-check"></i><b>31.5</b> k vecinos más cercanos (kNN)</a></li>
<li class="chapter" data-level="31.6" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-52"><i class="fa fa-check"></i><b>31.6</b> Ejercicios</a></li>
<li class="chapter" data-level="31.7" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-generativos"><i class="fa fa-check"></i><b>31.7</b> Modelos generativos</a><ul>
<li class="chapter" data-level="31.7.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#controlando-la-prevalencia"><i class="fa fa-check"></i><b>31.7.2</b> Controlando la prevalencia</a></li>
<li class="chapter" data-level="31.7.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-cuadrático"><i class="fa fa-check"></i><b>31.7.3</b> Análisis discriminante cuadrático</a></li>
<li class="chapter" data-level="31.7.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-lineal"><i class="fa fa-check"></i><b>31.7.4</b> Análisis discriminante lineal</a></li>
<li class="chapter" data-level="31.7.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#conexión-a-distancia"><i class="fa fa-check"></i><b>31.7.5</b> Conexión a distancia</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#estudio-de-caso-más-de-tres-clases"><i class="fa fa-check"></i><b>31.8</b> Estudio de caso: más de tres clases</a></li>
<li class="chapter" data-level="31.9" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-53"><i class="fa fa-check"></i><b>31.9</b> Ejercicios</a></li>
<li class="chapter" data-level="31.10" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-y-regresión-cart"><i class="fa fa-check"></i><b>31.10</b> Árboles de clasificación y regresión (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#la-maldición-de-la-dimensionalidad"><i class="fa fa-check"></i><b>31.10.1</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="31.10.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#motivación-cart"><i class="fa fa-check"></i><b>31.10.2</b> Motivación CART</a></li>
<li class="chapter" data-level="31.10.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-regresión"><i class="fa fa-check"></i><b>31.10.3</b> Árboles de regresión</a></li>
<li class="chapter" data-level="31.10.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-decisión"><i class="fa fa-check"></i><b>31.10.4</b> Árboles de clasificación (decisión)</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#bosques-aleatorios"><i class="fa fa-check"></i><b>31.11</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="31.12" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-54"><i class="fa fa-check"></i><b>31.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html"><i class="fa fa-check"></i><b>32</b> Machine learning en la práctica</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#preprocesamiento"><i class="fa fa-check"></i><b>32.1</b> Preprocesamiento</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#k-vecino-más-cercano-y-bosque-aleatorio"><i class="fa fa-check"></i><b>32.2</b> k-vecino más cercano y bosque aleatorio</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#importancia-variable"><i class="fa fa-check"></i><b>32.3</b> Importancia variable</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#evaluaciones-visuales"><i class="fa fa-check"></i><b>32.4</b> Evaluaciones visuales</a></li>
<li class="chapter" data-level="32.5" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#conjuntos"><i class="fa fa-check"></i><b>32.5</b> Conjuntos</a></li>
<li class="chapter" data-level="32.6" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#ejercicios-55"><i class="fa fa-check"></i><b>32.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html"><i class="fa fa-check"></i><b>33</b> Grandes sets de datos</a><ul>
<li class="chapter" data-level="33.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Álgebra matricial</a><ul>
<li class="chapter" data-level="33.1.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#notación-2"><i class="fa fa-check"></i><b>33.1.1</b> Notación</a></li>
<li class="chapter" data-level="33.1.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#convertir-un-vector-en-una-matriz"><i class="fa fa-check"></i><b>33.1.2</b> Convertir un vector en una matriz</a></li>
<li class="chapter" data-level="33.1.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#resúmenes-de-filas-y-columnas"><i class="fa fa-check"></i><b>33.1.3</b> Resúmenes de filas y columnas</a></li>
<li class="chapter" data-level="33.1.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#filtrar-columnas-basado-en-resúmenes"><i class="fa fa-check"></i><b>33.1.5</b> Filtrar columnas basado en resúmenes</a></li>
<li class="chapter" data-level="33.1.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#indexación-con-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexación con matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#binarizar-los-datos"><i class="fa fa-check"></i><b>33.1.7</b> Binarizar los datos</a></li>
<li class="chapter" data-level="33.1.8" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#vectorización-para-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorización para matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#operaciones-de-álgebra-matricial"><i class="fa fa-check"></i><b>33.1.9</b> Operaciones de álgebra matricial</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-56"><i class="fa fa-check"></i><b>33.2</b> Ejercicios</a></li>
<li class="chapter" data-level="33.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia"><i class="fa fa-check"></i><b>33.3</b> Distancia</a><ul>
<li class="chapter" data-level="33.3.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.1</b> Distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia-en-dimensiones-superiores"><i class="fa fa-check"></i><b>33.3.2</b> Distancia en dimensiones superiores</a></li>
<li class="chapter" data-level="33.3.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejemplo-de-distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.3</b> Ejemplo de distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Espacio predictor</a></li>
<li class="chapter" data-level="33.3.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#distancia-entre-predictores"><i class="fa fa-check"></i><b>33.3.5</b> Distancia entre predictores</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-57"><i class="fa fa-check"></i><b>33.4</b> Ejercicios</a></li>
<li class="chapter" data-level="33.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#reducción-de-dimensiones"><i class="fa fa-check"></i><b>33.5</b> Reducción de dimensiones</a><ul>
<li class="chapter" data-level="33.5.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#preservando-la-distancia"><i class="fa fa-check"></i><b>33.5.1</b> Preservando la distancia</a></li>
<li class="chapter" data-level="33.5.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#transformaciones-lineales-avanzado"><i class="fa fa-check"></i><b>33.5.2</b> Transformaciones lineales (avanzado)</a></li>
<li class="chapter" data-level="33.5.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#transformaciones-ortogonales-avanzado"><i class="fa fa-check"></i><b>33.5.3</b> Transformaciones ortogonales (avanzado)</a></li>
<li class="chapter" data-level="33.5.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#pca"><i class="fa fa-check"></i><b>33.5.4</b> Análisis de componentes principales</a></li>
<li class="chapter" data-level="33.5.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejemplo-lirio"><i class="fa fa-check"></i><b>33.5.5</b> Ejemplo lirio</a></li>
<li class="chapter" data-level="33.5.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejemplo-de-mnist"><i class="fa fa-check"></i><b>33.5.6</b> Ejemplo de MNIST</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-58"><i class="fa fa-check"></i><b>33.6</b> Ejercicios</a></li>
<li class="chapter" data-level="33.7" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#sistemas-de-recomendación"><i class="fa fa-check"></i><b>33.7</b> Sistemas de recomendación</a><ul>
<li class="chapter" data-level="33.7.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#datos-de-movielens"><i class="fa fa-check"></i><b>33.7.1</b> Datos de Movielens</a></li>
<li class="chapter" data-level="33.7.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#sistemas-de-recomendación-como-un-desafío-de-machine-learning"><i class="fa fa-check"></i><b>33.7.2</b> Sistemas de recomendación como un desafío de machine learning</a></li>
<li class="chapter" data-level="33.7.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Función de pérdida</a></li>
<li class="chapter" data-level="33.7.4" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#un-primer-modelo"><i class="fa fa-check"></i><b>33.7.4</b> Un primer modelo</a></li>
<li class="chapter" data-level="33.7.5" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#modelando-los-efectos-de-películas"><i class="fa fa-check"></i><b>33.7.5</b> Modelando los efectos de películas</a></li>
<li class="chapter" data-level="33.7.6" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#efectos-de-usuario"><i class="fa fa-check"></i><b>33.7.6</b> Efectos de usuario</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-59"><i class="fa fa-check"></i><b>33.8</b> Ejercicios</a></li>
<li class="chapter" data-level="33.9" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#regularización"><i class="fa fa-check"></i><b>33.9</b> Regularización</a><ul>
<li class="chapter" data-level="33.9.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#motivación"><i class="fa fa-check"></i><b>33.9.1</b> Motivación</a></li>
<li class="chapter" data-level="33.9.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#mínimos-cuadrados-penalizados"><i class="fa fa-check"></i><b>33.9.2</b> Mínimos cuadrados penalizados</a></li>
<li class="chapter" data-level="33.9.3" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#cómo-elegir-los-términos-de-penalización"><i class="fa fa-check"></i><b>33.9.3</b> Cómo elegir los términos de penalización</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-60"><i class="fa fa-check"></i><b>33.10</b> Ejercicios</a></li>
<li class="chapter" data-level="33.11" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#factorización-de-matrices"><i class="fa fa-check"></i><b>33.11</b> Factorización de matrices</a><ul>
<li class="chapter" data-level="33.11.1" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#factor-analysis"><i class="fa fa-check"></i><b>33.11.1</b> Análisis de factores</a></li>
<li class="chapter" data-level="33.11.2" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#conexión-a-svd-y-pca"><i class="fa fa-check"></i><b>33.11.2</b> Conexión a SVD y PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="grandes-sets-de-datos.html"><a href="grandes-sets-de-datos.html#ejercicios-61"><i class="fa fa-check"></i><b>33.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Agrupación</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#agrupación-jerárquica"><i class="fa fa-check"></i><b>34.1</b> Agrupación jerárquica</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#mapas-de-calor"><i class="fa fa-check"></i><b>34.3</b> Mapas de calor</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#filtrando-atributos"><i class="fa fa-check"></i><b>34.4</b> Filtrando atributos</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#ejercicios-62"><i class="fa fa-check"></i><b>34.5</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>VI Herramientas de productividad</b></span></li>
<li class="chapter" data-level="35" data-path="introducción-a-las-herramientas-de-productividad.html"><a href="introducción-a-las-herramientas-de-productividad.html"><i class="fa fa-check"></i><b>35</b> Introducción a las herramientas de productividad</a></li>
<li class="chapter" data-level="36" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>36</b> Instalación de R y RStudio</a><ul>
<li class="chapter" data-level="36.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#instalando-r"><i class="fa fa-check"></i><b>36.1</b> Instalando R</a></li>
<li class="chapter" data-level="36.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#instalación-de-rstudio"><i class="fa fa-check"></i><b>36.2</b> Instalación de RStudio</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html"><i class="fa fa-check"></i><b>37</b> Accediendo al terminal e instalando Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accediendo al terminal en una Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#instalando-git-en-la-mac"><i class="fa fa-check"></i><b>37.2</b> Instalando Git en la Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#instalación-de-git-y-git-bash-en-windows"><i class="fa fa-check"></i><b>37.3</b> Instalación de Git y <em>Git Bash</em> en Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accediendo el terminal en Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizando con Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#convención-de-nomenclatura"><i class="fa fa-check"></i><b>38.1</b> Convención de nomenclatura</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> El terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> El sistema de archivos</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directorios-y-subdirectorios"><i class="fa fa-check"></i><b>38.3.1</b> Directorios y subdirectorios</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#el-directorio-home"><i class="fa fa-check"></i><b>38.3.2</b> El directorio <em>home</em></a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Directorio de trabajo</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Rutas</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#comandos-de-unix"><i class="fa fa-check"></i><b>38.4</b> Comandos de Unix</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listado-de-contenido-del-directorio"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listado de contenido del directorio</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-y-rmdir-crear-y-eliminar-un-directorio"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> y <code>rmdir</code>: crear y eliminar un directorio</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navegando-por-el-sistema-de-archivos-cambiando-directorios"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: navegando por el sistema de archivos cambiando directorios</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#algunos-ejemplos"><i class="fa fa-check"></i><b>38.5</b> Algunos ejemplos</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#más-comandos-de-unix"><i class="fa fa-check"></i><b>38.6</b> Más comandos de Unix</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-mover-archivos"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: mover archivos</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copiando-archivos"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copiando archivos</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-eliminar-archivos"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: eliminar archivos</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-mirando-un-archivo"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: mirando un archivo</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparación para un proyecto de ciencia de datos</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#unix-avanzado"><i class="fa fa-check"></i><b>38.8</b> Unix avanzado</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#argumentos"><i class="fa fa-check"></i><b>38.8.1</b> Argumentos</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#obtener-ayuda"><i class="fa fa-check"></i><b>38.8.2</b> Obtener ayuda</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#el-pipe-1"><i class="fa fa-check"></i><b>38.8.3</b> El <em>pipe</em></a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#comodines"><i class="fa fa-check"></i><b>38.8.4</b> Comodines</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#variables-de-entorno"><i class="fa fa-check"></i><b>38.8.5</b> Variables de entorno</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#shells"><i class="fa fa-check"></i><b>38.8.6</b> <em>Shells</em></a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#ejecutables"><i class="fa fa-check"></i><b>38.8.7</b> Ejecutables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permisos-y-tipos-de-archivo"><i class="fa fa-check"></i><b>38.8.8</b> Permisos y tipos de archivo</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#comandos-que-deben-aprender"><i class="fa fa-check"></i><b>38.8.9</b> Comandos que deben aprender</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#manipulación-de-archivos-en-r"><i class="fa fa-check"></i><b>38.8.10</b> Manipulación de archivos en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git y GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#por-qué-usar-git-y-github"><i class="fa fa-check"></i><b>39.1</b> ¿Por qué usar Git y GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#cuentas-github"><i class="fa fa-check"></i><b>39.2</b> Cuentas GitHub</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> Repositorios de GitHub</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Descripción general de Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clonar"><i class="fa fa-check"></i><b>39.4.1</b> Clonar</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Inicializando un directorio Git</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Usando Git y GitHub en RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Proyectos reproducibles con RStudio y R Markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#proyectos-de-rstudio"><i class="fa fa-check"></i><b>40.1</b> Proyectos de RStudio</a></li>
<li class="chapter" data-level="40.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>40.2</b> R Markdown</a><ul>
<li class="chapter" data-level="40.2.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#el-encabezado"><i class="fa fa-check"></i><b>40.2.1</b> El encabezado</a></li>
<li class="chapter" data-level="40.2.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#fragmentos-de-código-r"><i class="fa fa-check"></i><b>40.2.2</b> Fragmentos de código R</a></li>
<li class="chapter" data-level="40.2.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#opciones-globales"><i class="fa fa-check"></i><b>40.2.3</b> Opciones globales</a></li>
<li class="chapter" data-level="40.2.4" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#más-sobre-r-markdown"><i class="fa fa-check"></i><b>40.2.5</b> Más sobre R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizando un proyecto de ciencia de datos</a><ul>
<li class="chapter" data-level="40.3.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#crear-directorios-en-unix"><i class="fa fa-check"></i><b>40.3.1</b> Crear directorios en Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#crear-un-proyecto-rstudio"><i class="fa fa-check"></i><b>40.3.2</b> Crear un proyecto RStudio</a></li>
<li class="chapter" data-level="40.3.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#editar-algunos-scripts-r"><i class="fa fa-check"></i><b>40.3.3</b> Editar algunos scripts R</a></li>
<li class="chapter" data-level="40.3.4" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#crear-más-directorios-usando-unix"><i class="fa fa-check"></i><b>40.3.4</b> Crear más directorios usando Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#agregar-un-archivo-readme"><i class="fa fa-check"></i><b>40.3.5</b> Agregar un archivo README</a></li>
<li class="chapter" data-level="40.3.6" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#inicializando-un-directorio-git"><i class="fa fa-check"></i><b>40.3.6</b> Inicializando un directorio Git</a></li>
<li class="chapter" data-level="40.3.7" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#add-commit-y-push-archivos-con-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> <em>Add</em>, <em>commit</em> y <em>push</em> archivos con RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción a la Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ejemplos-de-algoritmos" class="section level1">
<h1><span class="header-section-number">Capítulo 31</span> Ejemplos de algoritmos</h1>
<p>Hay docenas de algoritmos de <em>machine learning</em>. Aquí ofrecemos algunos ejemplos que abarcan enfoques bastante diferentes. A lo largo del capítulo usaremos los dos datos de dígitos predictores presentados en la Sección <a href="introducción-a-machine-learning.html#two-or-seven">27.8</a> para demostrar cómo funcionan los algoritmos.</p>
<div class="sourceCode" id="cb1095"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1095-1"><a href="ejemplos-de-algoritmos.html#cb1095-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1095-2"><a href="ejemplos-de-algoritmos.html#cb1095-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1095-3"><a href="ejemplos-de-algoritmos.html#cb1095-3"></a><span class="kw">library</span>(caret)</span>
<span id="cb1095-4"><a href="ejemplos-de-algoritmos.html#cb1095-4"></a><span class="kw">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</span></code></pre></div>
<div id="regresión-lineal" class="section level2">
<h2><span class="header-section-number">31.1</span> Regresión lineal</h2>
<p>La regresión lineal puede considerarse un algoritmo de <em>machine learning</em>. En la Sección <a href="introducción-a-machine-learning.html#two-or-seven">27.8</a>, demostramos cómo la regresión lineal a veces es demasiada rígida para ser útil. Esto es generalmente cierto, pero para algunos desafíos funciona bastante bien. También sirve como enfoque de partida: si no podemos mejorarlo con un enfoque más complejo, probablemente querremos continuar con la regresión lineal. Para establecer rápidamente la conexión entre la regresión y el <em>machine learning</em>, reformularemos el estudio de Galton con alturas, una variable continua.</p>
<div class="sourceCode" id="cb1096"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1096-1"><a href="ejemplos-de-algoritmos.html#cb1096-1"></a><span class="kw">library</span>(HistData)</span>
<span id="cb1096-2"><a href="ejemplos-de-algoritmos.html#cb1096-2"></a></span>
<span id="cb1096-3"><a href="ejemplos-de-algoritmos.html#cb1096-3"></a><span class="kw">set.seed</span>(<span class="dv">1983</span>)</span>
<span id="cb1096-4"><a href="ejemplos-de-algoritmos.html#cb1096-4"></a>galton_heights &lt;-<span class="st"> </span>GaltonFamilies <span class="op">%&gt;%</span></span>
<span id="cb1096-5"><a href="ejemplos-de-algoritmos.html#cb1096-5"></a><span class="st">  </span><span class="kw">filter</span>(gender <span class="op">==</span><span class="st"> &quot;male&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1096-6"><a href="ejemplos-de-algoritmos.html#cb1096-6"></a><span class="st">  </span><span class="kw">group_by</span>(family) <span class="op">%&gt;%</span></span>
<span id="cb1096-7"><a href="ejemplos-de-algoritmos.html#cb1096-7"></a><span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb1096-8"><a href="ejemplos-de-algoritmos.html#cb1096-8"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></span>
<span id="cb1096-9"><a href="ejemplos-de-algoritmos.html#cb1096-9"></a><span class="st">  </span><span class="kw">select</span>(father, childHeight) <span class="op">%&gt;%</span></span>
<span id="cb1096-10"><a href="ejemplos-de-algoritmos.html#cb1096-10"></a><span class="st">  </span><span class="kw">rename</span>(<span class="dt">son =</span> childHeight)</span></code></pre></div>
<p>Supongamos que tienen la tarea de construir un algoritmo de <em>machine learning</em> que prediga la altura del hijo <span class="math inline">\(Y\)</span> usando la altura del padre <span class="math inline">\(X\)</span>. Generemos sets de evaluación y de entrenamiento:</p>
<div class="sourceCode" id="cb1097"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1097-1"><a href="ejemplos-de-algoritmos.html#cb1097-1"></a>y &lt;-<span class="st"> </span>galton_heights<span class="op">$</span>son</span>
<span id="cb1097-2"><a href="ejemplos-de-algoritmos.html#cb1097-2"></a>test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1097-3"><a href="ejemplos-de-algoritmos.html#cb1097-3"></a></span>
<span id="cb1097-4"><a href="ejemplos-de-algoritmos.html#cb1097-4"></a>train_set &lt;-<span class="st"> </span>galton_heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>test_index)</span>
<span id="cb1097-5"><a href="ejemplos-de-algoritmos.html#cb1097-5"></a>test_set &lt;-<span class="st"> </span>galton_heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(test_index)</span></code></pre></div>
<p>En este caso, si solo estuviéramos ignorando la altura del padre y adivinando la altura del hijo, adivinaríamos la altura promedio de los hijos.</p>
<div class="sourceCode" id="cb1098"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1098-1"><a href="ejemplos-de-algoritmos.html#cb1098-1"></a>m &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>son)</span>
<span id="cb1098-2"><a href="ejemplos-de-algoritmos.html#cb1098-2"></a>m</span>
<span id="cb1098-3"><a href="ejemplos-de-algoritmos.html#cb1098-3"></a><span class="co">#&gt; [1] 69.2</span></span></code></pre></div>
<p>Nuestro error cuadrático medio es:</p>
<div class="sourceCode" id="cb1099"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1099-1"><a href="ejemplos-de-algoritmos.html#cb1099-1"></a><span class="kw">mean</span>((m <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>son)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb1099-2"><a href="ejemplos-de-algoritmos.html#cb1099-2"></a><span class="co">#&gt; [1] 7.65</span></span></code></pre></div>
<p>¿Podemos mejorar? En el capítulo de regresión, aprendimos que si el par <span class="math inline">\((X,Y)\)</span> sigue una distribución normal de dos variables, la expectativa condicional (lo que queremos estimar) es equivalente a la línea de regresión:</p>
<p><span class="math display">\[
f(x) = \mbox{E}( Y \mid X= x ) = \beta_0 + \beta_1 x
\]</span></p>
<p>En la Sección <a href="modelos-lineales.html#lse">18.3</a>, introdujimos los mínimos cuadrados como método para estimar la pendiente <span class="math inline">\(\beta_0\)</span> y el intercepto <span class="math inline">\(\beta_1\)</span>:</p>
<div class="sourceCode" id="cb1100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1100-1"><a href="ejemplos-de-algoritmos.html#cb1100-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(son <span class="op">~</span><span class="st"> </span>father, <span class="dt">data =</span> train_set)</span>
<span id="cb1100-2"><a href="ejemplos-de-algoritmos.html#cb1100-2"></a>fit<span class="op">$</span>coef</span>
<span id="cb1100-3"><a href="ejemplos-de-algoritmos.html#cb1100-3"></a><span class="co">#&gt; (Intercept)      father </span></span>
<span id="cb1100-4"><a href="ejemplos-de-algoritmos.html#cb1100-4"></a><span class="co">#&gt;      35.976       0.482</span></span></code></pre></div>
<p>Esto nos da una estimado de la expectativa condicional:</p>
<p><span class="math display">\[ \hat{f}(x) = 52 + 0.25 x \]</span></p>
<p>Podemos ver que esto realmente provee una mejora sobre adivinar.</p>
<div class="sourceCode" id="cb1101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1101-1"><a href="ejemplos-de-algoritmos.html#cb1101-1"></a>y_hat &lt;-<span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>test_set<span class="op">$</span>father</span>
<span id="cb1101-2"><a href="ejemplos-de-algoritmos.html#cb1101-2"></a><span class="kw">mean</span>((y_hat <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>son)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb1101-3"><a href="ejemplos-de-algoritmos.html#cb1101-3"></a><span class="co">#&gt; [1] 6.47</span></span></code></pre></div>
<div id="la-función-predict" class="section level3">
<h3><span class="header-section-number">31.1.1</span> La función <code>predict</code></h3>
<p>La función <code>predict</code> es muy útil para aplicaciones de <em>machine learning</em>. Esta función toma como argumentos el resultado de funciones que ajustan modelos como <code>lm</code> o <code>glm</code> (aprenderemos sobre <code>glm</code> pronto) y un <em>data frame</em> con los nuevos predictores para los cuales predecir. Entonces, en nuestro ejemplo actual, usaríamos <code>predict</code> así:</p>
<div class="sourceCode" id="cb1102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1102-1"><a href="ejemplos-de-algoritmos.html#cb1102-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, test_set)</span></code></pre></div>
<p>Utilizando <code>predict</code>, podemos obtener los mismos resultados que obtuvimos anteriormente:</p>
<div class="sourceCode" id="cb1103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1103-1"><a href="ejemplos-de-algoritmos.html#cb1103-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, test_set)</span>
<span id="cb1103-2"><a href="ejemplos-de-algoritmos.html#cb1103-2"></a><span class="kw">mean</span>((y_hat <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>son)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb1103-3"><a href="ejemplos-de-algoritmos.html#cb1103-3"></a><span class="co">#&gt; [1] 6.47</span></span></code></pre></div>
<p><code>predict</code> no siempre devuelve objetos del mismo tipo; depende del tipo de objeto que se le envíe. Para conocer los detalles, deben consultar el archivo de ayuda específico para el tipo de objeto de ajuste que se está utilizando. <code>predict</code> es un tipo de función especial en R (denominada <em>función genérica</em>) que llama a otras funciones según el tipo de objeto que recibe. Así que si <code>predict</code> recibe un objeto producido por la función <code>lm</code>, llamará <code>predict.lm</code>. Si recibe un objeto producido por la función <code>glm</code>, llamará <code>predict.glm</code>. Estas dos funciones son similares pero con algunas diferencias. Pueden obtener más información sobre las diferencias leyendo los archivos de ayuda:</p>
<div class="sourceCode" id="cb1104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1104-1"><a href="ejemplos-de-algoritmos.html#cb1104-1"></a>?predict.lm</span>
<span id="cb1104-2"><a href="ejemplos-de-algoritmos.html#cb1104-2"></a>?predict.glm</span></code></pre></div>
<p>Hay muchas otras versiones de <code>predict</code> y muchos algoritmos de <em>machine learning</em> tienen una función <code>predict</code>.</p>
</div>
</div>
<div id="ejercicios-50" class="section level2">
<h2><span class="header-section-number">31.2</span> Ejercicios</h2>
<p>1. Cree un set de datos con el siguiente código.</p>
<div class="sourceCode" id="cb1105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1105-1"><a href="ejemplos-de-algoritmos.html#cb1105-1"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb1105-2"><a href="ejemplos-de-algoritmos.html#cb1105-2"></a>Sigma &lt;-<span class="st"> </span><span class="dv">9</span><span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1105-3"><a href="ejemplos-de-algoritmos.html#cb1105-3"></a>dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma) <span class="op">%&gt;%</span></span>
<span id="cb1105-4"><a href="ejemplos-de-algoritmos.html#cb1105-4"></a><span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>))</span></code></pre></div>
<p>Use el paquete <strong>caret</strong> para dividirlo en un set de evaluación y uno de entrenamiento del mismo tamaño. Entrene un modelo lineal e indique el RMSE. Repita este ejercicio 100 veces y haga un histograma de los RMSE e indique el promedio y la desviación estándar. Sugerencia: adapte el código mostrado anteriormente así:</p>
<div class="sourceCode" id="cb1106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1106-1"><a href="ejemplos-de-algoritmos.html#cb1106-1"></a>y &lt;-<span class="st"> </span>dat<span class="op">$</span>y</span>
<span id="cb1106-2"><a href="ejemplos-de-algoritmos.html#cb1106-2"></a>test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1106-3"><a href="ejemplos-de-algoritmos.html#cb1106-3"></a>train_set &lt;-<span class="st"> </span>dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>test_index)</span>
<span id="cb1106-4"><a href="ejemplos-de-algoritmos.html#cb1106-4"></a>test_set &lt;-<span class="st"> </span>dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(test_index)</span>
<span id="cb1106-5"><a href="ejemplos-de-algoritmos.html#cb1106-5"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> train_set)</span>
<span id="cb1106-6"><a href="ejemplos-de-algoritmos.html#cb1106-6"></a>y_hat &lt;-<span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>test_set<span class="op">$</span>x</span>
<span id="cb1106-7"><a href="ejemplos-de-algoritmos.html#cb1106-7"></a><span class="kw">mean</span>((y_hat <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>y)<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>y póngalo dentro de una llamada a <code>replicate</code>.</p>
<p>2. Ahora repetiremos lo anterior pero usando sets de datos más grandes. Repita el ejercicio 1 pero para sets de datos con <code>n &lt;- c(100, 500, 1000, 5000, 10000)</code>. Guarde el promedio y la desviación estándar de RMSE de estas 100 repeticiones para cada <code>n</code>. Sugerencia: use las funciones <code>sapply</code> o <code>map</code>.</p>
<p>3. Describa lo que observa con el RMSE a medida que aumenta el tamaño del set de datos.</p>
<ol style="list-style-type: lower-alpha">
<li>En promedio, el RMSE no cambia mucho ya que <code>n</code> se hace más grande, mientras que la variabilidad de RMSE disminuye.</li>
<li>Debido a la ley de los grandes números, el RMSE disminuye: más datos significa estimados más precisos.</li>
<li><code>n = 10000</code> no es lo suficientemente grande. Para ver una disminución en RMSE, necesitamos hacerla más grande.</li>
<li>El RMSE no es una variable aleatoria.</li>
</ol>
<p>4. Ahora repita el ejercicio 1, pero esta vez haga la correlación entre <code>x</code> y <code>y</code> más grande cambiando <code>Sigma</code> así:</p>
<div class="sourceCode" id="cb1107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1107-1"><a href="ejemplos-de-algoritmos.html#cb1107-1"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb1107-2"><a href="ejemplos-de-algoritmos.html#cb1107-2"></a>Sigma &lt;-<span class="st"> </span><span class="dv">9</span><span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1107-3"><a href="ejemplos-de-algoritmos.html#cb1107-3"></a>dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma) <span class="op">%&gt;%</span></span>
<span id="cb1107-4"><a href="ejemplos-de-algoritmos.html#cb1107-4"></a><span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>))</span></code></pre></div>
<p>Repita el ejercicio y observe lo que le sucede al RMSE ahora.</p>
<p>5. ¿Cuál de las siguientes explica mejor por qué el RMSE en el ejercicio 4 es mucho más bajo que en el ejercicio 1?</p>
<ol style="list-style-type: lower-alpha">
<li>Es solo suerte. Si lo hacemos nuevamente, será más grande.</li>
<li>El teorema del límite central nos dice que el RMSE es normal.</li>
<li>Cuando aumentamos la correlación entre <code>x</code> y <code>y</code>, <code>x</code> tiene más poder predictivo y, por lo tanto, provee un mejor estimado de <code>y</code>. Esta correlación tiene un efecto mucho mayor en RMSE que <code>n</code>. <code>n</code> grande simplemente ofrece estimados más precisos de los coeficientes del modelo lineal.</li>
<li>Ambos son ejemplos de regresión, por lo que el RMSE tiene que ser el mismo.</li>
</ol>
<p>6. Cree un set de datos con el siguiente código:</p>
<div class="sourceCode" id="cb1108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1108-1"><a href="ejemplos-de-algoritmos.html#cb1108-1"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb1108-2"><a href="ejemplos-de-algoritmos.html#cb1108-2"></a>Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb1108-3"><a href="ejemplos-de-algoritmos.html#cb1108-3"></a>dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), Sigma) <span class="op">%&gt;%</span></span>
<span id="cb1108-4"><a href="ejemplos-de-algoritmos.html#cb1108-4"></a><span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;x_1&quot;</span>, <span class="st">&quot;x_2&quot;</span>))</span></code></pre></div>
<p>Tenga en cuenta que <code>y</code> está correlacionado con ambos <code>x_1</code> y <code>x_2</code>, pero los dos predictores son independientes entre sí.</p>
<div class="sourceCode" id="cb1109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1109-1"><a href="ejemplos-de-algoritmos.html#cb1109-1"></a><span class="kw">cor</span>(dat)</span></code></pre></div>
<p>Use el paquete <strong>caret</strong> para dividir en un set de evaluación y un set de entrenamiento del mismo tamaño. Compare el RMSE al usar solo <code>x_1</code>, sólo <code>x_2</code>, y ambos <code>x_1</code> y <code>x_2</code>. Entrene un modelo lineal e indique el RMSE.</p>
<p>7. Repita el ejercicio 6 pero ahora cree un ejemplo en el que <code>x_1</code> y <code>x_2</code> están altamente correlacionados:</p>
<div class="sourceCode" id="cb1110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1110-1"><a href="ejemplos-de-algoritmos.html#cb1110-1"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb1110-2"><a href="ejemplos-de-algoritmos.html#cb1110-2"></a>Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.75</span>, <span class="fl">0.75</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>, <span class="fl">0.95</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>, <span class="fl">1.0</span>), <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb1110-3"><a href="ejemplos-de-algoritmos.html#cb1110-3"></a>dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), Sigma) <span class="op">%&gt;%</span></span>
<span id="cb1110-4"><a href="ejemplos-de-algoritmos.html#cb1110-4"></a><span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;x_1&quot;</span>, <span class="st">&quot;x_2&quot;</span>))</span></code></pre></div>
<p>Use el paquete <strong>caret</strong> para dividir en un set de evaluación y uno de entrenamiento del mismo tamaño. Compare el RMSE al usar solo <code>x_1</code>, sólo <code>x_2</code> y ambos <code>x_1</code> y <code>x_2</code>. Entrene un modelo lineal e indique el RMSE.</p>
<p>8. Compare los resultados del ejercicio 6 y 7 y elija la declaración con la que está de acuerdo:</p>
<ol style="list-style-type: lower-alpha">
<li>Agregar predictores adicionales puede mejorar sustancialmente RMSE, pero no cuando están altamente correlacionados con otro predictor.</li>
<li>Agregar predictores adicionales mejora las predicciones por igual en ambos ejercicios.</li>
<li>Agregar predictores adicionales da como resultado un ajuste excesivo.</li>
<li>A menos que incluyamos todos los predictores, no tenemos poder de predicción.</li>
</ol>
</div>
<div id="regresión-logística" class="section level2">
<h2><span class="header-section-number">31.3</span> Regresión logística</h2>
<p>El enfoque de regresión puede extenderse a datos categóricos. En esta sección, primero ilustramos cómo, para datos binarios, simplemente se pueden asignar valores numéricos de 0 y 1 a los resultados <span class="math inline">\(y\)</span>. Entonces, se aplica la regresión como si los datos fueran continuos. Más tarde, señalaremos una limitación de este enfoque y presentaremos la <em>regresión logística</em> como una solución. La regresión logística es un caso específico de un set de <em>modelos lineales generalizados</em>. Para ilustrar la regresión logística, la aplicaremos a nuestro ejemplo anterior de predicción de sexo basado en altura definido en la Sección <a href="introducción-a-machine-learning.html#training-test">27.4.1</a>.</p>
<p>Si definimos el resultado <span class="math inline">\(Y\)</span> como 1 para mujeres y 0 para hombres, y <span class="math inline">\(X\)</span> como la altura, nos interesa la probabilidad condicional:</p>
<p><span class="math display">\[
\mbox{Pr}( Y = 1 \mid X = x)
\]</span></p>
<p>Como ejemplo, ofrecemos una predicción para un estudiante que mide 66 pulgadas de alto. ¿Cuál es la probabilidad condicional de ser mujer si mide 66 pulgadas de alto? En nuestro set de datos, podemos estimar esto redondeando a la pulgada más cercana y calculando:</p>
<div class="sourceCode" id="cb1111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1111-1"><a href="ejemplos-de-algoritmos.html#cb1111-1"></a>train_set <span class="op">%&gt;%</span></span>
<span id="cb1111-2"><a href="ejemplos-de-algoritmos.html#cb1111-2"></a><span class="st">  </span><span class="kw">filter</span>(<span class="kw">round</span>(height) <span class="op">==</span><span class="st"> </span><span class="dv">66</span>) <span class="op">%&gt;%</span></span>
<span id="cb1111-3"><a href="ejemplos-de-algoritmos.html#cb1111-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">y_hat =</span> <span class="kw">mean</span>(sex<span class="op">==</span><span class="st">&quot;Female&quot;</span>))</span>
<span id="cb1111-4"><a href="ejemplos-de-algoritmos.html#cb1111-4"></a><span class="co">#&gt;   y_hat</span></span>
<span id="cb1111-5"><a href="ejemplos-de-algoritmos.html#cb1111-5"></a><span class="co">#&gt; 1 0.347</span></span></code></pre></div>
<p>Para construir un algoritmo de predicción, queremos estimar la proporción de la población femenina para cualquier altura dada <span class="math inline">\(X=x\)</span>, que escribimos como la probabilidad condicional descrita anteriormente: <span class="math inline">\(\mbox{Pr}( Y = 1 | X=x)\)</span>. Veamos cómo se ve esto para varios valores de <span class="math inline">\(x\)</span> (eliminaremos estratos de <span class="math inline">\(x\)</span> con pocos puntos de datos):</p>
<div class="sourceCode" id="cb1112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1112-1"><a href="ejemplos-de-algoritmos.html#cb1112-1"></a>heights <span class="op">%&gt;%</span></span>
<span id="cb1112-2"><a href="ejemplos-de-algoritmos.html#cb1112-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">round</span>(height)) <span class="op">%&gt;%</span></span>
<span id="cb1112-3"><a href="ejemplos-de-algoritmos.html#cb1112-3"></a><span class="st">  </span><span class="kw">group_by</span>(x) <span class="op">%&gt;%</span></span>
<span id="cb1112-4"><a href="ejemplos-de-algoritmos.html#cb1112-4"></a><span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1112-5"><a href="ejemplos-de-algoritmos.html#cb1112-5"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop =</span> <span class="kw">mean</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1112-6"><a href="ejemplos-de-algoritmos.html#cb1112-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, prop)) <span class="op">+</span></span>
<span id="cb1112-7"><a href="ejemplos-de-algoritmos.html#cb1112-7"></a><span class="st">  </span><span class="kw">geom_point</span>()</span>
<span id="cb1112-8"><a href="ejemplos-de-algoritmos.html#cb1112-8"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span></code></pre></div>
<p><img src="libro_files/figure-html/height-and-sex-conditional-probabilities-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Dado que los resultados del gráfico anterior son casi lineal, y es el único enfoque que conocemos actualmente, intentaremos la regresión lineal. Suponemos que:</p>
<p><span class="math display">\[p(x) = \mbox{Pr}( Y = 1 | X=x) = \beta_0 + \beta_1 x\]</span></p>
<p>Noten: como <span class="math inline">\(p_0(x) = 1 - p_1(x)\)</span>, solo estimaremos <span class="math inline">\(p_1(x)\)</span> y eliminaremos el índice <span class="math inline">\(_1\)</span>.</p>
<p>Si convertimos los factores a 0s y 1s, podemos estimar <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> usando mínimos cuadrados.</p>
<div class="sourceCode" id="cb1113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1113-1"><a href="ejemplos-de-algoritmos.html#cb1113-1"></a>lm_fit &lt;-<span class="st"> </span><span class="kw">mutate</span>(train_set, <span class="dt">y =</span> <span class="kw">as.numeric</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1113-2"><a href="ejemplos-de-algoritmos.html#cb1113-2"></a><span class="st">  </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>height, <span class="dt">data =</span> .)</span></code></pre></div>
<p>Una vez que tengamos estimados <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span>, podemos obtener una predicción real. Nuestro estimado de la probabilidad condicional <span class="math inline">\(p(x)\)</span> es:</p>
<p><span class="math display">\[
\hat{p}(x) = \hat{\beta}_0+ \hat{\beta}_1 x
\]</span></p>
<p>Para formar una predicción, definimos una <em>regla de decisión</em>: predecir mujer si <span class="math inline">\(\hat{p}(x) &gt; 0.5\)</span>. Podemos comparar nuestras predicciones con los resultados usando:</p>
<div class="sourceCode" id="cb1114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1114-1"><a href="ejemplos-de-algoritmos.html#cb1114-1"></a>p_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(lm_fit, test_set)</span>
<span id="cb1114-2"><a href="ejemplos-de-algoritmos.html#cb1114-2"></a>y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">factor</span>()</span>
<span id="cb1114-3"><a href="ejemplos-de-algoritmos.html#cb1114-3"></a><span class="kw">confusionMatrix</span>(y_hat, test_set<span class="op">$</span>sex)<span class="op">$</span>overall[[<span class="st">&quot;Accuracy&quot;</span>]]</span>
<span id="cb1114-4"><a href="ejemplos-de-algoritmos.html#cb1114-4"></a><span class="co">#&gt; [1] 0.798</span></span></code></pre></div>
<p>Vemos que este método funciona sustancialmente mejor que adivinar.</p>
<div id="modelos-lineales-generalizados" class="section level3">
<h3><span class="header-section-number">31.3.1</span> Modelos lineales generalizados</h3>
<p>La función <span class="math inline">\(\beta_0 + \beta_1 x\)</span> puede tomar cualquier valor, incluyendo negativos y valores mayores que 1. De hecho, el estimado <span class="math inline">\(\hat{p}(x)\)</span> calculado en la sección de regresión lineal se vuelve negativo alrededor de 76 pulgadas.</p>
<div class="sourceCode" id="cb1115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1115-1"><a href="ejemplos-de-algoritmos.html#cb1115-1"></a>heights <span class="op">%&gt;%</span></span>
<span id="cb1115-2"><a href="ejemplos-de-algoritmos.html#cb1115-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">round</span>(height)) <span class="op">%&gt;%</span></span>
<span id="cb1115-3"><a href="ejemplos-de-algoritmos.html#cb1115-3"></a><span class="st">  </span><span class="kw">group_by</span>(x) <span class="op">%&gt;%</span></span>
<span id="cb1115-4"><a href="ejemplos-de-algoritmos.html#cb1115-4"></a><span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1115-5"><a href="ejemplos-de-algoritmos.html#cb1115-5"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop =</span> <span class="kw">mean</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1115-6"><a href="ejemplos-de-algoritmos.html#cb1115-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, prop)) <span class="op">+</span></span>
<span id="cb1115-7"><a href="ejemplos-de-algoritmos.html#cb1115-7"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb1115-8"><a href="ejemplos-de-algoritmos.html#cb1115-8"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> lm_fit<span class="op">$</span>coef[<span class="dv">1</span>], <span class="dt">slope =</span> lm_fit<span class="op">$</span>coef[<span class="dv">2</span>])</span>
<span id="cb1115-9"><a href="ejemplos-de-algoritmos.html#cb1115-9"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span></code></pre></div>
<p><img src="libro_files/figure-html/regression-prediction-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>El rango es:</p>
<div class="sourceCode" id="cb1116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1116-1"><a href="ejemplos-de-algoritmos.html#cb1116-1"></a><span class="kw">range</span>(p_hat)</span>
<span id="cb1116-2"><a href="ejemplos-de-algoritmos.html#cb1116-2"></a><span class="co">#&gt; [1] -0.578  1.262</span></span></code></pre></div>
<p>Pero estamos estimando una probabilidad: <span class="math inline">\(\mbox{Pr}( Y = 1 \mid X = x)\)</span> que está restringida entre 0 y 1.</p>
<p>La idea de los modelos lineales generalizados (<em>generalized linear models</em> o GLM por sus siglas en inglés) es 1) definir una distribución de <span class="math inline">\(Y\)</span> que sea consistente con sus posibles resultados y
2) encontrar una función <span class="math inline">\(g\)</span> tal que <span class="math inline">\(g(\mbox{Pr}( Y = 1 \mid X = x))\)</span> se pueda modelar como una combinación lineal de predictores.
La regresión logística es el GLM más utilizado. Es una extensión de regresión lineal que asegura que el estimado de <span class="math inline">\(\mbox{Pr}( Y = 1 \mid X = x)\)</span> esté entre 0 y 1. Este enfoque utiliza la transformación <em>logística</em> introducida en la Sección <a href="gapminder.html#logit">9.8.1</a>:</p>
<p><span class="math display">\[ g(p) = \log \frac{p}{1-p}\]</span></p>
<p>Esta transformación logística convierte probabilidad en logaritmo del riesgo relativo. Como se discutió en la sección de visualización de datos, el riesgo relativo nos dice cuánto más probable es que algo suceda en comparación con no suceder. <span class="math inline">\(p=0.5\)</span> significa que las probabilidades son de 1 a 1; por lo tanto, el riesgo relativo es 1. Si <span class="math inline">\(p=0.75\)</span>, las probabilidades son de 3 a 1. Una buena característica de esta transformación es que convierte las probabilidades en simétricas alrededor de 0. Aquí hay un gráfico de <span class="math inline">\(g(p)\)</span> versus <span class="math inline">\(p\)</span>:</p>
<p><img src="libro_files/figure-html/p-versus-logistic-of-p-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Con la <em>regresión logística</em>, modelamos la probabilidad condicional directamente con:</p>
<p><span class="math display">\[
g\left\{ \mbox{Pr}(Y = 1 \mid X=x) \right\} = \beta_0 + \beta_1 x
\]</span></p>
<p>Con este modelo, ya no podemos usar mínimos cuadrados. En su lugar, calculamos el <em>estimado de máxima verosimilitud</em> (<em>maximum likelihood estimation</em> o MLE por sus siglas en inglés). Pueden aprender más sobre este concepto en un libro de texto de teoría estadística<a href="#fn109" class="footnote-ref" id="fnref109"><sup>109</sup></a>.</p>
<p>En R, podemos ajustar el modelo de regresión logística con la función <code>glm</code>: modelos lineales generalizados (<em>generalized linear models</em> o GLM por sus siglas en inglés). Esta función puede ajustar varios modelos, no solo regresión logística, por lo cual tenemos que especificar el modelo que queremos a través del argumento <code>family</code>:</p>
<div class="sourceCode" id="cb1117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1117-1"><a href="ejemplos-de-algoritmos.html#cb1117-1"></a>glm_fit &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1117-2"><a href="ejemplos-de-algoritmos.html#cb1117-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">as.numeric</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1117-3"><a href="ejemplos-de-algoritmos.html#cb1117-3"></a><span class="st">  </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>height, <span class="dt">data =</span> ., <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
<p>Podemos obtener predicciones usando la función <code>predict</code>:</p>
<div class="sourceCode" id="cb1118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1118-1"><a href="ejemplos-de-algoritmos.html#cb1118-1"></a>p_hat_logit &lt;-<span class="st"> </span><span class="kw">predict</span>(glm_fit, <span class="dt">newdata =</span> test_set, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<p>Cuando usamos <code>predict</code> con un objeto <code>glm</code>, tenemos que especificar que queremos <code>type="response"</code> si queremos las probabilidades condicionales, ya que por defecto la función devuelve los valores luego de la transformación logística.</p>
<p>Este modelo se ajusta a los datos un poco mejor que la línea:</p>
<pre><code>#&gt; `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<p><img src="libro_files/figure-html/conditional-prob-glm-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Como tenemos un estimado <span class="math inline">\(\hat{p}(x)\)</span>, podemos obtener predicciones:</p>
<div class="sourceCode" id="cb1120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1120-1"><a href="ejemplos-de-algoritmos.html#cb1120-1"></a>y_hat_logit &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat_logit <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>factor</span>
<span id="cb1120-2"><a href="ejemplos-de-algoritmos.html#cb1120-2"></a><span class="kw">confusionMatrix</span>(y_hat_logit, test_set<span class="op">$</span>sex)<span class="op">$</span>overall[[<span class="st">&quot;Accuracy&quot;</span>]]</span>
<span id="cb1120-3"><a href="ejemplos-de-algoritmos.html#cb1120-3"></a><span class="co">#&gt; [1] 0.808</span></span></code></pre></div>
<p>Las predicciones resultantes son similares. Esto se debe a que los dos estimados de <span class="math inline">\(p(x)\)</span> mayores que 1/2 en aproximadamente la misma región de x:</p>
<p><img src="libro_files/figure-html/glm-prediction-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Las regresiones lineales y logísticas proveen un estimado de la expectativa condicional:</p>
<p><span class="math display">\[
\mbox{E}(Y \mid X=x)
\]</span>
que en el caso de datos binarios es equivalente a la probabilidad condicional:</p>
<p><span class="math display">\[
\mbox{Pr}(Y = 1 \mid X = x)
\]</span></p>
</div>
<div id="regresión-logística-con-más-de-un-predictor" class="section level3">
<h3><span class="header-section-number">31.3.2</span> Regresión logística con más de un predictor</h3>
<p>En esta sección, aplicamos la regresión logística a los datos “2 o 7” introducidos en la Sección <a href="introducción-a-machine-learning.html#two-or-seven">27.8</a>. En este caso, estamos interesados en estimar una probabilidad condicional que depende de dos variables. El modelo de regresión logística estándar en este caso supondrá que:</p>
<p><span class="math display">\[
g\{p(x_1, x_2)\}= g\{\mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2)\} =
\beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span>
con <span class="math inline">\(g(p) = \log \frac{p}{1-p}\)</span>, la función logística descrita en la sección anterior. Para ajustar el modelo, usamos el siguiente código:</p>
<div class="sourceCode" id="cb1121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1121-1"><a href="ejemplos-de-algoritmos.html#cb1121-1"></a>fit_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x_<span class="dv">2</span>, <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb1121-2"><a href="ejemplos-de-algoritmos.html#cb1121-2"></a>p_hat_glm &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_glm, mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb1121-3"><a href="ejemplos-de-algoritmos.html#cb1121-3"></a>y_hat_glm &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(p_hat_glm <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">7</span>, <span class="dv">2</span>))</span>
<span id="cb1121-4"><a href="ejemplos-de-algoritmos.html#cb1121-4"></a><span class="kw">confusionMatrix</span>(y_hat_glm, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1121-5"><a href="ejemplos-de-algoritmos.html#cb1121-5"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1121-6"><a href="ejemplos-de-algoritmos.html#cb1121-6"></a><span class="co">#&gt;     0.76</span></span></code></pre></div>
<p>Comparando con los resultados que obtuvimos en la Sección <a href="introducción-a-machine-learning.html#two-or-seven">27.8</a>, vemos que la regresión logística funciona de manera similar a la regresión. Esto no es sorprendente dado que el estimado de <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> se ve similar también:</p>
<div class="sourceCode" id="cb1122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1122-1"><a href="ejemplos-de-algoritmos.html#cb1122-1"></a>p_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_glm, <span class="dt">newdata =</span> mnist_<span class="dv">27</span><span class="op">$</span>true_p, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb1122-2"><a href="ejemplos-de-algoritmos.html#cb1122-2"></a>mnist_<span class="dv">27</span><span class="op">$</span>true_p <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">p_hat =</span> p_hat) <span class="op">%&gt;%</span></span>
<span id="cb1122-3"><a href="ejemplos-de-algoritmos.html#cb1122-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">z=</span>p_hat, <span class="dt">fill=</span>p_hat)) <span class="op">+</span></span>
<span id="cb1122-4"><a href="ejemplos-de-algoritmos.html#cb1122-4"></a><span class="st">  </span><span class="kw">geom_raster</span>() <span class="op">+</span></span>
<span id="cb1122-5"><a href="ejemplos-de-algoritmos.html#cb1122-5"></a><span class="st">  </span><span class="kw">scale_fill_gradientn</span>(<span class="dt">colors=</span><span class="kw">c</span>(<span class="st">&quot;#F8766D&quot;</span>,<span class="st">&quot;white&quot;</span>,<span class="st">&quot;#00BFC4&quot;</span>)) <span class="op">+</span></span>
<span id="cb1122-6"><a href="ejemplos-de-algoritmos.html#cb1122-6"></a><span class="st">  </span><span class="kw">stat_contour</span>(<span class="dt">breaks=</span><span class="kw">c</span>(<span class="fl">0.5</span>), <span class="dt">color=</span><span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/logistic-p-hat-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Al igual que con la regresión lineal, la regla de decisión es una línea, un hecho que puede corroborarse matemáticamente ya que:</p>
<p><span class="math display">\[
g^{-1}(\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2) = 0.5 \implies
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = g(0.5) = 0 \implies
x_2 = -\hat{\beta}_0/\hat{\beta}_2 -\hat{\beta}_1/\hat{\beta}_2 x_1
\]</span></p>
<p>Por eso <span class="math inline">\(x_2\)</span> es una función lineal de <span class="math inline">\(x_1\)</span>. Esto implica que, al igual que la regresión, nuestro enfoque de regresión logística no tiene ninguna posibilidad de capturar la naturaleza no lineal de la verdadera <span class="math inline">\(p(x_1,x_2)\)</span>. Una vez que pasemos a ejemplos más complejos, veremos que la regresión lineal y la regresión lineal generalizada son limitadas y no lo suficientemente flexibles como para ser útiles para la mayoría de los desafíos de <em>machine learning</em>. Las nuevas técnicas que aprendemos son esencialmente enfoques para estimar la probabilidad condicional de una manera más flexible.</p>
</div>
</div>
<div id="ejercicios-51" class="section level2">
<h2><span class="header-section-number">31.4</span> Ejercicios</h2>
<p>1. Defina el siguiente set de datos:</p>
<div class="sourceCode" id="cb1123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1123-1"><a href="ejemplos-de-algoritmos.html#cb1123-1"></a>make_data &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">p =</span> <span class="fl">0.5</span>,</span>
<span id="cb1123-2"><a href="ejemplos-de-algoritmos.html#cb1123-2"></a>                      <span class="dt">mu_0 =</span> <span class="dv">0</span>, <span class="dt">mu_1 =</span> <span class="dv">2</span>,</span>
<span id="cb1123-3"><a href="ejemplos-de-algoritmos.html#cb1123-3"></a>                      <span class="dt">sigma_0 =</span> <span class="dv">1</span>, <span class="dt">sigma_1 =</span> <span class="dv">1</span>){</span>
<span id="cb1123-4"><a href="ejemplos-de-algoritmos.html#cb1123-4"></a>  y &lt;-<span class="st"> </span><span class="kw">rbinom</span>(n, <span class="dv">1</span>, p)</span>
<span id="cb1123-5"><a href="ejemplos-de-algoritmos.html#cb1123-5"></a>  f_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, mu_<span class="dv">0</span>, sigma_<span class="dv">0</span>)</span>
<span id="cb1123-6"><a href="ejemplos-de-algoritmos.html#cb1123-6"></a>  f_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, mu_<span class="dv">1</span>, sigma_<span class="dv">1</span>)</span>
<span id="cb1123-7"><a href="ejemplos-de-algoritmos.html#cb1123-7"></a>  x &lt;-<span class="st"> </span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, f_<span class="dv">1</span>, f_<span class="dv">0</span>)</span>
<span id="cb1123-8"><a href="ejemplos-de-algoritmos.html#cb1123-8"></a>  test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1123-9"><a href="ejemplos-de-algoritmos.html#cb1123-9"></a>  <span class="kw">list</span>(<span class="dt">train =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> <span class="kw">as.factor</span>(y)) <span class="op">%&gt;%</span></span>
<span id="cb1123-10"><a href="ejemplos-de-algoritmos.html#cb1123-10"></a><span class="st">         </span><span class="kw">slice</span>(<span class="op">-</span>test_index),</span>
<span id="cb1123-11"><a href="ejemplos-de-algoritmos.html#cb1123-11"></a>       <span class="dt">test =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> <span class="kw">as.factor</span>(y)) <span class="op">%&gt;%</span></span>
<span id="cb1123-12"><a href="ejemplos-de-algoritmos.html#cb1123-12"></a><span class="st">         </span><span class="kw">slice</span>(test_index))</span>
<span id="cb1123-13"><a href="ejemplos-de-algoritmos.html#cb1123-13"></a>}</span>
<span id="cb1123-14"><a href="ejemplos-de-algoritmos.html#cb1123-14"></a>dat &lt;-<span class="st"> </span><span class="kw">make_data</span>()</span></code></pre></div>
<p>Noten que hemos definido una variable <code>x</code> que es predictiva de un resultado binario <code>y</code>.</p>
<div class="sourceCode" id="cb1124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1124-1"><a href="ejemplos-de-algoritmos.html#cb1124-1"></a>dat<span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, <span class="dt">color =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_density</span>()</span></code></pre></div>
<p>Compare la exactitud de la regresión lineal y la regresión logística.</p>
<p>2. Repita la simulación del primer ejercicio 100 veces y compare la exactitud promedio para cada método. Observe como dan prácticamente la misma respuesta.</p>
<p>3. Genere 25 sets de datos diferentes cambiando la diferencia entre las dos clases: <code>delta &lt;- seq(0, 3, len = 25)</code>. Grafique exactitud versus <code>delta</code>.</p>

</div>
<div id="k-vecinos-más-cercanos-knn" class="section level2">
<h2><span class="header-section-number">31.5</span> k vecinos más cercanos (kNN)</h2>
<p>Introdujimos el algoritmo kNN en la Sección <a href="cross-validation.html#knn-cv-intro">29.1</a> y demostramos cómo usamos la validación cruzada para elegir <span class="math inline">\(k\)</span> en la Sección <a href="caret.html#caret-cv">30.2</a>. Aquí revisamos rápidamente cómo ajustamos un modelo kNN usando el paquete <strong>caret</strong>. En la Sección <a href="caret.html#caret-cv">30.2</a>, presentamos el siguiente código para que se ajuste a/para ajustar un modelo kNN:</p>
<div class="sourceCode" id="cb1125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1125-1"><a href="ejemplos-de-algoritmos.html#cb1125-1"></a>train_knn &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb1125-2"><a href="ejemplos-de-algoritmos.html#cb1125-2"></a>                   <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train,</span>
<span id="cb1125-3"><a href="ejemplos-de-algoritmos.html#cb1125-3"></a>                   <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">9</span>, <span class="dv">71</span>, <span class="dv">2</span>)))</span></code></pre></div>
<p>Vimos que el parámetro que maximizaba la exactitud estimada era:</p>
<div class="sourceCode" id="cb1126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1126-1"><a href="ejemplos-de-algoritmos.html#cb1126-1"></a>train_knn<span class="op">$</span>bestTune</span>
<span id="cb1126-2"><a href="ejemplos-de-algoritmos.html#cb1126-2"></a><span class="co">#&gt;     k</span></span>
<span id="cb1126-3"><a href="ejemplos-de-algoritmos.html#cb1126-3"></a><span class="co">#&gt; 10 27</span></span></code></pre></div>
<p>Este modelo resulta en una exactitud mejor que la de regresión y de regresión logística:</p>
<div class="sourceCode" id="cb1127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1127-1"><a href="ejemplos-de-algoritmos.html#cb1127-1"></a><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_knn, mnist_<span class="dv">27</span><span class="op">$</span>test, <span class="dt">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb1127-2"><a href="ejemplos-de-algoritmos.html#cb1127-2"></a>                mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1127-3"><a href="ejemplos-de-algoritmos.html#cb1127-3"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1127-4"><a href="ejemplos-de-algoritmos.html#cb1127-4"></a><span class="co">#&gt;    0.835</span></span></code></pre></div>
<p>Un gráfico de la probabilidad condicional estimada muestra que el estimado de kNN es lo suficientemente flexible para capturar la forma de la probabilidad condicional verdadera.</p>
<p><img src="libro_files/figure-html/best-knn-fit-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="ejercicios-52" class="section level2">
<h2><span class="header-section-number">31.6</span> Ejercicios</h2>
<p>1. Anteriormente utilizamos regresión logística para predecir el sexo basado en la altura. Use kNN para hacer lo mismo. Use el código descrito en este capítulo para seleccionar la medida <span class="math inline">\(F_1\)</span> y graficarla contra <span class="math inline">\(k\)</span>. Compare con el <span class="math inline">\(F_1\)</span> de aproximadamente 0.6 que obtuvimos con regresión.</p>
<p>2. Cargue el siguiente set de datos:</p>
<div class="sourceCode" id="cb1128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1128-1"><a href="ejemplos-de-algoritmos.html#cb1128-1"></a><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)</span></code></pre></div>
<p>Este set de datos incluye una matriz <code>x</code>:</p>
<div class="sourceCode" id="cb1129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1129-1"><a href="ejemplos-de-algoritmos.html#cb1129-1"></a><span class="kw">dim</span>(tissue_gene_expression<span class="op">$</span>x)</span></code></pre></div>
<p>con la expresión génica medida en 500 genes para 189 muestras biológicas que representan siete tejidos diferentes. El tipo de tejido se almacena en <code>y</code>:</p>
<div class="sourceCode" id="cb1130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1130-1"><a href="ejemplos-de-algoritmos.html#cb1130-1"></a><span class="kw">table</span>(tissue_gene_expression<span class="op">$</span>y)</span></code></pre></div>
<p>Divida los datos en sets de entrenamiento y de evaluación. Luego use kNN para predecir el tipo de tejido y ver qué exactitud obtiene. Pruébelo para <span class="math inline">\(k = 1, 3, \dots, 11\)</span>.</p>

</div>
<div id="modelos-generativos" class="section level2">
<h2><span class="header-section-number">31.7</span> Modelos generativos</h2>
<p>Hemos descrito cómo, cuando se usa la función de pérdida cuadrática, las expectativas/probabilidades condicionales ofrecen el mejor enfoque para desarrollar una regla de decisión. En un caso binario, el error verdadero más pequeño que podemos lograr está determinado por la regla de Bayes, que es una regla de decisión basada en la probabilidad condicional verdadera:</p>
<p><span class="math display">\[
p(\mathbf{x}) = \mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})
\]</span></p>
<p>Hemos descrito varios enfoques para estimar <span class="math inline">\(p(\mathbf{x})\)</span>. En todos estos, estimamos la probabilidad condicional directamente y no consideramos la distribución de los predictores. En <em>machine learning</em>, estos se denominan enfoques <em>discriminativos</em>.</p>
<p>Sin embargo, el teorema de Bayes nos dice que conocer la distribución de los predictores <span class="math inline">\(\mathbf{X}\)</span> puede ser útil. Métodos que modelan la distribución conjunta de <span class="math inline">\(Y\)</span> y <span class="math inline">\(\mathbf{X}\)</span> se denominan <em>modelos generativos</em> (modelamos cómo todos los datos, <span class="math inline">\(\mathbf{X}\)</span> e <span class="math inline">\(Y\)</span>, se generan). Comenzamos describiendo el modelo generativo más general, <em>Naive Bayes</em>, y luego describimos dos casos específicos: el análisis discriminante cuadrático (<em>quadratic discriminant analysis</em> o QDA en por sus siglas en inglés) y el análisis discriminante lineal (<em>linear discriminant analysis</em> o LDA por sus siglas en inglés).</p>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">31.7.1</span> Naive Bayes</h3>
<p>Recordemos que la regla de Bayes nos dice que podemos reescribir <span class="math inline">\(p(\mathbf{x})\)</span> así:</p>
<p><span class="math display">\[
p(\mathbf{x}) = \mbox{Pr}(Y=1|\mathbf{X}=\mathbf{x}) = \frac{f_{\mathbf{X}|Y=1}(\mathbf{x}) \mbox{Pr}(Y=1)}
{ f_{\mathbf{X}|Y=0}(\mathbf{x})\mbox{Pr}(Y=0) + f_{\mathbf{X}|Y=1}(\mathbf{x})\mbox{Pr}(Y=1) }
\]</span></p>
<p>con <span class="math inline">\(f_{\mathbf{X}|Y=1}\)</span> y <span class="math inline">\(f_{\mathbf{X}|Y=0}\)</span> representando las funciones de distribución del predictor <span class="math inline">\(\mathbf{X}\)</span> para las dos clases <span class="math inline">\(Y=1\)</span> y <span class="math inline">\(Y=0\)</span>. La fórmula implica que si podemos estimar estas distribuciones condicionales de los predictores, podemos desarrollar una poderosa regla de decisión. Sin embargo, esto es un gran “si”. A medida que avancemos, encontraremos ejemplos en los que <span class="math inline">\(\mathbf{X}\)</span> tiene muchas dimensiones y no tenemos mucha información sobre la distribución. En estos casos, <em>Naive Bayes</em> será prácticamente imposible de implementar. Sin embargo, hay casos en los que tenemos un pequeño número de predictores (no más de 2) y muchas categorías en las que los modelos generativos pueden ser bastante poderosos. Describimos dos ejemplos específicos y utilizamos nuestros estudios de caso descritos anteriormente para ilustrarlos.</p>
<p>Comencemos con un caso muy sencillo y poco interesante, pero ilustrativo: el ejemplo relacionado con la predicción del sexo basado en la altura.</p>
<div class="sourceCode" id="cb1131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1131-1"><a href="ejemplos-de-algoritmos.html#cb1131-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1131-2"><a href="ejemplos-de-algoritmos.html#cb1131-2"></a><span class="kw">library</span>(caret)</span>
<span id="cb1131-3"><a href="ejemplos-de-algoritmos.html#cb1131-3"></a></span>
<span id="cb1131-4"><a href="ejemplos-de-algoritmos.html#cb1131-4"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1131-5"><a href="ejemplos-de-algoritmos.html#cb1131-5"></a><span class="kw">data</span>(<span class="st">&quot;heights&quot;</span>)</span>
<span id="cb1131-6"><a href="ejemplos-de-algoritmos.html#cb1131-6"></a></span>
<span id="cb1131-7"><a href="ejemplos-de-algoritmos.html#cb1131-7"></a>y &lt;-<span class="st"> </span>heights<span class="op">$</span>height</span>
<span id="cb1131-8"><a href="ejemplos-de-algoritmos.html#cb1131-8"></a><span class="kw">set.seed</span>(<span class="dv">1995</span>)</span>
<span id="cb1131-9"><a href="ejemplos-de-algoritmos.html#cb1131-9"></a>test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1131-10"><a href="ejemplos-de-algoritmos.html#cb1131-10"></a>train_set &lt;-<span class="st"> </span>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>test_index)</span>
<span id="cb1131-11"><a href="ejemplos-de-algoritmos.html#cb1131-11"></a>test_set &lt;-<span class="st"> </span>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(test_index)</span></code></pre></div>
<p>En este caso, el enfoque <em>Naive Bayes</em> es particularmente apropiado porque sabemos que la distribución normal es una buena aproximación para las distribuciones condicionales de altura dado el sexo para ambas clases <span class="math inline">\(Y=1\)</span> (mujer) y <span class="math inline">\(Y=0\)</span> (hombre). Esto implica que podemos aproximar las distribuciones condicionales <span class="math inline">\(f_{X|Y=1}\)</span> y <span class="math inline">\(f_{X|Y=0}\)</span> al simplemente estimar los promedios y las desviaciones estándar de los datos:</p>
<div class="sourceCode" id="cb1132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1132-1"><a href="ejemplos-de-algoritmos.html#cb1132-1"></a>params &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1132-2"><a href="ejemplos-de-algoritmos.html#cb1132-2"></a><span class="st">  </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span></span>
<span id="cb1132-3"><a href="ejemplos-de-algoritmos.html#cb1132-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg =</span> <span class="kw">mean</span>(height), <span class="dt">sd =</span> <span class="kw">sd</span>(height))</span>
<span id="cb1132-4"><a href="ejemplos-de-algoritmos.html#cb1132-4"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1132-5"><a href="ejemplos-de-algoritmos.html#cb1132-5"></a>params</span>
<span id="cb1132-6"><a href="ejemplos-de-algoritmos.html#cb1132-6"></a><span class="co">#&gt; # A tibble: 2 x 3</span></span>
<span id="cb1132-7"><a href="ejemplos-de-algoritmos.html#cb1132-7"></a><span class="co">#&gt;   sex      avg    sd</span></span>
<span id="cb1132-8"><a href="ejemplos-de-algoritmos.html#cb1132-8"></a><span class="co">#&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb1132-9"><a href="ejemplos-de-algoritmos.html#cb1132-9"></a><span class="co">#&gt; 1 Female  64.8  4.14</span></span>
<span id="cb1132-10"><a href="ejemplos-de-algoritmos.html#cb1132-10"></a><span class="co">#&gt; 2 Male    69.2  3.57</span></span></code></pre></div>
<p>La prevalencia, que denotaremos con <span class="math inline">\(\pi = \mbox{Pr}(Y=1)\)</span>, puede estimarse a partir de los datos con:</p>
<div class="sourceCode" id="cb1133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1133-1"><a href="ejemplos-de-algoritmos.html#cb1133-1"></a>pi &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">pi=</span><span class="kw">mean</span>(sex<span class="op">==</span><span class="st">&quot;Female&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(pi)</span>
<span id="cb1133-2"><a href="ejemplos-de-algoritmos.html#cb1133-2"></a>pi</span>
<span id="cb1133-3"><a href="ejemplos-de-algoritmos.html#cb1133-3"></a><span class="co">#&gt; [1] 0.212</span></span></code></pre></div>
<p>Ahora podemos usar nuestros estimados de promedio y desviación estándar para obtener una regla:</p>
<div class="sourceCode" id="cb1134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1134-1"><a href="ejemplos-de-algoritmos.html#cb1134-1"></a>x &lt;-<span class="st"> </span>test_set<span class="op">$</span>height</span>
<span id="cb1134-2"><a href="ejemplos-de-algoritmos.html#cb1134-2"></a></span>
<span id="cb1134-3"><a href="ejemplos-de-algoritmos.html#cb1134-3"></a>f0 &lt;-<span class="st"> </span><span class="kw">dnorm</span>(x, params<span class="op">$</span>avg[<span class="dv">2</span>], params<span class="op">$</span>sd[<span class="dv">2</span>])</span>
<span id="cb1134-4"><a href="ejemplos-de-algoritmos.html#cb1134-4"></a>f1 &lt;-<span class="st"> </span><span class="kw">dnorm</span>(x, params<span class="op">$</span>avg[<span class="dv">1</span>], params<span class="op">$</span>sd[<span class="dv">1</span>])</span>
<span id="cb1134-5"><a href="ejemplos-de-algoritmos.html#cb1134-5"></a></span>
<span id="cb1134-6"><a href="ejemplos-de-algoritmos.html#cb1134-6"></a>p_hat_bayes &lt;-<span class="st"> </span>f1<span class="op">*</span>pi<span class="op">/</span><span class="st"> </span>(f1<span class="op">*</span>pi <span class="op">+</span><span class="st"> </span>f0<span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>pi))</span></code></pre></div>
<p>Nuestro estimado de <em>Naive Bayes</em> <span class="math inline">\(\hat{p}(x)\)</span> se parece mucho a nuestro estimado de regresión logística:</p>
<pre><code>#&gt; `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<p><img src="libro_files/figure-html/conditional-prob-glm-fit-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>De hecho, podemos mostrar que el enfoque de <em>Naive Bayes</em> es matemáticamente similar a la predicción de regresión logística. Sin embargo, dejamos la demostración a un texto más avanzado, como <em>Elements of Statistical Learning</em><a href="#fn110" class="footnote-ref" id="fnref110"><sup>110</sup></a>. Podemos ver que son similares empíricamente al comparar las dos curvas resultantes.</p>
</div>
<div id="controlando-la-prevalencia" class="section level3">
<h3><span class="header-section-number">31.7.2</span> Controlando la prevalencia</h3>
<p>Una característica útil del enfoque <em>Naive Bayes</em> es que incluye un parámetro para tomar en cuenta las diferencias en la prevalencia. Usando nuestra muestra, estimamos <span class="math inline">\(f_{X|Y=1}\)</span>, <span class="math inline">\(f_{X|Y=0}\)</span> y <span class="math inline">\(\pi\)</span>. Si usamos sombreros para denotar los estimados, podemos escribir <span class="math inline">\(\hat{p}(x)\)</span> como:</p>
<p><span class="math display">\[
\hat{p}(x)= \frac{\hat{f}_{X|Y=1}(x) \hat{\pi}}
{ \hat{f}_{X|Y=0}(x)(1-\hat{\pi}) + \hat{f}_{X|Y=1}(x)\hat{\pi} }
\]</span></p>
<p>Como discutimos anteriormente, nuestra muestra tiene una prevalencia mucho menor, 0.21, que la población general. Entonces si usamos la regla <span class="math inline">\(\hat{p}(x)&gt;0.5\)</span> para predecir mujeres, nuestra exactitud se verá afectada debido a la baja sensibilidad:</p>
<div class="sourceCode" id="cb1136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1136-1"><a href="ejemplos-de-algoritmos.html#cb1136-1"></a>y_hat_bayes &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat_bayes <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>)</span>
<span id="cb1136-2"><a href="ejemplos-de-algoritmos.html#cb1136-2"></a><span class="kw">sensitivity</span>(<span class="dt">data =</span> <span class="kw">factor</span>(y_hat_bayes), <span class="dt">reference =</span> <span class="kw">factor</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1136-3"><a href="ejemplos-de-algoritmos.html#cb1136-3"></a><span class="co">#&gt; [1] 0.213</span></span></code></pre></div>
<p>Nuevamente, esto se debe a que el algoritmo da más peso a la especificidad para tomar en cuenta la baja prevalencia:</p>
<div class="sourceCode" id="cb1137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1137-1"><a href="ejemplos-de-algoritmos.html#cb1137-1"></a><span class="kw">specificity</span>(<span class="dt">data =</span> <span class="kw">factor</span>(y_hat_bayes), <span class="dt">reference =</span> <span class="kw">factor</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1137-2"><a href="ejemplos-de-algoritmos.html#cb1137-2"></a><span class="co">#&gt; [1] 0.967</span></span></code></pre></div>
<p>Esto se debe principalmente al hecho de que <span class="math inline">\(\hat{\pi}\)</span> es sustancialmente menor que 0.5, por lo que tendemos a predecir <code>Male</code> más a menudo. Tiene sentido que un algoritmo de <em>machine learning</em> haga esto en nuestra muestra porque tenemos un mayor porcentaje de hombres. Pero si tuviéramos que extrapolar esto a una población general, nuestra exactitud general se vería afectada por la baja sensibilidad.</p>
<p>El enfoque <em>Naive Bayes</em> nos da una forma directa de corregir esto, ya que simplemente podemos forzar <span class="math inline">\(\hat{\pi}\)</span> a ser el valor que queremos. Entonces, para equilibrar la especificidad y la sensibilidad, en lugar de cambiar el umbral en la regla de decisión, simplemente podríamos cambiar <span class="math inline">\(\hat{\pi}\)</span> a 0.5 así:</p>
<div class="sourceCode" id="cb1138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1138-1"><a href="ejemplos-de-algoritmos.html#cb1138-1"></a>p_hat_bayes_unbiased &lt;-<span class="st"> </span>f1 <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span><span class="op">/</span><span class="st"> </span>(f1 <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span>f0 <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span>))</span>
<span id="cb1138-2"><a href="ejemplos-de-algoritmos.html#cb1138-2"></a>y_hat_bayes_unbiased &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat_bayes_unbiased<span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>)</span></code></pre></div>
<p>Tengan en cuenta la diferencia de sensibilidad con un mejor equilibrio:</p>
<div class="sourceCode" id="cb1139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1139-1"><a href="ejemplos-de-algoritmos.html#cb1139-1"></a><span class="kw">sensitivity</span>(<span class="kw">factor</span>(y_hat_bayes_unbiased), <span class="kw">factor</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1139-2"><a href="ejemplos-de-algoritmos.html#cb1139-2"></a><span class="co">#&gt; [1] 0.693</span></span>
<span id="cb1139-3"><a href="ejemplos-de-algoritmos.html#cb1139-3"></a><span class="kw">specificity</span>(<span class="kw">factor</span>(y_hat_bayes_unbiased), <span class="kw">factor</span>(test_set<span class="op">$</span>sex))</span>
<span id="cb1139-4"><a href="ejemplos-de-algoritmos.html#cb1139-4"></a><span class="co">#&gt; [1] 0.832</span></span></code></pre></div>
<p>La nueva regla también nos da un umbral muy intuitivo entre 66-67, que es aproximadamente la mitad de las alturas promedio de hombres y mujeres:</p>
<div class="sourceCode" id="cb1140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1140-1"><a href="ejemplos-de-algoritmos.html#cb1140-1"></a><span class="kw">qplot</span>(x, p_hat_bayes_unbiased, <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>) <span class="op">+</span></span>
<span id="cb1140-2"><a href="ejemplos-de-algoritmos.html#cb1140-2"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="fl">0.5</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb1140-3"><a href="ejemplos-de-algoritmos.html#cb1140-3"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">67</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/naive-with-good-prevalence-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="análisis-discriminante-cuadrático" class="section level3">
<h3><span class="header-section-number">31.7.3</span> Análisis discriminante cuadrático</h3>
<p>El <em>análisis discriminante cuadrático</em> (QDA) es una versión de <em>Naive Bayes</em> en la cual suponemos que las distribuciones <span class="math inline">\(p_{\mathbf{X}|Y=1}(x)\)</span> y <span class="math inline">\(p_{\mathbf{X}|Y=0}(\mathbf{x})\)</span> siguen una distribución normal de múltiples variables. El ejemplo sencillo que describimos en la sección anterior es QDA. Veamos ahora un caso un poco más complicado: el ejemplo “2 o 7”.</p>
<div class="sourceCode" id="cb1141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1141-1"><a href="ejemplos-de-algoritmos.html#cb1141-1"></a><span class="kw">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</span></code></pre></div>
<p>En este caso, tenemos dos predictores, por lo que suponemos que cada uno sigue una distribución normal de dos variables. Esto implica que necesitamos estimar dos promedios, dos desviaciones estándar y una correlación para cada caso <span class="math inline">\(Y=1\)</span> y <span class="math inline">\(Y=0\)</span>. Una vez que tengamos estos, podemos aproximar las distribuciones <span class="math inline">\(f_{X_1,X_2|Y=1}\)</span> y <span class="math inline">\(f_{X_1, X_2|Y=0}\)</span>. Podemos estimar fácilmente los parámetros a partir de los datos:</p>
<div class="sourceCode" id="cb1142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1142-1"><a href="ejemplos-de-algoritmos.html#cb1142-1"></a>params &lt;-<span class="st"> </span>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span></span>
<span id="cb1142-2"><a href="ejemplos-de-algoritmos.html#cb1142-2"></a><span class="st">  </span><span class="kw">group_by</span>(y) <span class="op">%&gt;%</span></span>
<span id="cb1142-3"><a href="ejemplos-de-algoritmos.html#cb1142-3"></a><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg_1 =</span> <span class="kw">mean</span>(x_<span class="dv">1</span>), <span class="dt">avg_2 =</span> <span class="kw">mean</span>(x_<span class="dv">2</span>),</span>
<span id="cb1142-4"><a href="ejemplos-de-algoritmos.html#cb1142-4"></a>            <span class="dt">sd_1=</span> <span class="kw">sd</span>(x_<span class="dv">1</span>), <span class="dt">sd_2 =</span> <span class="kw">sd</span>(x_<span class="dv">2</span>),</span>
<span id="cb1142-5"><a href="ejemplos-de-algoritmos.html#cb1142-5"></a>            <span class="dt">r =</span> <span class="kw">cor</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>))</span>
<span id="cb1142-6"><a href="ejemplos-de-algoritmos.html#cb1142-6"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1142-7"><a href="ejemplos-de-algoritmos.html#cb1142-7"></a>params</span>
<span id="cb1142-8"><a href="ejemplos-de-algoritmos.html#cb1142-8"></a><span class="co">#&gt; # A tibble: 2 x 6</span></span>
<span id="cb1142-9"><a href="ejemplos-de-algoritmos.html#cb1142-9"></a><span class="co">#&gt;   y     avg_1 avg_2   sd_1   sd_2     r</span></span>
<span id="cb1142-10"><a href="ejemplos-de-algoritmos.html#cb1142-10"></a><span class="co">#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb1142-11"><a href="ejemplos-de-algoritmos.html#cb1142-11"></a><span class="co">#&gt; 1 2     0.129 0.283 0.0702 0.0578 0.401</span></span>
<span id="cb1142-12"><a href="ejemplos-de-algoritmos.html#cb1142-12"></a><span class="co">#&gt; 2 7     0.234 0.288 0.0719 0.105  0.455</span></span></code></pre></div>
<p>Aquí ofrecemos una forma visual de mostrar el enfoque. Graficamos los datos y usamos gráficos de contorno (<em>contour plots</em> en inglés) para dar una idea de cómo son las dos densidades normales estimadas (mostramos la curva que representa una región que incluye el 95% de los puntos):</p>
<div class="sourceCode" id="cb1143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1143-1"><a href="ejemplos-de-algoritmos.html#cb1143-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y)) <span class="op">%&gt;%</span></span>
<span id="cb1143-2"><a href="ejemplos-de-algoritmos.html#cb1143-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">fill =</span> y, <span class="dt">color=</span>y)) <span class="op">+</span></span>
<span id="cb1143-3"><a href="ejemplos-de-algoritmos.html#cb1143-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb1143-4"><a href="ejemplos-de-algoritmos.html#cb1143-4"></a><span class="st">  </span><span class="kw">stat_ellipse</span>(<span class="dt">type=</span><span class="st">&quot;norm&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/qda-explained-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Esto define el siguiente estimado de <span class="math inline">\(f(x_1, x_2)\)</span>.</p>
<p>Podemos usar la función <code>train</code> del paquete <strong>caret</strong> para ajustar el modelo y obtener predictores:</p>
<div class="sourceCode" id="cb1144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1144-1"><a href="ejemplos-de-algoritmos.html#cb1144-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1144-2"><a href="ejemplos-de-algoritmos.html#cb1144-2"></a>train_qda &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;qda&quot;</span>, <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)</span></code></pre></div>
<p>Vemos que obtenemos una exactitud relativamente buena:</p>
<div class="sourceCode" id="cb1145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1145-1"><a href="ejemplos-de-algoritmos.html#cb1145-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(train_qda, mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb1145-2"><a href="ejemplos-de-algoritmos.html#cb1145-2"></a><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1145-3"><a href="ejemplos-de-algoritmos.html#cb1145-3"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1145-4"><a href="ejemplos-de-algoritmos.html#cb1145-4"></a><span class="co">#&gt;     0.82</span></span></code></pre></div>
<p>La probabilidad condicional estimada se ve relativamente bien, aunque no se ajusta tan bien como los suavizadores de <em>kernel</em>:</p>
<p><img src="libro_files/figure-html/qda-estimate-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Una razón por la que QDA no funciona tan bien como los métodos de <em>kernel</em> es quizás porque la presunción de normalidad no es válida. Aunque para los 2 parece razonable, para los 7 no lo parece. Observen la ligera curvatura en los puntos para los 7:</p>
<div class="sourceCode" id="cb1146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1146-1"><a href="ejemplos-de-algoritmos.html#cb1146-1"></a>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y)) <span class="op">%&gt;%</span></span>
<span id="cb1146-2"><a href="ejemplos-de-algoritmos.html#cb1146-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">fill =</span> y, <span class="dt">color=</span>y)) <span class="op">+</span></span>
<span id="cb1146-3"><a href="ejemplos-de-algoritmos.html#cb1146-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb1146-4"><a href="ejemplos-de-algoritmos.html#cb1146-4"></a><span class="st">  </span><span class="kw">stat_ellipse</span>(<span class="dt">type=</span><span class="st">&quot;norm&quot;</span>) <span class="op">+</span></span>
<span id="cb1146-5"><a href="ejemplos-de-algoritmos.html#cb1146-5"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>y)</span></code></pre></div>
<p><img src="libro_files/figure-html/qda-does-not-fit-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>QDA puede funcionar bien aquí, pero se vuelve más difícil de usar a medida que aumente el número de predictores. Aquí tenemos 2 predictores y tuvimos que calcular 4 medias, 4 desviaciones estándar y 2 correlaciones. ¿Cuántos parámetros tendríamos si en lugar de 2 predictores tuviéramos 10?
El principal problema proviene del estimado de correlaciones para 10 predictores. Con 10, tenemos 45 correlaciones para cada clase. En general, la fórmula es <span class="math inline">\(K\times p(p-1)/2\)</span>, que se hace grande rápidamente. Una vez el número de parámetros se acerque al tamaño de nuestros datos, el método deja de ser práctico debido al sobreajuste.</p>
</div>
<div id="análisis-discriminante-lineal" class="section level3">
<h3><span class="header-section-number">31.7.4</span> Análisis discriminante lineal</h3>
<p>Una solución relativamente sencilla para el problema de tener demasiados parámetros es suponer que la estructura de correlación es la misma para todas las clases, lo que reduce el número de parámetros que necesitamos estimar.</p>
<p>En este caso, calcularíamos solo un par de desviaciones estándar y una correlación,
<!--so the parameters would look something like this:


```r
params <- mnist_27$train %>%
group_by(y) %>%
summarize(avg_1 = mean(x_1), avg_2 = mean(x_2),
sd_1= sd(x_1), sd_2 = sd(x_2),
r = cor(x_1,x_2))
#> `summarise()` ungrouping output (override with `.groups` argument)

params <- params %>% mutate(sd_1 = mean(sd_1), sd_2=mean(sd_2), r=mean(r))
params
#> # A tibble: 2 x 6
#>   y     avg_1 avg_2   sd_1   sd_2     r
#>   <fct> <dbl> <dbl>  <dbl>  <dbl> <dbl>
#> 1 2     0.129 0.283 0.0710 0.0813 0.428
#> 2 7     0.234 0.288 0.0710 0.0813 0.428
```
-->
y las distribuciones se ven así:</p>
<p><img src="libro_files/figure-html/lda-explained-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Ahora el tamaño de las elipses y el ángulo son iguales. Esto se debe a que tienen las mismas desviaciones estándar y correlaciones.</p>
<p>Podemos ajustar el modelo LDA usando <strong>caret</strong>:</p>
<div class="sourceCode" id="cb1147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1147-1"><a href="ejemplos-de-algoritmos.html#cb1147-1"></a>train_lda &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;lda&quot;</span>, <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)</span>
<span id="cb1147-2"><a href="ejemplos-de-algoritmos.html#cb1147-2"></a>y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(train_lda, mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb1147-3"><a href="ejemplos-de-algoritmos.html#cb1147-3"></a><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1147-4"><a href="ejemplos-de-algoritmos.html#cb1147-4"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1147-5"><a href="ejemplos-de-algoritmos.html#cb1147-5"></a><span class="co">#&gt;     0.75</span></span></code></pre></div>
<p>Cuando forzamos esta suposición, podemos mostrar matemáticamente que la frontera es una línea, al igual que con la regresión logística. Por esta razón, llamamos al método <em>análisis lineal discriminante</em> (LDA). Del mismo modo, para QDA, podemos mostrar que la frontera debe ser una función cuadrática.</p>
<p><img src="libro_files/figure-html/lda-estimate-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>En el caso de LDA, la falta de flexibilidad no nos permite capturar la no linealidad en la verdadera función de probabilidad condicional.</p>
</div>
<div id="conexión-a-distancia" class="section level3">
<h3><span class="header-section-number">31.7.5</span> Conexión a distancia</h3>
<p>La densidad normal es:</p>
<p><span class="math display">\[
p(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp\left\{ - \frac{(x-\mu)^2}{\sigma^2}\right\}
\]</span></p>
<p>Si eliminamos la constante <span class="math inline">\(1/(\sqrt{2\pi} \sigma)\)</span> y luego tomamos el logaritmo, obtenemos:</p>
<p><span class="math display">\[
- \frac{(x-\mu)^2}{\sigma^2}
\]</span></p>
<p>que es el negativo de una distancia al cuadrado escalada por la desviación estándar. Para dimensiones mayores, lo mismo es cierto, excepto que la escala es más compleja e implica correlaciones.</p>
</div>
</div>
<div id="estudio-de-caso-más-de-tres-clases" class="section level2">
<h2><span class="header-section-number">31.8</span> Estudio de caso: más de tres clases</h2>
<p>Podemos generar un ejemplo con tres categorías así:</p>
<div class="sourceCode" id="cb1148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1148-1"><a href="ejemplos-de-algoritmos.html#cb1148-1"></a><span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()</span>
<span id="cb1148-2"><a href="ejemplos-de-algoritmos.html#cb1148-2"></a><span class="kw">set.seed</span>(<span class="dv">3456</span>)</span>
<span id="cb1148-3"><a href="ejemplos-de-algoritmos.html#cb1148-3"></a>index_<span class="dv">127</span> &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">which</span>(mnist<span class="op">$</span>train<span class="op">$</span>labels <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">7</span>)), <span class="dv">2000</span>)</span>
<span id="cb1148-4"><a href="ejemplos-de-algoritmos.html#cb1148-4"></a>y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels[index_<span class="dv">127</span>]</span>
<span id="cb1148-5"><a href="ejemplos-de-algoritmos.html#cb1148-5"></a>x &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>images[index_<span class="dv">127</span>,]</span>
<span id="cb1148-6"><a href="ejemplos-de-algoritmos.html#cb1148-6"></a>index_train &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">p=</span><span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1148-7"><a href="ejemplos-de-algoritmos.html#cb1148-7"></a><span class="co">## get the quadrants</span></span>
<span id="cb1148-8"><a href="ejemplos-de-algoritmos.html#cb1148-8"></a>row_column &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">row=</span><span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">28</span>)</span>
<span id="cb1148-9"><a href="ejemplos-de-algoritmos.html#cb1148-9"></a>upper_left_ind &lt;-<span class="st"> </span><span class="kw">which</span>(row_column<span class="op">$</span>col <span class="op">&lt;=</span><span class="st"> </span><span class="dv">14</span> <span class="op">&amp;</span><span class="st"> </span>row_column<span class="op">$</span>row <span class="op">&lt;=</span><span class="st"> </span><span class="dv">14</span>)</span>
<span id="cb1148-10"><a href="ejemplos-de-algoritmos.html#cb1148-10"></a>lower_right_ind &lt;-<span class="st"> </span><span class="kw">which</span>(row_column<span class="op">$</span>col <span class="op">&gt;</span><span class="st"> </span><span class="dv">14</span> <span class="op">&amp;</span><span class="st"> </span>row_column<span class="op">$</span>row <span class="op">&gt;</span><span class="st"> </span><span class="dv">14</span>)</span>
<span id="cb1148-11"><a href="ejemplos-de-algoritmos.html#cb1148-11"></a><span class="co">## binarize the values. Above 200 is ink, below is no ink</span></span>
<span id="cb1148-12"><a href="ejemplos-de-algoritmos.html#cb1148-12"></a>x &lt;-<span class="st"> </span>x <span class="op">&gt;</span><span class="st"> </span><span class="dv">200</span></span>
<span id="cb1148-13"><a href="ejemplos-de-algoritmos.html#cb1148-13"></a><span class="co">## proportion of pixels in lower right quadrant</span></span>
<span id="cb1148-14"><a href="ejemplos-de-algoritmos.html#cb1148-14"></a>x &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rowSums</span>(x[ ,upper_left_ind])<span class="op">/</span><span class="kw">rowSums</span>(x),</span>
<span id="cb1148-15"><a href="ejemplos-de-algoritmos.html#cb1148-15"></a>           <span class="kw">rowSums</span>(x[ ,lower_right_ind])<span class="op">/</span><span class="kw">rowSums</span>(x))</span>
<span id="cb1148-16"><a href="ejemplos-de-algoritmos.html#cb1148-16"></a><span class="co">##save data</span></span>
<span id="cb1148-17"><a href="ejemplos-de-algoritmos.html#cb1148-17"></a>train_set &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y[index_train]),</span>
<span id="cb1148-18"><a href="ejemplos-de-algoritmos.html#cb1148-18"></a>                        <span class="dt">x_1 =</span> x[index_train,<span class="dv">1</span>], <span class="dt">x_2 =</span> x[index_train,<span class="dv">2</span>])</span>
<span id="cb1148-19"><a href="ejemplos-de-algoritmos.html#cb1148-19"></a>test_set &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y[<span class="op">-</span>index_train]),</span>
<span id="cb1148-20"><a href="ejemplos-de-algoritmos.html#cb1148-20"></a>                       <span class="dt">x_1 =</span> x[<span class="op">-</span>index_train,<span class="dv">1</span>], <span class="dt">x_2 =</span> x[<span class="op">-</span>index_train,<span class="dv">2</span>])</span></code></pre></div>
<p>Aquí están los datos de entrenamiento:</p>
<div class="sourceCode" id="cb1149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1149-1"><a href="ejemplos-de-algoritmos.html#cb1149-1"></a>train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">color=</span>y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="libro_files/figure-html/mnist-27-training-data-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Podemos usar el paquete <strong>caret</strong> para entrenar el modelo QDA:</p>
<div class="sourceCode" id="cb1150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1150-1"><a href="ejemplos-de-algoritmos.html#cb1150-1"></a>train_qda &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;qda&quot;</span>, <span class="dt">data =</span> train_set)</span></code></pre></div>
<p>Ahora estimamos tres probabilidades condicionales (aunque tienen que sumar a 1):</p>
<div class="sourceCode" id="cb1151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1151-1"><a href="ejemplos-de-algoritmos.html#cb1151-1"></a><span class="kw">predict</span>(train_qda, test_set, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</span>
<span id="cb1151-2"><a href="ejemplos-de-algoritmos.html#cb1151-2"></a><span class="co">#&gt;        1       2       7</span></span>
<span id="cb1151-3"><a href="ejemplos-de-algoritmos.html#cb1151-3"></a><span class="co">#&gt; 1 0.7655 0.23043 0.00405</span></span>
<span id="cb1151-4"><a href="ejemplos-de-algoritmos.html#cb1151-4"></a><span class="co">#&gt; 2 0.2031 0.72514 0.07175</span></span>
<span id="cb1151-5"><a href="ejemplos-de-algoritmos.html#cb1151-5"></a><span class="co">#&gt; 3 0.5396 0.45909 0.00132</span></span>
<span id="cb1151-6"><a href="ejemplos-de-algoritmos.html#cb1151-6"></a><span class="co">#&gt; 4 0.0393 0.09419 0.86655</span></span>
<span id="cb1151-7"><a href="ejemplos-de-algoritmos.html#cb1151-7"></a><span class="co">#&gt; 5 0.9600 0.00936 0.03063</span></span>
<span id="cb1151-8"><a href="ejemplos-de-algoritmos.html#cb1151-8"></a><span class="co">#&gt; 6 0.9865 0.00724 0.00623</span></span></code></pre></div>
<p>Nuestras predicciones son una de las tres clases:</p>
<div class="sourceCode" id="cb1152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1152-1"><a href="ejemplos-de-algoritmos.html#cb1152-1"></a><span class="kw">predict</span>(train_qda, test_set) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()</span>
<span id="cb1152-2"><a href="ejemplos-de-algoritmos.html#cb1152-2"></a><span class="co">#&gt; [1] 1 2 1 7 1 1</span></span>
<span id="cb1152-3"><a href="ejemplos-de-algoritmos.html#cb1152-3"></a><span class="co">#&gt; Levels: 1 2 7</span></span></code></pre></div>
<p>La matriz de confusión es, por lo tanto, una tabla de 3 por 3:</p>
<div class="sourceCode" id="cb1153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1153-1"><a href="ejemplos-de-algoritmos.html#cb1153-1"></a><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_qda, test_set), test_set<span class="op">$</span>y)<span class="op">$</span>table</span>
<span id="cb1153-2"><a href="ejemplos-de-algoritmos.html#cb1153-2"></a><span class="co">#&gt;           Reference</span></span>
<span id="cb1153-3"><a href="ejemplos-de-algoritmos.html#cb1153-3"></a><span class="co">#&gt; Prediction   1   2   7</span></span>
<span id="cb1153-4"><a href="ejemplos-de-algoritmos.html#cb1153-4"></a><span class="co">#&gt;          1 111   9  11</span></span>
<span id="cb1153-5"><a href="ejemplos-de-algoritmos.html#cb1153-5"></a><span class="co">#&gt;          2  10  86  21</span></span>
<span id="cb1153-6"><a href="ejemplos-de-algoritmos.html#cb1153-6"></a><span class="co">#&gt;          7  21  28 102</span></span></code></pre></div>
<p>La exactitud es 0.749.</p>
<p>Tengan en cuenta que para la sensibilidad y especificidad, tenemos un par de valores para <strong>cada</strong> clase. Para definir estos términos, necesitamos un resultado binario. Por lo tanto, tenemos tres columnas: una para cada clase como positivos y las otras dos como negativas.</p>
<p>Para visualizar qué partes de la región se llaman 1, 2 y 7, ahora necesitamos tres colores:</p>
<p><img src="libro_files/figure-html/three-classes-plot-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>La exactitud para LDA,
0.629,
es mucho peor porque el modelo es más rígido. Aquí vemos como se ve la regla de decisión:</p>
<p><img src="libro_files/figure-html/lda-too-rigid-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Los resultados para kNN:</p>
<div class="sourceCode" id="cb1154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1154-1"><a href="ejemplos-de-algoritmos.html#cb1154-1"></a>train_knn &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">data =</span> train_set,</span>
<span id="cb1154-2"><a href="ejemplos-de-algoritmos.html#cb1154-2"></a>                   <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">15</span>, <span class="dv">51</span>, <span class="dv">2</span>)))</span></code></pre></div>
<p>son mucho mejores con una exactitud de
0.749. La regla de decisión se ve así:</p>
<p><img src="libro_files/figure-html/three-classes-knn-better-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Noten que una de las limitaciones de los modelos generativos mostrados aquí se debe a la falta de ajuste del supuesto normal, en particular para la clase 1.</p>
<div class="sourceCode" id="cb1155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1155-1"><a href="ejemplos-de-algoritmos.html#cb1155-1"></a>train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y)) <span class="op">%&gt;%</span></span>
<span id="cb1155-2"><a href="ejemplos-de-algoritmos.html#cb1155-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">fill =</span> y, <span class="dt">color=</span>y)) <span class="op">+</span></span>
<span id="cb1155-3"><a href="ejemplos-de-algoritmos.html#cb1155-3"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb1155-4"><a href="ejemplos-de-algoritmos.html#cb1155-4"></a><span class="st">  </span><span class="kw">stat_ellipse</span>(<span class="dt">type=</span><span class="st">&quot;norm&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/three-classes-lack-of-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Los modelos generativos pueden ser muy útiles, pero solo cuando somos capaces de aproximar con éxito la distribución de predictores condicionados en cada clase.</p>
</div>
<div id="ejercicios-53" class="section level2">
<h2><span class="header-section-number">31.9</span> Ejercicios</h2>
<p>Vamos a aplicar LDA y QDA al set de datos <code>tissue_gene_expression</code>. Comenzaremos con ejemplos sencillos basados en este set de datos y luego desarrollaremos un ejemplo realista.</p>
<p>1. Cree un set de datos con solo las clases “cerebellum” e “hippocampus” (dos partes del cerebro) y una matriz de predicción con 10 columnas seleccionadas al azar.</p>
<div class="sourceCode" id="cb1156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1156-1"><a href="ejemplos-de-algoritmos.html#cb1156-1"></a><span class="kw">set.seed</span>(<span class="dv">1993</span>)</span>
<span id="cb1156-2"><a href="ejemplos-de-algoritmos.html#cb1156-2"></a><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)</span>
<span id="cb1156-3"><a href="ejemplos-de-algoritmos.html#cb1156-3"></a>tissues &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;cerebellum&quot;</span>, <span class="st">&quot;hippocampus&quot;</span>)</span>
<span id="cb1156-4"><a href="ejemplos-de-algoritmos.html#cb1156-4"></a>ind &lt;-<span class="st"> </span><span class="kw">which</span>(tissue_gene_expression<span class="op">$</span>y <span class="op">%in%</span><span class="st"> </span>tissues)</span>
<span id="cb1156-5"><a href="ejemplos-de-algoritmos.html#cb1156-5"></a>y &lt;-<span class="st"> </span><span class="kw">droplevels</span>(tissue_gene_expression<span class="op">$</span>y[ind])</span>
<span id="cb1156-6"><a href="ejemplos-de-algoritmos.html#cb1156-6"></a>x &lt;-<span class="st"> </span>tissue_gene_expression<span class="op">$</span>x[ind, ]</span>
<span id="cb1156-7"><a href="ejemplos-de-algoritmos.html#cb1156-7"></a>x &lt;-<span class="st"> </span>x[, <span class="kw">sample</span>(<span class="kw">ncol</span>(x), <span class="dv">10</span>)]</span></code></pre></div>
<p>Utilice la función <code>train</code> para estimar la exactitud de LDA.</p>
<p>2. En este caso, LDA se ajusta a dos distribuciones normales de 10 dimensiones. Mire el modelo ajustado mirando el componente <code>finalModel</code> del resultado de <code>train</code>. Observe que hay un componente llamado <code>means</code> que incluye el estimado de los promedios de ambas distribuciones. Grafique este vector de promedios uno contra el otro y determine qué predictores (genes) parecen estar impulsando el algoritmo.</p>
<p>3. Repita el ejercicio 1 con QDA. ¿Tiene mejor exactitud que LDA?</p>
<p>4. ¿Los mismos predictores (genes) impulsan el algoritmo? Haga un gráfico como en el ejercicio 2.</p>
<p>5. Algo que vemos en el gráfico anterior es que el valor de los predictores se correlaciona en ambos grupos: algunos predictores son bajos en ambos grupos, mientras que otros son altos en ambos grupos. El valor medio de cada predictor, <code>colMeans(x)</code>, no es informativo ni útil para la predicción, y para fines de interpretación, a menudo es útil centrar o escalar cada columna. Esto se puede lograr con el argumento <code>preProcessing</code> en <code>train</code>. Vuelva a ejecutar LDA con <code>preProcessing = "scale"</code>. Tenga en cuenta que la exactitud no cambia, pero vea cómo es más fácil identificar los predictores que difieren más entre los grupos en el gráfico realizada en el ejercicio 4.</p>
<p>6. En los ejercicios anteriores, vimos que ambos enfoques funcionaron bien. Grafique los valores predictores para los dos genes con las mayores diferencias entre los dos grupos en un diagrama de dispersión para ver cómo parecen seguir una distribución normal de dos variables como se supone para los enfoques LDA y QDA. Coloree los puntos por el resultado.</p>
<p>7. Ahora vamos a aumentar un poco la complejidad del desafío: consideraremos todos los tipos de tejidos.</p>
<div class="sourceCode" id="cb1157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1157-1"><a href="ejemplos-de-algoritmos.html#cb1157-1"></a><span class="kw">set.seed</span>(<span class="dv">1993</span>)</span>
<span id="cb1157-2"><a href="ejemplos-de-algoritmos.html#cb1157-2"></a><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)</span>
<span id="cb1157-3"><a href="ejemplos-de-algoritmos.html#cb1157-3"></a>y &lt;-<span class="st"> </span>tissue_gene_expression<span class="op">$</span>y</span>
<span id="cb1157-4"><a href="ejemplos-de-algoritmos.html#cb1157-4"></a>x &lt;-<span class="st"> </span>tissue_gene_expression<span class="op">$</span>x</span>
<span id="cb1157-5"><a href="ejemplos-de-algoritmos.html#cb1157-5"></a>x &lt;-<span class="st"> </span>x[, <span class="kw">sample</span>(<span class="kw">ncol</span>(x), <span class="dv">10</span>)]</span></code></pre></div>
<p>¿Qué exactitud obtiene con LDA?</p>
<p>8. Vemos que los resultados son ligeramente peores. Utilice la función <code>confusionMatrix</code> para aprender qué tipo de errores estamos cometiendo.</p>
<p>9. Grafique una imagen de los centros de las siete distribuciones normales de 10 dimensiones.</p>

</div>
<div id="árboles-de-clasificación-y-regresión-cart" class="section level2">
<h2><span class="header-section-number">31.10</span> Árboles de clasificación y regresión (CART)</h2>
<div id="la-maldición-de-la-dimensionalidad" class="section level3">
<h3><span class="header-section-number">31.10.1</span> La maldición de la dimensionalidad</h3>
<p>Describimos cómo métodos como LDA y QDA no deben usarse con muchos predictores <span class="math inline">\(p\)</span> porque el número de parámetros que necesitamos estimar se vuelve demasiado grande. Por ejemplo, con el ejemplo de dígitos <span class="math inline">\(p=784\)</span>, tendríamos más de 600,000 parámetros con LDA, y lo multiplicaríamos por el número de clases para QDA. Los métodos de <em>kernel</em>, como kNN o regresión local, no tienen parámetros de modelo para estimar. Sin embargo, también se enfrentan a un desafío cuando se utilizan predictores múltiples debido a lo que se conoce como la <em>maldición de la dimensionalidad</em>. La <em>dimensión</em> aquí se refiere al hecho de que cuando tenemos <span class="math inline">\(p\)</span> predictores, la distancia entre dos observaciones se calcula en un espacio <span class="math inline">\(p\)</span>-dimensional.</p>
<p>Una forma útil de entender la maldición de la dimensionalidad es considerar cuán grande tenemos que hacer un <em>span</em>/vecindario/ventana para incluir un porcentaje dado de los datos. Recuerden que con vecindarios más grandes, nuestros métodos pierden flexibilidad.</p>
<p>Por ejemplo, supongamos que tenemos un predictor continuo con puntos igualmente espaciados en el intervalo [0,1] y queremos crear ventanas que incluyan 1/10 de datos. Entonces es fácil ver que nuestras ventanas tienen que ser de tamaño 0.1:</p>
<p><img src="libro_files/figure-html/curse-of-dim-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Ahora, para dos predictores, si decidimos mantener el vecindario igual de pequeño, 10% para cada dimensión, incluimos solo 1 punto. Si queremos incluir el 10% de los datos, entonces necesitamos aumentar el tamaño de cada lado del cuadrado a <span class="math inline">\(\sqrt{.10} \approx .316\)</span>:</p>
<p><img src="libro_files/figure-html/curse-of-dim-2-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Usando la misma lógica, si queremos incluir el 10% de los datos en un espacio tridimensional, entonces el lado de cada cubo es <span class="math inline">\(\sqrt[3]{.10} \approx 0.464\)</span>.
En general, para incluir el 10% de los datos en un caso con <span class="math inline">\(p\)</span> dimensiones, necesitamos un intervalo con cada lado del tamaño <span class="math inline">\(\sqrt[p]{.10}\)</span> del total. Esta proporción se acerca a 1 rápidamente, y si la proporción es 1, significa que incluimos todos los datos y ya no estamos suavizando.</p>
<div class="sourceCode" id="cb1158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1158-1"><a href="ejemplos-de-algoritmos.html#cb1158-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1158-2"><a href="ejemplos-de-algoritmos.html#cb1158-2"></a>p &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span></span>
<span id="cb1158-3"><a href="ejemplos-de-algoritmos.html#cb1158-3"></a><span class="kw">qplot</span>(p, <span class="fl">.1</span><span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>p), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span></code></pre></div>
<p><img src="libro_files/figure-html/curse-of-dim-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Cuando llegamos a 100 predictores, el vecindario ya no es muy local, puesto que cada lado cubre casi todo el set de datos.</p>
<p>Aquí observamos un conjunto de métodos elegantes y versátiles que se adaptan a dimensiones más altas y también permiten que estas regiones tomen formas más complejas mientras producen modelos que son interpretables. Estos son métodos muy populares, conocidos y estudiados. Nos concentraremos en los árboles de regresión y decisión y su extensión a bosques aleatorios.</p>
</div>
<div id="motivación-cart" class="section level3">
<h3><span class="header-section-number">31.10.2</span> Motivación CART</h3>
<p>Para motivar esta sección, utilizaremos un nuevo set de datos
que incluye el desglose de la composición del aceite de oliva en 8 ácidos grasos:</p>
<div class="sourceCode" id="cb1159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1159-1"><a href="ejemplos-de-algoritmos.html#cb1159-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1159-2"><a href="ejemplos-de-algoritmos.html#cb1159-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1159-3"><a href="ejemplos-de-algoritmos.html#cb1159-3"></a><span class="kw">data</span>(<span class="st">&quot;olive&quot;</span>)</span>
<span id="cb1159-4"><a href="ejemplos-de-algoritmos.html#cb1159-4"></a><span class="kw">names</span>(olive)</span>
<span id="cb1159-5"><a href="ejemplos-de-algoritmos.html#cb1159-5"></a><span class="co">#&gt;  [1] &quot;region&quot;      &quot;area&quot;        &quot;palmitic&quot;    &quot;palmitoleic&quot;</span></span>
<span id="cb1159-6"><a href="ejemplos-de-algoritmos.html#cb1159-6"></a><span class="co">#&gt;  [5] &quot;stearic&quot;     &quot;oleic&quot;       &quot;linoleic&quot;    &quot;linolenic&quot;  </span></span>
<span id="cb1159-7"><a href="ejemplos-de-algoritmos.html#cb1159-7"></a><span class="co">#&gt;  [9] &quot;arachidic&quot;   &quot;eicosenoic&quot;</span></span></code></pre></div>
<p>Con fines ilustrativos, intentaremos predecir la región utilizando los valores de composición de ácidos grasos como predictores.</p>
<div class="sourceCode" id="cb1160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1160-1"><a href="ejemplos-de-algoritmos.html#cb1160-1"></a><span class="kw">table</span>(olive<span class="op">$</span>region)</span>
<span id="cb1160-2"><a href="ejemplos-de-algoritmos.html#cb1160-2"></a><span class="co">#&gt; </span></span>
<span id="cb1160-3"><a href="ejemplos-de-algoritmos.html#cb1160-3"></a><span class="co">#&gt; Northern Italy       Sardinia Southern Italy </span></span>
<span id="cb1160-4"><a href="ejemplos-de-algoritmos.html#cb1160-4"></a><span class="co">#&gt;            151             98            323</span></span></code></pre></div>
<p>Quitamos la columna <code>area</code> porque no la usaremos como predictor.</p>
<div class="sourceCode" id="cb1161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1161-1"><a href="ejemplos-de-algoritmos.html#cb1161-1"></a>olive &lt;-<span class="st"> </span><span class="kw">select</span>(olive, <span class="op">-</span>area)</span></code></pre></div>
<p>Intentemos rápidamente predecir la región usando kNN:</p>
<div class="sourceCode" id="cb1162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1162-1"><a href="ejemplos-de-algoritmos.html#cb1162-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1162-2"><a href="ejemplos-de-algoritmos.html#cb1162-2"></a>fit &lt;-<span class="st"> </span><span class="kw">train</span>(region <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb1162-3"><a href="ejemplos-de-algoritmos.html#cb1162-3"></a>             <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">2</span>)),</span>
<span id="cb1162-4"><a href="ejemplos-de-algoritmos.html#cb1162-4"></a>             <span class="dt">data =</span> olive)</span>
<span id="cb1162-5"><a href="ejemplos-de-algoritmos.html#cb1162-5"></a><span class="kw">ggplot</span>(fit)</span></code></pre></div>
<p><img src="libro_files/figure-html/olive-knn-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que usando solo un vecino, podemos predecir relativamente bien. Sin embargo, un poco de exploración de datos revela que deberíamos poder hacerlo aún mejor. Por ejemplo, si observamos la distribución de cada predictor estratificado por región, vemos que el <em>eicosenoic</em> solo está presente en el sur de Italia y que el <em>linoleic</em> separa el norte de Italia de Cerdeña.</p>
<div class="sourceCode" id="cb1163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1163-1"><a href="ejemplos-de-algoritmos.html#cb1163-1"></a>olive <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(fatty_acid, percentage, <span class="op">-</span>region) <span class="op">%&gt;%</span></span>
<span id="cb1163-2"><a href="ejemplos-de-algoritmos.html#cb1163-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(region, percentage, <span class="dt">fill =</span> region)) <span class="op">+</span></span>
<span id="cb1163-3"><a href="ejemplos-de-algoritmos.html#cb1163-3"></a><span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span></span>
<span id="cb1163-4"><a href="ejemplos-de-algoritmos.html#cb1163-4"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>fatty_acid, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>, <span class="dt">ncol =</span> <span class="dv">4</span>) <span class="op">+</span></span>
<span id="cb1163-5"><a href="ejemplos-de-algoritmos.html#cb1163-5"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_blank</span>(), <span class="dt">legend.position=</span><span class="st">&quot;bottom&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/olive-eda-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>¡Esto implica que deberíamos ser capaces de construir un algoritmo que prediga perfectamente! Podemos ver esto claramente al graficar los valores para <em>eicosenoic</em> y <em>linoleic</em>.</p>
<div class="sourceCode" id="cb1164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1164-1"><a href="ejemplos-de-algoritmos.html#cb1164-1"></a>olive <span class="op">%&gt;%</span></span>
<span id="cb1164-2"><a href="ejemplos-de-algoritmos.html#cb1164-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(eicosenoic, linoleic, <span class="dt">color =</span> region)) <span class="op">+</span></span>
<span id="cb1164-3"><a href="ejemplos-de-algoritmos.html#cb1164-3"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb1164-4"><a href="ejemplos-de-algoritmos.html#cb1164-4"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.065</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span></span>
<span id="cb1164-5"><a href="ejemplos-de-algoritmos.html#cb1164-5"></a><span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="fl">-0.2</span>, <span class="dt">y =</span> <span class="fl">10.54</span>, <span class="dt">xend =</span> <span class="fl">0.065</span>, <span class="dt">yend =</span> <span class="fl">10.54</span>,</span>
<span id="cb1164-6"><a href="ejemplos-de-algoritmos.html#cb1164-6"></a>               <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/olive-two-predictors-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>En la Sección <a href="grandes-sets-de-datos.html#predictor-space">33.3.4</a>, definimos espacios predictores. El espacio predictor aquí consiste en puntos de ocho dimensiones con valores entre 0 y 100. En el gráfico anterior, mostramos el espacio definido por los dos predictores <em>eicosenoic</em> y <em>linoleic</em> y, a simple vista,
podemos construir una regla de predicción que divida el espacio del predictor para que cada partición contenga solo resultados de una categoría. Esto a su vez se puede utilizar para definir un algoritmo con una precisión perfecta. Específicamente, definimos la siguiente regla de decisión. Si el <em>eicosenoic</em> es mayor que 0.065, predecimos el sur de Italia. Si no, entonces si <em>linoleic</em> es más grande que <span class="math inline">\(10.535\)</span>, predecimos Cerdeña, y si es más bajo, predecimos el norte de Italia. Podemos dibujar este árbol de decisión así:</p>
<p><img src="libro_files/figure-html/olive-tree-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Los árboles de decisión como este se usan a menudo en la práctica. Por ejemplo, para determinar el riesgo de una persona de tener un mal resultado después de un ataque cardíaco, los médicos usan lo siguiente:</p>
<p><img src="ml/img/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png" width="50%" style="display: block; margin: auto;" /></p>
<p>(Fuente: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184<a href="#fn111" class="footnote-ref" id="fnref111"><sup>111</sup></a>.)</p>
<p>Un árbol es básicamente un diagrama de flujo con preguntas de sí o no. La idea general de los métodos que estamos describiendo es definir un algoritmo que use datos para crear estos árboles con predicciones en los extremos, conocidos como <em>nodos</em> (<em>nodes</em> en inglés). Los árboles de regresión y de decisión operan prediciendo una variable de resultado <span class="math inline">\(Y\)</span> al dividir los predictores.</p>
</div>
<div id="árboles-de-regresión" class="section level3">
<h3><span class="header-section-number">31.10.3</span> Árboles de regresión</h3>
<p>Cuando el resultado es continuo, llamamos al método un árbol de <em>regresión</em>. Para introducir árboles de regresión, utilizaremos los datos de la encuesta de 2008 que usamos en secciones anteriores para describir la idea básica de cómo construimos estos algoritmos. Al igual que con otros algoritmos de <em>machine learning</em>, intentaremos estimar la expectativa condicional <span class="math inline">\(f(x) = \mbox{E}(Y | X = x)\)</span> con <span class="math inline">\(Y\)</span> el margen de la encuesta y <span class="math inline">\(x\)</span> el dia.</p>
<div class="sourceCode" id="cb1165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1165-1"><a href="ejemplos-de-algoritmos.html#cb1165-1"></a><span class="kw">data</span>(<span class="st">&quot;polls_2008&quot;</span>)</span>
<span id="cb1165-2"><a href="ejemplos-de-algoritmos.html#cb1165-2"></a><span class="kw">qplot</span>(day, margin, <span class="dt">data =</span> polls_<span class="dv">2008</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-again-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>La idea general aquí es construir un árbol de decisión y, al final de cada <em>nodo</em>, obtener un predictor <span class="math inline">\(\hat{y}\)</span>. Una forma matemática de describir esto es decir que estamos dividiendo el espacio predictivo en <span class="math inline">\(J\)</span> regiones no superpuestas, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>, y luego para cualquier predictor <span class="math inline">\(x\)</span> que caiga dentro de la región <span class="math inline">\(R_j\)</span>, estimar <span class="math inline">\(f(x)\)</span> con el promedio de las observaciones de entrenamiento <span class="math inline">\(y_i\)</span> para el cual el predictor asociado <span class="math inline">\(x_i\)</span> también está en <span class="math inline">\(R_j\)</span>.</p>
<p>¿Pero cómo decidimos la partición <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> y como elegimos <span class="math inline">\(J\)</span>? Aquí es donde el algoritmo se vuelve un poco complicado.</p>
<p>Los árboles de regresión crean particiones de manera recursiva. Comenzamos el algoritmo con una partición, el espacio predictor completo. En nuestro primer ejemplo sencillo, este espacio es el intervalo [-155, 1]. Pero después del primer paso, tendremos dos particiones. Después del segundo paso, dividiremos una de estas particiones en dos y tendremos tres particiones, luego cuatro, entonces cinco, y así sucesivamente. Describimos cómo elegimos la partición para una partición adicional, y cuándo parar, más adelante.</p>
<p>Después de seleccionar una partición <span class="math inline">\(\mathbf{x}\)</span> para dividir a fin de crear las nuevas particiones, encontramos un predictor <span class="math inline">\(j\)</span> y un valor <span class="math inline">\(s\)</span> que definen dos nuevas particiones, que llamaremos <span class="math inline">\(R_1(j,s)\)</span> y <span class="math inline">\(R_2(j,s)\)</span> y que dividen nuestras observaciones en la partición actual al preguntar si <span class="math inline">\(x_j\)</span> es mayor que <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{ and } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span></p>
<p>En nuestro ejemplo actual, solo tenemos un predictor, por lo que siempre elegiremos <span class="math inline">\(j=1\)</span>, pero en general este no será el caso. Ahora, después de definir las nuevas particiones <span class="math inline">\(R_1\)</span> y <span class="math inline">\(R_2\)</span> y parar el proceso de particionar, calculamos predictores tomando el promedio de todas las observaciones <span class="math inline">\(y\)</span> para el cual el <span class="math inline">\(\mathbf{x}\)</span> asociado está en <span class="math inline">\(R_1\)</span> y <span class="math inline">\(R_2\)</span>. Nos referimos a estos dos como <span class="math inline">\(\hat{y}_{R_1}\)</span> y <span class="math inline">\(\hat{y}_{R_2}\)</span> respectivamente.</p>
<p>¿Pero cómo elegimos <span class="math inline">\(j\)</span> y <span class="math inline">\(s\)</span>? Básicamente, encontramos el par que minimiza la suma de errores cuadrados (<em>residual sum of squares</em> o RSS por sus siglas en inglés):
<span class="math display">\[
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span></p>
<p>Esto se aplica de manera recursiva a las nuevas regiones <span class="math inline">\(R_1\)</span> y <span class="math inline">\(R_2\)</span>. Describimos cómo paramos más tarde, pero una vez que terminemos de dividir el espacio del predictor en regiones, en cada región se realiza una predicción utilizando las observaciones en esa región.</p>
<p>Echemos un vistazo a lo que hace este algoritmo en los datos de la encuesta de las elecciones presidenciales de 2008. Utilizaremos la funcion <code>rpart</code> del paquete <strong>rpart</strong>.</p>
<div class="sourceCode" id="cb1166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1166-1"><a href="ejemplos-de-algoritmos.html#cb1166-1"></a><span class="kw">library</span>(rpart)</span>
<span id="cb1166-2"><a href="ejemplos-de-algoritmos.html#cb1166-2"></a>fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>)</span></code></pre></div>
<p>Aquí, solo hay un predictor y, por lo tanto, no tenemos que decidir cuál dividir. Simplemente tenemos que decidir qué valor <span class="math inline">\(s\)</span> utilizaremos para dividir. Podemos ver visualmente dónde se hicieron las divisiones:</p>
<div class="sourceCode" id="cb1167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1167-1"><a href="ejemplos-de-algoritmos.html#cb1167-1"></a><span class="kw">plot</span>(fit, <span class="dt">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb1167-2"><a href="ejemplos-de-algoritmos.html#cb1167-2"></a><span class="kw">text</span>(fit, <span class="dt">cex =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-tree-1.png" width="60%" style="display: block; margin: auto;" /></p>
<p>La primera división se realiza el día 39.5. Una de esas regiones se divide en el día 86.5. Las dos nuevas particiones que resultan se dividen en los días 49.5 y 117.5, respectivamente, y así sucesivamente. Terminamos con 8 particiones. El estimado final <span class="math inline">\(\hat{f}(x)\)</span> se ve así:</p>
<div class="sourceCode" id="cb1168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1168-1"><a href="ejemplos-de-algoritmos.html#cb1168-1"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span></span>
<span id="cb1168-2"><a href="ejemplos-de-algoritmos.html#cb1168-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span></span>
<span id="cb1168-3"><a href="ejemplos-de-algoritmos.html#cb1168-3"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb1168-4"><a href="ejemplos-de-algoritmos.html#cb1168-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb1168-5"><a href="ejemplos-de-algoritmos.html#cb1168-5"></a><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-tree-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Observen que el algoritmo paró a las 8 particiones. Ahora explicamos cómo se toma esa decisión.</p>
<p>Primero necesitamos definir el término <em>parámetro de complejidad</em> (<em>complexity parameter</em> o CP por sus siglas en inglés). Cada vez que dividimos y definimos dos nuevas particiones, nuestro set de entrenamiento RSS disminuye. Esto se debe a que con más particiones, nuestro modelo tiene más flexibilidad para adaptarse a los datos de entrenamiento. De hecho, si se divide hasta que cada punto sea su propia partición, entonces RSS baja hasta 0 ya que el promedio de un valor es el mismo valor. Para evitar esto, el algoritmo establece un mínimo de cuánto debe mejorar el RSS para que se agregue otra partición. Este parámetro se conoce como <em>parámetro de complejidad</em>. El RSS debe mejorar por un factor de CP para que se agregue la nueva partición. Por lo tanto, los valores grandes de CP obligarán al algoritmo a detenerse antes, lo que resulta en menos nodos.</p>
<p>Sin embargo, CP no es el único parámetro utilizado para decidir si debemos dividir una partición existente. Otro parámetro común es el número mínimo de observaciones requeridas en una partición antes de dividirla más. El argumento que se usa en la función <code>rpart</code> es <code>minsplit</code> y el valor predeterminado es 20. La implementación <code>rpart</code> de árboles de regresión también permite a los usuarios determinar un número mínimo de observaciones en cada nodo. El argumento es <code>minbucket</code> y por defecto usa el valor <code>round(minsplit/3)</code>.</p>
<p>Como se esperaba, si establecemos <code>cp = 0</code> y <code>minsplit = 2</code>, nuestra predicción es lo más flexible posible y nuestros predictores son nuestros datos originales:</p>
<div class="sourceCode" id="cb1169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1169-1"><a href="ejemplos-de-algoritmos.html#cb1169-1"></a>fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>,</span>
<span id="cb1169-2"><a href="ejemplos-de-algoritmos.html#cb1169-2"></a>             <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="dv">0</span>, <span class="dt">minsplit =</span> <span class="dv">2</span>))</span>
<span id="cb1169-3"><a href="ejemplos-de-algoritmos.html#cb1169-3"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span></span>
<span id="cb1169-4"><a href="ejemplos-de-algoritmos.html#cb1169-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span></span>
<span id="cb1169-5"><a href="ejemplos-de-algoritmos.html#cb1169-5"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb1169-6"><a href="ejemplos-de-algoritmos.html#cb1169-6"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb1169-7"><a href="ejemplos-de-algoritmos.html#cb1169-7"></a><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-tree-over-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Intuitivamente, sabemos que este no es un buen enfoque, ya que generalmente dará como resultado un entrenamiento excesivo. Estos tres parámetros, <code>cp</code>, <code>minsplit</code> y <code>minbucket</code>, se pueden usar para controlar la variabilidad de los predictores finales. Entre más grandes sean estos valores, más datos se promedian para calcular un predictor y, por lo tanto, reducir la variabilidad. El inconveniente es que restringe la flexibilidad.</p>
<p>Entonces, ¿cómo elegimos estos parámetros? Podemos usar validación cruzada, descrita en el Capítulo <a href="cross-validation.html#cross-validation">29</a>, como con cualquier parámetro de ajuste. Aquí tenemos un ejemplo del uso de validación cruzada para elegir CP:</p>
<div class="sourceCode" id="cb1170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1170-1"><a href="ejemplos-de-algoritmos.html#cb1170-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1170-2"><a href="ejemplos-de-algoritmos.html#cb1170-2"></a>train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(margin <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb1170-3"><a href="ejemplos-de-algoritmos.html#cb1170-3"></a>                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb1170-4"><a href="ejemplos-de-algoritmos.html#cb1170-4"></a>                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="fl">0.05</span>, <span class="dt">len =</span> <span class="dv">25</span>)),</span>
<span id="cb1170-5"><a href="ejemplos-de-algoritmos.html#cb1170-5"></a>                     <span class="dt">data =</span> polls_<span class="dv">2008</span>)</span>
<span id="cb1170-6"><a href="ejemplos-de-algoritmos.html#cb1170-6"></a><span class="kw">ggplot</span>(train_rpart)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-tree-train-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Para ver el árbol que resulta, accedemos <code>finalModel</code> y lo graficamos:</p>
<div class="sourceCode" id="cb1171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1171-1"><a href="ejemplos-de-algoritmos.html#cb1171-1"></a><span class="kw">plot</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb1171-2"><a href="ejemplos-de-algoritmos.html#cb1171-2"></a><span class="kw">text</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">cex =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-final-model-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Y debido a que solo tenemos un predictor, podemos graficar <span class="math inline">\(\hat{f}(x)\)</span>:</p>
<div class="sourceCode" id="cb1172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1172-1"><a href="ejemplos-de-algoritmos.html#cb1172-1"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span></span>
<span id="cb1172-2"><a href="ejemplos-de-algoritmos.html#cb1172-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(train_rpart)) <span class="op">%&gt;%</span></span>
<span id="cb1172-3"><a href="ejemplos-de-algoritmos.html#cb1172-3"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb1172-4"><a href="ejemplos-de-algoritmos.html#cb1172-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb1172-5"><a href="ejemplos-de-algoritmos.html#cb1172-5"></a><span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-final-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Tengan en cuenta que si ya tenemos un árbol y queremos aplicar un valor de CP más alto, podemos usar la función <code>prune</code>. Llamamos a esto <em>podar</em> (<em>pruning</em> en inglés) un árbol porque estamos cortando particiones que no cumplen con un criterio <code>cp</code>. Anteriormente creamos un árbol que usaba un <code>cp = 0</code> y lo guardamos en <code>fit</code>. Podemos podarlo así:</p>
<div class="sourceCode" id="cb1173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1173-1"><a href="ejemplos-de-algoritmos.html#cb1173-1"></a>pruned_fit &lt;-<span class="st"> </span><span class="kw">prune</span>(fit, <span class="dt">cp =</span> <span class="fl">0.01</span>)</span></code></pre></div>
</div>
<div id="árboles-de-clasificación-decisión" class="section level3">
<h3><span class="header-section-number">31.10.4</span> Árboles de clasificación (decisión)</h3>
<p>Los árboles de clasificación, o árboles de decisión, se usan en problemas de predicción donde el resultado es categórico. Utilizamos el mismo principio de partición con algunas diferencias para tomar en cuenta el hecho de que ahora estamos trabajando con un resultado categórico.</p>
<p>La primera diferencia es que formamos predicciones calculando qué clase es la más común entre las observaciones del set de entrenamiento dentro de la partición, en lugar de tomar el promedio en cada partición (ya que no podemos tomar el promedio de las categorías).</p>
<p>La segunda es que ya no podemos usar RSS para elegir la partición. Si bien podríamos utilizar el enfoque simplista de buscar particiones que minimicen el error de entrenamiento, los enfoques de mejor desempeño utilizan métricas más sofisticadas. Dos de los más populares son el <em>índice de Gini</em> (<em>Gini Index</em> en inglés) y <em>entropia</em> (<em>entropy</em> en inglés).</p>
<p>En una situación perfecta, los resultados en cada una de nuestras particiones son todos de la misma categoría, ya que esto permitirá una exactitud perfecta. El <em>índice de Gini</em> será 0 en este caso, y se hará más grande a medida que nos desviamos de este escenario. Para definir el índice de Gini, definimos <span class="math inline">\(\hat{p}_{j,k}\)</span> como la proporción de observaciones en partición <span class="math inline">\(j\)</span> que son de clase <span class="math inline">\(k\)</span>. Especificamente el índice de Gini se define como:</p>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p>Si estudian la fórmula cuidadosamente, verán que, de hecho, es 0 en la situación perfecta descrita anteriormente.</p>
<p><em>Entropia</em> es una cantidad muy similar, definida como:</p>
<p><span class="math display">\[
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
\]</span></p>
<p>Veamos cómo funciona un árbol de clasificación en el ejemplo de dígitos que examinamos antes:</p>
<p>Podemos usar este código para ejecutar el algoritmo y graficar el árbol resultante:</p>
<div class="sourceCode" id="cb1174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1174-1"><a href="ejemplos-de-algoritmos.html#cb1174-1"></a>train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb1174-2"><a href="ejemplos-de-algoritmos.html#cb1174-2"></a>                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb1174-3"><a href="ejemplos-de-algoritmos.html#cb1174-3"></a>                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="dt">len =</span> <span class="dv">25</span>)),</span>
<span id="cb1174-4"><a href="ejemplos-de-algoritmos.html#cb1174-4"></a>                     <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)</span>
<span id="cb1174-5"><a href="ejemplos-de-algoritmos.html#cb1174-5"></a><span class="kw">plot</span>(train_rpart)</span></code></pre></div>
<p><img src="libro_files/figure-html/mnist-27-tree-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>La exactitud que logramos con este enfoque es mejor que la que obtuvimos con la regresión, pero no es tan buena como la que obtuvimos con los métodos <em>kernel</em>:</p>
<div class="sourceCode" id="cb1175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1175-1"><a href="ejemplos-de-algoritmos.html#cb1175-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(train_rpart, mnist_<span class="dv">27</span><span class="op">$</span>test)</span>
<span id="cb1175-2"><a href="ejemplos-de-algoritmos.html#cb1175-2"></a><span class="kw">confusionMatrix</span>(y_hat, mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1175-3"><a href="ejemplos-de-algoritmos.html#cb1175-3"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1175-4"><a href="ejemplos-de-algoritmos.html#cb1175-4"></a><span class="co">#&gt;     0.82</span></span></code></pre></div>
<p>El gráfico de la probabilidad condicional estimada nos muestra las limitaciones de los árboles de clasificación:</p>
<p><img src="libro_files/figure-html/rf-cond-prob-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Tengan en cuenta que con los árboles de decisión, es difícil suavizar los límites ya que cada partición crea una discontinuidad.</p>
<p>Los árboles de clasificación tienen ciertas ventajas que los hacen muy útiles. Son altamente interpretables, incluso más que los modelos lineales. Son fáciles de visualizar (si son lo suficientemente pequeños). Finalmente, pueden modelar procesos de decisión humana y no requieren el uso de predictores ficticios para variables categóricas. Por otro lado, si usamos particiones recursivas es muy posible que sobreentrenemos y, por lo tanto, es un poco más difícil de entrenar que, por ejemplo, la regresión lineal o kNN. Además, en términos de exactitud, rara vez es el método de mejor rendimiento ya que no es muy flexible y es muy inestable a los cambios en los datos de entrenamiento. Los bosques aleatorios, explicados a continuación, mejoran varias de estas deficiencias.</p>
</div>
</div>
<div id="bosques-aleatorios" class="section level2">
<h2><span class="header-section-number">31.11</span> Bosques aleatorios</h2>
<p>Los bosques aleatorios son un enfoque de <em>machine learning</em> <strong>muy popular</strong> que abordan las deficiencias de los árboles de decisión utilizando una idea inteligente. El objetivo es mejorar la predicción y reducir la inestabilidad mediante <em>el promedio</em> de múltiples árboles de decisión (un bosque de árboles construido con aleatoriedad). Tienen dos atributos que ayudan a lograr esto.</p>
<p>El primer paso es <em>bootstrap aggregation</em> o <em>bagging</em>. La idea general es generar muchos predictores, cada uno utilizando árboles de regresión o de clasificación, y luego formar una predicción final basada en la predicción promedio de todos estos árboles. Para asegurar que los árboles individuales no sean iguales, utilizamos el <em>bootstrap</em> para inducir aleatoriedad. Estos dos atributos combinados explican el nombre: el <em>bootstrap</em> hace que los árboles individuales sean <strong>aleatorios</strong> y la combinación de árboles es el <strong>bosque</strong>. Los pasos específicos son los siguientes.</p>
<p>1. Construyan <span class="math inline">\(B\)</span> árboles de decisión utilizando el set de entrenamiento. Nos referimos a los modelos ajustados como <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. Luego, explicamos cómo nos aseguramos de que sean diferentes.</p>
<p>2. Para cada observación en el set de evaluación, formen una predicción <span class="math inline">\(\hat{y}_j\)</span> usando el árbol <span class="math inline">\(T_j\)</span>.</p>
<p>3. Para resultados continuos, formen una predicción final con el promedio <span class="math inline">\(\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j\)</span>. Para la clasificación de datos categóricos, predigan <span class="math inline">\(\hat{y}\)</span> con voto mayoritario (clase más frecuente entre <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p>
<p>Entonces, ¿cómo obtenemos diferentes árboles de decisión de un solo set de entrenamiento? Para esto, usamos la aleatoriedad en dos maneras que explicamos en los pasos a continuación. Dejen que <span class="math inline">\(N\)</span> sea el número de observaciones en el set de entrenamiento. Para crear <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> del set de entrenamiento, hagan lo siguiente:</p>
<p>1. Creen un set de entrenamiento de <em>bootstrap</em> al mostrar <span class="math inline">\(N\)</span> observaciones del set de entrenamiento <strong>con reemplazo</strong>. Esta es la primera forma de inducir aleatoriedad.</p>
<p>2. Una gran cantidad de atributos es típico en los desafíos de <em>machine learning</em>. A menudo, muchos atributos pueden ser informativos, pero incluirlos todos en el modelo puede resultar en un sobreajuste. La segunda forma en que los bosques aleatorios inducen aleatoriedad es seleccionando al azar los atributos que se incluirán en la construcción de cada árbol. Se selecciona un subconjunto aleatorio diferente para cada árbol. Esto reduce la correlación entre los árboles en el bosque, mejorando así la exactitud de la predicción.</p>
<p>Para ilustrar cómo los primeros pasos pueden dar como resultado estimados más uniformes, demostraremos ajustando un bosque aleatorio a los datos de las encuestas de 2008. Utilizaremos la función <code>randomForest</code> en el paquete <strong>randomForest</strong>:</p>
<div class="sourceCode" id="cb1176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1176-1"><a href="ejemplos-de-algoritmos.html#cb1176-1"></a><span class="kw">library</span>(randomForest)</span>
<span id="cb1176-2"><a href="ejemplos-de-algoritmos.html#cb1176-2"></a>fit &lt;-<span class="st"> </span><span class="kw">randomForest</span>(margin<span class="op">~</span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>)</span></code></pre></div>
<p>Noten que si aplicamos la función <code>plot</code> al objeto resultante, almacenado en <code>fit</code>, vemos cómo cambia la tasa de error de nuestro algoritmo a medida que agregamos árboles.</p>
<div class="sourceCode" id="cb1177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1177-1"><a href="ejemplos-de-algoritmos.html#cb1177-1"></a>rafalib<span class="op">::</span><span class="kw">mypar</span>()</span>
<span id="cb1177-2"><a href="ejemplos-de-algoritmos.html#cb1177-2"></a><span class="kw">plot</span>(fit)</span></code></pre></div>
<p><img src="libro_files/figure-html/more-trees-better-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Podemos ver que en este caso, la exactitud mejora a medida que agregamos más árboles hasta unos 30 árboles donde la exactitud se estabiliza.</p>
<p>El estimado resultante para este bosque aleatorio puede verse así:</p>
<div class="sourceCode" id="cb1178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1178-1"><a href="ejemplos-de-algoritmos.html#cb1178-1"></a>polls_<span class="dv">2008</span> <span class="op">%&gt;%</span></span>
<span id="cb1178-2"><a href="ejemplos-de-algoritmos.html#cb1178-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit, <span class="dt">newdata =</span> polls_<span class="dv">2008</span>)) <span class="op">%&gt;%</span></span>
<span id="cb1178-3"><a href="ejemplos-de-algoritmos.html#cb1178-3"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb1178-4"><a href="ejemplos-de-algoritmos.html#cb1178-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span></span>
<span id="cb1178-5"><a href="ejemplos-de-algoritmos.html#cb1178-5"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/polls-2008-rf-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Observen que el estimado del bosque aleatorio es mucho más uniforme que lo que logramos con el árbol de regresión en la sección anterior. Esto es posible porque el promedio de muchas funciones de escalón puede ser suave. Podemos ver esto examinando visualmente cómo cambia el estimado a medida que agregamos más árboles. En el siguiente gráfico, pueden ver cada una de las muestras de <em>bootstrap</em> para varios valores de <span class="math inline">\(b\)</span> y para cada una vemos el árbol que se ajusta en gris, los árboles anteriores que se ajustaron en gris más claro y el resultado de tomar el promedio de todos los árboles estimados hasta ese punto.</p>
<p><img src="ml/img/rf.gif" width="100%" style="display: block; margin: auto;" /></p>
<p>Aquí está el ajuste del bosque aleotorio para nuestro ejemplo de dígitos basado en dos predictores:</p>
<div class="sourceCode" id="cb1179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1179-1"><a href="ejemplos-de-algoritmos.html#cb1179-1"></a><span class="kw">library</span>(randomForest)</span>
<span id="cb1179-2"><a href="ejemplos-de-algoritmos.html#cb1179-2"></a>train_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train)</span>
<span id="cb1179-3"><a href="ejemplos-de-algoritmos.html#cb1179-3"></a></span>
<span id="cb1179-4"><a href="ejemplos-de-algoritmos.html#cb1179-4"></a><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf, mnist_<span class="dv">27</span><span class="op">$</span>test),</span>
<span id="cb1179-5"><a href="ejemplos-de-algoritmos.html#cb1179-5"></a>                mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1179-6"><a href="ejemplos-de-algoritmos.html#cb1179-6"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1179-7"><a href="ejemplos-de-algoritmos.html#cb1179-7"></a><span class="co">#&gt;     0.79</span></span></code></pre></div>
<p>Así es como se ven las probabilidades condicionales:</p>
<p><img src="libro_files/figure-html/cond-prob-rf-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>La visualización del estimado muestra que, aunque obtenemos una alta exactitud, parece que podemos mejorar al hacer que el estimado sea más uniforme. Esto podría lograrse cambiando el parámetro que controla el número mínimo de puntos de datos en los nodos del árbol. Mientras más grande sea este mínimo, más suave será el estimado final. Podemos entrenar los parámetros del bosque aleatorio. A continuación, utilizamos el paquete <strong>caret</strong> para optimizar el tamaño mínimo del nodo. Debido a que este no es uno de los parámetros que el paquete <strong>caret</strong> optimiza por defecto, escribiremos nuestro propio código:</p>
<div class="sourceCode" id="cb1180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1180-1"><a href="ejemplos-de-algoritmos.html#cb1180-1"></a>nodesize &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">51</span>, <span class="dv">10</span>)</span>
<span id="cb1180-2"><a href="ejemplos-de-algoritmos.html#cb1180-2"></a>acc &lt;-<span class="st"> </span><span class="kw">sapply</span>(nodesize, <span class="cf">function</span>(ns){</span>
<span id="cb1180-3"><a href="ejemplos-de-algoritmos.html#cb1180-3"></a>  <span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train,</span>
<span id="cb1180-4"><a href="ejemplos-de-algoritmos.html#cb1180-4"></a>        <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">mtry =</span> <span class="dv">2</span>),</span>
<span id="cb1180-5"><a href="ejemplos-de-algoritmos.html#cb1180-5"></a>        <span class="dt">nodesize =</span> ns)<span class="op">$</span>results<span class="op">$</span>Accuracy</span>
<span id="cb1180-6"><a href="ejemplos-de-algoritmos.html#cb1180-6"></a>})</span>
<span id="cb1180-7"><a href="ejemplos-de-algoritmos.html#cb1180-7"></a><span class="kw">qplot</span>(nodesize, acc)</span></code></pre></div>
<p><img src="libro_files/figure-html/acc-versus-nodesize-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Ahora podemos ajustar el bosque aleatorio con el tamaño de nodo mínimo optimizado a todos los datos de entrenamiento y evaluar el rendimiento en los datos de evaluación.</p>
<div class="sourceCode" id="cb1181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1181-1"><a href="ejemplos-de-algoritmos.html#cb1181-1"></a>train_rf_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">randomForest</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train,</span>
<span id="cb1181-2"><a href="ejemplos-de-algoritmos.html#cb1181-2"></a>                           <span class="dt">nodesize =</span> nodesize[<span class="kw">which.max</span>(acc)])</span>
<span id="cb1181-3"><a href="ejemplos-de-algoritmos.html#cb1181-3"></a></span>
<span id="cb1181-4"><a href="ejemplos-de-algoritmos.html#cb1181-4"></a><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf_<span class="dv">2</span>, mnist_<span class="dv">27</span><span class="op">$</span>test),</span>
<span id="cb1181-5"><a href="ejemplos-de-algoritmos.html#cb1181-5"></a>                mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1181-6"><a href="ejemplos-de-algoritmos.html#cb1181-6"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1181-7"><a href="ejemplos-de-algoritmos.html#cb1181-7"></a><span class="co">#&gt;    0.815</span></span></code></pre></div>
<p>El modelo seleccionado mejora la exactitud y provee un estimado más uniforme.</p>
<p><img src="libro_files/figure-html/cond-prob-final-rf-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Tengan en cuenta que podemos evitar escribir nuestro propio código utilizando otras implementaciones de bosques aleatorios como se describe en el manual <strong>caret</strong><a href="#fn112" class="footnote-ref" id="fnref112"><sup>112</sup></a>.</p>
<p>El bosque aleatorio funciona mejor en todos los ejemplos que hemos considerado. Sin embargo, una desventaja de los bosques aleatorios es que perdemos interpretabilidad. Un enfoque que ayuda con la interpretabilidad es examinar la <em>importancia de la variable</em> (<em>variable importance</em> en inglés). Para definir <em>importancia</em> contamos cuán frecuentemente se usa el predictor en los árboles individuales. Pueden obtener más información sobre <em>importancia</em> en un libro de <em>machine learning</em> avanzado<a href="#fn113" class="footnote-ref" id="fnref113"><sup>113</sup></a>. El paquete <strong>caret</strong> incluye la función <code>varImp</code> que extrae la importancia de cada variable de cualquier modelo en el que se implementa el cálculo. Ofecemos un ejemplo de cómo usamos la importancia en la siguiente sección.</p>
</div>
<div id="ejercicios-54" class="section level2">
<h2><span class="header-section-number">31.12</span> Ejercicios</h2>
<p>1. Cree un set de datos sencillo donde el resultado crece 0.75 unidades en promedio por cada aumento en un predictor:</p>
<div class="sourceCode" id="cb1182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1182-1"><a href="ejemplos-de-algoritmos.html#cb1182-1"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb1182-2"><a href="ejemplos-de-algoritmos.html#cb1182-2"></a>sigma &lt;-<span class="st"> </span><span class="fl">0.25</span></span>
<span id="cb1182-3"><a href="ejemplos-de-algoritmos.html#cb1182-3"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1182-4"><a href="ejemplos-de-algoritmos.html#cb1182-4"></a>y &lt;-<span class="st"> </span><span class="fl">0.75</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb1182-5"><a href="ejemplos-de-algoritmos.html#cb1182-5"></a>dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)</span></code></pre></div>
<p>Utilice <code>rpart</code> para ajustar un árbol de regresión y guardar el resultado en <code>fit</code>.</p>
<p>2. Grafique el árbol final para que pueda ver dónde ocurrieron las particiones.</p>
<p>3. Haga un diagrama de dispersión de <code>y</code> versus <code>x</code> junto con los valores predichos basados en el ajuste.</p>
<p>4. Ahora modele con un bosque aleatorio en lugar de un árbol de regresión usando <code>randomForest</code> del paquete <strong>randomForest</strong> y rehaga el diagrama de dispersión con la línea de predicción.</p>
<p>5. Use la función <code>plot</code> para ver si el bosque aleatorio ha convergido o si necesitamos más árboles.</p>
<p>6. Parece que los valores predeterminados para el bosque aleatorio dan como resultado un estimado demasiado flexible (no uniforme). Vuelva a ejecutar el bosque aleatorio pero esta vez con <code>nodesize</code> fijado en 50 y <code>maxnodes</code> fijado en 25. Rehaga el gráfico.</p>
<p>7. Vemos que esto produce resultados más suaves. Usemos la función <code>train</code> para ayudarnos a elegir estos valores. Del manual <strong>caret</strong><a href="#fn114" class="footnote-ref" id="fnref114"><sup>114</sup></a> vemos que no podemos ajustar el parámetro <code>maxnodes</code> ni el argumento <code>nodesize</code> con la función <code>randomForest</code>, así que usaremos el paquete <strong>Rborist</strong> y ajustaremos el argumento <code>minNode</code>. Utilice la función <code>train</code> para probar valores <code>minNode &lt;- seq(5, 250, 25)</code>. Vea qué valor minimiza el RMSE estimado.</p>
<p>8. Haga un diagrama de dispersión junto con la predicción del modelo mejor ajustado.</p>
<p>9. Utilice la función <code>rpart</code> para ajustar un árbol de clasificación al set de datos <code>tissue_gene_expression</code>. Utilice la función <code>train</code> para estimar la exactitud. Pruebe valores <code>cp</code> de <code>seq(0, 0.05, 0.01)</code>. Grafique la exactitud para indicar los resultados del mejor modelo.</p>
<p>10. Estudie la matriz de confusión para el árbol de clasificación de mejor ajuste. ¿Qué observa que sucede con la placenta?</p>
<p>11. Tenga en cuenta que las placentas se llaman endometrio con más frecuencia que placenta. Además, noten que la cantidad de placentas es solo seis y que, de forma predeterminada, <code>rpart</code> requiere 20 observaciones antes de dividir un nodo. Por lo tanto, no es posible con estos parámetros tener un nodo en el que las placentas sean la mayoría. Vuelva a ejecutar el análisis anterior, pero esta vez permita que <code>rpart</code> divida cualquier nodo usando el argumento <code>control = rpart.control(minsplit = 0)</code>. ¿Aumenta la exactitud? Mire la matriz de confusión de nuevo.</p>
<p>12. Grafique el árbol del modelo de mejor ajuste obtenido en el ejercicio 11.</p>
<p>13. Podemos ver que con solo seis genes, podemos predecir el tipo de tejido. Ahora veamos si podemos hacerlo aún mejor con un bosque aleatorio. Utilice la función <code>train</code> y el método <code>rf</code> para entrenar un bosque aleatorio. Pruebe valores de <code>mtry</code> que van desde, al menos, <code>seq(50, 200, 25)</code>. ¿Qué valor de <code>mtry</code> maximiza la exactitud? Para permitir que pequeños <code>nodesize</code> crezcan como lo hicimos con los árboles de clasificación, use el siguiente argumento: <code>nodesize = 1</code>. Esto tardará varios segundos en ejecutarse. Si desea probarlo, intente usar valores más pequeños con <code>ntree</code>. Fije la semilla en 1990.</p>
<p>14. Use la función <code>varImp</code> en el resultado de <code>train</code> y guárdelo en un objeto llamado <code>imp</code>.</p>
<p>15. El modelo <code>rpart</code> que ejecutamos anteriormente produjo un árbol que utilizaba solo seis predictores. Extraer los nombres de los predictores no es sencillo, pero se puede hacer. Si el resultado de la llamada a <code>train</code> fue <code>fit_rpart</code>, podemos extraer los nombres así:</p>
<div class="sourceCode" id="cb1183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1183-1"><a href="ejemplos-de-algoritmos.html#cb1183-1"></a>ind &lt;-<span class="st"> </span><span class="op">!</span>(fit_rpart<span class="op">$</span>finalModel<span class="op">$</span>frame<span class="op">$</span>var <span class="op">==</span><span class="st"> &quot;&lt;leaf&gt;&quot;</span>)</span>
<span id="cb1183-2"><a href="ejemplos-de-algoritmos.html#cb1183-2"></a>tree_terms &lt;-</span>
<span id="cb1183-3"><a href="ejemplos-de-algoritmos.html#cb1183-3"></a><span class="st">  </span>fit_rpart<span class="op">$</span>finalModel<span class="op">$</span>frame<span class="op">$</span>var[ind] <span class="op">%&gt;%</span></span>
<span id="cb1183-4"><a href="ejemplos-de-algoritmos.html#cb1183-4"></a><span class="st">  </span><span class="kw">unique</span>() <span class="op">%&gt;%</span></span>
<span id="cb1183-5"><a href="ejemplos-de-algoritmos.html#cb1183-5"></a><span class="st">  </span><span class="kw">as.character</span>()</span>
<span id="cb1183-6"><a href="ejemplos-de-algoritmos.html#cb1183-6"></a>tree_terms</span></code></pre></div>
<p>¿Cuál es la importancia de variable para estos predictores? ¿Cuáles son sus rangos?</p>
<p>16. Avanzado: extraiga los 50 predictores principales según la importancia, tome un subconjunto de <code>x</code> con solo estos predictores y aplique la función <code>heatmap</code> para ver cómo se comportan estos genes a través de los tejidos. Presentaremos la función <code>heatmap</code> en el Capítulo <a href="clustering.html#clustering">34</a>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="109">
<li id="fn109"><p><a href="http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428" class="uri">http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428</a><a href="ejemplos-de-algoritmos.html#fnref109" class="footnote-back">↩︎</a></p></li>
<li id="fn110"><p><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" class="uri">https://web.stanford.edu/~hastie/Papers/ESLII.pdf</a><a href="ejemplos-de-algoritmos.html#fnref110" class="footnote-back">↩︎</a></p></li>
<li id="fn111"><p><a href="https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2" class="uri">https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2</a> <a href="ejemplos-de-algoritmos.html#fnref111" class="footnote-back">↩︎</a></p></li>
<li id="fn112"><p><a href="http://topepo.github.io/caret/available-models.html" class="uri">http://topepo.github.io/caret/available-models.html</a><a href="ejemplos-de-algoritmos.html#fnref112" class="footnote-back">↩︎</a></p></li>
<li id="fn113"><p><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" class="uri">https://web.stanford.edu/~hastie/Papers/ESLII.pdf</a><a href="ejemplos-de-algoritmos.html#fnref113" class="footnote-back">↩︎</a></p></li>
<li id="fn114"><p><a href="https://topepo.github.io/caret/available-models.html" class="uri">https://topepo.github.io/caret/available-models.html</a><a href="ejemplos-de-algoritmos.html#fnref114" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="caret.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="machine-learning-en-la-práctica.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dslibro/edit/master/ml/regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
