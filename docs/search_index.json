[
["index.html", "Introducción a la Ciencia de Datos Análisis de datos y algoritmos de predicción con R Prefacio", " Introducción a la Ciencia de Datos Análisis de datos y algoritmos de predicción con R Rafael A. Irizarry 2020-05-29 Prefacio Este libro comenzó como las notas utilizadas para enseñar la clase de HarvardX Data Science Series1. El código Rmarkdown que se usó para generar el libro está disponible en GitHub2. Tengan en cuenta que el tema gráfico utilizado para los gráficos a lo largo del libro se pueden recrear utilizando la función ds_theme_set() del paquete dslabs. Un PDF de la versión en inglés de este libro está disponible en Leanpub3. Una copia impresa de la versión en inglés de este libro está disponible en CRC Press4. Este trabajo se publica bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Internacional CC BY-NC-SA 4.0. Hacemos anuncios relacionados al libro en Twitter. Para la información más reciente, siga @rafalab. https://www.edx.org/professional-certificate/harvardx-data-science↩ https://github.com/rafalab/dsbook↩ https://leanpub.com/datasciencebook↩ https://www.crcpress.com/Introduction-to-Data-Science-Data-Analysis-and-Prediction-Algorithms-with/Irizarry/p/book/9780367357986↩ "],
["agradecimientos.html", "Agradecimientos", " Agradecimientos Este libro está dedicado a todas las personas involucradas en la construcción y el mantenimiento de R y los paquetes R que utilizamos en el texto. Un agradecimiento especial a los desarrolladores y los mantenedores de base R, el tidyverse y el paquete caret. Un agradecimiento especial a mi tidyverse gurú David Robinson y a Amy Gill por docenas de comentarios, cambios y sugerencias. Además, muchas gracias a Stephanie Hicks, que dos veces sirvió como co-instructora en mis clases de ciencias de datos, y a Yihui Xie, que pacientemente toleró mis multiples preguntas sobre bookdown. Gracias también a Karl Broman, de quien tomé prestadas ideas para las secciones sobre la visualización de datos y las herramientas de productividad, y a Hector Corrada-Bravo, por sus consejos sobre cómo mejor enseñar machine learning. Gracias a Peter Aldhous, de quien tomé prestadas ideas para la sección sobre los principios de la visualización de datos y a Jenny Bryan por escribir Happy Git y GitHub for the useR, que influyeron en nuestros capítulos de Git. Gracias a Alyssa Frazee por ayudar a crear el problema de tarea que se convirtió en el capítulo sobre los sistemas de recomendación y a Amanda Cox por proporcionar los datos de los exámenes de los Regentes de Nueva York. Además, muchas gracias a Jeff Leek, Roger Peng y Brian Caffo, cuya clase inspiró la forma en que se divide este libro y a Garrett Grolemund y Hadley Wickham por abrir el código de reserva para su libro R for Data Science. Finalmente, gracias a Alex Nones por corregir el manuscrito durante sus diversas etapas. Este libro fue concebido durante la enseñanza de varios cursos de estadística aplicada, comenzando hace más de quince años. Los profesores asistentes que trabajaron conmigo a lo largo de los años hicieron importantes contribuciones indirectas a este libro. La última versión de este curso es una serie de HarvardX coordinada por Heather Sternshein y Zofia Gajdos. Les agradecemos sus contribuciones. También estamos agradecidos a todos los estudiantes cuyas preguntas y comentarios nos ayudaron a mejorar el libro. Los cursos fueron parcialmente financiados por el subsidio del NIH R25GM114818. Agradecemos los Institutos Nacionales de Salud por su apoyo. Un agradecimiento especial a todos aquellos que editaron el libro a través de pull requests de GitHub o hicieron sugerencias creando un issue o enviando un correo electrónico: nickyfoto (Huang Qiang) desautm (Marc-André Désautels), michaschwab (Michail Schwab) alvarolarreategui (Alvaro Larreategui), jakevc (Jake VanCampen), omerta (Guillermo Lengemann), espinielli (Enrico Spinielli), asimumba(Aaron Simumba) braunschweig (Maldewar), gwierzchowski (Grzegorz Wierzchowski), technocrat (Richard Careaga) atzakas, defeit (David Emerson Feit), shiraamitchell (Shira Mitchell) Nathalie-S, andreashandel (Andreas Handel) berkowitze (Elias Berkowitz) Dean-Webb (Dean Webber), mohayusuf, jimrothstein, mPloenzke (Matthew Ploenzke), NicholasDowand (Nicholas Dow) kant (Darío Hereñú), debbieyuster (Debbie Yuster), tuanchauict (Tuan Chau), phzeller, David D. Kane, El Mustapha El Abbassi y Vadim Zipunnikov. La traducción del libro al español estuvo a cargo de Alex Nones. Agradecemos a todos los que contribuyeron a esta traducción. Ilia Ushkin and Dustin Tingley generaron un primer borrador usando un programa de traducción automática. A través de Twitter @R4DS_es y @lacion (Laura Ación) proveyeron importante información sobre recursos existentes. Varios otros contribuyeron a traves de Twitter, GitHub, o email: @hortizzuazaga (Humberto Ortiz), @ribnikov (Jose Matestadístico), @jarangoo (Julián A.), @DiegoV_O_ (Diego), @BrunoContrerasM (BContreras Moreira), @a2kimura (Alejandro Kimura), @Emilio_NTN (Emilio García Morán), @beto_bfr (betofogo), @jdieramon (Jose V. Die), @yabellini (Yanina Bellini Saibene), @symusicgroup (Ismael Rudas), @criztinaz (Cristina Zenteno), @controlnegativo (Cristina de Dios), @d_olivaw (Elio Campitelli), @aguerri_jc (Jesús C. Aguerri), @pincheippie (Francisco, en casa) @compBiology (Pedro Madrigal), @RLadiesCuerna (RLadies Cuernavaca), @thecarpentries, eead-csic-compbio (CSIC &amp; Fundación ARAID), pablormier (Pablo R. Mier), josschavezf (Joselyn Chavez), jmcastagnetto (Jesus M. Castagnetto), ismaelrudas, AnaBVA (Ana B. Villaseñor Altamirano), (???) (Pablo Gutierrez), Héctor Corrada-Bravo, Rafael A. Arce Nazario, Luis R. Pericchi Guerra, María E. Perez Hernández, Juan Carlos Perdomo, Anamari Irizarry, and Amed Irizarry. "],
["introducción.html", "Introducción Los casos de estudio ¿Quién encontrará útil este libro? ¿Que cubre este libro? ¿Qué no cubre este libro?", " Introducción La demanda de profesionales cualificados en ciencias de datos en la industria, la academia y el gobierno está creciendo rápidamente. Este libro presenta conceptos y destrezas que pueden ayudarles a enfrentar los desafíos del análisis de datos en situaciones reales. El texto abarca los conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. También les ayudará a desarrollar destrezas como la programación en R, el wrangling de datos, dplyr, la visualización de datos con ggplot2, la creación de algoritmos con caret, la organización de archivos con UNIX/Linux shell, el control de versiones con Git y GitHub y la preparación de documentos reproducibles con knitr y R markdown. El libro se divide en seis partes: R, Visualización de datos, Wrangling de datos, Estadísticas con R, Machine Learning y Herramientas de productividad. Cada parte tiene varios capítulos que se deben presentar como una sola clase e incluye docenas de ejercicios distribuidos a través de los capítulos. Los casos de estudio A lo largo del libro, utilizamos casos de estudio motivantes. En cada caso de estudio, intentamos imitar de manera realista la experiencia de los científicos de datos. Para cada uno de los conceptos que discutimos, comenzamos haciendo preguntas específicas a las que entonces respondemos mediante un análisis de datos. Aprendemos los conceptos como un medio para responder a las preguntas. Ejemplos de los casos de estudio que incluimos en este libro son: Caso de estudio Concepto Tasas de asesinatos en Estados Unidos por estado Conceptos básicos de R Alturas de estudiantes Resúmenes estadísticos Tendencias en la salud y la economía mundial Visualización de datos El impacto de las vacunas en las tasas de enfermedades infecciosas Visualización de datos La crisis financiera de 2007-2008 Probabilidad Previsión de elecciones Inferencia estadística Alturas autoreportadas de estudiantes Wrangling de datos Money Ball: Construyendo un equipo de béisbol Regresión lineal MNIST: Procesamiento de imagen de dígitos escritos a mano Machine Learning Sistemas de recomendación de películas Machine Learning ¿Quién encontrará útil este libro? El próposito de este libro es servir como un texto para un primer curso de ciencia de datos. No es necesario tener conocimientos previos de R, aunque algo de experiencia en la programación puede ser útil. Los conceptos estadísticos utilizados para responder a las preguntas de los casos de estudio se presentan solo brevemente, y por tanto recomendamos un libro de texto de Probabilidad y Estadística para los que quieran entender a fondo estos conceptos. Al leer y comprender todos los capítulos y completar todos los ejercicios, los estudiantes estarán bien posicionados para realizar tareas básicas de análisis de datos y aprender los conceptos y las destrezas más avanzadas que son necesarios para convertirse en expertos. ¿Que cubre este libro? Comenzamos repasando los conceptos básicos de R y el tidyverse. Aprenderán R a lo largo del libro, pero en la primera parte nos dedicamos a revisar los componentes básicos necesarios para seguir aprendiendo. La creciente disponibilidad de conjuntos de datos informativos y de herramientas de software ha llevado a una mayor dependencia de la visualización de datos en muchos campos. En la segunda parte, demostramos cómo usar ggplot2 para generar gráficos y describir principios importantes de la visualización de datos. En la tercera parte demostramos la importancia de las estadísticas en el análisis de datos respondiendo a preguntas de estudios de caso usando la probabilidad, la inferencia y la regresión con R. La cuarta parte utiliza varios ejemplos para familiarizar a los lectores con el wrangling de datos. Entre las destrezas específicas que estudiamos están la extracción de la web (web scraping en inglés), el uso de expresiones regulares y la unión y la remodelación de tablas de datos. Hacemos esto usando las herramientas de tidyverse. En la quinta parte presentamos varios desafíos que nos llevan a introducir el machine learning. Aprendemos a usar el paquete caret para construir algoritmos de predicción que incluyen K-nearest neighbors y random forests. En la parte final, ofrecemos una breve introducción a las herramientas de productividad que usamos diariamente en los proyectos de ciencia de datos. Estas son RStudio, UNIX/Linux shell, Git y GitHub, y knitr y R Markdown. ¿Qué no cubre este libro? Este libro se enfoca en los aspectos del análisis de datos de la ciencia de datos. Por consiguiente, no discutimos aspectos relacionados con el manejo de datos (data management en inglés) o la ingeniería. Aunque la programación en R es una parte esencial del libro, no enseñamos temas informáticos más avanzados como las estructuras de datos, la optimización y la teoría de algoritmos. Del mismo modo, no discutimos temas como los servicios web, los gráficos interactivos, la computación paralela y el procesamiento de flujos de datos (data streaming processing en inglés). Los conceptos estadísticos se presentan principalmente como herramientas para resolver problemas y no se incluyen descripciones teóricas en profundidad en este libro. "],
["getting-started.html", "Capítulo 1 Comenzando con R y RStudio 1.1 ¿Por qué R? 1.2 La consola R 1.3 scripts 1.4 RStudio 1.5 Instalación de paquetes de R", " Capítulo 1 Comenzando con R y RStudio 1.1 ¿Por qué R? R no es un lenguaje de programación como C o Java. No fue creado por ingenieros de software para el desarrollo de software, sino por estadísticos como un ambiente interactivo para el análisis de datos. Pueden leer la historia completa en el artículo “Una breve historia de S”5. La interactividad es una característica indispensable en la ciencia de datos porque, como pronto aprenderán, la capacidad de explorar rápidamente los datos es necesario para el éxito en este campo. Sin embargo, igual que en otros lenguajes de programación, en R pueden guardar su trabajo como scripts, o secuencias de comandos, que se pueden ejecutar fácilmente en cualquier momento. Estos scripts sirven como un registro del análisis que realizaron, una característica clave que facilita el trabajo reproducible. Los programadores expertos no deben esperar que R siga las convenciones a que están acostumbrados, ya que se sentirán decepcionados. Si son pacientes, apreciarán la gran ventaja de R cuando se trata del análisis de datos y, específicamente, de la visualización de datos. Otras características atractivas de R son: R es gratuito y de código abierto6. Se ejecuta en todas las plataformas principales: Windows, Mac Os, UNIX/ Linux. Los scripts y los objetos de datos se pueden compartir sin problemas entre plataformas. Existe una comunidad grande, creciente y activa de usuarios de R y, como resultado, hay numerosos recursos para aprender y hacer preguntas789. Es fácil para otras personas contribuir complementos (add-ons en inglés) que les permiten a los desarrolladores compartir implementaciones de software de nuevas metodologías de ciencia de datos. Esto les da a los usuarios de R acceso temprano a los últimos métodos y herramientas que se desarrollan para una amplia variedad de disciplinas, incluyendo la ecología, la biología molecular, las ciencias sociales y la geografía, entre otros campos. 1.2 La consola R El análisis de datos interactivo generalmente ocurre en la consola R que ejecuta comandos a medida que los escriban. Hay varias formas de obtener acceso a una consola R. Una es simplemente iniciando R en su computadora. La consola se ve así: Como ejemplo rápido, intenten usar la consola para calcular una propina de 15% en una comida que cuesta $19.71: 0.15 * 19.71 #&gt; [1] 2.96 ** Ojo: En este libro, los cuadros grises se utilizan para mostrar el código R escrito en la consola R. El símbolo #&gt; se usa para denotar el output de la consola R. ** 1.3 scripts Una de las grandes ventajas de R sobre el software de análisis de apuntar y hacer clic es que pueden guardar su trabajo como scripts, que entonces pueden editar y guardar con un editor de texto. El material de este libro se desarrolló utilizando el Integrated Development Environment (IDE) de RStudio10. RStudio incluye un editor con muchas características específicas de R, una consola para ejecutar su código y otros paneles útiles, incluso uno para mostrar figuras. La mayoría de consolas de R disponibles en la web también incluyen un panel para editar scripts, pero no todas les permiten guardar los scripts para su uso posterior. Todos los scripts de R utilizados para generar este libro se pueden encontrar en GitHub11. 1.4 RStudio RStudio será nuestra plataforma de lanzamiento para los proyectos de ciencia de datos. No sólo nos provee un editor para crear y editar nuestros scripts, sino que también ofrece muchas otras herramientas útiles. En esta sección, repasaremos algunos de los conceptos básicos. 1.4.1 Paneles Cuando inicien RStudio por primera vez, verán tres paneles. El panel izquierdo muestra la consola R. A la derecha, el panel superior incluye pestañas como Environment y History, mientras que el panel inferior muestra cinco pestañas: File, Plots, Packages, Help y Viewer (estas pestañas pueden ser diferentes en las nuevas versiones de RStudio). Pueden hacer clic en cada pestaña para moverse por las diferentes opciones. Para iniciar un nuevo script, hagan clic en File, entonces New File y luego R Script. Esto inicia un nuevo panel a la izquierda y es aquí donde pueden comenzar a escribir su script. 1.4.2 Key bindings Muchas de las tareas que realizamos con el mouse se pueden lograr con una combinación de teclas (key bindings en inglés). Por ejemplo, acabamos de mostrar cómo usar el mouse para iniciar un nuevo script, pero también se puede usar la siguiente combinación de teclas: Ctrl + Shift + N en Windows y Command + Shift + N en la Mac. Aunque en este tutorial a menudo mostramos cómo usar el mouse, recomendamos encarecidamente que memoricen las combinaciones de teclas para las operaciones que usan con mayor frecuencia. RStudio incluye una hoja de referencia (cheat sheet en inglés) útil con los comandos más utilizados. Pueden obtenerla directamente de RStudio así: Recomendamos tener esto a mano para poder buscar las combinaciones de teclas cuando se encuentren apuntando y haciendo clic repetidas veces. 1.4.3 Cómo ejecutar comandos mientras edita scripts Hay muchos editores diseñados específicamente para la codificación. Estos son útiles porque el color y la indentación se agregan automáticamente para que el código sea más legible. RStudio es uno de estos editores y se desarrolló específicamente para R. Una de las principales ventajas que RStudio tiene sobre otros editores es que podemos probar nuestro código fácilmente mientras editamos nuestros scripts. A continuación ofrecemos un ejemplo. Comencemos abriendo un nuevo script como lo hicimos antes. Entonces, nombremos el script. Podemos hacer esto a través del editor guardando el nuevo script actual sin nombre. Para empezar, hagan clic en el icono de guardar o usando la combinación de teclas Ctrl + S en Windows y Command + S en la Mac. Al intentar guardar el documento por primera vez, RStudio le pedirá un nombre. Una buena convención es usar un nombre descriptivo, con letras minúsculas, sin espacios, sólo guiones para separar las palabras y luego seguido del sufijo .R. Llamaremos a este script: my-first-script.R. Ahora estamos listos para comenzar a editar nuestro primer script. Las primeras líneas de código en un script de R se dedican a cargar los paquetes que usaremos. Otra característica útil de RStudio es que una vez escribimos library(), RStudio comienza a completar automáticamente lo que estamos escribiendo con los paquetes que hemos instalado. Observen lo que sucede cuando escribimos library(ti): Otra característica que pueden haber notado es que cuando escriben library( el segundo paréntesis se agrega automáticamente. Esto les ayudará a evitar uno de los errores más comunes en la codificación: olvidar cerrar un paréntesis. Ahora podemos continuar escribiendo código. Como ejemplo, crearemos un gráfico que muestre los totales de asesinatos versus los totales de población por estado de EE.UU.. Una vez que hayan terminado de escribir el código necesario para hacer este gráfico, pueden probarlo ejecutando el código. Para hacer esto, hagan clic en el botón Run en la parte derecha superior del panel de edición. También pueden usar la combinación de teclas: Ctrl + Shift + Enter en Windows o Command + Shift + Return en Mac. Tan pronto corran el código, verán que este aparece en la consola R y, en este caso, el gráfico que resulta aparece en la consola de gráficos. Recuerden que la consola de gráficos tiene una interfaz útil que le permite hacer clic hacia delante o hacia atrás en diferentes gráficos, hacer zoom en el gráfico o guardar los gráficos como archivos. Para ejecutar una línea a la vez en lugar del script completo, pueden usar Control-Enter en Windows y Command-Return en Mac. 1.4.4 Cómo cambiar las opciones globales Pueden cambiar el aspecto y la funcionalidad de RStudio bastante. Para cambiar las opciones globales, hagan clic en Tools y luego en Global Options …. Como ejemplo, mostramos cómo hacer un cambio que sumamente recomendamos: cambiar el Save workspace to .RData on exit a Never y desmarcar Restore .RData into workspace at start. Por defecto, cuando uno sale de R, el programa guarda todos los objetos que ha creado en un archivo llamado .RData. Esto ocurre para que, cuando reinicien la sesión en el mismo archivo, el programa cargue estos objetos. Sin embargo, encontramos que esto causa confusión, especialmente cuando compartimos código con colegas y suponemos que tienen este archivo .RData. Para cambiar estas opciones, hagan que su configuración General se vea así: 1.5 Instalación de paquetes de R La funcionalidad que una nueva instalación de R ofrece es sólo una pequeña fracción de lo que es posible. De hecho, nos referimos a lo que obtienen después de su primera instalación como base R. La funcionalidad adicional proviene de complementos disponibles de los desarrolladores. Actualmente hay cientos de estos disponibles de CRAN y muchos otros compartidos a través de otros repositorios como GitHub. Sin embargo, debido a que no todo el mundo necesita todas las funciones disponibles, R pone a disposición diferentes componentes a través de paquetes (packages en inglés). R facilita la instalación de paquetes desde R. Por ejemplo, para instalar el paquete dslabs, que usamos para compartir los sets de datos y códigos relacionados con este libro, deben escribir: install.packages(&quot;dslabs&quot;) En RStudio, pueden navegar a la pestaña Tools y seleccionar Install packages. Luego podemos cargar el paquete en nuestras sesiones R usando la función library: library(dslabs) A medida que vayan leyendo este libro, verán que cargamos paquetes sin instalarlos. Esto se debe a que una vez que instalen un paquete, permanece instalado y sólo necesita cargarse con library. El paquete permanece cargado hasta que terminemos con la sesión R. Si intentan cargar un paquete y obtienen un error, probablemente significa que no lo han instalado. Podemos instalar más de un paquete a la vez proveyendo un vector de caracteres a esta función: install.packages(c(&quot;tidyverse&quot;, &quot;dslabs&quot;)) Tengan en cuenta que la instalación de tidyverse en realidad instala varios paquetes. Esto ocurre comúnmente cuando un paquete tiene * dependencias *, o usa funciones de otros paquetes. Cuando cargan un paquete usando library, también cargan sus dependencias. Una vez que los paquetes estén instalados, pueden cargarlos en R y no necesitan instalarlos nuevamente, a menos que instalen una versión nueva de R. Recuerden que los paquetes están instalados en R y no en RStudio. Es útil mantener una lista de todos los paquetes que necesitan para su trabajo en un script porque si tienen que realizar una instalación nueva de R, pueden reinstalar todos sus paquetes simplemente ejecutando un script. Pueden ver todos los paquetes que han instalado utilizando la siguiente función: installed.packages() https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf↩ https://opensource.org/history↩ https://stats.stackexchange.com/questions/138/free-resources-for-learning-r↩ https://www.r-project.org/help.html↩ https://stackoverflow.com/documentation/r/topics↩ https://www.rstudio.com/↩ https://github.com/rafalab/dsbook↩ "],
["r-basics.html", "Capítulo 2 Lo básico de R 2.1 Caso de estudio: los asesinatos con armas en EE. UU. 2.2 Lo básico 2.3 Ejercicios 2.4 Tipos de datos 2.5 Ejercicios 2.6 Vectores 2.7 La conversión forzada 2.8 Ejercicios 2.9 Sorting 2.10 Ejercicios 2.11 Aritmética de vectores 2.12 Ejercicios 2.13 Indexación 2.14 Ejercicios 2.15 Gráficos básicos 2.16 Ejercicios", " Capítulo 2 Lo básico de R En este libro, utilizaremos el ambiente de software R para todo nuestro análisis. Aprenderán R y las técnicas de análisis de datos simultáneamente. Por tanto, para continuar necesitarán acceso a R. También recomendamos el uso de un Integrated Development Environment (IDE), como RStudio, para guardar su trabajo. Recuerden que es común que un curso o taller ofrezca acceso a un ambiente de R y a un IDE a través de su navegador de web, como lo hace RStudio cloud12. Si tienen acceso a dicho recurso, no necesitan instalar R ni RStudio. Sin embargo, si eventualmente quieren convertirse en analistas expertos de datos, recomendamos instalar estas herramientas en su computadora13. Tanto R como RStudio son gratuitos y están disponibles en línea. 2.1 Caso de estudio: los asesinatos con armas en EE. UU. Imagínese que vive en Europa y se le ofrece un trabajo en una empresa estadounidense con muchas ubicaciones por todo EE. UU.. Es un gran trabajo, pero noticias con titulares como Tasa de homicidios con armas de fuego de EE. UU. más alta que en otros países desarrollados14. ¿Se preocupa? Gráficos como el siguiente pueden preocuparle aún más: O peor aún, esta versión de everytown.org: Pero entonces se recuerda que Estados Unidos es un país grande y diverso, con 50 estados muy diferentes, además del Distrito de Columbia (DC). California, por ejemplo, tiene una población más grande que Canadá, y 20 estados de EE. UU. tienen poblaciones más grandes que la de Noruega. En algunos aspectos, la variabilidad entre los estados de EE. UU. es parecida a la variabilidad entre los países de Europa. Además, aunque no se incluyen en los cuadros anteriores, las tasas de asesinatos en Lituania, Ucrania y Rusia son superiores a cuatro por cada 100.000. Entonces, es posible que las noticias que le preocupan sean demasiado superficiales. Tiene opciones de dónde puede vivir y desea determinar la seguridad de cada estado en particular. Obtendremos algunas ideas al examinar los datos relacionados con homicidios con armas de fuego de EE. UU. en 2010 usando R. Antes de comenzar con nuestro ejemplo, necesitamos discutir la logística, así como algunos de los componentes básicos necesarios para obtener destrezas más avanzadas de R. Recuerden que la utilidad de algunos de estos componentes básicos no siempre es inmediatamente obvia, pero luego en el libro apreciarán haber dominado estas destrezas. 2.2 Lo básico Antes de empezar con el set de datos motivante, necesitamos repasar los conceptos básicos de R. 2.2.1 Objetos Supongamos que unos estudiantes de secundaria nos piden ayuda para resolver varias ecuaciones cuadráticas de la forma \\(ax^2+bx+c = 0\\). La fórmula cuadrática nos ofrece las soluciones: \\[ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\] que por supuesto cambian dependiendo de los valores de \\(a\\), \\(b\\) y \\(c\\). Una ventaja de los lenguajes de programación es poder definir variables y escribir expresiones con estas, como se hace en las matemáticas, para obtener una solución numérica. Escribiremos un código general para la ecuación cuadrática a continuación, pero si nos piden resolver \\(x^2 + x -1 = 0\\), entonces definimos: a &lt;- 1 b &lt;- 1 c &lt;- -1 que almacena los valores para su uso posterior. Usamos &lt;- para asignar valores a las variables. También podemos asignar valores usando = en lugar de &lt;-, pero recomendamos no usar = para evitar confusión. Copien y peguen el código anterior en su consola para definir las tres variables. Tengan en cuenta que R no imprime nada cuando hacemos esta asignación. Esto significa que los objetos se definieron con éxito. Si hubieran cometido un error, habría aparecido un mensaje de error. Para ver el valor almacenado en una variable, simplemente le pedimos a R que evalúe a y este muestra el valor almacenado: a #&gt; [1] 1 Una forma más explícita de pedirle a R que nos muestre el valor almacenado en a es usar print así: print(a) #&gt; [1] 1 Usamos el término objeto para describir cosas que están almacenadas en R. Las variables son ejemplos, pero los objetos también pueden ser entidades más complicadas como las funciones, que se describen más adelante. 2.2.2 El espacio de trabajo A medida que definimos objetos en la consola, estamos cambiando el espacio de trabajo (workspace en inglés). Pueden ver todas las variables guardadas en su espacio de trabajo al escribir: ls() #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;dat&quot; &quot;img_path&quot; &quot;murders&quot; En RStudio, la pestaña Environment muestra los valores: Deberíamos ver a, b y c. Si intentan recuperar el valor de una variable que no está en sus espacios de trabajo, recibirán un mensaje de error. Por ejemplo, si escriben x, verán lo siguiente: Error: object 'x' not found. Ahora, dado que estos valores se guardan en variables, para resolver nuestra ecuación, utilizamos la fórmula cuadrática: (-b + sqrt(b^2 - 4*a*c) )/ ( 2*a ) #&gt; [1] 0.618 (-b - sqrt(b^2 - 4*a*c) )/ ( 2*a ) #&gt; [1] -1.62 2.2.3 Funciones Una vez que definan las variables, el proceso de análisis de datos generalmente se puede describir como una serie de funciones aplicadas a los datos. R incluye varias funciones predefinidas y la mayoría de las líneas de análisis que construimos hacen uso extensivo de ellas. Ya usamos las funciones install.packages, library y ls. También usamos la función sqrt para solucionar la ecuación cuadrática anterior. Hay muchas más funciones predefinidas y hasta más se pueden añadir a través de paquetes. Estas no aparecen en sus espacios de trabajo porque no las definieron, pero están disponibles para su uso inmediato. En general, necesitamos usar paréntesis para evaluar una función. Si escriben ls, la función no se evalúa y en cambio R les muestra el código que la define. Si escriben ls(), la función se evalúa y, como ya se mostró, vemos objetos en el espacio de trabajo. A diferencia de ls, la mayoría de las funciones requieren uno o más argumentos. A continuación se muestra un ejemplo de cómo asignamos un objeto al argumento de la función log. Recuerden que anteriormente definimos a como 1: log(8) #&gt; [1] 2.08 log(a) #&gt; [1] 0 Pueden averiguar lo que la función espera y lo que hace revisando unos manuales muy útiles incluidos en R. Pueden obtener ayuda utilizando la función help así: help(&quot;log&quot;) Para la mayoría de las funciones, también podemos usar esta abreviatura: ?log La página de ayuda les mostrará qué argumentos espera la función. Por ejemplo, log necesita x y base para correr. Sin embargo, algunos argumentos son obligatorios y otros son opcionales. Pueden determinar cuáles son opcionales notando en el documento de ayuda cuáles valores predeterminados se asignan con =. Definir estos es opcional. Por ejemplo, la base de la función log por defecto es base = exp(1) haciendo log el logaritmo natural por defecto. Para echar un vistazo rápido a los argumentos sin abrir el sistema de ayuda, pueden escribir: args(log) #&gt; function (x, base = exp(1)) #&gt; NULL Pueden cambiar los valores predeterminados simplemente asignando otro objeto: log(8, base = 2) #&gt; [1] 3 Recuerden que no hemos estado especificando el argumento x como tal: log(x = 8, base = 2) #&gt; [1] 3 El código anterior funciona, pero también podemos ahorrarnos un poco de tecleo: si no usan un nombre de argumento, R supone que están ingresando argumentos en el orden en que se muestran en la página de ayuda o por args. Entonces, al no usar los nombres, R supone que los argumentos son x seguido por base: log(8,2) #&gt; [1] 3 Si usan los nombres de los argumentos, podemos incluirlos en el orden en que queramos: log(base = 2, x = 8) #&gt; [1] 3 Para especificar argumentos, debemos usar = y no &lt;-. Hay algunas excepciones a la regla de que las funciones necesitan los paréntesis para ser evaluadas. Entre estas, las más utilizados son los operadores aritméticos y relacionales. Por ejemplo: 2^3 #&gt; [1] 8 Pueden ver los operadores aritméticos al escribir: help(&quot;+&quot;) o ?&quot;+&quot; y los operadores relacionales al escribir: help(&quot;&gt;&quot;) o ?&quot;&gt;&quot; 2.2.4 Otros objetos predefinidos Hay varios sets de datos que se incluyen para que los usuarios practiquen y prueben las funciones. Pueden ver todos los sets de datos disponibles escribiendo: data() Esto les muestra el nombre del objeto para estos sets de datos. Estos sets de datos son objetos que se pueden usar simplemente escribiendo el nombre. Por ejemplo, si escriben: co2 R les mostrará los datos de concentración de CO2 atmosférico de Mauna Loa. Otros objetos predefinidos son cantidades matemáticas, como la constante \\(\\pi\\) y \\(\\infty\\): pi #&gt; [1] 3.14 Inf+1 #&gt; [1] Inf 2.2.5 Nombres de variables Hemos usado las letras a, b y c como nombres de variables, pero estos pueden ser casi cualquier cosa. Algunas reglas básicas en R son que los nombres de variables tienen que comenzar con una letra, no pueden contener espacios y no deben ser variables predefinidas en R. Por ejemplo, no nombren una de sus variables install.packages escribiendo algo como: install.packages &lt;- 2. Una buena convención a seguir es usar palabras significativas que describan lo que están almacenado, usar solo minúsculas y usar guiones bajos como sustituto de espacios. Para las ecuaciones cuadráticas, podríamos usar algo como: solution_1 &lt;- (-b + sqrt(b^2 - 4*a*c))/ (2*a) solution_2 &lt;- (-b - sqrt(b^2 - 4*a*c))/ (2*a) Para obtener más consejos, recomendamos estudiar la guía de estilo de Hadley Wickham15. 2.2.6 Cómo guardar su espacio de trabajo Los valores permanecen en el espacio de trabajo hasta que finalicen sus sesiones o las borren con la función rm. Pero los espacios de trabajo también se pueden guardar para su uso posterior. De hecho, al salir de R, el programa les pregunta si desean guardar su espacio de trabajo. Si lo guardan, la próxima vez que inicien R, el programa restaurará el espacio de trabajo. Recomendamos no guardar el espacio de trabajo así porque, a medida que comiencen a trabajar en diferentes proyectos, será más difícil darle seguimiento de lo que se guarda. En cambio, les recomendamos que le asignen al espacio de trabajo un nombre específico. Pueden hacer esto usando las funciones save o save.image. Para cargar, usen la función load. Al guardar un espacio de trabajo, recomendamos el sufijo rda o RData. En RStudio, también pueden hacerlo navegando a la pestaña Session y eligiendo Save Workspace as. Luego pueden cargarlo usando las opciones Load Workspace en la misma pestaña. Para aprender más, lean las páginas de ayuda (help pages en inglés) en save, save.image y load. 2.2.7 Scripts motivantes Para resolver otra ecuación como \\(3x^2 + 2x -1\\), podemos copiar y pegar el código anterior, redefinir las variables y volver a calcular la solución: a &lt;- 3 b &lt;- 2 c &lt;- -1 (-b + sqrt(b^2 - 4*a*c))/ (2*a) (-b - sqrt(b^2 - 4*a*c))/ (2*a) Al crear y guardar un script con el código anterior, ya no tendrán que volver a escribirlo todo cada vez, sino simplemente cambiar los nombres de las variables. Intenten escribir la secuencia de comandos anteriores en un editor y observen lo fácil que es cambiar las variables y recibir una respuesta. 2.2.8 Cómo comentar su código Si una línea de código R comienza con el símbolo #, no se evalúa. Podemos usar esto para escribir recordatorios de por qué escribimos un código particular. Por ejemplo, en el script anterior, podríamos agregar: ## Código para calcular la solución a la ecuación cuadrática de la forma ax^2 + bx + c ## definir las variables a &lt;- 3 b &lt;- 2 c &lt;- -1 ## ahora calcule la solución (-b + sqrt(b^2 - 4*a*c))/ (2*a) (-b - sqrt(b^2 - 4*a*c))/ (2*a) 2.3 Ejercicios 1. ¿Cuál es la suma de los primeros 100 números enteros positivos? La fórmula para la suma de enteros \\(1\\) a \\(n\\) es \\(n(n+1)/2\\). Defina \\(n=100\\) y luego use R para calcular la suma de \\(1\\) a \\(100\\) usando la fórmula. ¿Cuál es la suma? 2. Ahora use la misma fórmula para calcular la suma de los enteros del 1 a 1000. 3. Mire el resultado de escribir el siguiente código en R: n &lt;- 1000 x &lt;- seq(1, n) sum(x) Basado en el resultado, ¿qué cree que hacen las funciones seq y sum? Puede usar help. sum crea una lista de números y seq los suma. seq crea una lista de números y sum los suma. seq crea una lista aleatoria y sum calcula la suma de 1 a 1000. sum siempre devuelve el mismo número. 4. En las matemáticas y la programación, decimos que evaluamos una función cuando reemplazamos el argumento con un número dado. Entonces si escribimos sqrt(4), evaluamos la función sqrt. En R, se puede evaluar una función dentro de otra función. Las evaluaciones suceden de adentro hacia afuera. Use una línea de código para calcular el logaritmo, en base 10, de la raíz cuadrada de 100. 5. ¿Cuál de los siguientes siempre devolverá el valor numérico almacenado en x? Puede intentar los ejemplos y usar el sistema de ayuda si lo desea. log(10^x) log10(x^10) log(exp(x)) exp(log(x, base = 2)) 2.4 Tipos de datos Las variables en R pueden ser de diferentes tipos. Por ejemplo, necesitamos distinguir números de cadenas de caracteres y tablas de listas sencillas de números. La función class nos ayuda a determinar qué tipo de objeto tenemos: a &lt;- 2 class(a) #&gt; [1] &quot;numeric&quot; Para trabajar eficientemente en R, es importante aprender los diferentes tipos de variables y qué podemos hacer con ellos. 2.4.1 data frames Hasta ahora, las variables que hemos definido son solo un número. Esto no es muy útil para almacenar datos. La forma más común de almacenar un set de datos en R es usando un data frame. Conceptualmente, podemos pensar en un data frame como una tabla con filas que representan observaciones y con columnas que representan las diferentes variables recopiladas para cada observación. Los data frames son particularmente útiles para sets de datos porque podemos combinar diferentes tipos de datos en un solo objeto. Una gran proporción de los retos del análisis de datos comienza con datos almacenados en un data frame. Por ejemplo, almacenamos los datos para nuestro ejemplo motivante en un data frame. Pueden tener acceso a este set de datos cargando el paquete dslabs y entonces utilizando la función data para cargar el set de datos murders : library(dslabs) data(murders) Para verificar que esto es un data frame, escribimos: class(murders) #&gt; [1] &quot;data.frame&quot; 2.4.2 Cómo examinar un objeto La función str es útil para obtener más información sobre la estructura de un objeto: str(murders) #&gt; &#39;data.frame&#39;: 51 obs. of 5 variables: #&gt; $ state : chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... #&gt; $ abb : chr &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ... #&gt; $ region : Factor w/ 4 levels &quot;Northeast&quot;,&quot;South&quot;,..: 2 4 4 2 4 4 1 2 2 #&gt; 2 ... #&gt; $ population: num 4779736 710231 6392017 2915918 37253956 ... #&gt; $ total : num 135 19 232 93 1257 ... Esto nos dice mucho más sobre el objeto. Vemos que la tabla tiene 51 filas (50 estados más DC) y cinco variables. Podemos mostrar las primeras seis líneas usando la función head: head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 En este set de datos, cada estado se considera una observación y se incluyen cinco variables para cada estado. Antes de continuar con la respuesta a nuestra pregunta original sobre los diferentes estados, repasemos más sobre los componentes de este objeto. 2.4.3 El operador de acceso: $ Para nuestro análisis, necesitaremos acceso a las diferentes variables representadas por columnas incluidas en este data frame. Para hacer esto, utilizamos el operador de acceso $ de la siguiente manera: murders$population #&gt; [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 #&gt; [8] 897934 601723 19687653 9920000 1360301 1567582 12830632 #&gt; [15] 6483802 3046355 2853118 4339367 4533372 1328361 5773552 #&gt; [22] 6547629 9883640 5303925 2967297 5988927 989415 1826341 #&gt; [29] 2700551 1316470 8791894 2059179 19378102 9535483 672591 #&gt; [36] 11536504 3751351 3831074 12702379 1052567 4625364 814180 #&gt; [43] 6346105 25145561 2763885 625741 8001024 6724540 1852994 #&gt; [50] 5686986 563626 ¿Pero cómo supimos usar population? Anteriormente, aplicando la función str al objeto murders, revelamos los nombres de cada una de las cinco variables almacenadas en esta tabla. Podemos tener acceso rápido a los nombres de las variables usando: names(murders) #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; Es importante saber que el orden de las entradas en murders$population conserva el orden de las filas en nuestra tabla de datos. Esto luego nos permitirá manipular una variable basada en los resultados de otra. Por ejemplo, podremos ordenar los nombres de los estados según el número de asesinatos. Consejo: R viene con una muy buena funcionalidad de autocompletar que nos ahorra la molestia de escribir todos los nombres. Escriban murders$p y luego presionen la tecla tab en su teclado. Esta funcionalidad y muchas otras características útiles de autocompletar están disponibles en RStudio. 2.4.4 Vectores: numéricos, de caracteres y lógicos El objeto murders$population no es un número sino varios. Llamamos vectores a este tipo de objeto. Un solo número es técnicamente un vector de longitud 1, pero en general usamos el término vectores para referirnos a objetos con varias entradas. La función length les dice cuántas entradas hay en el vector: pop &lt;- murders$population length(pop) #&gt; [1] 51 Este vector particular es numérico ya que los tamaños de la población son números: class(pop) #&gt; [1] &quot;numeric&quot; En un vector numérico, cada entrada debe ser un número. Para almacenar una cadena de caracteres, los vectores también pueden ser de la clase carácter. Por ejemplo, los nombres de los estados son caracteres: class(murders$state) #&gt; [1] &quot;character&quot; Al igual que con los vectores numéricos, todas las entradas en un vector de caracteres deben ser un carácter. Otro tipo importante de vectores son los vectores lógicos. Estos deben ser TRUE o FALSE. z &lt;- 3 == 2 z #&gt; [1] FALSE class(z) #&gt; [1] &quot;logical&quot; Aquí el == es un operador relacional que pregunta si 3 es igual a 2. En R, usar solo un = asigna una variable, pero si usan dos == entonces evalúa si los objetos son iguales. Pueden ver esto al escribir: ?Comparison En futuras secciones, observarán lo útil que pueden ser los operadores relacionales. Discutimos las características más importantes de los vectores después de los siguientes ejercicios. Avanzado: Matemáticamente, los valores en pop son números enteros y hay una clase de enteros en R. Sin embargo, por defecto, a los números se les asigna una clase numérica incluso cuando son enteros redondos. Por ejemplo, class(1) devuelve numérico. Pueden convertirlo en un entero de clase con la función as.integer() o agregando un L así: 1L. Tengan en cuenta la clase escribiendo: class(1L) 2.4.5 Factores En el set de datos murders, se podría esperar que la región también sea un vector de caracteres. Sin embargo, no lo es: class(murders$region) #&gt; [1] &quot;factor&quot; Es un factor. Los factores son útiles para almacenar datos categóricos. Podemos ver que solo hay cuatro regiones al utilizar la función levels: levels(murders$region) #&gt; [1] &quot;Northeast&quot; &quot;South&quot; &quot;North Central&quot; &quot;West&quot; En el fondo, R almacena estos levels como números enteros y mantiene un mapa para llevar un registro de las etiquetas. Esto es más eficiente en terminos de memoria que almacenar todos los caracteres. Tengan en cuenta que los niveles tienen un orden diferente al orden de aparición en el factor. En R, por defecto, los niveles se ordenan alfabéticamente. Sin embargo, a menudo queremos que los niveles sigan un orden diferente. Pueden especificar un orden a través del argumento levels cuando crean el factor con la función factor. Por ejemplo, en el set de datos de asesinatos, las regiones se ordenan de este a oeste. La función ’reorder` nos permite cambiar el orden de los niveles de un factor según un resumen calculado en un vector numérico. Demostraremos esto con un ejemplo sencillo y veremos otros más avanzados en la parte de visualización de datos del libro. Supongamos que queremos los levels de la región ordenados según el número total de asesinatos en vez de por orden alfabético. Si hay valores asociados con cada level, podemos usar reorder y especificar un resumen de datos para determinar el orden. El siguiente código toma la suma del total de asesinatos en cada región y reordena el factor según estas sumas. region &lt;- murders$region value &lt;- murders$total region &lt;- reorder(region, value, FUN = sum) levels(region) #&gt; [1] &quot;Northeast&quot; &quot;North Central&quot; &quot;West&quot; &quot;South&quot; El nuevo orden concuerda con el hecho de que hay menos asesinatos en el Noreste y más en el Sur. Advertencia: Los factores pueden causar confusión ya que a veces se comportan como caracteres y otras veces no. Como resultado, estos son una fuente común de errores. 2.4.6 Listas Los data frames son un caso especial de listas. Cubriremos las listas con más detalle más adelante, pero sabemos que son útiles porque pueden almacenar cualquier combinación de diferentes tipos de datos. Aquí creamos una lista para servir de ejemplo: record #&gt; $name #&gt; [1] &quot;John Doe&quot; #&gt; #&gt; $student_id #&gt; [1] 1234 #&gt; #&gt; $grades #&gt; [1] 95 82 91 97 93 #&gt; #&gt; $final_grade #&gt; [1] &quot;A&quot; class(record) #&gt; [1] &quot;list&quot; Al igual que con los data frames, pueden extraer los componentes de una lista con el operador de acceso: $. De hecho, los data frames son un tipo de lista. record$student_id #&gt; [1] 1234 También podemos usar corchetes dobles ( [[) así: record[[&quot;student_id&quot;]] #&gt; [1] 1234 Deben acostumbrarse al hecho de que en R a menudo hay varias formas de hacer lo mismo, como tener acceso a las entradas. También pueden encontrar listas sin nombres de variables: record2 #&gt; [[1]] #&gt; [1] &quot;John Doe&quot; #&gt; #&gt; [[2]] #&gt; [1] 1234 Si una lista no tiene nombres, no pueden extraer los elementos con $, pero pueden usar el método de corchetes. En vez de proveer el nombre de la variable, pueden proveer el índice de la lista de la siguiente manera: record2[[1]] #&gt; [1] &quot;John Doe&quot; No usaremos listas hasta más tarde, pero es posible que encuentren una en sus propias exploraciones de R. Por eso, les mostramos algunos conceptos básicos aquí. 2.4.7 Matrices Las matrices son otro tipo de objeto común en R. Las matrices son similares a los data frames en que son bidimensionales: tienen filas y columnas. Sin embargo, al igual que los vectores numéricos, de caracteres y lógicos, las entradas en las matrices deben ser del mismo tipo. Por esta razón, los data frames son mucho más útiles para almacenar datos, ya que podemos tener caracteres, factores y números en ellos. No obstante, las matrices tienen una gran ventaja sobre los data frames: podemos realizar operaciones de álgebra de matrices, una técnica matemática poderosa. No describimos estas operaciones en este libro, pero gran parte de lo que sucede en segundo plano cuando realiza un análisis de datos involucra matrices. Cubrimos las matrices con más detalle en el Capítulo ?? pero también las discutimos brevemente aquí, ya que algunas de las funciones que aprenderemos devuelven matrices. Podemos definir una matriz usando la función matrix. Necesitamos especificar el número de filas y columnas: mat &lt;- matrix(1:12, 4, 3) mat #&gt; [,1] [,2] [,3] #&gt; [1,] 1 5 9 #&gt; [2,] 2 6 10 #&gt; [3,] 3 7 11 #&gt; [4,] 4 8 12 Pueden acceder a entradas específicas en una matriz usando corchetes ( [). Si desean la segunda fila, tercera columna, usen: mat[2, 3] #&gt; [1] 10 Si desean toda la segunda fila, dejen vacío el lugar de la columna: mat[2, ] #&gt; [1] 2 6 10 Tengan en cuenta que esto devuelve un vector, no una matriz. Del mismo modo, si desean la tercera columna completa, dejen el lugar de la fila vacío: mat[, 3] #&gt; [1] 9 10 11 12 Esto también es un vector, no una matriz. Pueden acceder a más de una columna o más de una fila si lo desean. Esto les dará una nueva matriz. mat[, 2:3] #&gt; [,1] [,2] #&gt; [1,] 5 9 #&gt; [2,] 6 10 #&gt; [3,] 7 11 #&gt; [4,] 8 12 Pueden crear subconjuntos basados tanto en las filas como en las columnas: mat[1:2, 2:3] #&gt; [,1] [,2] #&gt; [1,] 5 9 #&gt; [2,] 6 10 Podemos convertir las matrices en data frames usando la función as.data.frame: as.data.frame(mat) #&gt; V1 V2 V3 #&gt; 1 1 5 9 #&gt; 2 2 6 10 #&gt; 3 3 7 11 #&gt; 4 4 8 12 También podemos usar corchetes individuales ( [) para acceder a las filas y las columnas de un data frame: data(&quot;murders&quot;) murders[25, 1] #&gt; [1] &quot;Mississippi&quot; murders[2:3, ] #&gt; state abb region population total #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 2.5 Ejercicios 1. Cargue el set de datos de asesinatos de EE. UU.. library(dslabs) data(murders) Use la función str para examinar la estructura del objeto murders. ¿Cuál de las siguientes opciones describe mejor las variables representadas en este data frame? Los 51 estados. Las tasas de asesinatos para los 50 estados y DC. El nombre del estado, la abreviatura del nombre del estado, la región del estado y la población y el número total de asesinatos para 2010 del estado. str no muestra información relevante. 2. ¿Cuáles son los nombres de las columnas utilizados por el data frame para estas cinco variables? 3. Use el operador de acceso $ para extraer las abreviaturas de los estados y asignarlas al objeto a. ¿Cuál es la clase de este objeto? 4. Ahora use los corchetes para extraer las abreviaturas de los estados y asignarlas al objeto b. Utilice la función identical para determinar si a y b son iguales. 5. Vimos que la columna region almacena un factor. Puede corroborar esto escribiendo: class(murders$region) Con una línea de código, use las funciones levels y length para determinar el número de regiones definidas por este set de datos. 6. La función table toma un vector y devuelve la frecuencia de cada elemento. Puede ver rápidamente cuántos estados hay en cada región aplicando esta función. Use esta función en una línea de código para crear una tabla de estados por región. 2.6 Vectores En R, los objetos más básicos disponibles para almacenar datos son vectores. Como hemos visto, los sets de datos complejos generalmente se pueden dividir en componentes que son vectores. Por ejemplo, en un data frame, cada columna es un vector. Aquí aprendemos más sobre esta clase importante. 2.6.1 Cómo crear vectores Podemos crear vectores usando la función c, que significa concatenar. Usamos c para concatenar entradas de la siguiente manera: codes &lt;- c(380, 124, 818) codes #&gt; [1] 380 124 818 También podemos crear vectores de caracteres. Usamos las comillas para denotar que las entradas son caracteres en lugar de nombres de variables. country &lt;- c(&quot;italy&quot;, &quot;canada&quot;, &quot;egypt&quot;) En R también pueden usar comillas sencillas: country &lt;- c(&#39;italy&#39;, &#39;canada&#39;, &#39;egypt&#39;) Pero tengan cuidado de no confundir la comilla sencilla ’ con el back quote `. A estas alturas ya deberían saber que si escriben: country &lt;- c(italy, canada, egypt) reciben un error porque las variables italy, canada y egypt no están definidas. Si no usamos las comillas, R busca variables con esos nombres y devuelve un error. 2.6.2 Nombres A veces es útil nombrar las entradas de un vector. Por ejemplo, al definir un vector de códigos de paises, podemos usar los nombres para conectar los dos: codes &lt;- c(italy = 380, canada = 124, egypt = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 El objeto codes sigue siendo un vector numérico: class(codes) #&gt; [1] &quot;numeric&quot; pero con nombres: names(codes) #&gt; [1] &quot;italy&quot; &quot;canada&quot; &quot;egypt&quot; Si el uso de cadenas sin comillas parece confuso, sepan que también pueden usar las comillas: codes &lt;- c(&quot;italy&quot; = 380, &quot;canada&quot; = 124, &quot;egypt&quot; = 818) codes #&gt; italy canada egypt #&gt; 380 124 818 No hay diferencia entre esta llamada a función (function call en inglés) y la anterior. Esta es una de las muchas formas en que R es peculiar en comparación con otros lenguajes. También podemos asignar nombres usando la función names: codes &lt;- c(380, 124, 818) country &lt;- c(&quot;italy&quot;,&quot;canada&quot;,&quot;egypt&quot;) names(codes) &lt;- country codes #&gt; italy canada egypt #&gt; 380 124 818 2.6.3 Secuencias Otra función útil para crear vectores genera secuencias: seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 El primer argumento define el inicio y el segundo define el final que incluimos. El valor por defecto es subir en incrementos de 1, pero un tercer argumento nos permite determinar cuánto saltar: seq(1, 10, 2) #&gt; [1] 1 3 5 7 9 Si queremos enteros consecutivos, podemos usar la siguiente taquigrafía: 1:10 #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Cuando usamos estas funciones, R produce números enteros, no numéricos, porque generalmente se usan para indexar algo: class(1:10) #&gt; [1] &quot;integer&quot; Sin embargo, si creamos una secuencia que incluye no enteros, la clase cambia: class(seq(1, 10, 0.5)) #&gt; [1] &quot;numeric&quot; 2.6.4 Cómo crear un subconjunto Usamos los corchetes para tener acceso a elementos específicos de un vector. Para el vector codes que definimos anteriormente, podemos tener acceso al segundo elemento usando: codes[2] #&gt; canada #&gt; 124 Pueden obtener más de una entrada utilizando un vector de entradas múltiples como índice: codes[c(1,3)] #&gt; italy egypt #&gt; 380 818 Las secuencias definidas anteriormente son particularmente útiles si necesitamos acceso, digamos, a los dos primeros elementos: codes[1:2] #&gt; italy canada #&gt; 380 124 Si los elementos tienen nombres, también podemos acceder a las entradas utilizando estos nombres. A continuación ofrecemos dos ejemplos: codes[&quot;canada&quot;] #&gt; canada #&gt; 124 codes[c(&quot;egypt&quot;,&quot;italy&quot;)] #&gt; egypt italy #&gt; 818 380 2.7 La conversión forzada En general, la conversión forzada (coercion en inglés) es un intento de R de ser flexible con los tipos de datos. Cuando una entrada no coincide con lo esperado, algunas de las funciones predefinidas de R tratan de adivinar lo que uno intentaba antes de devolver un mensaje de error. Esto también puede causar confusión. Al no entender la conversión forzada, los programadores pueden volverse locos cuando codifican en R, ya que R se comporta de manera bastante diferente a la mayoría de los otros idiomas en cuanto a esto. Aprendamos más con unos ejemplos. Dijimos que los vectores deben ser todos del mismo tipo. Entonces, si tratamos de combinar, por ejemplo, números y caracteres, pueden esperar un error: x &lt;- c(1, &quot;canada&quot;, 3) ¡Pero no nos da uno, ni siquiera una advertencia! ¿Que pasó? Mire x y su clase: x #&gt; [1] &quot;1&quot; &quot;canada&quot; &quot;3&quot; class(x) #&gt; [1] &quot;character&quot; R forzó una conversión de los datos a caracteres. Como pusimos una cadena de caracteres en el vector, R adivinó que nuestra intención era que el 1 y el 3 fueran las cadenas de caracteres &quot;1&quot; y “3”. El hecho de que ni siquiera emitiera una advertencia es un ejemplo de cómo la conversión forzada puede causar muchos errores inadvertidos en R. R también ofrece funciones para cambiar de un tipo a otro. Por ejemplo, pueden convertir números en caracteres con: x &lt;- 1:5 y &lt;- as.character(x) y #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; Pueden revertir a lo anterior con as.numeric: as.numeric(y) #&gt; [1] 1 2 3 4 5 Esta función es muy útil ya que los sets de datos que incluyen números como cadenas de caracteres son comunes. 2.7.1 Not available (NA) Cuando una función intenta forzar una conversión de un tipo a otro y encuentra un caso imposible, generalmente nos da una advertencia y convierte la entrada en un valor especial llamado NA que significa no disponible (Not Available en inglés). Por ejemplo: x &lt;- c(&quot;1&quot;, &quot;b&quot;, &quot;3&quot;) as.numeric(x) #&gt; Warning: NAs introducidos por coerción #&gt; [1] 1 NA 3 R no sabe el número que querían cuando escribieron b y no lo intenta adivinar. Como científicos de datos, se encontrarán con el NA frecuentemente ya que se usa generalmente para datos faltantes (missing data en inglés), un problema común en los sets de datos del mundo real. 2.8 Ejercicios 1. Use la función c para crear un vector con las temperaturas altas promedio en enero para Beijing, Lagos, París, Río de Janeiro, San Juan y Toronto, que son 35, 88, 42, 84, 81 y 30 grados Fahrenheit. Llame al objeto temp. 2. Ahora cree un vector con los nombres de las ciudades y llame al objeto city. 3. Utilice la función names y los objetos definidos en los ejercicios anteriores para asociar los datos de temperatura con su ciudad correspondiente. 4. Utilice los operadores [ y : para acceder a la temperatura de las tres primeras ciudades de la lista. 5. Utilice el operador [ para acceder a la temperatura de París y San Juan. 6. Utilice el operador : para crear una secuencia de números \\(12,13,14,\\dots,73\\). 7. Cree un vector que contenga todos los números impares positivos menores que 100. 8. Cree un vector de números que comience en 6, no pase 55 y agregue números en incrementos de 4/7: 6, 6 + 4/7, 6 + 8/7, y así sucesivamente. ¿Cuántos números tiene la lista? Sugerencia: use seq y length. 9. ¿Cuál es la clase del siguiente objeto? a &lt;- seq(1, 10, 0.5)? 10. ¿Cuál es la clase del siguiente objeto? a &lt;- seq(1, 10)? 11. La clase de class(a&lt;-1) es numérica, no entero. R por defecto es numérico y para forzar un número entero, debe agregar la letra L. Confirme que la clase de 1L es entero. 12. Defina el siguiente vector: x &lt;- c(&quot;1&quot;, &quot;3&quot;, &quot;5&quot;) y oblíguelo a obtener enteros. 2.9 Sorting Ahora que hemos dominado algunos conocimientos básicos de R, intentemos obtener algunos conocimientos sobre la seguridad de los distintos estados en el contexto de los asesinatos con armas de fuego. 2.9.1 sort Digamos que queremos clasificar los estados desde el menor hasta el mayor según los asesinatos con armas de fuego. La función sort ordena un vector en orden creciente. Por lo tanto, podemos ver la mayor cantidad de asesinatos con armas escribiendo: library(dslabs) data(murders) sort(murders$total) #&gt; [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 #&gt; [14] 27 32 36 38 53 63 65 67 84 93 93 97 97 #&gt; [27] 99 111 116 118 120 135 142 207 219 232 246 250 286 #&gt; [40] 293 310 321 351 364 376 413 457 517 669 805 1257 Sin embargo, esto no nos da información sobre qué estados tienen qué total de asesinatos. Por ejemplo, no sabemos qué estado tuvo 1257. 2.9.2 order La función order es mas apropiada para lo que queremos hacer. order toma un vector como entrada, y devuelve el vector de índices que clasifica el vector de entrada. Esto puede ser un poco confuso, así que estudiemos un ejemplo sencillo. Podemos crear un vector y ordenarlo (sort en inglés): x &lt;- c(31, 4, 15, 92, 65) sort(x) #&gt; [1] 4 15 31 65 92 En lugar de ordenar el vector de entrada, la función order devuelve el índice que ordena el vector de entrada: index &lt;- order(x) x[index] #&gt; [1] 4 15 31 65 92 Este es el mismo resultado que le devuelve sort(x). Si miramos este índice, vemos por qué funciona: x #&gt; [1] 31 4 15 92 65 order(x) #&gt; [1] 2 3 1 5 4 La segunda entrada de x es la más pequeña, entonces order(x) comienza con 2. La siguiente más pequeña es la tercera entrada, por lo que la segunda entrada es 3 y así sigue. ¿Cómo nos ayuda esto a ordenar los estados por asesinatos? Primero, recuerden que las entradas de vectores a las que acceden con $ siguen el mismo orden que las filas de la tabla. Por ejemplo, estos dos vectores que contienen el nombre de los estados y sus abreviaturas, respectivamente, siguen el mismo orden: murders$state[1:6] #&gt; [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; &quot;California&quot; #&gt; [6] &quot;Colorado&quot; murders$abb[1:6] #&gt; [1] &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; &quot;CA&quot; &quot;CO&quot; Esto significa que podemos ordenar los nombres de estado según el total de asesinatos. Primero obtenemos el índice que ordena los vectores por el total de asesinatos y luego ponemos el vector de nombres de estado en un índice: ind &lt;- order(murders$total) murders$abb[ind] #&gt; [1] &quot;VT&quot; &quot;ND&quot; &quot;NH&quot; &quot;WY&quot; &quot;HI&quot; &quot;SD&quot; &quot;ME&quot; &quot;ID&quot; &quot;MT&quot; &quot;RI&quot; &quot;AK&quot; &quot;IA&quot; &quot;UT&quot; #&gt; [14] &quot;WV&quot; &quot;NE&quot; &quot;OR&quot; &quot;DE&quot; &quot;MN&quot; &quot;KS&quot; &quot;CO&quot; &quot;NM&quot; &quot;NV&quot; &quot;AR&quot; &quot;WA&quot; &quot;CT&quot; &quot;WI&quot; #&gt; [27] &quot;DC&quot; &quot;OK&quot; &quot;KY&quot; &quot;MA&quot; &quot;MS&quot; &quot;AL&quot; &quot;IN&quot; &quot;SC&quot; &quot;TN&quot; &quot;AZ&quot; &quot;NJ&quot; &quot;VA&quot; &quot;NC&quot; #&gt; [40] &quot;MD&quot; &quot;OH&quot; &quot;MO&quot; &quot;LA&quot; &quot;IL&quot; &quot;GA&quot; &quot;MI&quot; &quot;PA&quot; &quot;NY&quot; &quot;FL&quot; &quot;TX&quot; &quot;CA&quot; De acuerdo con lo anterior, California tuvo la mayor cantidad de asesinatos. 2.9.3 max y which.max Si solo estamos interesados en la entrada con el mayor valor, podemos usar max para ese valor: max(murders$total) #&gt; [1] 1257 y which.max para el índice del valor mayor: i_max &lt;- which.max(murders$total) murders$state[i_max] #&gt; [1] &quot;California&quot; Como mínimo, podemos usar min y which.min del mismo modo. ¿Esto significa que California es el estado más peligroso? En una próxima sección, planteamos que deberíamos considerar las tasas en lugar de los totales. Antes de hacer eso, presentamos una última función relacionada con el orden: rank. 2.9.4 rank Aunque no se usa con tanta frecuencia como order y sort, la función rank también está relacionada con el orden y puede ser útil. Para cualquier vector dado, rank devuelve un vector con el rango de la primera entrada, segunda entrada, etc., del vector de entrada. Aquí tenemos un ejemplo sencillo: x &lt;- c(31, 4, 15, 92, 65) rank(x) #&gt; [1] 3 1 2 5 4 Para resumir, veamos los resultados de las tres funciones que hemos discutido: original sort order rank 31 4 2 3 4 15 3 1 15 31 1 2 92 65 5 5 65 92 4 4 2.9.5 Cuidado con el reciclaje Otra fuente común de errores inadvertidos en R es el uso de reciclaje (recycling en inglés). Hemos visto como los vectores se agregan por elementos. Entonces, si los vectores no coinciden en longitud, es natural suponer que obtendramos un error. Pero ese no es el caso. Vean lo que pasa: x &lt;- c(1,2,3) y &lt;- c(10, 20, 30, 40, 50, 60, 70) x+y #&gt; Warning in x + y: longitud de objeto mayor no es múltiplo de la longitud #&gt; de uno menor #&gt; [1] 11 22 33 41 52 63 71 Recibimos una advertencia, pero no hay error. Para el output, R ha reciclado los números en x. Observen el último dígito de números en el output. 2.10 Ejercicios Para estos ejercicios usaremos el set de datos de asesinatos de EE. UU.. Asegúrese de cargarlo antes de comenzar. library(dslabs) data(&quot;murders&quot;) 1. Utilice el operador $ para acceder a los datos del tamaño de la población y almacenarlos como el objeto pop. Luego use la función sort para redefinir pop para que esté en orden alfabético. Finalmente, use el operador [ para indicar el tamaño de población más pequeño. 2. Ahora, en lugar del tamaño de población más pequeño, encuentre el índice de la entrada con el tamaño de población más pequeño. Sugerencia: use order en lugar de sort. 3. Podemos realizar la misma operación que en el ejercicio anterior usando la función which.min. Escriba una línea de código que haga esto. 4. Ahora sabemos cuán pequeño es el estado más pequeño y qué fila lo representa. ¿Qué estado es? Defina una variable states para ser los nombres de los estados del data frame murders. Indique el nombre del estado con la población más pequeña. 5. Puede crear un data frame utilizando la función data.frame. Aquí un ejemplo: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Utilice la función rank para determinar el rango de población de cada estado desde el menos poblado hasta el más poblado. Guarde estos rangos en un objeto llamado ranks, luego cree un data frame con el nombre del estado y su rango. Nombre el data frame my_df. 6. Repita el ejercicio anterior, pero esta vez ordene my_df para que los estados estén en orden de menos poblado a más poblado. Sugerencia: cree un objeto ind que almacene los índices necesarios para poner en orden los valores de la población. Luego use el operador de corchete [ para reordenar cada columna en el data frame. 7. El vector na_example representa una serie de conteos. Puede examinar rápidamente el objeto usando: data(&quot;na_example&quot;) str(na_example) #&gt; int [1:1000] 2 1 3 2 1 3 1 4 3 2 ... Sin embargo, cuando calculamos el promedio con la función mean, obtenemos un NA: mean(na_example) #&gt; [1] NA La función is.na devuelve un vector lógico que nos dice qué entradas son NA. Asigne este vector lógico a un objeto llamado ind y determine cuántos NAs tiene na_example. 8. Ahora calcule nuevamente el promedio, pero solo para las entradas que no son NA. Sugerencia: recuerde el operador !. 2.11 Aritmética de vectores California tuvo la mayor cantidad de asesinatos, pero ¿esto significa que es el estado más peligroso? ¿Qué pasa si solo tiene muchas más personas que cualquier otro estado? Podemos confirmar rápidamente que California tiene la mayor población: library(dslabs) data(&quot;murders&quot;) murders$state[which.max(murders$population)] #&gt; [1] &quot;California&quot; con más de 37 millones de habitantes. Por lo tanto, es injusto comparar los totales si estamos interesados en saber cuán seguro es el estado. Lo que realmente deberíamos calcular son los asesinatos per cápita. Los informes que describimos en la sección motivante utilizan asesinatos por cada 100,000 como la unidad. Para calcular esta cantidad, usamos las poderosas capacidades aritméticas de vectores de R. 2.11.1 Rescaling un vector En R, las operaciones aritméticas en vectores ocurren elemento por elemento. Como ejemplo, supongamos que tenemos la altura en pulgadas (inches en inglés): inches &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70) y queremos convertirla a centímetros. Observen lo que sucede cuando multiplicamos inches por 2.54: inches * 2.54 #&gt; [1] 175 157 168 178 178 185 170 185 170 178 Arriba, multiplicamos cada elemento por 2.54. Del mismo modo, si para cada entrada queremos calcular cuántas pulgadas más alto, o cuántas más corto, que 69 pulgadas (la altura promedio para hombres), podemos restarlo de cada entrada de esta manera: inches - 69 #&gt; [1] 0 -7 -3 1 1 4 -2 4 -2 1 2.11.2 Dos vectores Si tenemos dos vectores de la misma longitud y los sumamos en R, se agregarán entrada por entrada de la siguiente manera: \\[ \\begin{pmatrix} a\\\\ b\\\\ c\\\\ d \\end{pmatrix} + \\begin{pmatrix} e\\\\ f\\\\ g\\\\ h \\end{pmatrix} = \\begin{pmatrix} a +e\\\\ b + f\\\\ c + g\\\\ d + h \\end{pmatrix} \\] Lo mismo aplica para otras operaciones matemáticas, como -, * y /. Esto implica que para calcular las tasas de asesinatos (murder rates en inglés) simplemente podemos escribir: murder_rate &lt;- murders$total/ murders$population * 100000 Al hacer esto, notamos que California ya no está cerca de la parte superior de la lista. De hecho, podemos usar lo que hemos aprendido para poner a los estados en orden por tasa de asesinatos: murders$abb[order(murder_rate)] #&gt; [1] &quot;VT&quot; &quot;NH&quot; &quot;HI&quot; &quot;ND&quot; &quot;IA&quot; &quot;ID&quot; &quot;UT&quot; &quot;ME&quot; &quot;WY&quot; &quot;OR&quot; &quot;SD&quot; &quot;MN&quot; &quot;MT&quot; #&gt; [14] &quot;CO&quot; &quot;WA&quot; &quot;WV&quot; &quot;RI&quot; &quot;WI&quot; &quot;NE&quot; &quot;MA&quot; &quot;IN&quot; &quot;KS&quot; &quot;NY&quot; &quot;KY&quot; &quot;AK&quot; &quot;OH&quot; #&gt; [27] &quot;CT&quot; &quot;NJ&quot; &quot;AL&quot; &quot;IL&quot; &quot;OK&quot; &quot;NC&quot; &quot;NV&quot; &quot;VA&quot; &quot;AR&quot; &quot;TX&quot; &quot;NM&quot; &quot;CA&quot; &quot;FL&quot; #&gt; [40] &quot;TN&quot; &quot;PA&quot; &quot;AZ&quot; &quot;GA&quot; &quot;MS&quot; &quot;MI&quot; &quot;DE&quot; &quot;SC&quot; &quot;MD&quot; &quot;MO&quot; &quot;LA&quot; &quot;DC&quot; 2.12 Ejercicios 1. Anteriormente creamos este data frame: temp &lt;- c(35, 88, 42, 84, 81, 30) city &lt;- c(&quot;Beijing&quot;, &quot;Lagos&quot;, &quot;Paris&quot;, &quot;Rio de Janeiro&quot;, &quot;San Juan&quot;, &quot;Toronto&quot;) city_temps &lt;- data.frame(name = city, temperature = temp) Vuelva a crear el data frame utilizando el código anterior, pero agregue una línea que convierta la temperatura de Fahrenheit a Celsius. La conversión es \\(C = \\frac{5}{9} \\times (F - 32)\\). 2. ¿Cuál es la siguiente suma? \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Sugerencia: gracias a Euler, sabemos que debería estar cerca de \\(\\pi^2/6\\). 3. Calcule la tasa de asesinatos por cada 100,000 para cada estado y almacénela en el objeto murder_rate. Luego calcule la tasa promedio de asesinatos para EE. UU. con la función mean. ¿Cuál es el promedio? 2.13 Indexación R provee una forma poderosa y conveniente de indexar vectores. Podemos, por ejemplo, crear un subconjunto de un vector según las propiedades de otro vector. En esta sección, continuamos trabajando con nuestro ejemplo de asesinatos en EE. UU., que podemos cargar así: library(dslabs) data(&quot;murders&quot;) 2.13.1 Crear subconjuntos con lógicos Ahora hemos calculado la tasa de asesinatos usando: murder_rate &lt;- murders$total/ murders$population * 100000 Imagine que se muda de Italia donde, según un informe de noticias, la tasa de asesinatos es solo 0.71 por 100,000. Preferiría mudarse a un estado con una tasa de homicidios similar. Otra característica poderosa de R es que podemos usar lógicas para indexar vectores. Si comparamos un vector con un solo número, R realiza la prueba para cada entrada. Aquí tenemos un ejemplo relacionado con la pregunta anterior: ind &lt;- murder_rate &lt; 0.71 Si en cambio queremos saber si un valor es menor o igual, podemos usar: ind &lt;- murder_rate &lt;= 0.71 Recuerden que recuperamos un vector lógico con TRUE para cada entrada menor o igual a 0.71. Para ver qué estados son estos, podemos aprovechar el hecho de que los vectores se pueden indexar con lógicos. murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Iowa&quot; &quot;New Hampshire&quot; &quot;North Dakota&quot; #&gt; [5] &quot;Vermont&quot; Para contar cuántos son TRUE, la función sum devuelve la suma de las entradas de un vector y fuerza una conversión de los vectores lógicos a numéricos con TRUE codificado como 1 y FALSE como 0. Así podemos contar los estados usando: sum(ind) #&gt; [1] 5 2.13.2 Operadores lógicos Supongamos que nos gustan las montañas y queremos mudarnos a un estado seguro en la región occidental del país. Queremos que la tasa de asesinatos sea como máximo 1. En este caso, queremos que dos cosas diferentes sean ciertas. Aquí podemos usar el operador lógico and, que en R se representa con &amp;. Esta operación da como resultado TRUE solo cuando ambas lógicas son TRUE, es decir ciertas. Para ver esto, considere este ejemplo: TRUE &amp; TRUE #&gt; [1] TRUE TRUE &amp; FALSE #&gt; [1] FALSE FALSE &amp; FALSE #&gt; [1] FALSE Para nuestro ejemplo, podemos formar dos lógicos: west &lt;- murders$region == &quot;West&quot; safe &lt;- murder_rate &lt;= 1 y podemos usar el &amp; para obtener un vector lógico que nos dice qué estados satisfacen ambas condiciones: ind &lt;- safe &amp; west murders$state[ind] #&gt; [1] &quot;Hawaii&quot; &quot;Idaho&quot; &quot;Oregon&quot; &quot;Utah&quot; &quot;Wyoming&quot; 2.13.3 which Supongamos que queremos ver la tasa de asesinatos de California. Para este tipo de operación, es conveniente convertir vectores lógicos en índices en lugar de mantener vectores lógicos largos. La función which nos dice qué entradas de un vector lógico son TRUE. Entonces podemos escribir: ind &lt;- which(murders$state == &quot;California&quot;) murder_rate[ind] #&gt; [1] 3.37 2.13.4 match Si en lugar de un solo estado queremos averiguar las tasas de homicidio de varios estados, digamos Nueva York, Florida y Texas, podemos usar la función match. Esta función nos dice qué índices de un segundo vector coinciden con cada una de las entradas de un primer vector: ind &lt;- match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) ind #&gt; [1] 33 10 44 Ahora podemos ver las tasas de asesinatos: murder_rate[ind] #&gt; [1] 2.67 3.40 3.20 2.13.5 %in% Si en lugar de un índice queremos un lógico que nos diga si cada elemento de un primer vector está en un segundo vector, podemos usar la función %in%. Imaginemos que no están seguros si Boston, Dakota y Washington son estados. Pueden averiguar así: c(&quot;Boston&quot;, &quot;Dakota&quot;, &quot;Washington&quot;) %in% murders$state #&gt; [1] FALSE FALSE TRUE Tengan en cuenta que estaremos usando %in% frecuentemente a lo largo del libro. Avanzado: Hay una conexión entre match y %in% mediante which. Para ver esto, observen que las siguientes dos líneas producen el mismo índice (aunque en orden diferente): match(c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;), murders$state) #&gt; [1] 33 10 44 which(murders$state%in%c(&quot;New York&quot;, &quot;Florida&quot;, &quot;Texas&quot;)) #&gt; [1] 10 33 44 2.14 Ejercicios Empiece cargando el paquete y los datos. library(dslabs) data(murders) 1. Calcule la tasa de asesinatos por cada 100,000 para cada estado y almacénela en un objeto llamado murder_rate. Luego use operadores lógicos para crear un vector lógico llamado low que nos dice qué entradas de murder_rate son inferiores a 1. 2. Ahora use los resultados del ejercicio anterior y la función which para determinar los índices de murder_rate asociados con valores inferiores a 1. 3. Use los resultados del ejercicio anterior para indicar los nombres de los estados con tasas de asesinatos inferiores a 1. 4. Ahora extienda el código de los ejercicios 2 y 3 para indicar los estados del noreste con tasas de homicidios inferiores a 1. Sugerencia: use el vector lógico predefinido low y el operador lógico &amp;. 5. En un ejercicio anterior, calculamos la tasa de homicidios para cada estado y el promedio de estos números. ¿Cuántos estados están por debajo del promedio? 6. Use la función match para identificar los estados con abreviaturas AK, MI e IA. Sugerencia: comience definiendo un índice de las entradas de murders$abb que coinciden con las tres abreviaturas. Entonces use el operador [ para extraer los estados. 7. Utilice el operador %in% para crear un vector lógico que responda a la pregunta: ¿cuáles de las siguientes son abreviaturas reales: MA, ME, MI, MO, MU? 8. Extienda el código que usó en el ejercicio 7 para averiguar la única entrada que no es una abreviatura real. Sugerencia: use el operador !, que convierte FALSE a TRUE y viceversa, y entonces which para obtener un índice. 2.15 Gráficos básicos En el capitulo 7 describimos un paquete complementario que ofrece un enfoque poderoso para producir gráficos (plots en inglés) en R. Luego tenemos una parte completa, “Visualización de datos”, en la que ofrecemos muchos ejemplos. Aquí describimos brevemente algunas de las funciones disponibles en una instalación básica de R. 2.15.1 plot La función plot se puede utilizar para hacer diagramas de dispersión (scatterplots en inglés). Aquí hay un gráfico de total de asesinatos versus población. x &lt;- murders$population/ 10^6 y &lt;- murders$total plot(x, y) Para crear un gráfico rápido que no accede a las variables dos veces, podemos usar la función with: with(murders, plot(population, total)) La función with nos permite usar los nombres en la columna murders en la función plot. También funciona con cualquier data frame y cualquier función. 2.15.2 hist x &lt;- with(murders, total/ population * 100000) hist(x) Podemos ver que hay una amplia gama de valores con la mayoría de ellos entre 2 y 3 y un caso muy extremo con una tasa de asesinatos de más de 15: murders$state[which.max(x)] #&gt; [1] &quot;District of Columbia&quot; 2.15.3 boxplot Los diagramas de caja (boxplots en inglés) también se describirán en la parte del libro “Visualización de datos”. Estos proveen un resumen más conciso que los histogramas, pero son más fáciles de apilar con otros diagramas de caja. Por ejemplo, aquí podemos usarlos para comparar las diferentes regiones: murders$rate &lt;- with(murders, total/ population * 100000) boxplot(rate~region, data = murders) Podemos ver que el Sur tiene tasas de asesinatos más altas que las otras tres regiones. 2.15.4 image La función image muestra los valores en una matriz usando color. Aquí mostramos un ejemplo rápido: x &lt;- matrix(1:120, 12, 10) image(x) 2.16 Ejercicios 1. Hicimos un gráfico de asesinatos totales versus población y notamos una fuerte relación. No es sorprendente que los estados con poblaciones más grandes hayan tenido más asesinatos. library(dslabs) data(murders) population_in_millions &lt;- murders$population/10^6 total_gun_murders &lt;- murders$total plot(population_in_millions, total_gun_murders) Recuerden que muchos estados tienen poblaciones inferiores a 5 millones y están agrupados. Podemos obtener más información al hacer este gráfico en la escala logarítmica. Transforme las variables usando la transformación log10 y luego cree un gráfico de los resultados. 2. Cree un histograma de las poblaciones estatales. 3. Genere diagramas de caja de las poblaciones estatales por región. https://rstudio.cloud↩ https://rafalab.github.io/dsbook/installing-r-rstudio.html↩ http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩ http://adv-r.had.co.nz/Style.html↩ "],
["conceptos-básicos-de-programación.html", "Capítulo 3 Conceptos básicos de programación 3.1 Expresiones condicionales 3.2 Cómo definir funciones 3.3 Namespaces 3.4 Bucles-for 3.5 Vectorización y funcionales 3.6 Ejercicios", " Capítulo 3 Conceptos básicos de programación Enseñamos R porque facilita enormemente el análisis de datos, el tema principal de este libro. Al codificar en R, podemos eficientemente realizar análisis de datos exploratorios, construir canales de análisis de datos y preparar la visualización de datos para comunicar los resultados. Sin embargo, R no es solo un ambiente de análisis de datos sino un lenguaje de programación. Los programadores avanzados de R pueden desarrollar paquetes complejos e incluso mejorar R, aunque no cubrimos la programación avanzada en este libro. No obstante, en esta sección, presentamos tres conceptos claves de programación: las expresiones condicionales, los bucles-for (for-loops en inglés) y las funciones. Estos no son solo componentes básicos claves para la programación avanzada, sino que a veces son útiles durante el análisis de datos. También notamos que hay varias funciones que se usan ampliamente para programar en R pero que no discutiremos en este libro. Éstos incluyen split, cut, do.call y Reduce, así como el paquete data.table. Vale la pena aprender como usarlos si quieren convertirse en programadores expertos de R. 3.1 Expresiones condicionales Las expresiones condicionales son una de las características básicas de la programación. Se utilizan para lo que se denomina flow control. La expresión condicional más común es la declaración if-else. En R, podemos realizar mucho análisis de datos sin condicionales. Sin embargo, aparecen ocasionalmente y los necesitarán una vez comiencen a escribir sus propias funciones y paquetes. Aquí presentamos un ejemplo muy sencillo que muestra la estructura general de una instrucción if-else. La idea básica es imprimir el recíproco de a a menos que a sea 0: a &lt;- 0 if(a!=0){ print(1/a) } else{ print(&quot;No reciprocal for 0.&quot;) } #&gt; [1] &quot;No reciprocal for 0.&quot; Veamos otro ejemplo usando el set de datos de asesinatos de EE. UU.: library(dslabs) data(murders) murder_rate &lt;- murders$total/ murders$population*100000 Aquí ofrecemos un ejemplo muy sencillo que nos dice qué estados, si los hay, tienen una tasa de homicidios inferior a 0.5 por 100,000. Las declaraciones if nos protegen del caso en el que ningún estado satisface la condición. ind &lt;- which.min(murder_rate) if(murder_rate[ind] &lt; 0.5){ print(murders$state[ind]) } else{ print(&quot;No state has murder rate that low&quot;) } #&gt; [1] &quot;Vermont&quot; Si lo intentamos nuevamente con una tasa de 0.25, obtenemos una respuesta diferente: if(murder_rate[ind] &lt; 0.25){ print(murders$state[ind]) } else{ print(&quot;No state has a murder rate that low.&quot;) } #&gt; [1] &quot;No state has a murder rate that low.&quot; Una función relacionada que es muy útil es ifelse. Esta función toma tres argumentos: un lógico y dos posibles respuestas. Si el lógico es TRUE, devuelve el valor en el segundo argumento, y si es FALSE, devuelve el valor en el tercer argumento. Aquí tenemos un ejemplo: a &lt;- 0 ifelse(a &gt; 0, 1/a, NA) #&gt; [1] NA Esta función es particularmente útil porque sirve para vectores. Esta examina cada entrada del vector lógico y devuelve elementos del vector proporcionados en el segundo argumento, si la entrada es TRUE, o elementos del vector proporcionados en el tercer argumento, si la entrada es FALSE. a &lt;- c(0, 1, 2, -4, 5) result &lt;- ifelse(a &gt; 0, 1/a, NA) Esta tabla nos ayuda a ver qué sucedió: a is_a_positive answer1 answer2 result 0 FALSE Inf NA NA 1 TRUE 1.00 NA 1.0 2 TRUE 0.50 NA 0.5 -4 FALSE -0.25 NA NA 5 TRUE 0.20 NA 0.2 Aquí hay un ejemplo de cómo esta función se puede usar fácilmente para reemplazar todos los valores faltantes en un vector con ceros: data(na_example) no_nas &lt;- ifelse(is.na(na_example), 0, na_example) sum(is.na(no_nas)) #&gt; [1] 0 Otras dos funciones útiles son any y all. La función any toma un vector de lógicos y devuelve TRUE si alguna de las entradas es TRUE. La función all toma un vector de lógicos y devuelve TRUE si todas las entradas son TRUE. Aquí ofrecemos un ejemplo: z &lt;- c(TRUE, TRUE, FALSE) any(z) #&gt; [1] TRUE all(z) #&gt; [1] FALSE 3.2 Cómo definir funciones A medida que adquieran más experiencia, necesitarán realizar las mismas operaciones una y otra vez. Un ejemplo sencillo es el cálculo de promedios. Podemos calcular el promedio de un vector x utilizando las funciones sum y length: sum(x)/length(x). Debido a que hacemos esto repetidas veces, es mucho más eficiente escribir una función que realice esta operación. Esta operación particular es tan común que alguien ya escribió la función mean y se incluye en la base R. Sin embargo, se encontrarán con situaciones en las que la función aún no existe, por lo que R les permite escribir una. Se puede definir una versión sencilla de una función que calcula el promedio así: avg &lt;- function(x){ s &lt;- sum(x) n &lt;- length(x) s/n } Ahora avg es una función que calcula el promedio: x &lt;- 1:100 identical(mean(x), avg(x)) #&gt; [1] TRUE Observen que las variables definidas dentro de una función no se guardan en el espacio de trabajo. Entonces mientras usamos s y n cuando llamamos (call en inglés) avg, los valores se crean y cambian solo durante la llamada. Aquí podemos ver un ejemplo ilustrativo: s &lt;- 3 avg(1:10) #&gt; [1] 5.5 s #&gt; [1] 3 Noten cómo s todavía es 3 después de que llamamos avg. En general, las funciones son objetos, por lo que les asignamos nombres de variables con &lt;-. La función function le dice a R que está a punto de definir una función. La forma general de la definición de una función es así: my_function &lt;- function(VARIABLE_NAME){ perform operations on VARIABLE_NAME and calculate VALUE VALUE } Las funciones que definan pueden tener múltiples argumentos, así como valores predeterminados. Por ejemplo, podemos definir una función que calcule el promedio aritmético o geométrico dependiendo de una variable definida por usuarios como esta: avg &lt;- function(x, arithmetic = TRUE){ n &lt;- length(x) ifelse(arithmetic, sum(x)/n, prod(x)^(1/n)) } Aprenderemos más sobre cómo crear funciones a través de la experiencia a medida que nos enfrentemos a tareas más complejas. 3.3 Namespaces Una vez que comiencen a convertirse en usuarios expertos de R, es probable que necesiten cargar varios complementos de paquetes (add-ons en inglés) para algunos de sus análisis. Tan pronto hagan eso, es probable que descubran que dos paquetes usen el mismo nombre para dos funciones diferentes. Y a menudo estas funciones hacen cosas completamente diferentes. De hecho, ya hemos visto esto porque ambos paquetes de base R dplyr y stats definen una función filter. Hay otros cinco ejemplos en dplyr. Sabemos esto porque cuando cargamos dplyr por primera vez, vemos el siguiente mensaje: The following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union Entonces, ¿qué hace R cuando escribimos filter? ¿Utiliza la función dplyr o la función stats? De nuestro trabajo anterior sabemos que usa dplyr. Pero, ¿qué pasa si queremos usar stats? Estas funciones viven en diferentes namespaces. R seguirá un cierto orden cuando busque una función en estos namespaces. Pueden ver el orden escribiendo: search() La primera entrada en esta lista es el ambiente global que incluye todos los objetos que definan. Entonces, ¿qué pasa si queremos usar el filter stats en lugar del filter dplyr pero dplyr aparece primero en la lista de búsqueda? Pueden forzar el uso de un namespace específico utilizando dos puntos dobles ( ::) así: stats::filter Si queremos estar absolutamente seguros de que usamos el filter dplyr, podemos usar: dplyr::filter Recuerden que si queremos usar una función en un paquete sin cargar el paquete completo, también podemos usar los dos puntos dobles. Para más información sobre este tema más avanzado, recomendamos el libro de paquetes R16. 3.4 Bucles-for La fórmula para la suma de la serie \\(1+2+\\dots+n\\) es \\(n(n+1)/2\\). ¿Qué pasaría si no estuviéramos seguros de que esa era la función correcta? ¿Cómo podríamos verificar? Usando lo que aprendimos sobre las funciones, podemos crear una que calcule \\(S_n\\): compute_s_n &lt;- function(n){ x &lt;- 1:n sum(x) } ¿Cómo podemos calcular \\(S_n\\) para varios valores de \\(n\\), digamos \\(n=1,\\dots,25\\)? ¿Escribimos 25 líneas de código llamando compute_s_n? No. Para eso están los bucles-for en la programación. En este caso, estamos realizando exactamente la misma tarea una y otra vez, y lo único que está cambiando es el valor de \\(n\\). Los bucles-for nos permiten definir el rango que toma nuestra variable (en nuestro ejemplo \\(n=1,\\dots,10\\)), luego cambiar el valor y evaluar la expresión a medida que realice un bucle. Quizás el ejemplo más sencillo de un bucle-for es este código inútil: for(i in 1:5){ print(i) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 Aquí está el bucle-for que escribiríamos para nuestro ejemplo \\(S_n\\): m &lt;- 25 s_n &lt;- vector(length = m) # create an empty vector for(n in 1:m){ s_n[n] &lt;- compute_s_n(n) } En cada iteración \\(n=1\\), \\(n=2\\), etc …, calculamos \\(S_n\\) y lo guardamos en la entrada \\(n\\) de s_n. Ahora podemos crear un gráfico para buscar un patrón: n &lt;- 1:m plot(n, s_n) Si notaron que parece ser cuadrático, van por buen camino porque la fórmula es \\(n(n+1)/2\\). 3.5 Vectorización y funcionales Aunque los bucles-for son un concepto importante para entender, no se usa mucho en R. A medida que aprendan más R, se darán cuenta de que la vectorización es preferible a los bucles-for puesto que resulta en un código más corto y claro. Ya vimos ejemplos en la sección de aritmética de vectores. Una función vectorizada es una función que aplicará la misma operación en cada uno de los vectores. x &lt;- 1:10 sqrt(x) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 y &lt;- 1:10 x*y #&gt; [1] 1 4 9 16 25 36 49 64 81 100 Para hacer este cálculo, no necesitamos los bucles-for. Sin embargo, no todas las funciones funcionan de esta manera. Por ejemplo, la función que acabamos de escribir, compute_s_n, no funciona elemento por elemento ya que espera un escalar. Este fragmento de código no ejecuta la función en cada entrada de n: n &lt;- 1:25 compute_s_n(n) Los funcionales son funciones que nos ayudan a aplicar la misma función a cada entrada en un vector, matriz, data frame o lista. Aquí cubrimos el funcional que opera en vectores numéricos, lógicos y de caracteres: sapply. La función sapply nos permite realizar operaciones basadas en elementos (element-wise en inglés) en cualquier función. Aquí podemos ver como funciona: x &lt;- 1:10 sapply(x, sqrt) #&gt; [1] 1.00 1.41 1.73 2.00 2.24 2.45 2.65 2.83 3.00 3.16 Cada elemento de x se pasa a la función sqrt y devuelve el resultado. Estos resultados se concatenan. En este caso, el resultado es un vector de la misma longitud que el original, x. Esto implica que el bucle-for anterior puede escribirse de la siguiente manera: n &lt;- 1:25 s_n &lt;- sapply(n, compute_s_n) Otros funcionales son apply, lapply, tapply, mapply, vapply y replicate. Usamos principalmente sapply, apply y replicate en este libro, pero recomendamos familiarizarse con los demás ya que pueden ser muy útiles. 3.6 Ejercicios 1. ¿Qué devolverá esta expresión condicional? x &lt;- c(1,2,-3,4) if(all(x&gt;0)){ print(&quot;All Postives&quot;) } else{ print(&quot;Not all positives&quot;) } 2. ¿Cuál de las siguientes expresiones es siempre FALSE cuando al menos una entrada de un vector lógico x es TRUE? all(x) any(x) any(!x) all(!x) 3. La función nchar le dice cuántos caracteres tiene un vector de caracteres. Escriba una línea de código que le asigne al objeto new_names la abreviatura del estado cuando el nombre del estado tiene más de 8 caracteres. 4. Cree una función sum_n que para cualquier valor dado, digamos \\(n\\), calcule la suma de los enteros de 1 a n (inclusive). Use la función para determinar la suma de los enteros de 1 a 5,000. 5. Cree una función altman_plot que toma dos argumentos, x y y, y grafica la diferencia contra la suma. 6. Después de ejecutar el siguiente código, ¿cuál es el valor de x? x &lt;- 3 my_func &lt;- function(y){ x &lt;- 5 y+5 } 7. Escriba una función compute_s_n que para cualquier \\(n\\) calcula la suma \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Indique el valor de la suma cuando \\(n=10\\). 8. Defina un vector numérico vacío s_n de tamaño 25 usando s_n &lt;- vector(&quot;numeric&quot;, 25) y almacene los resultados de \\(S_1, S_2, \\dots S_{25}\\) usando un bucle-for. 9. Repita el ejercicio 8, pero esta vez use sapply. 10. Repita el ejercicio 8, pero esta vez use map_dbl. 11. Grafica \\(S_n\\) versus \\(n\\). Use puntos definidos por \\(n=1,\\dots,25\\). 12. Confirme que la fórmula para esta suma es \\(S_n= n(n+1)(2n+1)/6\\). http://r-pkgs.had.co.nz/namespace.html↩ "],
["tidyverse.html", "Capítulo 4 tidyverse 4.1 Data tidy 4.2 Ejercicios 4.3 Cómo manipular los data frames 4.4 Ejercicios 4.5 El pipe: %&gt;% 4.6 Ejercicios 4.7 Cómo resumir datos 4.8 Cómo ordenar los data frames 4.9 Ejercicios 4.10 Tibbles 4.11 El operador punto 4.12 do 4.13 El paquete purrr 4.14 Los condicionales de tidyverse 4.15 Ejercicios", " Capítulo 4 tidyverse Hasta ahora hemos estado manipulando vectores reordenándolos y creando subconjuntos mediante la indexación. Sin embargo, una vez comencemos los análisis más avanzados, la unidad preferida para el almacenamiento de datos no es el vector sino el data frame. En este capítulo aprenderemos a trabajar directamente con data frames, que facilitan enormemente la organización de información. Utilizaremos data frames para la mayoría de este libro. Nos enfocaremos en un formato de datos específico denominado tidy y en una colección específica de paquetes que son particularmente útiles para trabajar con data tidy que se denomina el tidyverse. Podemos cargar todos los paquetes tidyverse a la vez al instalar y cargar el paquete tidyverse: library(tidyverse) Aprenderemos cómo implementar el enfoque tidyverse a lo largo del libro, pero antes de profundizar en los detalles, en este capítulo presentamos algunos de los aspectos más utilizadas del tidyverse, comenzando con el paquete dplyr para manipular los data frames y el paquete purrr para trabajar con las funciones. Tengan en cuenta que el tidyverse también incluye un paquete para graficar, ggplot2, que presentaremos más adelante en el Capítulo 7 en la parte “Visualización de datos” del libro; el paquete readr discutido en el Capítulo 5; y muchos otros. En este capítulo, primero presentamos el concepto de data tidy y luego demostramos cómo usamos el tidyverse para trabajar con data frames en este formato. 4.1 Data tidy Decimos que una tabla de datos está en formato tidy si cada fila representa una observación y las columnas representan las diferentes variables disponibles para cada una de estas observaciones. El set de datos murders es un ejemplo de un tidy data frame. #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 Cada fila representa un estado con cada una de las cinco columnas proveyendo una variable diferente relacionada con estos estados: nombre, abreviatura, región, población y total de asesinatos. Para ver cómo se puede proveer la misma información en diferentes formatos, consideren el siguiente ejemplo: #&gt; country year fertility #&gt; 1 Germany 1960 2.41 #&gt; 2 South Korea 1960 6.16 #&gt; 3 Germany 1961 2.44 #&gt; 4 South Korea 1961 5.99 #&gt; 5 Germany 1962 2.47 #&gt; 6 South Korea 1962 5.79 Este set de datos tidy ofrece tasas de fertilidad para dos países a lo largo de los años. Se considera un set de datos tidy porque cada fila presenta una observación con las tres variables: país, año y tasa de fecundidad. Sin embargo, este set de datos originalmente vino en otro formato y le cambiamos la forma para distribuir a través del paquete dslabs. Originalmente, los datos estaban en el siguiente formato: #&gt; country 1960 1961 1962 #&gt; 1 Germany 2.41 2.44 2.47 #&gt; 2 South Korea 6.16 5.99 5.79 Se provee la misma información, pero hay dos diferencias importantes en el formato: 1) cada fila incluye varias observaciones y 2) una de las variables, año, se almacena en el encabezado. Para que los paquetes del tidyverse se utilicen de manera óptima, le tenemos que cambiar la forma a los datos para que estén en formato tidy, que aprenderán a hacer en la sección “Wrangling de datos” del libro. Hasta entonces, utilizaremos ejemplos de sets de datos que ya están en formato tidy. Aunque no es inmediatamente obvio, a medida que avancen en el libro comenzarán a apreciar las ventajas de trabajar en un marco en el que las funciones usan formatos tidy tanto para inputs como para outputs. Verán cómo esto permite que los analistas de datos se enfoquen en los aspectos más importantes del análisis en lugar del formato de los datos. 4.2 Ejercicios 1. Examine el set de datos incluidos en base R co2. ¿Cuál de los siguientes es cierto? co2 son datos tidy: tiene un año para cada fila. co2 no es tidy: necesitamos al menos una columna con un vector de caracteres. co2 no es tidy: es una matriz en lugar de un data frame. co2 no es tidy: para ser tidy tendríamos que cambiarle la forma (wrangle it en inglés) para tener tres columnas (año, mes y valor), y entonces cada observación de CO2 tendría una fila. 2. Examine el set de datos incluidos en base R ChickWeight. ¿Cuál de los siguientes es cierto? ChickWeight no es tidy: cada pollito tiene más de una fila. ChickWeight es tidy: cada observación (un peso) está representada por una fila. El pollito de donde provino esta medida es una de las variables. ChickWeight no es tidy: nos falta la columna del año. ChickWeight es tidy: se almacena en un data frame. 3. Examine el set de datos predefinido BOD. ¿Cuál de los siguientes es cierto? BOD no es tidy: solo tiene seis filas. BOD no es tidy: la primera columna es solo un índice. BOD es tidy: cada fila es una observación con dos valores (tiempo y demanda) BOD es tidy: todos los sets de datos pequeños son tidy por definición. 4. ¿Cuál de los siguientes sets de datos integrados es tidy? Puede elegir más de uno. BJsales EuStockMarkets DNase Formaldehyde Orange UCBAdmissions 4.3 Cómo manipular los data frames El paquete dplyr del tidyverse ofrece funciones que realizan algunas de las operaciones más comunes cuando se trabaja con los data frames y usa nombres para estas funciones que son relativamente fáciles de recordar. Por ejemplo, para cambiar la tabla de datos agregando una nueva columna, utilizamos mutate. Para filtrar la tabla de datos a un subconjunto de filas, utilizamos filter. Finalmente, para subdividir los datos seleccionando columnas específicas, usamos select. 4.3.1 Cómo agregar una columna con mutate Queremos que toda la información necesaria para nuestro análisis se incluya en la tabla de datos. Entonces, la primera tarea es agregar las tasas de asesinatos a nuestro data frame de asesinatos. La función mutate toma el data frame como primer argumento y el nombre y los valores de la variable como segundo argumento usando la convención name = values. Entonces, para agregar tasas de asesinatos, usamos: library(dslabs) data(&quot;murders&quot;) murders &lt;- mutate(murders, rate = total/ population * 100000) Recuerden que aquí usamos total y population dentro de la función, que son objetos no definidos en nuestro espacio de trabajo. Pero, ¿por qué no recibimos un error? Esta es una de las principales características de dplyr. Las funciones en este paquete, como mutate, saben buscar variables en el data frame que el primer argumento le provee. En la llamada a mutate que vemos arriba, total tendrá los valores de murders$total. Este enfoque hace que el código sea mucho más legible. Podemos ver que se agrega la nueva columna: head(murders) #&gt; state abb region population total rate #&gt; 1 Alabama AL South 4779736 135 2.82 #&gt; 2 Alaska AK West 710231 19 2.68 #&gt; 3 Arizona AZ West 6392017 232 3.63 #&gt; 4 Arkansas AR South 2915918 93 3.19 #&gt; 5 California CA West 37253956 1257 3.37 #&gt; 6 Colorado CO West 5029196 65 1.29 Aunque hemos sobrescrito el original objeto murders, esto no cambia el objeto que se cargó con data(murders). Si cargamos la data murders nuevamente, el original sobrescribirá nuestra versión mutada. 4.3.2 Cómo crear subconjuntos con filter Ahora supongamos que queremos filtrar la tabla de datos para mostrar solo las entradas para las cuales la tasa de homicidios es inferior a 0.71. Para hacer esto usamos la función filter, que toma la tabla de datos como primer argumento y luego la declaración condicional como el segundo. Igual que con mutate, podemos usar los nombres de variables sin comillas de murders dentro de la función y esta sabrá que nos referimos a las columnas y no a los objetos en el espacio de trabajo. filter(murders, rate &lt;= 0.71) #&gt; state abb region population total rate #&gt; 1 Hawaii HI West 1360301 7 0.515 #&gt; 2 Iowa IA North Central 3046355 21 0.689 #&gt; 3 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Vermont VT Northeast 625741 2 0.320 4.3.3 Cómo seleccionar columnas con select Aunque nuestra tabla de datos solo tiene seis columnas, algunas tablas de datos incluyen cientos. Si queremos ver solo algunas columnas, podemos usar la función select de dplyr. En el siguiente código seleccionamos tres, asignamos el resultado a un nuevo objeto y luego filtramos este nuevo objeto: new_table &lt;- select(murders, state, region, rate) filter(new_table, rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 En la llamada a select, el primer argumento murders es un objeto, pero state, region y rate son nombres de variables. 4.4 Ejercicios 1. Cargue el paquete dplyr y el set de datos de asesinatos de EE.UU.. library(dplyr) library(dslabs) data(murders) Puede agregar columnas usando la función mutate de dplyr. Esta función reconoce los nombres de la columnas y dentro de la función puede llamarlos sin comillas: murders &lt;- mutate(murders, population_in_millions = population/ 10^6) Podemos escribir population en vez de murders$population. La función mutate sabe que estamos agarrando columnas de murders. Use la función mutate para agregar una columna de asesinatos llamada rate con la tasa de asesinatos por 100,000 como en el código del ejemplo anterior. Asegúrese de redefinir murders como se hizo en el código del ejemplo anterior (murders &lt;- [su código]) para que podamos seguir usando esta variable. 2. Si rank(x) le da el rango de las entradas de x de menor a mayor, rank(-x) le da los rangos de mayor a menor. Use la función mutate para agregar una columna rank que contiene el rango de la tasa de asesinatos de mayor a menor. Asegúrese de redefinir murders para poder seguir usando esta variable. 3. Con dplyr, podemos usar select para mostrar solo ciertas columnas. Por ejemplo, con este código solo mostraríamos los estados y los tamaños de población: select(murders, state, population) %&gt;% head() Utilice select para mostrar los nombres de los estados y las abreviaturas en murders. No redefina murders, solo muestre los resultados. 4. La función filter de dplyr se utiliza para elegir filas específicas del data frame para guardar. A diferencia de select que es para columnas, filter es para filas. Por ejemplo, puede mostrar solo la fila de Nueva York así: filter(murders, state == &quot;New York&quot;) Puede usar otros vectores lógicos para filtrar filas. Utilice filter para mostrar los cinco estados con las tasas de asesinatos más altas. Después de agregar la tasa y el rango de asesinatos, no cambie el set de datos de asesinatos de EE. UU., solo muestre el resultado. Recuerde que puede filtrar basándose en la columna rank. 5. Podemos eliminar filas usando el operador !=. Por ejemplo, para eliminar Florida, haríamos esto: no_florida &lt;- filter(murders, state != &quot;Florida&quot;) Cree un nuevo data frame con el nombre no_south que elimina los estados de la región sur. ¿Cuántos estados hay en esta categoría? Puede usar la función nrow para esto. 6. También podemos usar %in% para filtrar con dplyr. Por lo tanto, puede ver los datos de Nueva York y Texas de esta manera: filter(murders, state %in% c(&quot;New York&quot;, &quot;Texas&quot;)) Cree un nuevo data frame llamado murders_nw con solo los estados del noreste y oeste. ¿Cuántos estados hay en esta categoría? 7. Suponga que desea vivir en el noreste u oeste y desea que la tasa de homicidios sea inferior a 1. Queremos ver los datos de los estados que satisfacen estas opciones. Tenga en cuenta que puede usar operadores lógicos con filter. Aquí hay un ejemplo en el que filtramos para mantener solo estados pequeños en la región noreste. filter(murders, population &lt; 5000000 &amp; region == &quot;Northeast&quot;) Asegúrese que murders ha sido definido con rate y rank y todavía tiene todos los estados. Cree una tabla llamada my_states que contiene filas para los estados que satisfacen ambas condiciones: está en el noreste u oeste y la tasa de homicidios es inferior a 1. Use select para mostrar solo el nombre del estado, la tasa y el rango. 4.5 El pipe: %&gt;% Con dplyr podemos realizar una serie de operaciones, por ejemplo select y entonces filter, enviando los resultados de una función a otra usando lo que se llama el pipe operator: %&gt;%. Algunos detalles se incluyen a continuación. Escribimos el código anterior para mostrar tres variables (estado, región, tasa) para los estados que tienen tasas de asesinatos por debajo de 0.71. Para hacer esto, definimos el objeto intermedio new_table. En dplyr podemos escribir código que se parece más a una descripción de lo que queremos hacer sin objetos intermedios: \\[ \\mbox {original data } \\rightarrow \\mbox { select } \\rightarrow \\mbox { filter } \\] Para tal operación, podemos usar el pipe %&gt;%. El código se ve así: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) #&gt; state region rate #&gt; 1 Hawaii West 0.515 #&gt; 2 Iowa North Central 0.689 #&gt; 3 New Hampshire Northeast 0.380 #&gt; 4 North Dakota North Central 0.595 #&gt; 5 Vermont Northeast 0.320 Esta línea de código es equivalente a las dos líneas de código anteriores. ¿Qué está pasando aquí? En general, el pipe envía el resultado que se encuentra en el lado izquierdo del pipe para ser el primer argumento de la función en el lado derecho del pipe. Aquí vemos un ejemplo sencillo: 16 %&gt;% sqrt() #&gt; [1] 4 Podemos continuar canalizando (piping en inglés) valores a lo largo de: 16 %&gt;% sqrt() %&gt;% log2() #&gt; [1] 2 La declaración anterior es equivalente a log2(sqrt(16)). Recuerde que el pipe envía valores al primer argumento, por lo que podemos definir otros argumentos como si el primer argumento ya estuviera definido: 16 %&gt;% sqrt() %&gt;% log(base = 2) #&gt; [1] 2 Por lo tanto, al usar el pipe con data frames y dplyr, ya no necesitamos especificar el primer argumento requerido puesto que las funciones dplyr que hemos descrito toman todos los datos como el primer argumento. En el código que escribimos: murders %&gt;% select(state, region, rate) %&gt;% filter(rate &lt;= 0.71) murders es el primer argumento de la función select, y el nuevo data frame (anteriormente new_table) es el primer argumento de la función filter. Tengan en cuenta que el pipe funciona bien con las funciones donde el primer argumento son los datos de entrada. Las funciones en los paquetes tidyverse y dplyr tienen este formato y se pueden usar fácilmente con el pipe. 4.6 Ejercicios 1. El pipe %&gt;% se puede usar para realizar operaciones secuencialmente sin tener que definir objetos intermedios. Comience redefiniendo murders para incluir la tasa y el rango. murders &lt;- mutate(murders, rate = total/ population * 100000, rank = rank(-rate)) En la solución al ejercicio anterior, hicimos lo siguiente: my_states &lt;- filter(murders, region %in% c(&quot;Northeast&quot;, &quot;West&quot;) &amp; rate &lt; 1) select(my_states, state, rate, rank) El pipe %&gt;% nos permite realizar ambas operaciones secuencialmente sin tener que definir una variable intermedia my_states. Por lo tanto, podríamos haber mutado y seleccionado en la misma línea de esta manera: mutate(murders, rate = total/ population * 100000, rank = rank(-rate)) %&gt;% select(state, rate, rank) Note que select ya no tiene un data frame como primer argumento. Se supone que el primer argumento sea el resultado de la operación realizada justo antes de %&gt;%. Repita el ejercicio anterior, pero ahora, en lugar de crear un nuevo objeto, muestre el resultado y solo incluya las columnas de estado, velocidad y rango. Use un pipe %&gt;% para hacer esto en una sola línea. 2. Reinicie murders a la tabla original usando data(murders). Use un pipe para crear un nuevo data frame llamado my_states que considera solo los estados del noreste u oeste que tienen una tasa de homicidios inferior a 1 y contiene solo las columnas de estado, tasa y rango. El pipe también debe tener cuatro componentes separados por tres %&gt;%. El código debería verse algo similar a lo siguiente: my_states &lt;- murders %&gt;% mutate SOMETHING %&gt;% filter SOMETHING %&gt;% select SOMETHING 4.7 Cómo resumir datos Una parte importante del análisis exploratorio de datos es resumir los datos. La desviación promedio y estándar son dos ejemplos de estadísticas de resumen ampliamente utilizadas. A menudo se pueden obtener resúmenes más informativos dividiendo primero los datos en grupos. En esta sección, cubrimos dos nuevos verbos de dplyr que facilitan estos cálculos: summarize y group_by. Aprendemos a acceder a los valores resultantes utilizando la función pull. 4.7.1 summarize La función summarize de dplyr ofrece una forma de calcular estadísticas de resumen con código intuitivo y legible. Comenzamos con un ejemplo sencillo basado en alturas. El set de datos heights incluye las alturas y el sexo reportado por los estudiantes en una encuesta en clase. library(dplyr) library(dslabs) data(heights) El siguiente código calcula la desviación promedio y estándar para las hembras: s &lt;- heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) s #&gt; average standard_deviation #&gt; 1 64.9 3.76 Esto toma nuestra tabla de datos original como entrada, la filtra para incluir solo a las filas representando hembras y luego produce una nueva tabla resumida con solo el promedio y la desviación estándar de las alturas. Podemos elegir los nombres de las columnas de la tabla resultante. Por ejemplo, arriba decidimos usar average y standard_deviation, pero podríamos haber usado otros nombres de la misma manera. Como la tabla resultante almacenada en s es un data frame, podemos acceder a los componentes con el operador de acceso $: s$average #&gt; [1] 64.9 s$standard_deviation #&gt; [1] 3.76 Igual que con la mayoría de las otras funciones de dplyr, summarize conoce los nombres de las variables y podemos usarlas directamente. Entonces, cuando escribimos mean(height) dentro de la llamada a la función summarize, la función accede a la columna con el nombre “height”, o altura, y luego calcula el promedio del vector numérico resultante. Podemos calcular cualquier otro resumen que opera en vectores y devuelve un solo valor. Por ejemplo, podemos agregar las alturas mediana, mínima y máxima de esta manera: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(median = median(height), minimum = min(height), maximum = max(height)) #&gt; median minimum maximum #&gt; 1 65 51 79 Podemos obtener estos tres valores con solo una línea usando la función quantile: por ejemplo, quantile(x, c(0,0.5,1)) devuelve el mínimo (percentil 0), la mediana (percentil 50) y el máximo (percentil 100) del vector x. Sin embargo, si intentamos usar una función como esta que devuelve dos o más valores dentro de summarize: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(range = quantile(height, c(0, 0.5, 1))) recibiremos un error: Error: expecting result of length one, got : 2. Con la función summarize, solo podemos llamar a funciones que devuelven un solo valor. En la sección 4.12, aprenderemos cómo lidiar con funciones que devuelven más de un valor. Para otro ejemplo de cómo podemos usar la función summarize, calculemos la tasa promedio de asesinatos en Estados Unidos. Recuerden que nuestra tabla de datos incluye los asesinatos totales y el tamaño de la población para cada estado y ya hemos usado dplyr para agregar una columna de índice de asesinatos: murders &lt;- murders %&gt;% mutate(rate = total/population*100000) Recuerden que la tasa de asesinatos en EE. UU. no es el promedio de las tasas de asesinatos estatales: summarize(murders, mean(rate)) #&gt; mean(rate) #&gt; 1 2.78 Esto se debe a que en el cálculo anterior, los estados pequeños tienen el mismo peso que los grandes. La tasa de homicidios de Estados Unidos es el número total de asesinatos en Estados Unidos dividido por la población total. Entonces el cálculo correcto es: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 100000) us_murder_rate #&gt; rate #&gt; 1 3.03 Este cálculo cuenta estados más grandes proporcionalmente a su tamaño, lo que da como resultado un valor mayor. 4.7.2 pull El objeto us_murder_rate definido anteriormente representa solo un número. Sin embargo, lo estamos almacenando en un data frame: class(us_murder_rate) #&gt; [1] &quot;data.frame&quot; ya que, como la mayoría de las funciones de dplyr, summarize siempre devuelve un data frame. Esto podría ser problemático si queremos usar este resultado con funciones que requieren un valor numérico. Aquí mostramos un truco útil para acceder a los valores almacenados en los datos cuando usamos pipes: cuando un objeto de datos se canaliza (is piped en inglés), ese objeto y sus columnas se pueden acceder usando la función pull. Para entender lo que queremos decir, consideren esta línea de código: us_murder_rate %&gt;% pull(rate) #&gt; [1] 3.03 Esto devuelve el valor en la columna rate de us_murder_rate haciéndolo equivalente a us_murder_rate$rate. Para obtener un número de la tabla de datos original con una línea de código, podemos escribir: us_murder_rate &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 100000) %&gt;% pull(rate) us_murder_rate #&gt; [1] 3.03 que ahora es numérico: class(us_murder_rate) #&gt; [1] &quot;numeric&quot; 4.7.3 Cómo agrupar y luego resumir con group_by Una operación común en la exploración de datos es dividir primero los datos en grupos y luego calcular resúmenes para cada grupo. Por ejemplo, podemos querer calcular la desviación promedio y estándar para las alturas de hombres y mujeres por separado. La función group_by nos ayuda a hacer esto. Si escribimos esto: heights %&gt;% group_by(sex) #&gt; # A tibble: 1,050 x 2 #&gt; # Groups: sex [2] #&gt; sex height #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Male 75 #&gt; 2 Male 70 #&gt; 3 Male 68 #&gt; 4 Male 74 #&gt; 5 Male 61 #&gt; # … with 1,045 more rows El resultado no se ve muy diferente de heights, excepto que vemos Groups: sex [2] cuando imprimimos el objeto. Aunque no es inmediatamente obvio por su apariencia, esto ahora es un data frame especial llamado un grouped data frame, y las funciones de dplyr, en particular summarize, se comportarán de manera diferente cuando actúen sobre este objeto. Conceptualmente, pueden pensar en esta tabla como muchas tablas, con las mismas columnas pero no necesariamente el mismo número de filas, apiladas juntas en un objeto. Cuando resumimos los datos después de la agrupación, esto es lo que sucede: heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), standard_deviation = sd(height)) #&gt; # A tibble: 2 x 3 #&gt; sex average standard_deviation #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 64.9 3.76 #&gt; 2 Male 69.3 3.61 La función summarize aplica el resumen a cada grupo por separado. Para ver otro ejemplo, calculemos la tasa media de asesinatos en las cuatro regiones del país: murders %&gt;% group_by(region) %&gt;% summarize(median_rate = median(rate)) #&gt; # A tibble: 4 x 2 #&gt; region median_rate #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 Northeast 1.80 #&gt; 2 South 3.40 #&gt; 3 North Central 1.97 #&gt; 4 West 1.29 4.8 Cómo ordenar los data frames Al examinar un set de datos, a menudo es conveniente ordenar, numérica o alfabéticamente, basado en una o más de las columnas de la tabla. Conocemos las funciones order y sort, pero para ordenar tablas enteras, la función arrange de dplyr es útil. Por ejemplo, aquí ordenamos los estados según el tamaño de la población: murders %&gt;% arrange(population) %&gt;% head() #&gt; state abb region population total rate #&gt; 1 Wyoming WY West 563626 5 0.887 #&gt; 2 District of Columbia DC South 601723 99 16.453 #&gt; 3 Vermont VT Northeast 625741 2 0.320 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Alaska AK West 710231 19 2.675 #&gt; 6 South Dakota SD North Central 814180 8 0.983 Con arrange podemos decidir cuál columna usar para ordenar. Para ver los estados por población, de menor a mayor, organizamos por el rate : murders %&gt;% arrange(rate) %&gt;% head() #&gt; state abb region population total rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Hawaii HI West 1360301 7 0.515 #&gt; 4 North Dakota ND North Central 672591 4 0.595 #&gt; 5 Iowa IA North Central 3046355 21 0.689 #&gt; 6 Idaho ID West 1567582 12 0.766 Tengan en cuenta que el comportamiento por defecto es ordenar en orden ascendente. En dplyr, la función desc transforma un vector para que esté en orden descendente. Para ordenar la tabla en orden descendente, podemos escribir: murders %&gt;% arrange(desc(rate)) 4.8.1 Cómo ordenar anidadamente Si estamos ordenando una columna cuando hay empates, podemos usar una segunda columna para romper el empate. Del mismo modo, se puede usar una tercera columna para romper empates entre la primera y la segunda, y así sucesivamente. Aquí ordenamos por region, luego dentro de la región ordenamos por tasa de asesinatos: murders %&gt;% arrange(region, rate) %&gt;% head() #&gt; state abb region population total rate #&gt; 1 Vermont VT Northeast 625741 2 0.320 #&gt; 2 New Hampshire NH Northeast 1316470 5 0.380 #&gt; 3 Maine ME Northeast 1328361 11 0.828 #&gt; 4 Rhode Island RI Northeast 1052567 16 1.520 #&gt; 5 Massachusetts MA Northeast 6547629 118 1.802 #&gt; 6 New York NY Northeast 19378102 517 2.668 4.8.2 Los primeros \\(n\\) En el código anterior, usamos la función head para evitar que la página se llene con todo el set de datos. Si queremos ver una mayor proporción, podemos usar la funcióntop_n. Esta función toma un data frame como primer argumento, el número de filas para mostrar en el segundo y la variable para filtrar en el tercero. Aquí hay un ejemplo de cómo ver las 5 filas superiores: murders %&gt;% top_n(5, rate) #&gt; state abb region population total rate #&gt; 1 District of Columbia DC South 601723 99 16.45 #&gt; 2 Louisiana LA South 4533372 351 7.74 #&gt; 3 Maryland MD South 5773552 293 5.07 #&gt; 4 Missouri MO North Central 5988927 321 5.36 #&gt; 5 South Carolina SC South 4625364 207 4.48 Tengan en cuenta que las filas no están ordenadas por rate, solo filtradas. Si queremos ordenar, necesitamos usar arrange. Recuerden que si el tercer argumento se deja en blanco, top_n filtra por la última columna. 4.9 Ejercicios Para estos ejercicios, utilizaremos los datos de la encuesta recopilada por el Centro Nacional de Estadísticas de Salud de Estados Unidos (NCHS, por sus siglas en inglés). Este centro ha realizado una serie de encuestas de salud y nutrición desde la década de 1960. A partir de 1999, alrededor de 5,000 individuos de todas las edades han sido entrevistados cada año y completan el componente de examen de salud de la encuesta. Parte de los datos está disponible a través del paquete NHANES. Una vez que instalen el paquete NHANES, pueden cargar los datos así: library(NHANES) data(NHANES) Los datos NHANES tienen muchos valores faltantes. Las funciones mean y sd devolverán NA si alguna de las entradas del vector de entrada es un NA. Aquí hay un ejemplo: library(dslabs) data(na_example) mean(na_example) #&gt; [1] NA sd(na_example) #&gt; [1] NA Para ignorar los NAs podemos usar el argumento na.rm: mean(na_example, na.rm = TRUE) #&gt; [1] 2.3 sd(na_example, na.rm = TRUE) #&gt; [1] 1.22 Exploremos ahora los datos de NHANES. 1. Le ofrecemos algunos datos básicos sobre la presión arterial. Primero, seleccionemos un grupo para establecer el estándar. Utilizaremos hembras de 20 a 29 años. AgeDecade es una variable categórica con estas edades. Tenga en cuenta que la categoría está codificada como “20-29”, ¡con un espacio al frente! ¿Cuál es el promedio y la desviación estándar de la presión arterial sistólica según se guarda en el variable BPSysAve? Guárdela en una variable llamada ref. Sugerencia: use filter y summarize y use el argumento na.rm = TRUE al calcular el promedio y la desviación estándar. También puede filtrar los valores de NA utilizando filter. 2. Usando un pipe, asigne el promedio a una variable numérica ref_avg. Sugerencia: use el código similar al anterior y luego pull. 3. Ahora indique los valores mínimo y máximo para el mismo grupo. 4. Calcule el promedio y la desviación estándar para las hembras, pero para cada grupo de edad por separado en lugar de una década seleccionada como en la pregunta 1. Tenga en cuenta que los grupos de edad se definen por AgeDecade. Sugerencia: en lugar de filtrar por edad y género, filtre por Gender y luego use group_by. 5. Repita el ejercicio 4 para los varones. 6. Podemos combinar ambos resúmenes para los ejercicios 4 y 5 en una línea de código. Esto es porque group_by nos permite agrupar por más de una variable. Obtenga una gran tabla de resumen usando group_by(AgeDecade, Gender). 7. Para los varones entre las edades de 40-49, compare la presión arterial sistólica según raza, como aparece en la variable Race1. Ordene la tabla resultante según la presión arterial sistólica promedio de más baja a más alta. 4.10 Tibbles Los datos tidy deben almacenarse en data frames. Discutimos el data frame en la Sección 2.4.1 y hemos estado usando el data frame murders en todo el libro. En la sección 4.7.3 presentamos la función group_by, que permite estratificar los datos antes de calcular las estadísticas de resumen. Pero, ¿dónde se almacena la información del grupo en el data frame? murders %&gt;% group_by(region) #&gt; # A tibble: 51 x 6 #&gt; # Groups: region [4] #&gt; state abb region population total rate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Alabama AL South 4779736 135 2.82 #&gt; 2 Alaska AK West 710231 19 2.68 #&gt; 3 Arizona AZ West 6392017 232 3.63 #&gt; 4 Arkansas AR South 2915918 93 3.19 #&gt; 5 California CA West 37253956 1257 3.37 #&gt; # … with 46 more rows Observen que no hay columnas con esta información. Pero si miran el output anterior, verán la línea A tibble seguida por unas dimensiones. Podemos aprender la clase del objeto devuelto usando: murders %&gt;% group_by(region) %&gt;% class() #&gt; [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; El tbl es un tipo especial de data frame. Las funciones group_by y summarize siempre devuelven este tipo de data frame. La función group_by devuelve un tipo especial de tbl, el grouped_df. Discutiremos esto más adelante. Por coherencia, los verbos de manipulación dplyr ( select, filter, mutate y arrange) preservan la clase del input: si reciben un data frame regular, devuelven un data frame regular, mientras que si reciben un tibble, devuelven un tibble. Pero los tibbles son el formato preferido por el tidyverse y, como resultado, las funciones tidyverse que producen un data frame desde cero devuelven un tibble. Por ejemplo, en el capítulo 5 veremos que las funciones del tidyverse que se usan para importar datos crean tibbles. Los tibbles son muy similares a los data frames. De hecho, pueden pensar en ellos como una versión moderna de data frames. Sin embargo, hay tres diferencias importantes que describiremos a continuación. 4.10.1 Los tibbles se ven mejor El método de impresión para tibbles es más legible que el de un data frame. Para ver esto, comparen el output de escribir murders y el output de asesinatos si los convertimos en un tibble. Podemos hacer esto usando as_tibble(murders). Si usan RStudio, el output para un tibble se ajusta al tamaño de sus ventanas. Para ver esto, cambien el ancho de su consola R y observen cómo se muestran más/menos columnas. 4.10.2 Los subconjuntos de tibbles son tibbles Si creamos subconjuntos de las columnas de un data frame, le pueden devolver un objeto que no es un data frame, como un vector o escalar. Por ejemplo: class(murders[,4]) #&gt; [1] &quot;numeric&quot; no es un data frame. Con tibbles esto no sucede: class(as_tibble(murders)[,4]) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Esto es útil en el tidyverse ya que las funciones requieren data frames como input. Con tibbles, si desean acceder al vector que define una columna y no recuperar un data frame, deben usar el operador de acceso $: class(as_tibble(murders)$population) #&gt; [1] &quot;numeric&quot; Una característica relacionada es que tibbles les dará una advertencia si intentan acceder a una columna que no existe. Por ejemplo, si escribimos accidentalmente Population en lugar de population vemos que: murders$Population #&gt; NULL devuelve un NULL sin advertencia, lo que puede dificultar la depuración. Por el contrario, si intentamos esto con un tibble, obtenemos una advertencia informativa: as_tibble(murders)$Population #&gt; Warning: Unknown or uninitialised column: `Population`. #&gt; NULL 4.10.3 Los tibbles pueden tener entradas complejas Si bien las columnas del data frame deben ser vectores de números, cadenas o valores lógicos, los tibbles pueden tener objetos más complejos, como listas o funciones. Además, podemos crear tibbles con funciones: tibble(id = c(1, 2, 3), func = c(mean, median, sd)) #&gt; # A tibble: 3 x 2 #&gt; id func #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;fn&gt; #&gt; 2 2 &lt;fn&gt; #&gt; 3 3 &lt;fn&gt; 4.10.4 Los tibbles se pueden agrupar La función group_by devuelve un tipo especial de tibble: un tibble agrupado. Esta clase almacena información que les permite saber qué filas están en qué grupos. Las funciones tidyverse, en particular la función summarize, son conscientes de la información del grupo. 4.10.5 Cómo crear un tibble usando tibble en lugar de data.frame A veces es útil para nosotros crear nuestros propios data frames. Para crear un data frame en formato tibble, pueden utilizar la función tibble. grades &lt;- tibble(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) Tengan en cuenta que la base R (sin paquetes cargados) tiene una función con un nombre muy similar, data.frame, que se puede usar para crear un data frame regular en vez de un tibble. Otra diferencia importante es que por defecto data.frame fuerza una conversion de caracteres en factores sin proveer una advertencia o un mensaje: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) class(grades$names) #&gt; [1] &quot;factor&quot; Para evitar esto, usamos el argumento bastante engorroso stringsAsFactors: grades &lt;- data.frame(names = c(&quot;John&quot;, &quot;Juan&quot;, &quot;Jean&quot;, &quot;Yao&quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90), stringsAsFactors = FALSE) class(grades$names) #&gt; [1] &quot;character&quot; Para convertir un data frame normal en un tibble, pueden usar la función as_tibble. as_tibble(grades) %&gt;% class() #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 4.11 El operador punto Una de las ventajas de utilizar el pipe %&gt;% es que no tenemos que seguir nombrando nuevos objetos mientras manipulamos el data frame. Recuerden que si queremos calcular la tasa de asesinatos promedio para los estados del sur, en lugar de escribir: tab_1 &lt;- filter(murders, region == &quot;South&quot;) tab_2 &lt;- mutate(tab_1, rate = total/ population * 10^5) rates &lt;- tab_2$rate median(rates) #&gt; [1] 3.4 podemos evitar definir nuevos objetos intermedios escribiendo: filter(murders, region == &quot;South&quot;) %&gt;% mutate(rate = total/ population * 10^5) %&gt;% summarize(median = median(rate)) %&gt;% pull(median) #&gt; [1] 3.4 Podemos hacer esto porque cada una de estas funciones toma un data frame como primer argumento. Pero, ¿qué pasa si queremos acceder a un componente del data frame? Por ejemplo, ¿qué pasa si la función pull no está disponible y queremos acceder tab_2$rate? ¿Qué nombre de data frame usamos? La respuesta es el operador punto (dot operator en inglés). Por ejemplo, para acceder al vector de velocidad sin la función pull, podríamos usar: rates &lt;-filter(murders, region == &quot;South&quot;) %&gt;% mutate(rate = total/ population * 10^5) %&gt;% .$rate median(rates) #&gt; [1] 3.4 En la siguiente sección, veremos otras instancias en las que usar el . es útil. 4.12 do Las funciones del tidyverse saben interpretar tibbles agrupados. Además, para facilitar la secuencia de comandos a través del pipe %&gt;%, las funciones del tidyverse constantemente devuelven data frames, ya que esto asegura que el output de una función sea aceptada como el input de otra. Pero la mayoría de las funciones de R no reconocen los tibbles agrupados ni devuelven data frames. La función quantile es un ejemplo que describimos en la Sección 4.7.1. La función do sirve como puente entre las funciones de R, como quantile y el tidyverse. La función doentiende tibbles agrupados y siempre devuelve un data frame. En la sección 4.7.1, notamos que si intentamos usar quantile para obtener el mínimo, la mediana y el máximo en una llamada, recibiremos un error: Error: expecting result of length one, got : 2. data(heights) heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% summarize(range = quantile(height, c(0, 0.5, 1))) Podemos usar la función do para arreglar esto. Primero tenemos que escribir una función que se ajuste al enfoque del tidyverse: es decir, recibe un data frame y devuelve un data frame. my_summary &lt;- function(dat){ x &lt;- quantile(dat$height, c(0, 0.5, 1)) tibble(min = x[1], median = x[2], max = x[3]) } Ahora podemos aplicar la función al set de datos de alturas para obtener los resúmenes: heights %&gt;% group_by(sex) %&gt;% my_summary #&gt; # A tibble: 1 x 3 #&gt; min median max #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 50 68.5 82.7 Pero esto no es lo que queremos. Queremos un resumen para cada sexo y el código devolvió solo un resumen. Esto es porque my_summary no es parte del tidyverse y no sabe cómo manejar los tibbles agrupados. do hace esta conexión: heights %&gt;% group_by(sex) %&gt;% do(my_summary(.)) #&gt; # A tibble: 2 x 4 #&gt; # Groups: sex [2] #&gt; sex min median max #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 51 65.0 79 #&gt; 2 Male 50 69 82.7 Recuerden que aquí necesitamos usar el operador punto. El tibble creado por group_by se canaliza a do. Dentro de la llamada a do, el nombre de este tibble es . y queremos enviarlo a my_summary. Si no usan el punto, entonces my_summary no tiene ningún argumento y devuelve un error que nos dice que falta el argument &quot;dat&quot; . Pueden ver el error escribiendo: heights %&gt;% group_by(sex) %&gt;% do(my_summary()) Si no usan el paréntesis, entonces la función no se ejecuta y en su lugar do intenta devolver la función. Esto da un error porque do siempre debe devolver un data frame. Pueden ver el error escribiendo: heights %&gt;% group_by(sex) %&gt;% do(my_summary) 4.13 El paquete purrr En la sección 3.5 aprendimos sobre la función sapply, que nos permitió aplicar la misma función a cada elemento de un vector. Construimos una función y utilizamos sapply para calcular la suma de los primeros n enteros para varios valores de n así: compute_s_n &lt;- function(n){ x &lt;- 1:n sum(x) } n &lt;- 1:25 s_n &lt;- sapply(n, compute_s_n) Este tipo de operación, que aplica la misma función o procedimiento a elementos de un objeto, es bastante común en el análisis de datos. El paquete purrr incluye funciones similares a sapply pero que interactúan mejor con otras funciones del tidyverse. La principal ventaja es que podemos controlar mejor el tipo de resultado de las funciones. En contraste, sapply puede devolver varios tipos de objetos diferentes, convirtiéndolos cuando sea conveniente. Las funciones de purrr nunca harán esto: devolverán objetos de un tipo específico o devolverán un error si esto no es posible. La primera función de purrr que aprenderemos es map, que funciona muy similar a sapply pero siempre, sin excepción, devuelve una lista: library(purrr) s_n &lt;- map(n, compute_s_n) class(s_n) #&gt; [1] &quot;list&quot; Si queremos un vector numérico, podemos usar map_dbl que siempre devuelve un vector de valores numéricos. s_n &lt;- map_dbl(n, compute_s_n) class(s_n) #&gt; [1] &quot;numeric&quot; Esto produce los mismos resultados que la llamada sapply que vemos arriba. Una función de purrr particularmente útil para interactuar con el resto del tidyverse es map_df, que siempre devuelve un tibble data frame. Sin embargo, la función que se llama debe devolver un vector o una lista con nombres. Por esta razón, el siguiente código daría como resultado un error Argument 1 must have names: s_n &lt;- map_df(n, compute_s_n) Necesitamos cambiar la función para arreglar esto: compute_s_n &lt;- function(n){ x &lt;- 1:n tibble(sum = sum(x)) } s_n &lt;- map_df(n, compute_s_n) El paquete purrr ofrece mucha más funcionalidad no discutida aquí. Para obtener más detalles, puede consultar este recurso en línea. 4.14 Los condicionales de tidyverse Un análisis de datos típicos frecuentemente implicará una o más operaciones condicionales. En la sección 3.1 describimos la función ifelse, que utilizaremos ampliamente en este libro. En esta sección presentamos dos funciones de dplyr que ofrecen una funcionalidad adicional para realizar operaciones condicionales. 4.14.1 case_when La función case_when es útil para vectorizar declaraciones condicionales. Esto es similar a ifelse pero puede generar cualquier cantidad de valores, en lugar de solo TRUE o FALSE. Aquí hay un ejemplo que divide los números en negativo, positivo y 0: x &lt;- c(-2, -1, 0, 1, 2) case_when(x &lt; 0 ~ &quot;Negative&quot;, x &gt; 0 ~ &quot;Positive&quot;, TRUE ~ &quot;Zero&quot;) #&gt; [1] &quot;Negative&quot; &quot;Negative&quot; &quot;Zero&quot; &quot;Positive&quot; &quot;Positive&quot; Un uso común de esta función es definir unas variables categóricas basadas en variables existentes. Por ejemplo, supongamos que queremos comparar las tasas de homicidios en cuatro grupos de estados: New England, West Coast, South y Other. Para cada estado, primero preguntamos si está en New England. Si la respuesta es no, entonces preguntamos si está en el West Coast, y si no, preguntamos si está en el South y, si no, entonces asignamos ninguna de las anteriores (Other). Aquí vemos como usamos case_when para hacer esto: murders %&gt;% mutate(group = case_when( abb %in% c(&quot;ME&quot;, &quot;NH&quot;, &quot;VT&quot;, &quot;MA&quot;, &quot;RI&quot;, &quot;CT&quot;) ~ &quot;New England&quot;, abb %in% c(&quot;WA&quot;, &quot;OR&quot;, &quot;CA&quot;) ~ &quot;West Coast&quot;, region == &quot;South&quot; ~ &quot;South&quot;, TRUE ~ &quot;Other&quot;)) %&gt;% group_by(group) %&gt;% summarize(rate = sum(total)/ sum(population) * 10^5) #&gt; # A tibble: 4 x 2 #&gt; group rate #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 New England 1.72 #&gt; 2 Other 2.71 #&gt; 3 South 3.63 #&gt; 4 West Coast 2.90 4.14.2 between Una operación común en el análisis de datos es determinar si un valor cae dentro de un intervalo. Podemos verificar esto usando condicionales. Por ejemplo, para verificar si los elementos de un vector x están entre a y b podemos escribir: x &gt;= a &amp; x &lt;= b Sin embargo, esto puede volverse engorroso, especialmente dentro del enfoque tidyverse. La función between realiza la misma operación: between(x, a, b) 4.15 Ejercicios 1. Cargue el set de datos murders. ¿Cuál de los siguientes es cierto? murders está en formato tidy y se almacena en un tibble. murders está en formato tidy y se almacena en un data frame. murders no está en formato tidy y se almacena en un tibble. murders no está en formato tidy y se almacena en un data frame. 2. Utilice as_tibble para convertir la tabla de datos murders en un tibble y guárdelo en un objeto llamado murders_tibble. 3. Utilice la función group_by para convertir murders en un tibble que se agrupa por región. 4. Escriba el código tidyverse que es equivalente a este código: exp(mean(log(murders$population))) Escríbalo usando el pipe para que cada función se llame sin argumentos. Use el operador punto para acceder a la población. Sugerencia: el código debe comenzar con murders %&gt;%. 5. Utilice el map_df para crear un data frame con tres columnas que se denominan n, s_n y s_n_2. La primera columna debe contener los números del 1 al 100. La segunda y la tercera columna deben contener la suma del 1 al 100 \\(n\\) con \\(n\\) representando el número de fila. "],
["importing-data.html", "Capítulo 5 Importando datos 5.1 Las rutas y el directorio de trabajo 5.2 Los paquetes readr y readxl 5.3 Ejercicios 5.4 Cómo descargar archivos 5.5 Las funciones de importación de base R 5.6 Archivos de texto versus archivos binarios 5.7 Unicode versus ASCII 5.8 Cómo organizar datos con hojas de cálculo 5.9 Ejercicios", " Capítulo 5 Importando datos Hemos estado usando sets de datos ya almacenados como objetos R. Los científicos de datos rara vez tendrán tanta suerte y frecuentemente tendrán que importar datos a R desde un archivo, una base de datos u otras fuentes. Actualmente, una de las formas más comunes de almacenar y compartir datos para el análisis es a través de hojas de cálculo electrónicas. Una hoja de cálculo almacena datos en filas y columnas. Básicamente es una versión de archivo de un data frame. Al guardar dicha tabla en un archivo de computadora, uno necesita una manera de definir cuándo termina una nueva fila o columna y cuando comienza la otra. Esto a su vez define las celdas en las que se almacenan los valores individuales. Al crear hojas de cálculo con archivos de texto, como esas creadas con un editor de texto sencillo, se define una nueva fila con un return y se separan las columnas con un carácter especial predefinido. Los caracteres más comunes son coma ( ,), punto y coma ( ;), espacio ( ) y el tab (un número predeterminado de espacios o \\t). Aquí tenemos un ejemplo de cómo se ve un archivo separado por comas si lo abrimos con un editor básico de texto: La primera fila contiene nombres de columnas en lugar de datos. Nos referimos a esto como un encabezado (header en inglés), y cuando leemos (read-in en inglés) datos de una hoja de cálculo es importante saber si el archivo tiene un encabezado o no. La mayoría de las funciones de lectura suponen que hay un encabezado. Para saber si el archivo tiene un encabezado, miren el archivo antes de intentar leerlo. Esto se puede hacer con un editor de texto o con RStudio. En RStudio, podemos hacerlo abriendo el archivo en el editor o navegando a la ubicación del archivo, haciendo doble clic en el archivo y presionando View File. Sin embargo, no todos los archivos de hojas de cálculo están en formato de texto. Las hojas de cálculo de Google (Google Sheets en inglés), por ejemplo, se acceden con un navegador. Otro ejemplo es el formato propietario utilizado por Microsoft Excel, que no se puede ver con un editor de texto. A pesar de esto y debido a la popularidad del software Microsoft Excel, este formato se utiliza ampliamente. Comenzamos este capítulo describiendo las diferencias entre archivos de texto (ASCII), Unicode y binarios y cómo estas afectan la forma en que los importamos. Luego explicamos los conceptos de rutas de archivos y directorios de trabajo, que son esenciales para comprender cómo importar datos de manera efectiva. Entonces presentamos los paquetes readr y readxl y las funciones disponibles para importar hojas de cálculo en R. Finalmente, ofrecemos algunas recomendaciones sobre cómo almacenar y organizar datos en archivos. Los desafíos más complejos, sin embargo, como la extracción de datos de páginas web o de documentos PDF, se discutirán en la parte del libro “Wrangling de datos”. 5.1 Las rutas y el directorio de trabajo El primer paso para importar datos desde una hoja de cálculo es ubicar el archivo que contiene los datos. Aunque no lo recomendamos, pueden utilizar un enfoque similar al que usan para abrir archivos en Microsoft Excel haciendo clic en el menú de “File” de RStudio, haciendo clic en “Import Dataset” y luego haciendo clic en las carpetas hasta encontrar el archivo. Queremos poder escribir código en lugar de estar apuntando y haciendo clic. Las claves y los conceptos que necesitamos para aprender a hacer esto se describen en detalle en la parte del libro “Herramientas de productividad”. Aquí ofrecemos una descripción general de los conceptos básicos. El principal reto de este primer paso es permitir que las funciones de R que realizan la importación sepan dónde buscar el archivo que contiene los datos. La forma más sencilla de hacer esto es tener una copia del archivo en la carpeta donde las funciones de importación buscan por defecto. Una vez que hagamos esto, solo tenemos que proveerle el nombre del archivo a la función de importación. El paquete dslabs incluye una hoja de cálculo que contiene los datos de los asesinatos de EE. UU. Encontrar este archivo no es obvio, pero las siguientes líneas de código copian el archivo a la carpeta en la que R busca por defecto. A continuación explicamos cómo funcionan estas líneas. filename &lt;- &quot;murders.csv&quot; dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) fullpath &lt;- file.path(dir, filename) file.copy(fullpath, &quot;murders.csv&quot;) Este código no lee los datos en R, solo copia un archivo. Pero una vez copie el archivo, podemos importar los datos con solo una línea de código. Aquí usamos la función read_csv del paquete readr, que forma parte del tidyverse. library(tidyverse) dat &lt;- read_csv(filename) Los datos se importan y almacenan en dat. El resto de esta sección define algunos conceptos importantes y ofrece una visión general de cómo escribimos código para que R pueda encontrar los archivos que queremos importar. Capítulo ?? ofrece más detalles sobre este tema. 5.1.1 El sistema de archivos Pueden pensar en el sistema de archivos (file system en inglés) de su computadora como una serie de carpetas anidadas, cada una con otras carpetas y archivos. Los científicos de datos se refieren a las carpetas como directorios y a la carpeta que contiene todas las demás carpetas como el directorio raíz (root directory en inglés). El directorio en el que estamos ubicados actualmente se llama el directorio de trabajo (working directory en inglés). Por lo tanto, el directorio de trabajo cambia a medida que se muevan por las carpetas: considérenlo como su ubicación actual. 5.1.2 Las rutas relativas y completas La ruta (path en inglés) de un archivo es una lista de nombres de directorios que se pueden considerar instrucciones sobre en qué carpetas hacer clic y en qué orden encontrar el archivo. Si estas instrucciones son para encontrar el archivo desde el directorio raíz, nos referiremos a ellas como la ruta completa (full path en inglés). Si las instrucciones son para encontrar el archivo desde el directorio de trabajo, nos referimos a ellas como una ruta relativa (relative path en inglés). Sección ?? ofrece más detalles sobre este tema. Para ver un ejemplo de una ruta completa en sus sistemas, escriban lo siguiente: system.file(package = &quot;dslabs&quot;) Las cadenas separadas por barras son los nombres de los directorios. La primera barra diagonal representa el directorio raíz y sabemos que esta es una ruta completa porque comienza con una barra diagonal. Si el primer nombre del directorio aparece sin una barra diagonal en el comienzo, entonces R supone que la ruta es relativa. Podemos usar la función list.files para ver ejemplos de rutas relativas: dir &lt;- system.file(package = &quot;dslabs&quot;) list.files(path = dir) #&gt; [1] &quot;data&quot; &quot;DESCRIPTION&quot; &quot;extdata&quot; &quot;help&quot; #&gt; [5] &quot;html&quot; &quot;INDEX&quot; &quot;Meta&quot; &quot;NAMESPACE&quot; #&gt; [9] &quot;R&quot; &quot;script&quot; Estas rutas relativas nos dan la localización de los archivos o directorios si comenzamos en el directorio con la ruta completa. Por ejemplo, la ruta completa al directorio help en el ejemplo anterior es: /Library/Frameworks/R.framework/Versions/3.5/Resources/library/dslabs/help. Nota: Probablemente no harán mucho uso de la función system.file en su trabajo diario de análisis de datos. Lo presentamos en esta sección porque facilita el intercambio de hojas de cálculo al incluirlas en el paquete dslabs. Raramente tendrán el lujo de tener datos incluidos en paquetes que ya han instalado. Sin embargo, con frecuencia necesitarán navegar por rutas completas y relativas e importar datos con formato de hoja de cálculo. 5.1.3 El directorio de trabajo Recomendamos escribir solo rutas relativas en su código ya que las rutas completas son exclusivas de sus computadoras y Uds. quieren que su código sea portátil. Pueden obtener la ruta completa de su directorio de trabajo sin escribirla explícitamente utilizando la función getwd: wd &lt;- getwd() Si necesitan cambiar su directorio de trabajo, pueden usar la función setwd o pueden cambiarlo a través de RStudio haciendo clic en “Session”. 5.1.4 Cómo generar los nombres de ruta Otro ejemplo de cómo obtener una ruta completa sin escribirla explícitamente se ofreció arriba cuando creamos el objeto fullpath de esta manera: filename &lt;- &quot;murders.csv&quot; dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) fullpath &lt;- file.path(dir, filename) La función system.file provee la ruta completa de la carpeta que contiene todos los archivos y directorios relevantes para el paquete especificado por el argumento package. Al explorar los directorios en dir, nos encontramos con que extdata contiene el archivo que queremos: dir &lt;- system.file(package = &quot;dslabs&quot;) filename %in% list.files(file.path(dir, &quot;extdata&quot;)) #&gt; [1] TRUE La función system.file nos permite proveer un subdirectorio como primer argumento, para que podamos obtener la ruta completa del directorio extdata así: dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) La función file.path se usa para combinar los nombres de directorios para producir la ruta completa del archivo que queremos importar. fullpath &lt;- file.path(dir, filename) 5.1.5 Cómo copiar los archivos usando rutas La última línea de código que usamos para copiar el archivo en nuestro directorio de inicio usó la función file.copy. Esta toma dos argumentos: el nombre del archivo para copiar y el nombre que se usará en el nuevo directorio. file.copy(fullpath, &quot;murders.csv&quot;) #&gt; [1] TRUE Si un archivo se copia exitosamente, la función file.copy devuelve TRUE. Tengan en cuenta que le estamos dando al archivo el mismo nombre, murders.csv, pero podríamos haberle dado cualquier nombre. También recuerden que al no iniciar la cadena con una barra diagonal, R supone que esta es una ruta relativa y copia el archivo al directorio de trabajo. Deberían poder ver el archivo en su directorio de trabajo usando: list.files() 5.2 Los paquetes readr y readxl En esta sección presentamos las principales funciones de importación del tidyverse. Utilizaremos el archivo murders.csv del paquete dslabs como ejemplo. Para simplificar la ilustración, copiaremos el archivo a nuestro directorio de trabajo usando el siguiente código: filename &lt;- &quot;murders.csv&quot; dir &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) fullpath &lt;- file.path(dir, filename) file.copy(fullpath, &quot;murders.csv&quot;) 5.2.1 readr El paquete readr incluye funciones para leer datos almacenados en hojas de cálculo. readr es parte del paquete tidyverse, o pueden cargarlo directamente así: library(readr) Las siguientes funciones están disponibles para leer hojas de cálculo: Función Formato Sufijo típico read_table valores separados por espacios en blanco txt read_csv valores separados por comas csv read_csv2 valores separados por punto y coma csv read_tsv valores separados delimitados por tabulaciones tsv read_delim formato de archivo de texto general, debe definir delimitador txt Aunque el sufijo generalmente nos indica qué tipo de archivo es, no hay garantía de que estos siempre coincidan. Podemos abrir el archivo para echar un vistazo o usar la función read_lines para ver algunas líneas: read_lines(&quot;murders.csv&quot;, n_max = 3) #&gt; [1] &quot;state,abb,region,population,total&quot; #&gt; [2] &quot;Alabama,AL,South,4779736,135&quot; #&gt; [3] &quot;Alaska,AK,West,710231,19&quot; Esto también muestra que hay un encabezado. Ahora estamos listos para leer los datos en R. Del sufijo .csv y del vistazo al archivo, sabemos que tenemos que usar read_csv: dat &lt;- read_csv(filename) #&gt; Parsed with column specification: #&gt; cols( #&gt; state = col_character(), #&gt; abb = col_character(), #&gt; region = col_character(), #&gt; population = col_double(), #&gt; total = col_double() #&gt; ) Tengan en cuenta que recibimos un mensaje informándonos qué tipos de datos se utilizaron para cada columna. También observen que dat es un tibble, no solo un data frame. Esto es porque read_csv es un leedor (parser en inglés) del tidyverse. Podemos confirmar que los datos se han leído de la siguiente manera: View(dat) Finalmente, recuerden que también podemos usar la ruta completa para el archivo: dat &lt;- read_csv(fullpath) 5.2.2 readxl Pueden cargar el paquete readxl usando: library(readxl) El paquete ofrece funciones para leer formatos de Microsoft Excel: Función Formato Sufijo típico read_excel detectar automáticamente el formato xls, xlsx read_xls formato original xls read_xlsx nuevo formato xlsx Los formatos de Microsoft Excel le permiten tener más de una hoja de cálculo en un archivo. Estos se conocen como hojas (sheets en inglés). Las funciones enumeradas anteriormente leen la primera hoja por defecto, pero también podemos leer las otras. La función excel_sheets nos da los nombres de todas las hojas en un archivo de Excel. Estos nombres entonces se pueden pasar al argumento sheet en las tres funciones anteriores para leer hojas distintas a la primera. 5.3 Ejercicios 1. Utilice la función read_csv para leer cada uno de los archivos que el siguiente código guarda en el objeto files: path &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) files &lt;- list.files(path) files 2. Observe que el último, el archivo olive, nos da una advertencia. Esto se debe a que a la primera línea del archivo le falta el encabezado de la primera columna. Lea la página de ayuda para read_csv para aprender cómo leer el archivo sin leer este encabezado. Si omite el encabezado, no debería recibir esta advertencia. Guarde el resultado en un objeto llamado dat. 3. Un problema con el enfoque anterior es que no sabemos qué representan las columnas. Escriba: names(dat) para confirmar que los nombres no son informativos. Utilice la función readLines para leer solo la primera línea (luego aprenderemos cómo extraer valores del output). 5.4 Cómo descargar archivos Otro lugar común donde residen los datos es en el internet. Cuando estos datos están en archivos, podemos descargarlos y luego importarlos, o incluso leerlos directamente de la web. Por ejemplo, notamos que como nuestro paquete dslabs está en GitHub, el archivo que descargamos con el paquete tiene una URL: url &lt;- &quot;https://raw.githubusercontent.com/rafalab/dslabs/master/inst/ extdata/murders.csv&quot; El archivo read_csv puede leer estos archivos directamente: dat &lt;- read_csv(url) Si quieren tener una copia local del archivo, pueden usar la función download.file: download.file(url, &quot;murders.csv&quot;) Esto descargará el archivo y lo guardará en su sistema con el nombre murders.csv. Pueden usar cualquier nombre aquí, no necesariamente murders.csv. Recuerden que al usar download.file deben tener cuidado ya que sobrescribirá los archivos existentes sin previo aviso. Dos funciones que a veces son útiles al descargar datos del internet son tempdir y tempfile. La primera crea un directorio con un nombre aleatorio que es muy probable que sea único. Igualmente, tempfile crea una cadena de caracteres, no un archivo, que probablemente sea un nombre de archivo único. Entonces pueden ejecutar un comando, como el siguiente, que borra el archivo temporal una vez que importe los datos: tmp_filename &lt;- tempfile() download.file(url, tmp_filename) dat &lt;- read_csv(tmp_filename) file.remove(tmp_filename) 5.5 Las funciones de importación de base R La base R también provee funciones de importación. Estos tienen nombres similares a esas del tidyverse, por ejemplo read.table, read.csv y read.delim. Sin embargo, hay par de diferencias importantes. Para mostrar esto, leemos los datos con una función de base R: dat2 &lt;- read.csv(filename) Una diferencia importante es que los caracteres se convierten en factores: class(dat2$abb) #&gt; [1] &quot;factor&quot; class(dat2$region) #&gt; [1] &quot;factor&quot; Esto se puede evitar definiendo el argumento stringsAsFactors como FALSE. dat &lt;- read.csv(&quot;murders.csv&quot;, stringsAsFactors = FALSE) class(dat$state) #&gt; [1] &quot;character&quot; En nuestra experiencia, esto puede ser motivo de confusión ya que una variable que se guardó como caracteres en el archivo se convierte en factores, independientemente de lo que represente la variable. De hecho, es altamente recomendable siempre definir stringsAsFactors=FALSE para ser su enfoque por defecto cuando usan los leedores de base R. Pueden fácilmente convertir las columnas deseadas en factores después de importar datos. 5.5.1 scan Al leer hojas de cálculo, muchas cosas pueden salir mal. El archivo puede tener un encabezado multilíneal, pueden faltar celdas, o puede usar una codificación inesperada17. Les recomendamos que lean esta publicación sobre problemas comunes: https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about -unicode-and-character-sets-no-excuses/. Con experiencia, aprenderán a manejar los diferentes retos. Además, les ayudará leer detenidamente los archivos de ayuda para las funciones discutidas aquí. Con scan pueden leer cada celda de un archivo, como vemos aquí: path &lt;- system.file(&quot;extdata&quot;, package = &quot;dslabs&quot;) filename &lt;- &quot;murders.csv&quot; x &lt;- scan(file.path(path, filename), sep=&quot;,&quot;, what = &quot;c&quot;) x[1:10] #&gt; [1] &quot;state&quot; &quot;abb&quot; &quot;region&quot; &quot;population&quot; &quot;total&quot; #&gt; [6] &quot;Alabama&quot; &quot;AL&quot; &quot;South&quot; &quot;4779736&quot; &quot;135&quot; Noten que el tidyverse incluye read_lines, una función igualmente útil. 5.6 Archivos de texto versus archivos binarios En la ciencia de datos, los archivos generalmente se pueden clasificar en dos categorías: archivos de texto (también conocidos como archivos ASCII) y archivos binarios. Ya han trabajado con archivos de texto. Todos sus scripts de R son archivos de texto igual que los archivos de R markdown utilizados para crear este libro. Las tablas csv que han leído también son archivos de texto. Una gran ventaja de estos archivos es que podemos “mirarlos” fácilmente sin tener que comprar ningún tipo de software especial o seguir instrucciones complicadas. Se puede usar cualquier editor de texto para examinar un archivo de texto, incluyendo los editores disponibles gratuitamente como RStudio, Notepad, textEdit, vi, emacs, nano y pico. Para ver esto, intenten abrir un archivo csv con la herramienta de RStudio “Open file”. Deberían poder ver el contenido directamente en su editor. Sin embargo, si intentan abrir, digamos, un archivo Excel xls, jpg o png, no podrán ver nada inmediatamente útil. Estos son archivos binarios. Los archivos de Excel son carpetas comprimidas con varios archivos de texto dentro de ellas. Pero la principal distinción aquí es que los archivos de texto se pueden examinar fácilmente. Aunque R incluye herramientas para leer archivos binarios ampliamente utilizados, como archivos xls, en general es mejor encontrar sets de datos almacenados en archivos de texto. Del mismo modo, al compartir datos, es mejor que estén disponibles como archivos de texto siempre que el almacenamiento no sea un problema (los archivos binarios son mucho más eficientes para ahorrar espacio en su disco). En general, los formatos de texto facilitan el intercambio de datos, ya que no requieren software comercial para trabajar con los datos. Extraer datos de una hoja de cálculo almacenada como un archivo de texto es quizás la forma más fácil de llevar datos de un archivo a una sesión R. Desafortunadamente, las hojas de cálculo no siempre están disponibles y el hecho de que puedan ver los archivos de texto no necesariamente implica que extraer datos de ellos sea sencillo. En la parte del libro “Wrangling de datos” aprendemos a extraer datos de archivos de texto más complejos, como los archivos html. 5.7 Unicode versus ASCII Una trampa en la ciencia de datos es suponer que un archivo es un archivo de texto ASCII cuando en actualidad es otra cosa que puede parecerse mucho a un archivo de texto ASCII: un archivo de texto Unicode. Para comprender la diferencia entre estos, recuerden que todo en una computadora necesita convertirse eventualmente en 0s y 1s. ASCII es una codificación (encoding en inglés) que define una correspondencia entre caracteres y números. ASCII usa 7 bits (0s y 1s) que resulta en \\(2^7 = 128\\) elementos únicos, suficientes para codificar todos los caracteres en un teclado en inglés. Sin embargo, otros idiomas, como el español, usan caracteres no incluidos en esta codificación. Por ejemplo, las tildes no están codificadas por ASCII. Por esta razón, se definió una nueva codificación que utiliza más de 7 bits: Unicode. Cuando se utiliza Unicode, se puede elegir entre 8, 16 y 32 bits abreviados UTF-8, UTF-16 y UTF-32 respectivamente. RStudio usa la codificación UTF-8 por defecto. Aunque no entraremos en detalles sobre cómo lidiar con las diferentes codificaciones aquí, es importante que sepan que existen diferentes codificaciones para que pueden diagnosticar bien un problema si lo encuentran. Una forma en que se manifiestan los problemas es cuando surjen caracteres de “aspecto extraño” que no esperaban. Esta discusión de StackOverflow es un ejemplo: https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell. 5.8 Cómo organizar datos con hojas de cálculo Aunque este libro se enfoca casi exclusivamente en el análisis de datos, el manejo de datos también es una parte importante de la ciencia de datos. Como explicamos en la introducción, no cubrimos este tema. Sin embargo, con bastante frecuencia los analistas de datos necesitan recopilar datos, o trabajar con otros que recopilan datos, de manera que la forma más conveniente de almacenarlos es en una hoja de cálculo. Aunque completar una hoja de cálculo a mano es una práctica que no recomendamos y preferimos que el proceso se automatice lo más posible, a veces no queda otro remedio. Por lo tanto, en esta sección, ofrecemos recomendaciones sobre cómo organizar los datos en una hoja de cálculo. Aunque hay paquetes R diseñados para leer hojas de cálculo de Microsoft Excel, generalmente queremos evitar este formato. Recomendamos Google Sheets como una herramienta de software gratuita. Abajo resumimos las recomendaciones hechas en una publicación de Karl Broman y Kara Woo18. Favor de leer el artículo completo para más detalles importantes. Sea coherente - Antes de empezar a ingresar datos, tenga un plan. Una vez lo tenga, sea consistente y sígalo. Elija buenos nombres para las cosas: Los nombres que elija para los objetos, los archivos y los directorios deben ser memorables, fáciles de deletrear y descriptivos. Este es un equilibrio difícil de lograr y requiere tiempo y reflexión. Una regla importante a seguir es no usar espacios, usar guiones bajos _ o guiones en su lugar -. Además, evite los símbolos; es mejor utilizar las letras y los números. Escriba fechas como AAAA-MM-DD - Para evitar confusión, recomendamos utilizar el estándar global ISO 8601. Evite las celdas vacías - Llene todas las celdas y use un código común para los datos faltantes. Ponga solo una cosa en cada celda - Es mejor agregar columnas para almacenar la información adicional en vez de tener más de una pieza de información en una celda. Hazlo un rectángulo - La hoja de cálculo debe ser un rectángulo. Crea un diccionario de datos - Si necesita explicar cosas, por ejemplo cuáles son las columnas o cuáles son las etiquetas utilizadas para las variables categóricas, hágalo en un archivo separado. No haga cálculos en los archivos de datos sin procesar - Excel le permite realizar cálculos. No haga esto parte de su hoja de cálculo. El código para los cálculos debe estar en un script. No use color o resaltado como datos - La mayoría de funciones de importación no pueden importar esta información. En cambio, codifique esta información como una variable. Respalde su información: Respalde sus datos frecuentemente. Utilice la validación de datos para evitar errores - Aproveche las herramientas en su software de hoja de cálculo para que el proceso sea lo más libre posible de errores y de lesiones por estrés repetitivo. Guarde los datos como archivos de texto - Guarde los archivos para compartir en formato delimitado por comas o tabs. 5.9 Ejercicios 1. Elija una medida que pueda tomar regularmente. Por ejemplo, su peso diario o cuánto tiempo le toma correr 8 kilometros. Mantenga una hoja de cálculo que incluya la fecha, la hora, la medición y cualquier otra variable informativa que considere valiosa. Haga esto por 2 semanas. Luego haga un gráfico. https://en.wikipedia.org/wiki/Character_encoding↩ https://www.tandfonline.com/doi/abs/10.1080/00031305.2017.1375989↩ "],
["introducción-a-la-visualización-de-datos.html", "Capítulo 6 Introducción a la visualización de datos", " Capítulo 6 Introducción a la visualización de datos Raras veces es útil mirar los números y las cadenas de caracteres que definen un set de datos. Para confirmar esto, impriman y observen la tabla de datos de asesinatos de Estados Unidos: library(dslabs) data(murders) head(murders) #&gt; state abb region population total #&gt; 1 Alabama AL South 4779736 135 #&gt; 2 Alaska AK West 710231 19 #&gt; 3 Arizona AZ West 6392017 232 #&gt; 4 Arkansas AR South 2915918 93 #&gt; 5 California CA West 37253956 1257 #&gt; 6 Colorado CO West 5029196 65 ¿Qué aprenden de ver esta tabla? ¿Cuán rápido pueden determinar qué estados tienen las poblaciones más grandes? ¿Qué estados tienen las más pequeñas? ¿Cuán grande es un estado típico? ¿Existe una relación entre el tamaño de la población y los asesinatos totales? ¿Cómo varían las tasas de homicidios entre las regiones del país? Para la mayoría de cerebros humanos, es bastante difícil extraer esta información simplemente mirando los números. Por el contrario, las respuestas a todas las preguntas anteriores están fácilmente disponibles al examinar este gráfico: Esto nos recuerda del dicho “una imagen vale más que mil palabras”. La visualización de datos ofrece una forma poderosa de comunicar hallazgos basados en datos. En algunos casos, la visualización es tan convincente que no requiere un análisis de seguimiento. La creciente disponibilidad de sets de datos informativos y de herramientas de software ha conducido a una mayor dependencia de la visualizacion de datos en muchas industrias, academias y gobiernos. Un ejemplo destacado son las organizaciones de noticias, que están adoptando cada vez más el periodismo de datos e incluyendo infografías eficaces como parte de sus informes. Un ejemplo particularmente efectivo es un artículo del Wall Street Journal19 que muestra datos relacionados con el impacto de las vacunas en la lucha contra las enfermedades infecciosas. Uno de los gráficos muestra los casos de sarampión por estado de EE. UU. a lo largo de los años con una línea vertical que indica cuándo se introdujo la vacuna. Otro ejemplo notable proviene de un gráfico del New York Times20 que resume los resultados de los exámenes de los Regentes de la ciudad de Nueva York. Según el artículo21, estas puntuaciones se recopilan por varias razones, incluso para determinar si un estudiante se gradúa de escuela secundaria. En la ciudad de Nueva York, se necesita una puntuación mínima de 65 para aprobar. La distribución de las puntuaciones de las pruebas nos obliga a notar algo un poco problemático: La puntuación de prueba más común es la calificación mínima para aprobar, con muy pocas puntuaciones justo por debajo del umbral. Este resultado inesperado es consistente con el aumento de la puntuación de los estudiantes cerca de aprobar, pero sin obtener el mínimo de 65. Este es un ejemplo de cómo la visualización de datos puede conducir a descubrimientos que de otro modo se perderían si simplemente sometiéramos los datos a una serie de herramientas o procedimientos de análisis de datos. La visualización de datos es la herramienta más efectiva de lo que llamamos el análisis exploratorio de datos, o EDA por sus siglas en inglés. John W. Tukey22, considerado el padre de EDA, una vez dijo: “El mayor valor de una imagen es cuando nos obliga a notar lo que nunca esperábamos ver.” Muchas de las herramientas de análisis de datos más ampliamente utilizadas fueron inicialmente desarolladas gracias al EDA. Este es quizás la parte más importante del análisis de datos, pero a menudo se ignora. La visualización de datos ahora también es omnipresente en organizaciones filantrópicas y educativas. En las conferencias “New Insights on Poverty”23 y “The Best Stats You’ve Never Seen”24, Hans Rosling nos obliga a notar lo inesperado con una serie de gráficos relacionados con la salud y la economía mundial. En sus videos, Rosling usa unos gráficos animados para demostrar cómo el mundo está cambiando y cómo las viejas narrativas ya no son ciertas. También es importante recordar que las equivocaciones, los prejuicios, los errores sistemáticos y otros problemas inesperados a menudo conducen a datos que se deben analizar con cuidado. No descubrir estos problemas puede dar lugar a análisis defectuosos y descubrimientos falsos. Como ejemplo, consideren que los instrumentos de medición a veces fallan y que la mayoría de los procedimientos de análisis de datos no están diseñados para detectarlos. Sin embargo, estos procedimientos aún le darán una respuesta. El hecho de que puede ser difícil, o hasta imposible, notar un error solo a partir de los resultados que se reportan hace que la visualización de datos sea particularmente importante. En esta parte del libro, aprenderemos los conceptos básicos de la visualización de datos y del análisis exploratorio de datos mediante el uso de tres ejemplos motivantes. Usaremos el paquete ggplot2 para codificar. Para aprender los conceptos básicos, utilizaremos un ejemplo algo artificial: alturas reportadas por estudiantes. Luego discutiremos dos ejemplos mencionados anteriormente: 1) la salud y economía mundial y 2) las tendencias de enfermedades infecciosas en Estados Unidos. Por supuesto, la visualización de datos es mucho más de lo que cubrimos aquí. A continuación ofrecemos unas referencias para los que quieran aprender más: ER Tufte (1983) The visual display of quantitative information. Graphics Press. ER Tufte (1990) Envisioning information. Graphics Press. ER Tufte (1997) Visual explanations. Graphics Press. WS Cleveland (1993) Visualizing data. Hobart Press. WS Cleveland (1994) The elements of graphing data. CRC Press. A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130. NB Robbins (2004) Creating more effective graphs. Wiley. A Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders. N Yau (2013) Data points: Visualization that means something. Wiley. Finalmente, no discutiremos gráficos interactivos, un tema demasiado avanzado para este libro. Abajo incluímos algunos recursos útiles para aquellos interesados en aprender más sobre ese tema: https://shiny.rstudio.com https://d3js.org http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e↩ http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif↩ https://www.nytimes.com/2011/02/19/nyregion/19schools.html↩ https://en.wikipedia.org/wiki/John_Tukey↩ https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩ https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩ "],
["ggplot2.html", "Capítulo 7 ggplot2 7.1 Los componentes de un gráfico 7.2 objetos ggplot 7.3 Geometrías 7.4 Mapeos estéticos 7.5 Capas 7.6 Mapeos estéticos globales versus locales 7.7 Escalas 7.8 Etiquetas y títulos 7.9 Categorías como colores 7.10 Anotación, formas y ajustes 7.11 Paquetes complementarios 7.12 Cómo combinarlo todo 7.13 Gráficos rápidos con qplot 7.14 Cuadrículas de gráficos 7.15 Ejercicios", " Capítulo 7 ggplot2 La visualización de datos exploratorios es quizás la mayor ventaja de R. Uno puede pasar rápidamente de la idea a los datos al gráfico con un equilibrio único de flexibilidad y facilidad. Por ejemplo, Excel puede ser más fácil que R para algunos gráficos, pero no es tan flexible. D3.js puede ser más flexible y poderoso que R, pero se tarda mucho más en generar una gráfico. A lo largo del libro, crearemos gráficos usando el paquete ggplot225. library(dplyr) library(ggplot2) Hay muchas opciones para graficar disponibles en R. De hecho, las capacidades para graficar que vienen con una instalación básica de R ya son bastante poderosas. También hay otros paquetes para crear gráficos como grid y lattice. En este libro, decidimos usar ggplot2 porque divide los gráficos en componentes de una manera que le permite a los principiantes crear gráficos relativamente complejos y estéticamente agradables utilizando una sintaxis intuitiva y relativamente fácil de recordar. Una razón por la cual ggplot2 es generalmente más intuitiva para los principiantes es porque usa una gramática de gráficos26, el gg de ggplot2. Esto es análogo a la forma en que aprender gramática puede ayudar a un estudiante construir cientos de oraciones diferentes al aprender solo una pequeña cantidad de verbos, sustantivos y adjetivos, en vez de memorizar cada oración específica. Del mismo modo, al aprender una pequeña cantidad de los componentes básicos de ggplot2 y de su gramática, podrán crear cientos de gráficos diferentes. Otra razón por la cual ggplot2 es fácil para los principiantes es que su comportamiento por defecto se ha elegido cuidadosamente para satisfacer la gran mayoría de los casos y, además, es visualmente agradable. Como resultado, es posible crear gráficos informativos y elegantes con un código relativamente sencillo y legible. Una limitación de ggplot2 es que está diseñado para trabajar exclusivamente con tablas de datos en formato tidy (donde las filas son observaciones y las columnas son variables). Sin embargo, un porcentaje sustancial de sets de datos con los que los principiantes trabajan están en este formato o pueden convertirse a tal. Una ventaja de este enfoque es que, con tal que nuestros datos estén tidy, ggplot2 simplifica el código de graficar y el aprendizaje de gramática para una variedad de gráficos. Para usar ggplot2, tendrán que aprender varias funciones y argumentos. Estos son difíciles de memorizar, por lo que les recomendamos que tengan a mano la hoja de referencia de ggplot2. Pueden obtener una copia aquí: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf o simplemente realizar una búsqueda en internet de “ggplot2 cheat sheet”. 7.1 Los componentes de un gráfico Construiremos un gráfico que resume el set de datos de asesinatos con armas en Estados Unidos como el siguiente: Podemos ver claramente cuánto varían los estados según el tamaño de la población y el número total de asesinatos. No es sorprendente que también se observe una relación clara entre los totales de asesinatos y el tamaño de la población. Un estado que cae en la línea discontinua gris tiene la misma tasa de asesinatos que el promedio de EE. UU.. Las cuatro regiones geográficas se denotan con color, que señala cómo la mayoría de los estados del sur tienen tasas de asesinatos por encima del promedio. Esta visualización de datos nos muestra prácticamente toda la información de la tabla de datos. El código necesario para hacer el gráfico es relativamente sencillo. Aprenderemos a crearlo parte por parte. El primer paso para aprender ggplot2 es poder separar un gráfico en componentes. Empezaremos analizando el gráfico anterior e introduciendo algo de la terminología de ggplot2. Los tres componentes principales para considerar son: Data: Se está resumiendo el set de datos de asesinatos con armas de Estados Unidos. Nos referimos a esto como el componente data. Geometría: El gráfico anterior es un diagrama de dispersión. Esto se denomina el componente de geometría. Otras posibles geometrías son diagrama de barras, histograma, densidades suaves (smooth densities en inglés), gráfico Q-Q y diagrama de cajas. Mapeo estético: El gráfico usa varias señales visuales para representar la información proveída por el set de datos. Las dos señales más importantes en este gráfico son las posiciones de los puntos en el eje-x y el eje-y, que representan el tamaño de la población y el número total de asesinatos, respectivamente. Cada punto representa una observación diferente, y mapeamos los datos de estas observaciones y las señales visuales a las escalas x e y. El color es otra señal visual que asignamos a la región. Nos referimos a esto como el componente de mapeo estético. La forma en que definimos el mapeo depende de qué geometría estamos usando. También observamos que: Los puntos están etiquetados con las abreviaturas de los estados. El rango del eje-x y el eje-y parece estar definido por el rango de los datos. Ambos están en escalas logarítmicas. Hay etiquetas, un título, una leyenda y utilizamos el estilo de la revista “The Economist”. Ahora construiremos el gráfico parte por parte. Comenzemos cargando el set de datos: library(dslabs) data(murders) 7.2 objetos ggplot El primer paso para crear un gráfico ggplot2 es definir un objeto ggplot. Hacemos esto con la función ggplot, que inicializa el gráfico. Si leemos la página de ayuda para esta función, vemos que el primer argumento se usa para especificar qué datos están asociados con este objeto: ggplot(data = murders) También podemos pipe los datos como primer argumento. Entonces, esta línea de código es equivalente a la anterior: murders %&gt;% ggplot() El código crea un gráfico, en este caso una pizarra en blanco ya que no se ha definido la geometría. La única opción de estilo que vemos es un fondo gris. Lo que sucedió es que el objeto fue creado y, debido a que no fue asignado, se evaluó automáticamente. Pero podemos asignar nuestro gráfico a un objeto, por ejemplo así: p &lt;- ggplot(data = murders) class(p) #&gt; [1] &quot;gg&quot; &quot;ggplot&quot; Para representar el gráfico asociado con este objeto, simplemente imprimimos el objeto p. Cada una de las siguientes dos líneas de código produce el mismo gráfico que vemos arriba: print(p) p 7.3 Geometrías En ggplot2 creamos gráficos agregando capas (layers en inglés). Las capas pueden definir geometrías, calcular estadísticas de resumen, definir qué escalas (scales en inglés) usar o incluso cambiar estilos. Para agregar capas, usamos el símbolo +. En general, una línea de código se verá así: DATOS%&gt;% ggplot() + CAPA 1 + CAPA 2 + … + CAPA N Usualmente, la primera capa que agregamos define la geometría. Queremos hacer un diagrama de dispersión. ¿Qué geometría debemos utilizar? Echando un vistazo rápido a la hoja de referencia, vemos que la función utilizada para crear gráficos con esta geometría es geom_point. (Imagen cortesía de RStudio27. Licencia CC-BY-4.028.) Los nombres de las funciones de geometría siguen el patrón: geom_X donde X es el nombre de la geometría. Algunos ejemplos incluyen geom_point, geom_bar y geom_histogram. Para que geom_point funcione bien, necesitamos proveer datos y una correspondencia. Ya hemos conectado el objeto p con la tabla de datos murders, y si agregamos la capa geom_point, esta por defecto usa los datos de asesinatos. Para saber qué correspondencias se esperan, lean la sección Aesthetics de la página de ayuda de geom_point: &gt; Aesthetics &gt; &gt; geom_point understands the following aesthetics (required aesthetics are in bold): &gt; &gt; x &gt; &gt; y &gt; &gt; alpha &gt; &gt; colour y, como se esperaba, vemos que se requieren al menos dos argumentos x y y. 7.4 Mapeos estéticos Los mapeos estéticos (aesthetic mappings en inglés) describen cómo las propiedades de los datos se conectan con las características del gráfico, como la distancia a lo largo de un eje, el tamaño o el color. La función aes conecta los datos con lo que vemos en el gráfico mediante la definición de asignaciones estéticas y será una de las funciones que más utilizarán al graficar. El resultado de la función aes a menudo se utiliza como argumento de una función de geometría. Este ejemplo produce un diagrama de dispersión de asesinatos totales versus población en millones: murders %&gt;% ggplot() + geom_point(aes(x = population/10^6, y = total)) Podemos quitar el x = y y = si quisiéramos ya que estos son el primer y el segundo argumento esperado, como se ve en la página de ayuda. En lugar de definir nuestro gráfico desde cero, también podemos agregar una capa al objeto p que se definió anteriormente como p &lt;- ggplot(data = murders): p + geom_point(aes(population/10^6, total)) La escala y las etiquetas se definen por defecto al agregar esta capa. Al igual que las funciones de dplyr, aes también usa los nombres de variables del componente objeto: podemos usar population y total sin tener que llamarlos como murders$population and murders$total. El comportamiento de reconocer las variables del componente de datos es específico a aes. Con la mayoría de las funciones, si intentan acceder a los valores de population o total fuera de aes, recibirán un error. 7.5 Capas Una segunda capa en el gráfico que queremos hacer implica agregar una etiqueta a cada punto para identificar el estado. Las funciones geom_label y geom_text nos permiten agregar texto al gráfico con o sin un rectángulo detrás del texto, respectivamente. Debido a que cada punto (cada estado en este caso) tiene una etiqueta, necesitamos un mapeo estético para hacer la conexión entre los puntos y las etiquetas. Al leer la página de ayuda, aprendemos que el mapeo entre el punto y la etiqueta se provee a través del argumento label de aes. Entonces el código se ve así: p + geom_point(aes(population/10^6, total)) + geom_text(aes(population/10^6, total, label = abb)) Hemos agregado exitosamente una segunda capa al gráfico. Como ejemplo del comportamiento único de aes mencionado anteriormente, observen que esta llamada: p_test &lt;- p + geom_text(aes(population/10^6, total, label = abb)) está bien, mientras que esta llamada: p_test &lt;- p + geom_text(aes(population/10^6, total), label = abb) les dará un error ya que abb no se encuentra porque está fuera de la función aes. La capa geom_text no sabe dónde encontrar abb porque es un nombre de columna y no una variable global. 7.5.1 Cómo probar varios argumentos p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb)) size no es un mapeo: mientras los mapeos usan datos de observaciones específicas y necesitan estar dentro de aes(), las operaciones que queremos que afecten a todos los puntos de la misma manera no necesitan ser incluidas dentro aes. Ahora, debido a que los puntos son más grandes, es difícil ver las etiquetas. Si leemos la página de ayuda para geom_text, vemos que el argumento nudge_x mueve el texto ligeramente hacia la derecha o hacia la izquierda: p + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5) Esto es preferible puesto que facilita la lectura del texto. En la sección 7.11 aprenderemos una mejor manera de asegurarnos de que podemos ver los puntos y las etiquetas. 7.6 Mapeos estéticos globales versus locales En la línea anterior de código, definimos el mapeo aes(population/10^6, total) dos veces, una vez en cada geometría. Podemos evitar esto usando un mapeo estético global cuando definimos la pizarra en blanco que nos da el objeto ggplot. Recuerden que la función ggplot contiene un argumento que nos permite definir mapeos estéticos: args(ggplot) #&gt; function (data = NULL, mapping = aes(), ..., environment = parent.frame()) #&gt; NULL Si definimos un mapeo en ggplot, todas las geometrías que se agregan como capas se asignarán por defecto a este mapeo. Redefinimos p: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) y entonces podemos simplemente escribir el siguiente código para producir el gráfico anterior: p + geom_point(size = 3) + geom_text(nudge_x = 1.5) Mantenemos los argumentos size y nudge_x en geom_point y geom_text, respectivamente, porque solo queremos aumentar el tamaño de los puntos y ajustar la posición (nudge en inglés) de las etiquetas. Si ponemos esos argumentos en aes, entonces se aplicarán a ambos gráficos. También tengan en cuenta que la función geom_point no necesita un argumento label y por lo tanto ignora esa estética. Si es necesario, podemos anular el mapeo global definiendo un nuevo mapeo dentro de cada capa. Estas definiciones locales reemplazan a las globales. Aquí hay un ejemplo: p + geom_point(size = 3) + geom_text(aes(x = 10, y = 800, label = &quot;Hello there!&quot;)) Claramente, la segunda llamada a geom_text no usa population y total. 7.7 Escalas Primero, las escalas que queremos están en escala logarítmica. Este no es el valor predeterminado, por lo que este cambio debe agregarse a través de una capa de escalas. Una mirada rápida a la hoja de referencia revela que la función scale_x_continuous nos permite controlar el comportamiento de las escalas. La usamos así: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) Debido a que ahora estamos en la escala logarítmica, el ajuste a la posición debe hacerse más pequeño. Esta transformación particular es tan común que ggplot2 ofrece dos funciones especializadas scale_x_log10 y scale_y_log10, que podemos usar para reescribir el código de esta manera: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() 7.8 Etiquetas y títulos Del mismo modo, la hoja de referencia revela que para cambiar las etiquetas y agregar un título, utilizamos las siguientes funciones: p + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) ¡Casi terminamos! Lo único que nos falta es agregar color, leyenda y cambios opcionales al estilo. 7.9 Categorías como colores Podemos cambiar el color de los puntos usando el argumento col en la función geom_point. Para facilitar la demostración de características nuevas, redefiniremos p para ser todo excepto la capa de puntos: p &lt;- murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) y luego probaremos lo que sucede cuando agregamos diferentes llamadas a geom_point. Por ejemplo, podemos hacer que todos los puntos sean azules agregando el argumento color: p + geom_point(size = 3, color =&quot;blue&quot;) Sin embargo, no queremos esto. Queremos asignar color según la región geográfica. Un buen comportamiento por defecto de ggplot2 es que si asignamos una variable categórica al color, automáticamente asigna un color diferente a cada categoría, además de una leyenda. Dado que la elección del color está determinada por una característica de cada observación, este es un mapeo estético. Para asignar un color a cada punto, necesitamos usar aes. Usamos el siguiente código: p + geom_point(aes(col=region), size = 3) Los mapeos x y y se heredan de esos ya definidos en p, así que no los redefinimos. También movemos aes al primer argumento, ya que ahí es donde se esperan los mapeos en esta llamada. Aquí vemos otro comportamiento útil por defecto: ggplot2 automáticamente agrega una leyenda que asigna el color a la región. Para evitar agregar esta leyenda, establecemos el argumento geom_point como show.legend = FALSE. 7.10 Anotación, formas y ajustes A menudo queremos agregar formas o anotaciones a las figuras que no se derivan directamente del mapeo estético; algunos ejemplos incluyen etiquetas, cuadros, áreas sombreadas y líneas. Aquí queremos agregar una línea que represente la tasa promedio de asesinatos en todo el país. Una vez que determinemos la tasa por millón a ser \\(r\\), esta línea se define por la fórmula: \\(y = r x\\), con \\(y\\) y \\(x\\) nuestros ejes: asesinatos totales y población en millones, respectivamente. En la escala logarítmica, esta línea se convierte en: \\(\\log(y) = \\log(r) + \\log(x)\\). Entonces, en nuestro gráfico, es una línea con pendiente 1 e intercepto \\(\\log(r)\\). Para calcular este valor, utilizamos nuestros conocimientos de dplyr: r &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 10^6) %&gt;% pull(rate) Para agregar una línea, usamos la función geom_abline. ggplot2 utiliza ab en el nombre para recordarnos que estamos suministrando el intercepto (a) y el pendiente (b). La línea predeterminada tiene pendiente 1 e intercepto 0, por lo que solo tenemos que definir el intercepto: p + geom_point(aes(col=region), size = 3) + geom_abline(intercept = log10(r)) Aquí geom_abline no utiliza ninguna información del objeto de datos. Podemos cambiar el tipo de línea y el color de las líneas usando argumentos. Además, la dibujamos primero para que no tape nuestros puntos. p &lt;- p + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) Tengan en cuenta que hemos redefinido p y usaremos este nuevo p a continuación y en la siguiente sección. Los gráficos por defecto creados por ggplot2 ya son muy útiles. Sin embargo, con frecuencia necesitamos hacer pequeños ajustes al comportamiento predeterminado. Aunque no siempre es obvio cómo hacer esto aun con la hoja de referencia, ggplot2 es muy flexible. Por ejemplo, podemos hacer cambios a la leyenda a través de la función scale_color_discrete. En nuestro gráfico original, la palabra region está en mayúscula y podemos cambiarla así: p &lt;- p + scale_color_discrete(name = &quot;Region&quot;) 7.11 Paquetes complementarios El poder de ggplot2 se incrementa aún más debido a la disponibilidad de paquetes adicionales. Los cambios restantes necesarios para darle los toques finales a nuestro gráfico requieren los paquetes ggthemes y ggrepel. El estilo de un gráfico ggplot2 se puede cambiar usando las funciones de theme. Se incluyen varios temas (themes en inglés) como parte del paquete ggplot2. De hecho, para la mayoría de los gráficos de este libro, utilizamos una función del paquete dslabs que automáticamente establece un tema por defecto: ds_theme_set() El paquete ggthemes agrega muchos otros temas, incluso el tema theme_economist que escogimos. Después de instalar el paquete, pueden cambiar el estilo agregando una capa como la siguiente: library(ggthemes) p + theme_economist() Pueden ver cómo se ven algunos de los otros temas simplemente cambiando la función. Por ejemplo, pueden probar el tema theme_fivethirtyeight() en vez del anterior. La diferencia final tiene que ver con la posición de las etiquetas. En nuestro gráfico, algunas de las etiquetas se superponen. El paquete de complementos ggrepel incluye una geometría que agrega etiquetas a la vez que garantiza que no se superpongan entre sí. Para utilizarla, simplemente cambiamos geom_text a geom_text_repel. 7.12 Cómo combinarlo todo Ahora que hemos terminado las pruebas, podemos escribir un código que produzca nuestro gráfico deseado partiendo de cero. library(ggthemes) library(ggrepel) r &lt;- murders %&gt;% summarize(rate = sum(total)/ sum(population) * 10^6) %&gt;% pull(rate) murders %&gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_abline(intercept = log10(r), lty = 2, color = &quot;darkgrey&quot;) + geom_point(aes(col=region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(&quot;Populations in millions (log scale)&quot;) + ylab(&quot;Total number of murders (log scale)&quot;) + ggtitle(&quot;US Gun Murders in 2010&quot;) + scale_color_discrete(name = &quot;Region&quot;) + theme_economist() 7.13 Gráficos rápidos con qplot Hemos aprendido las poderosas técnicas de ggplot para generar visualizaciones. Sin embargo, hay casos en que sólo necesitamos un gráfico rápido de, por ejemplo, un histograma de los valores en un vector, un diagrama de dispersión de los valores en dos vectores o un diagrama de caja usando vectores categóricos y numéricos. Ya hemos demostrado cómo generar estos gráficos con hist, plot y boxplot. Sin embargo, si queremos ser consistentes con el estilo de ggplot, podemos usar la función qplot. Si tenemos valores en dos vectores como: data(murders) x &lt;- log10(murders$population) y &lt;- murders$total y queremos hacer un diagrama de dispersión con ggplot, tendríamos que escribir algo como: data.frame(x = x, y = y) %&gt;% ggplot(aes(x, y)) + geom_point() Esto parece ser demasiado código para una gráfico tan sencillo. La función qplot sacrifica la flexibilidad ofrecida por el enfoque de ggplot, pero nos permite rápidamente generar un gráfico. qplot(x, y) Aprenderemos más sobre qplot en la sección 8.16 7.14 Cuadrículas de gráficos A menudo tenemos que poner gráficos uno al lado del de otro. El paquete gridExtra nos permite hacer eso: library(gridExtra) p1 &lt;- qplot(x) p2 &lt;- qplot(x,y) grid.arrange(p1, p2, ncol = 2) 7.15 Ejercicios Comience cargando los paquetes dplyr y ggplot2, así como los datos murders y heights. library(dplyr) library(ggplot2) library(dslabs) data(heights) data(murders) 1. Con ggplot2, los gráficos se pueden guardar como objetos. Por ejemplo, podemos asociar un set de datos con un objeto de gráfico así: p &lt;- ggplot(data = murders) Como data es el primer argumento, no necesitamos explicarlo: p &lt;- ggplot(murders) y también podemos usar el pipe: p &lt;- murders %&gt;% ggplot() ¿Cuál es la clase del objeto p? 2. Recuerde que para imprimir un objeto puede usar el comando print o simplemente escribir el objeto. Imprima el objeto p definido en el ejercicio uno y describa lo que ve. No pasa nada. Una gráfico de pizarra en blanco. Un diagrama de dispersión. Un histograma. 3. Usando el pipe %&gt;%, cree un objeto p pero esta vez asociado con el set de datos heights en lugar del set de datos murders. 4. ¿Cuál es la clase del objeto p que acaba de crear? 5. Ahora vamos a agregar una capa y las mapeos estéticos correspondientes. Para los datos de asesinatos, graficamos asesinatos totales versus tamaños de población. Explore el set de datos murders para recordar cuáles son los nombres de estas dos variables y escoja la respuesta correcta. Sugerencia: Mire ?murders. state y abb total_murders y population_size total y population murders y size 6. Para crear el diagrama de dispersión, agregamos una capa con geom_point. Los mapeos estéticos requieren que definamos las variables del eje-x y del eje-y, respectivamente. Entonces el código se ve así: murders %&gt;% ggplot(aes(x = , y = )) + geom_point() excepto que tenemos que definir las dos variables x y y. Llene el espacio con los nombres correctos de las variables. 7. Recuerde que si no usamos nombres de argumentos, podemos obtener el mismo gráfico si ingresamos los nombres de las variables en el orden correcto de esta manera: murders %&gt;% ggplot(aes(population, total)) + geom_point() Vuelva a hacer el gráfico pero ahora con total en el eje-x y población en el eje-y. 8. Si en lugar de puntos queremos agregar texto, podemos usar las geometrías geom_text() o geom_label(). El siguiente código: murders %&gt;% ggplot(aes(population, total)) + geom_label() nos dará el mensaje de error: Error: geom_label requires the following missing aesthetics: label ¿Por qué ocurre esto? Necesitamos mapear un carácter a cada punto a través del argumento de etiqueta en aes. Necesitamos dejar que geom_label sepa qué carácter usar en el gráfico. La geometría geom_label no requiere valores del eje-x y del eje-y. geom_label no es un comando de ggplot2. 9. Reescriba el código anterior para que use la abreviatura como la etiqueta a través de aes. 10. Cambie el color de las etiquetas a azul. ¿Cómo se hace? Agregando una columna llamada blue a murders. Debido a que cada etiqueta necesita un color diferente, mapeamos los colores a través de aes. Utilizando el argumento color en ggplot. Como queremos que todos los colores sean azules, no necesitamos asignar colores, solo usar el argumento de color en geom_label. 11. Reescriba el código anterior para que las etiquetas sean azules. 12. Ahora supongamos que queremos usar color para representar las diferentes regiones. En este caso, ¿cuál de los siguientes es el más apropiado? Agregar una columna llamada color a murders con el color que queremos usar. Como cada etiqueta necesita un color diferente, mapear los colores a través del argumento de color de aes . Utilizar el argumento color en ggplot. Como queremos que todos los colores sean azules, no necesitamos asignar colores, solo usar el argumento de color en geom_label. 13. Reescriba el código anterior para que el color de las etiquetas sea determinado por la región del estado. 14. Ahora vamos a cambiar el eje-x a una escala logarítmica para tomar en cuenta el hecho de que la distribución de la población es asimétrica. Comencemos definiendo un objeto p guardando el gráfico que hemos hecho hasta ahora: p &lt;- murders %&gt;% ggplot(aes(population, total, label = abb, color = region)) + geom_label() Para cambiar el eje-y a una escala logarítmica, aprendimos sobre la función scale_x_log10(). Agregue esta capa al objeto p para cambiar la escala y crear el gráfico. 15. Repita el ejercicio anterior pero ahora cambie ambos ejes para que estén en la escala logarítmica. 16. Ahora edite el código anterior para agregar el título “Gun murder data” al argumento. Sugerencia: use la función ggtitle. https://ggplot2.tidyverse.org/↩ http://www.springer.com/us/book/9780387245447↩ https://github.com/rstudio/cheatsheets↩ https://github.com/rstudio/cheatsheets/blob/master/LICENSE↩ "],
["distributions.html", "Capítulo 8 Cómo visualizar distribuciones de datos 8.1 Tipos de variables 8.2 Estudio de caso: describiendo alturas de estudiantes 8.3 La función de distribución 8.4 Funciones de distribución acumulada 8.5 Histogramas 8.6 Densidad suave 8.7 Ejercicios 8.8 La distribución normal 8.9 Unidades estándar 8.10 Gráficos Q-Q 8.11 Percentiles 8.12 Diagramas de caja 8.13 Estratificación 8.14 Estudio de caso: descripción de alturas de estudiantes (continuación) 8.15 Ejercicios 8.16 Geometrías ggplot2 8.17 Ejercicios", " Capítulo 8 Cómo visualizar distribuciones de datos Los datos numéricos a menudo se resumen con el valor promedio. Por ejemplo, la calidad de una escuela secundaria a veces se resume con un solo número: la puntuación promedio en una prueba estandarizada. Ocasionalmente, se incluye un segundo número: la desviación estándar. Por ejemplo, pueden leer un informe que indique que las puntuaciones fueron 680 más o menos 50 (la desviación estándar). El informe ha resumido un vector completo de puntajes con solo dos números. ¿Es esto apropiado? ¿Hay alguna información importante que no estamos considerando al ver este resumen en lugar de la lista completa? Nuestro primer componente básico de visualización de datos es aprender a resumir listas de factores o vectores numéricos. Generalmente, la mejor manera de compartir o explorar este resumen es a través de la visualización de datos. El resumen estadístico más básico de una lista de objetos o números es su distribución. Una vez que un vector se haya resumido como una distribución, existen varias técnicas de visualización de datos para transmitir esta información de manera efectiva. En este capítulo, primero discutiremos las propiedades de una variedad de distribuciones y cómo visualizar las distribuciones usando un ejemplo motivante de alturas de estudiantes. Luego, en la Sección 8.16, discutiremos las geometrías de ggplot2 para estas visualizaciones. 8.1 Tipos de variables Trabajaremos con dos tipos de variables: categóricas y numéricas. Cada uno puede dividirse en otros dos grupos: las variables categóricas pueden ser ordinales o no, mientras que las numéricas pueden ser discretas o continuas. Cuando cada entrada en un vector proviene de uno de un pequeño número de grupos, nos referimos a los datos como datos categóricos. Dos ejemplos sencillos son el sexo (masculino o femenino) y las regiones (noreste, sur, norte central, oeste). Algunos datos categóricos se pueden ordenar aunque no sean números, por ejemplo cuán picante es una comida (poco, medio, muy). En los libros de texto de estadísticas, los datos categóricos ordenados se denominan datos ordinales. Ejemplos de datos numéricos son el tamaño de la población, las tasas de asesinatos y las alturas. Algunos datos numéricos se pueden tratar como ordenados categóricos. Podemos dividir aún más los datos numéricos en continuos y discretos. Las variables continuas son aquellas que pueden tomar cualquier valor, como las alturas, si se miden con suficiente precisión. Por ejemplo, un par de gemelos pueden medir 68.12 y 68.11 pulgadas, respectivamente. Los conteos, como el tamaño de la población, son discretos porque tienen que ser números redondos. Tengan en cuenta que los datos numéricos discretos pueden considerarse ordinales. Aunque esto es técnicamente cierto, generalmente reservamos el término datos ordinales para variables que pertenecen a un pequeño número de grupos diferentes, y cada grupo tiene muchos miembros. En contraste, cuando tenemos muchos grupos con pocos casos en cada grupo, generalmente nos referimos a ellos como variables numéricas discretas. Entonces, por ejemplo, el número de paquetes de cigarrillos que una persona fuma al día, redondeado al paquete más cercano, se consideraría ordinal, mientras que el número real de cigarrillos se consideraría una variable numérica. Sin embargo, hay ejemplos que pueden considerarse tanto numéricos como ordinales cuando se trata de visualizar datos. 8.2 Estudio de caso: describiendo alturas de estudiantes Aquí presentamos un nuevo problema motivante. Es artificial, pero nos ayudará ilustrar los conceptos necesarios para comprender las distribuciones. Imaginen que tenemos que describir las alturas de nuestros compañeros de clase a ET, un extraterrestre que nunca ha visto humanos. Como primer paso, necesitamos recopilar datos. Para hacer esto, les pedimos a los estudiantes que indiquen sus alturas en pulgadas. Les pedimos que nos provean información sobre su sexo biológico porque sabemos que hay dos distribuciones diferentes por sexo. Recopilamos los datos y los guardamos en el set de datos heights: library(tidyverse) library(dslabs) data(heights) Una forma de transmitir las alturas a ET es simplemente enviarle esta lista de 1,050 alturas. Sin embargo, hay formas mucho más efectivas de transmitir la información y, para lograr esto, nos ayudará a entender el concepto de distribuciones. Para simplificar la explicación, primero nos enfocamos en las alturas masculinas. Examinamos los datos de altura femenina en la Sección 8.14. 8.3 La función de distribución Resulta que, en algunos casos, el promedio y la desviación estándar son prácticamente todo lo que necesitamos para comprender los datos. Aprenderemos técnicas de visualización de datos que nos ayudarán a determinar cuándo este resumen de dos números es apropiado. Estas mismas técnicas servirán como una alternativa para cuando dos números no son suficientes. El resumen estadístico más básico de una lista de objetos o números es su distribución. La forma más sencilla de pensar en una distribución es como una descripción compacta de una lista con muchas entradas. Este concepto no debería ser nuevo para los lectores de este libro. Por ejemplo, con datos categóricos, la distribución simplemente describe la proporción de cada categoría única. El sexo representado en el set de datos de alturas es: #&gt; #&gt; Female Male #&gt; 0.227 0.773 Esta tabla de frecuencia de dos categorías es la forma más simple de una distribución. Realmente no necesitamos visualizarla ya que un número describe todo lo que necesitamos saber: 23% son mujeres y el resto son hombres. Cuando hay más categorías, un diagrama de barras sencillo describe la distribución. Aquí hay un ejemplo con las regiones estatales de EE. UU.: Este gráfico simplemente nos muestra cuatro números, uno para cada categoría. Usualmente usamos diagramas de barras cuando tenemos pocos números. Aunque este gráfico en particular no proporciona mucha más información que una tabla de frecuencias en sí, es un primer ejemplo de cómo convertimos un vector en un gráfico que resume de manera sucinta toda la información en el vector. Cuando los datos son numéricos, la tarea de mostrar distribuciones es más retante. 8.4 Funciones de distribución acumulada Los datos numéricos que no son categóricos también tienen distribuciones. En general, cuando los datos no son categóricos, indicar la frecuencia de cada entrada no es un resumen efectivo, ya que la mayoría de las entradas son únicas. En nuestro estudio de caso, por ejemplo, mientras varios estudiantes reportaron una altura de 68 pulgadas, un estudiante indicó una altura de 68.503937007874 pulgadas y otro una altura 68.8976377952756 pulgadas. Suponemos que convirtieron sus alturas de 174 y 175 centímetros, respectivamente. Los libros de texto de estadísticas nos enseñan que una forma más útil de definir una distribución de datos numéricos es definir una función que indique la proporción de los datos a continuación \\(a\\) para todos los valores posibles de \\(a\\). Esta función se llama la función de distribución acumulada, o CDF por sus siglas en inglés. En estadística, se usa la siguiente notación: \\[ F(a) = \\mbox{Pr}(x \\leq a) \\] Aquí vemos un gráfico de \\(F\\) para los datos de altura masculina: Similar a lo que hace la tabla de frecuencias para datos categóricos, el CDF define la distribución de datos numéricos. En el gráfico, podemos ver que 16% de los valores son menos de 65, ya que \\(F(66)=\\) 0.164, o que 84% de los valores son menos de 72, ya que \\(F(72)=\\) 0.841, y así. De hecho, podemos informar la proporción de valores entre dos alturas, digamos \\(a\\) y \\(b\\), al computar \\(F(b) - F(a)\\). Esto significa que si enviamos este diagrama a ET, tendrá toda la información necesaria para reconstruir la lista completa. Parafraseando la expresión “una imagen vale más que mil palabras”, en este caso una imagen es tan informativa como 812 números. Una nota final: debido a que los CDF pueden definirse matemáticamente, la palabra empírica se agrega para distinguir cuando se usan los datos. Por lo tanto, utilizamos el término CDF empírico, o eCDF por sus siglas en inglés. 8.5 Histogramas Aunque el concepto de CDF se discute ampliamente en los libros de texto de estadística, el gráfico no es muy popular en la práctica. La razón principal es que no transmite fácilmente características de interés como: ¿En qué valor se centra la distribución? ¿La distribución es simétrica? ¿Qué rangos contienen el 95% de los valores? Los histogramas son preferidos porque facilitan enormemente la respuesta a tales preguntas. Los histogramas sacrifican solo un poco de información para producir gráficos que son mucho más fáciles de interpretar. La forma más sencilla de hacer un histograma es dividir el span de nuestros datos en compartimientos (bins en inglés) no superpuestos del mismo tamaño. Luego, para cada compartimiento, contamos el número de valores que se encuentran en ese intervalo. El histograma grafica estos conteos como barras con la base de la barra definida por los intervalos. A continuación tenemos el histograma para los datos de altura que dividen el rango de valores en intervalos de una pulgada: \\([49.5, 50.5], [51.5,52.5],(53.5,54.5],...,(82.5,83.5]\\) Como pueden ver en la figura de arriba, un histograma es similar a un diagrama de barras, pero difiere en que el eje-x es numérico, no categórico. Si le enviamos este diagrama a ET, inmediatamente aprenderá algunos detalles importantes sobre nuestros datos. Primero, el rango de datos es de 50 a 84 con la mayoría (más del 95%) entre 63 y 75 pulgadas. Además, las alturas son casi simétricas alrededor de 69 pulgadas. Por último, al sumar conteos, ET puede obtener una muy buena aproximación de la proporción de los datos en cualquier intervalo. Por lo tanto, el histograma anterior no solo es fácil de interpretar, sino que también ofrece casi toda la información contenida en la lista cruda de 812 alturas con aproximadamente 30 conteos, uno por cada compartimiento. ¿Qué información perdemos? Recuerden que todos los valores en cada intervalo se tratan de la misma manera cuando se calculan las alturas del compartimiento. Entonces, por ejemplo, el histograma no distingue entre 64, 64.1 y 64.2 pulgadas. Dado que estas diferencias son casi imperceptibles a la vista, las implicaciones prácticas son insignificantes y pudimos resumir los datos en solo 23 números. Discutimos cómo codificar histogramas en la Sección 8.16. 8.6 Densidad suave Los gráficos de densidad suave son estéticamente más atractivos que los histogramas. A continuación vemos un gráfico de densidad suave para nuestros datos de altura: En este gráfico, ya no tenemos bordes afilados en los límites de intervalo y se han eliminado muchos de los picos locales. Además, la escala del eje-y cambió de conteos a densidad. Para entender las densidades suaves, tenemos que entender las estimaciones, un tema que no abordamos hasta más tarde. Sin embargo, ofrecemos una explicación heurística para ayudarles a entender los conceptos básicos y así poder utilizar esta herramienta útil de visualización de datos. El nuevo concepto principal que deben entender es que suponemos que nuestra lista de valores observados es un subconjunto de una lista mucho más grande de valores no observados. En el caso de las alturas, pueden imaginar que nuestra lista de 812 estudiantes varones proviene de una lista hipotética que contiene todas las alturas de todos los estudiantes varones en todo el mundo, medidos todos con mucha precisión. Digamos que hay 1,000,000 de estas medidas. Esta lista de valores tiene una distribución, como cualquier lista de valores, y esta distribución considerable es realmente lo que queremos transmitir a ET, ya que es mucho más general. Desafortunadamente, no podemos ver esa lista grandísima. Sin embargo, podemos hacer una suposición que quizás nos ayude a aproximarla. Si tuviéramos 1,000,000 valores, medidos con mucha precisión, podríamos hacer un histograma con compartimientos muy, muy pequeños. La suposición es que si mostramos esto, la altura de los compartimientos consecutivos será similar. Esto es lo que queremos decir con suave: no tenemos grandes saltos en las alturas de los compartimientos consecutivos. A continuación tenemos un histograma hipotético con compartimientos de tamaño 1: Entre más pequeños hacemos los compartimientos, más suave se vuelve el histograma. Aquí están los histogramas con ancho de compartimiento de 1, 0.5 y 0.1: La densidad suave es básicamente la curva que atraviesa la parte superior de las barras del histograma cuando los compartimientos son muy, muy pequeños. Para que la curva no dependa del tamaño hipotético de la lista hipotética, calculamos la curva usando frecuencias en lugar de conteos: Ahora, de vuelta a la realidad. No tenemos millones de medidas. En cambio, tenemos 812 y no podemos hacer un histograma con compartimientos muy pequeños. Por lo tanto, hacemos un histograma, utilizando tamaños de compartimiento apropiados para nuestros datos y frecuencias de cómputo en lugar de conteos. Además, dibujamos una curva suave que pasa por la parte superior de las barras del histograma. Los siguientes gráficos muestran los pasos que conducen a una densidad suave: Sin embargo, recuerden que suave (smooth en inglés) es un término relativo. De hecho, podemos controlar la suavidad de la curva cambiando el número de puntos en los compartimientos. Esta opción se conoce como bandwidth, span, window size o, en español, ventana de suavizado o parámetro de suavizado, y se puede ajustar en la función que calcula la curva de densidad suave. Aquí hay dos ejemplos que usan diferentes niveles de suavidad en el mismo histograma: Necesitamos tomar esta decisión con cuidado ya que las visualizaciones que resultan pueden cambiar nuestra interpretación de los datos. Debemos seleccionar un grado de suavidad que podamos defender como representativo de los datos subyacentes. En el caso de la altura, realmente tenemos razones para creer que la proporción de personas con alturas similares debería ser la misma. Por ejemplo, la proporción que es 72 pulgadas debería ser más similar a la proporción que es 71 que a la proporción que es 78 o 65. Esto implica que la curva debe ser bastante suave; es decir, la curva debería parecerse más al ejemplo de la derecha que al de la izquierda. Si bien el histograma es un resumen sin supuestos, la densidad suavizada se basa en algunos supuestos. 8.6.1 Cómo interpretar el eje-y Tengan en cuenta que interpretar el eje-y de un gráfico de densidad suave no es obvio. Se escala para que el área bajo la curva de densidad se sume a 1. Si imaginan que formamos un compartimiento con una base de 1 unidad de longitud, el valor del eje-y nos indica la proporción de valores en ese compartimiento. Sin embargo, esto solo es cierto para compartimientos de tamaño 1. Para intervalos de otro tamaño, la mejor manera de determinar la proporción de datos en ese intervalo es calculando la proporción del área total contenida en ese intervalo. Por ejemplo, aquí vemos la proporción de valores entre 65 y 68: La proporción de esta área es aproximadamente 0.3, lo que significa que aproximadamente 30% de las alturas masculinas están entre 65 y 68 pulgadas. Al comprender esto, estamos listos para usar la densidad suave como resumen. Para este set de datos, nos sentimos bastante cómodos suponiendo suavidad y, por ende, compartiendo esta figura estéticamente agradable con ET, que puede usarla para comprender nuestros datos de alturas masculinas: 8.6.2 Densidades permiten estratificación Como nota final, señalamos que una ventaja de las densidades suaves sobre los histogramas para fines de visualización es que las densidades facilitan la comparación de dos distribuciones. Esto se debe en gran parte a que los bordes irregulares del histograma agregan desorden. Aquí hay un ejemplo que compara las alturas masculinas y las femeninas: Con el argumento correcto, ggplot automáticamente sombrea la región de intersección con un color diferente. Mostraremos ejemplos de código de ggplot2 para densidades en la Sección 9 así como en la Sección 8.16. 8.7 Ejercicios 1. En el set de datos murders, la región es una variable categórica y la siguiente es su distribución: Redondeando al 5% más cercano, ¿qué proporción de los estados se encuentran en la región “North Central”? 2. ¿Cuál de los siguientes es cierto? El gráfico anterior es un histograma. El gráfico anterior muestra solo cuatro números con un diagrama de barras. Las categorías no son números, por lo que no tiene sentido graficar la distribución. Los colores, no la altura de las barras, describen la distribución. 3. El siguiente gráfico muestra el eCDF para las alturas masculinas: Según el gráfico, ¿qué porcentaje de hombres son más bajos que 75 pulgadas? 100% 95% 80% 72 pulgadas 4. A la pulgada más cercana, ¿qué altura m tiene la propiedad de que la mitad de los estudiantes varones son más altos que m y la mitad son más bajos? 61 pulgadas 64 pulgadas 69 pulgadas 74 pulgadas 5. Aquí hay un eCDF de las tasas de asesinatos en todos los estados: Sabiendo que hay 51 estados (contando DC) y basado en este gráfico, ¿cuántos estados tienen tasas de homicidio superiores a 10 por cada 100,000 personas? 1 5 10 50 6. Según el eCDF anterior, ¿cuál de las siguientes afirmaciones es cierta? Alrededor de la mitad de los estados tienen tasas de homicidios superiores a 7 por 100,000 y la otra mitad por debajo. La mayoría de los estados tienen tasas de homicidio de menos de 2 por 100,000. Todos los estados tienen tasas de asesinatos superiores a 2 por 100,000. Con la excepción de 4 estados, las tasas de asesinatos son inferiores a 5 por cada 100,000. 7. A continuación mostramos el histograma de alturas masculinas de nuestro set de datos heights: Según este gráfico, ¿cuántos hombres hay entre 63.5 y 65.5? 10 24 34 100 8. ¿Aproximadamente qué porcentaje son más bajos que 60 pulgadas? 1% 10% 25% 50% 9. Según el siguiente gráfico de densidad, ¿aproximadamente qué proporción de estados de EE. UU. tiene poblaciones con más de 10 millones de personas? 0.02 0.15 0.50 0.55 10. A continuación hay tres gráficos de densidad. ¿Es posible que sean del mismo set de datos? ¿Cuál de las siguientes afirmaciones es cierta? Es imposible que sean del mismo set de datos. Son del mismo set de datos, pero los gráficos son diferentes debido a errores de código. Son del mismo set de datos, pero el primer y el segundo gráfico suavizan de menos y el tercero sobresuaviza. Son del mismo set de datos, pero el primero no está en la escala logarítmica, el segundo suaviza de menos y el tercero sobresuaviza. 8.8 La distribución normal Los histogramas y los gráficos de densidad proveen excelentes resúmenes de una distribución. ¿Pero podemos resumir aún más? A menudo vemos el promedio y la desviación estándar utilizada como resumen estadístico: ¡un resumen de dos números! Para comprender cuáles son estos resúmenes y por qué se usan tanto, necesitamos comprender la distribución normal. La distribución normal, también conocida como curva de campana y distribución Gausiana, es uno de los conceptos matemáticos más famosos de la historia. Una razón para esto es que se producen distribuciones aproximadamente normales en muchas situaciones, incluyendo las ganancias de juego, las alturas, los pesos, la presión arterial, las puntuaciones en las pruebas estandarizadas y los errores de medición experimentales. Hay explicaciones para esto y las describiremos más adelante. Aquí nos enfocamos en cómo la distribución normal nos ayuda a resumir los datos. En vez de usar datos, la distribución normal se define con una fórmula matemática. Para cualquier intervalo \\((a,b)\\), la proporción de valores en ese intervalo se puede calcular utilizando esta fórmula: \\[\\mbox{Pr}(a &lt; x &lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\, dx\\] No necesitan memorizar o comprender los detalles de la fórmula. Sin embargo, recuerden que está completamente definida por solo dos parámetros: \\(m\\) y \\(s\\). El resto de los símbolos en la fórmula representan los extremos del intervalo que determinamos, \\(a\\) y \\(b\\) y las conocidas constantes matemáticas \\(\\pi\\) y \\(e\\). Estos dos parámetros, \\(m\\) y \\(s\\), se conocen respectivamente como el promedio (o la media) y la desviación estándar, o SD por sus siglas en inglés, de la distribución. La distribución es simétrica, centrada en el promedio y la mayoría de los valores (alrededor del 95%) están dentro de 2 SD del promedio. Así es como se ve la distribución normal cuando el promedio es 0 y la SD es 1: El hecho de que la distribución está definida por solo dos parámetros implica que si la distribución de un set de datos se puede aproximar con una distribución normal, toda la información necesaria para describir la distribución se puede codificar en solo dos números: el promedio y la desviación estándar. Ahora vamos a definir estos valores para una lista arbitraria de números. Para una lista de números contenidos en un vector x, el promedio se define como: m &lt;- sum(x)/ length(x) y la SD se define como: s &lt;- sqrt(sum((x-mu)^2)/ length(x)) que puede interpretarse como la distancia promedio entre los valores y su promedio. Calculemos los valores para la altura de los varones que almacenaremos en el objeto \\(x\\): index &lt;- heights$sex == &quot;Male&quot; x &lt;- heights$height[index] Se pueden usar las funciones predefinidas mean y sd (tengan en cuenta que por razones que se explican en la Sección ??, sd divide por length(x)-1 en vez de length(x)) : m &lt;- mean(x) s &lt;- sd(x) c(average = m, sd = s) #&gt; average sd #&gt; 69.31 3.61 A continuación tenemos un gráfico de la densidad suave y la distribución normal con media = 69.3 y SD = 3.6 trazado como una línea negra con la densidad suave de nuestras alturas de estudiantes en azul: La distribución normal parece ser una buena aproximación aquí. Ahora veremos cuán bien funciona esta aproximación para predecir la proporción de valores dentro de los intervalos. 8.9 Unidades estándar Para los datos que se distribuyen aproximadamente normalmente, es conveniente pensar en términos de unidades estándar. La unidad estándar de un valor nos dice cuántas desviaciones estándar se alejan del promedio. Específicamente, para un valor x de un vector X, definimos el valor de x en unidades estándar como z = (x - m)/s con m y s el promedio y la desviación estándar de X, respectivamente. ¿Por qué es conveniente hacer esto? Primero repasen la fórmula para la distribución normal y observen que lo que se está exponiendo es \\(-z^2/2\\) con \\(z\\) equivalente a \\(x\\) en unidades estándar. El hecho de que el máximo de \\(e^{-z^2/2}\\) es cuando \\(z=0\\) explica por qué la distribución ocurre en el promedio. También explica la simetría ya que \\(- z^2/2\\) es simétrico alrededor de 0. Además, noten que si convertimos los datos distribuidos normalmente a unidades estándar, podemos saber rápidamente si, por ejemplo, una persona es aproximadamente promedio ( \\(z=0\\)), entre los más altos ( \\(z \\approx 2\\)), entre los más pequeños ( \\(z \\approx -2\\)), o una ocurrencia extremadamente rara ( \\(z &gt; 3\\) o \\(z &lt; -3\\)). Recuerden que no importa cuáles sean las unidades originales, estas reglas aplican a cualquier dato que es aproximadamente normal. En R, podemos obtener unidades estándar usando la función scale: z &lt;- scale(x) Ahora para ver cuántos hombres hay dentro de 2 SD del promedio, simplemente escribimos: mean(abs(z) &lt; 2) #&gt; [1] 0.95 ¡La proporción es aproximadamente el 95%, que es lo que predice la distribución normal! Para tener hasta más confirmación de que la aproximación es precisa, podemos usar gráficos Q-Q (quantile-quantile plots en inglés). 8.10 Gráficos Q-Q Una forma sistemática de evaluar cuán bien se ajusta la distribución normal a los datos es verificar si las proporciones observadas y predecidas coinciden. En general, este es el acercamiento del gráfico Q-Q. Primero, definimos los cuantiles teóricos para la distribución normal. En los libros de estadísticas usamos el símbolo \\(\\Phi(x)\\) para definir la función que nos da la probabilidad de que una distribución normal estándar sea menos que \\(x\\). Así por ejemplo, \\(\\Phi(-1.96) = 0.025\\) y \\(\\Phi(1.96) = 0.975\\). En R, podemos evaluar \\(\\Phi\\) utilizando la función pnorm: pnorm(-1.96) #&gt; [1] 0.025 La función inversa \\(\\Phi^{-1}(x)\\) nos da los cuantiles teóricos para la distribución normal. Así por ejemplo, \\(\\Phi^{-1}(0.975) = 1.96\\). En R, podemos evaluar el inverso de \\(\\Phi\\) utilizando la función qnorm. qnorm(0.975) #&gt; [1] 1.96 Tengan en cuenta que estos cálculos son para la distribución normal estándar por defecto (media = 0, desviación estándar = 1), pero también podemos definirlos para cualquier distribución normal. Podemos hacer esto usando los argumentos mean y sd en las funciones pnorm y qnorm. Por ejemplo, podemos usar qnorm para determinar cuantiles de una distribución con promedio y desviación estándar específicos: qnorm(0.975, mean = 5, sd = 2) #&gt; [1] 8.92 Para la distribución normal, todos los cálculos relacionados con los cuantiles se realizan sin datos, de ahí el nombre de cuantiles teóricos. Pero los cuantiles se pueden definir para cualquier distribución, incluso una empírica. Entonces, si tenemos datos en un vector \\(x\\), podemos definir el cuantil asociado con cualquier proporción \\(p\\) como el \\(q\\) para el cual la proporción de valores por debajo de \\(q\\) es \\(p\\). Usando el código R, podemos definir q como el valor para el cual mean(x &lt;= q) = p. Observen que no todo \\(p\\) tiene un \\(q\\) para el cual la proporción es exactamente \\(p\\). Hay varias maneras de definir el mejor \\(q\\) como se discute en la página de ayuda para la función quantile. Como ejemplo rápido, para los datos de alturas masculinas, vemos que: mean(x &lt;= 69.5) #&gt; [1] 0.515 Entonces, alrededor del 50% son más bajos o iguales a 69 pulgadas. Esto implica que si \\(p=0.50\\), entonces \\(q=69.5\\). La idea de un gráfico Q-Q es que si sus datos están bien aproximados por la distribución normal, los cuantiles de sus datos deberían ser similares a los cuantiles de una distribución normal. Para construir un gráfico Q-Q, hacemos lo siguiente: Definimos un vector de \\(m\\) dimensiones \\(p_1, p_2, \\dots, p_m\\). Definimos un vector de cuantiles \\(q_1, \\dots, q_m\\) para las proporciones \\(p_1, \\dots, p_m\\) usando sus datos. Nos referimos a estos como los cuantiles muestrales (sample quantiles en inglés). Definimos un vector de cuantiles teóricos para las proporciones \\(p_1, \\dots, p_m\\) para una distribución normal con el mismo promedio y desviación estándar que los datos. Graficamos los cuantiles muestrales versus los cuantiles teóricos. Construyamos un diagrama Q-Q usando el código R. Comiencen definiendo el vector de proporciones. p &lt;- seq(0.05, 0.95, 0.05) Para obtener los cuantiles de los datos, podemos usar la función quantile así: sample_quantiles &lt;- quantile(x, p) Para obtener los cuantiles teóricos de distribución normal con promedio y SD correspondiente, utilizamos la función qnorm: theoretical_quantiles &lt;- qnorm(p, mean = mean(x), sd = sd(x)) Para ver si coinciden o no, los graficamos uno contra el otro y dibujamos la línea de identidad: qplot(theoretical_quantiles, sample_quantiles) + geom_abline() Noten que este código es mucho más sencillo si usamos unidades estándar: sample_quantiles &lt;- quantile(z, p) theoretical_quantiles &lt;- qnorm(p) qplot(theoretical_quantiles, sample_quantiles) + geom_abline() El código anterior se incluye para ayudar a describir los gráficos Q-Q. Sin embargo, en la práctica es más fácil usar el código ggplot2 descrito en la Sección 8.16: heights %&gt;% filter(sex == &quot;Male&quot;) %&gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline() Mientras que para la ilustración anterior usamos 20 cuantiles, el valor por defecto de la función geom_qq es utilizar la misma cantidad de cuantiles como datos. 8.11 Percentiles Antes de continuar, definamos algunos términos que se usan comúnmente en el análisis exploratorio de datos. Percentiles son casos especiales de cuantiles que se usan comúnmente. Los percentiles son los cuantiles que se obtienen al configurar el \\(p\\) a \\(0.01, 0.02, ..., 0.99\\). Denominamos, por ejemplo, el caso de \\(p=0.25\\) el cuartilo inferior, ya que nos da un número para el cual el 25% de los datos están por debajo. El percentil más famoso es el 50, también conocido como la mediana. Para la distribución normal, la mediana y el promedio son los mismos, pero generalmente este no es el caso. Otro caso especial que recibe un nombre son los cuartiles, que se obtienen al configurar \\(p=0.25,0.50\\) y \\(0.75\\). 8.12 Diagramas de caja Para presentar los diagramas de caja (boxplots en inglés) volveremos a los datos de asesinatos de EE. UU.. Supongamos que queremos resumir la distribución de la tasa de asesinatos. Usando la técnica de visualización de datos que hemos aprendido, podemos ver que la aproximación normal no aplica aquí: En este caso, el histograma anterior o un gráfico de densidad suave serviría como un resumen relativamente sucinto. Ahora supongamos que los que están acostumbrados a recibir solo dos números como resúmenes nos piden un resumen numérico más compacto. Aquí Tukey ofreció algunos consejos. Primero, recomendó proveer un resumen de cinco números compuestos por el rango junto con los cuartiles (los percentiles 25, 50 y 75). Además, Tukey sugirió ignorar los valores atípicos al calcular el rango y, en su lugar, graficarlos como puntos independientes. Ofreceremos una explicación detallada de los valores atípicos más adelante. Finalmente, recomendó que graficáramos estos números como una “caja” con “bigotes” así: con el cuadro definido por los percentiles 25% y 75% y los bigotes mostrando el rango. La distancia entre estos dos se llama el rango intercuartil. Los dos puntos son valores atípicos según la definición de Tukey. La mediana se muestra con una línea horizontal. A partir de este simple gráfico, hoy conocido como un diagrama de caja, sabemos que la mediana es de aproximadamente 2.5, que la distribución no es simétrica y que el rango es de 0 a 5 para la gran mayoría de los estados con dos excepciones. Discutimos cómo crear diagramas de caja en la Sección 8.16. 8.13 Estratificación En el análisis de datos, a menudo dividimos las observaciones en grupos según los valores de una o más variables asociadas con esas observaciones. Por ejemplo, en la siguiente sección dividimos los valores de altura en grupos según una variable de sexo: hembras y varones. Llamamos a este procedimiento estratificación y nos referimos a los grupos resultantes como estratos. La estratificación es común en la visualización de datos porque a menudo estamos interesados en cómo la distribución de variables difiere entre los diferentes subgrupos. Veremos varios ejemplos a lo largo de esta parte del libro. Revisaremos el concepto de estratificación cuando aprendamos regresión en el Capítulo ?? y en la parte de machine learning del libro. 8.14 Estudio de caso: descripción de alturas de estudiantes (continuación) Usando el histograma, los gráficos de densidad y los gráficos Q-Q, nos hemos convencido de que los datos de altura masculina se aproximan bien con una distribución normal. En este caso, le damos a ET un resumen muy sucinto: las alturas masculinas siguen una distribución normal con un promedio de 69.3 pulgadas y una SD de 3.6 pulgadas. Con esta información, ET tendrá una buena idea de qué esperar cuando conozca a nuestros estudiantes varones. Sin embargo, para proporcionar una imagen completa, también debemos proporcionar un resumen de las alturas femeninas. Aprendimos que los diagramas de caja son útiles cuando queremos comparar rápidamente dos o más distribuciones. Aquí vemos las alturas para varones y hembras: El diagrama inmediatamente demuestra que los varones son, en promedio, más altos que las hembras. Las desviaciones estándar parecen ser similares. Pero, ¿la aproximación normal también funciona para los datos de altura femenina recopilados por la encuesta? Esperamos que sigan una distribución normal, al igual que los varones. Sin embargo, los gráficos exploratorios revelan que la aproximación no es tan útil: Vemos algo que no observamos para los varones: el gráfico de densidad tiene una segunda protuberancia. Además, el gráfico Q-Q muestra que los puntos más altos tienden a ser más altos de lo esperado por la distribución normal. Finalmente, también vemos cinco puntos en el gráfico Q-Q que sugieren alturas más bajas de lo esperado para una distribución normal. Al nuevamente informar a ET, es posible que necesitemos proporcionar un histograma en lugar de solo el promedio y la desviación estándar para las alturas femeninas. Sin embargo, releemos la cita de Tukey y nos damos cuenta de que hemos notado lo que no esperábamos ver. Si observamos otras distribuciones de altura femenina, encontramos que están bien aproximadas con una distribución normal. Entonces, ¿por qué nuestras alumnas son diferentes? ¿Es nuestra clase un requisito para el equipo femenino de baloncesto? ¿Hay una proporción pequeña de mujeres que dicen ser más altas de lo que son? Otra explicación, quizás más probable, es que en el formulario en que los estudiantes ingresaron sus alturas, FEMALE era el sexo predeterminado y algunos varones ingresaron sus alturas, pero olvidaron cambiar la variable de sexo. En cualquier caso, la visualización de datos ha ayudado a descubrir una posible falla en nuestros datos. Con respecto a los cinco valores más pequeños, noten que estos valores son: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% top_n(5, desc(height)) %&gt;% pull(height) #&gt; [1] 51 53 55 52 52 Debido a que estas son alturas autoreportadas, una posibilidad es que las estudiantes quisieron ingresar 5'1&quot;, 5'2&quot;, 5'3&quot; o 5'5&quot;. 8.15 Ejercicios 1. Defina variables que contengan las alturas de varones y hembras de esta manera: library(dslabs) data(heights) male &lt;- heights$height[heights$sex == &quot;Male&quot;] female &lt;- heights$height[heights$sex == &quot;Female&quot;] ¿Cuántas medidas tenemos para cada una? 2. Supongamos que no podemos hacer un gráfico y queremos comparar las distribuciones una al lado de otra. No podemos simplemente enumerar todos los números. En cambio, veremos los percentiles. Cree una tabla de cinco filas que muestre female_percentiles y male_percentiles con los percentiles 10, 30, 50, …, 90 para cada sexo. Luego, cree un data frame con estas dos como columnas. 3. Estudie los siguientes diagramas de caja que muestran los tamaños de población por país: ¿Qué continente tiene el país con el mayor tamaño de población? 4. ¿Qué continente tiene la mediana de tamaño poblacional más grande? 5. ¿Cuál es la mediana del tamaño poblacional de África al millón más cercano? 6. ¿Qué proporción de países en Europa tienen poblaciones menos de 14 millones? 0.99 0.75 0.50 0.25 7. Si utilizamos una transformación logarítmica, ¿qué continente de los anteriores tiene el mayor rango intercuartil? 8. Cargue el set de datos de altura y cree un vector x con solo las alturas masculinas: library(dslabs) data(heights) x &lt;- heights$height[heights$sex==&quot;Male&quot;] ¿Qué proporción de los datos está entre 69 y 72 pulgadas (más alto que 69, pero más bajo o igual a 72)? Sugerencia: use un operador lógico y mean. 9. Supongamos que lo único que sabe sobre los datos es el promedio y la desviación estándar. Use la aproximación normal para estimar la proporción que acaba de calcular. Sugerencia: comience calculando el promedio y la desviación estándar. Luego use la función pnorm para predecir las proporciones. 10. Note que la aproximación calculada en la pregunta nueve está muy cerca del cálculo exacto en la primera pregunta. Ahora realice la misma tarea para valores más atípicos. Compare el cálculo exacto y la aproximación normal para el intervalo (79,81]. ¿Cuántas veces mayor es la proporción real que la aproximación? 11. Aproxime la distribución de hombres adultos en el mundo como distribución normal con un promedio de 69 pulgadas y una desviación estándar de 3 pulgadas. Usando esta aproximación, calcule la proporción de hombres adultos que miden 7 pies de alto o más, conocidos como seven footers. Sugerencia: use la función pnorm. 12. Hay alrededor de mil millones de hombres entre las edades de 18 y 40 en el mundo. Use su respuesta a la pregunta anterior para estimar cuántos de estos hombres (de 18 a 40 años) miden siete pies de altura o más en el mundo. 13. Hay alrededor de 10 jugadores de la Asociación Nacional de Baloncesto (NBA) que miden 7 pies de altura o más. Usando la respuesta a las dos preguntas anteriores, ¿qué proporción de los seven footers del mundo, entre 18 a 40 años, están en la NBA? 14. Repita los cálculos realizados en la pregunta anterior para la altura del baloncelista Lebron James: 6 pies y 8 pulgadas. Hay alrededor de 150 jugadores que son al menos tan altos. 15. Al responder a las preguntas anteriores, descubrimos que no es raro que un jugador de siete pies se convierta en jugador de la NBA. Entonces, ¿que sería una crítica justa de nuestros cálculos? La práctica y el talento son lo que hacen a un gran jugador de baloncesto, no la altura. La aproximación normal no es apropiada para alturas. Como se observa en la pregunta 10, la aproximación normal tiende a subestimar los valores atípicos. Es posible que haya más seven footers de lo que predijimos. Como se observa en la pregunta 10, la aproximación normal tiende a sobreestimar los valores atípicos. Es posible que haya menos seven footers de lo que predijimos. 8.16 Geometrías ggplot2 En el capitulo 7, presentamos el paquete ggplot2 para la visualización de datos. Aquí demostramos cómo generar gráficos relacionados con distribuciones, específicamente los gráficos que se muestran anteriormente en este capítulo. 8.16.1 Diagramas de barras Para generar un diagrama de barras (barplots en inglés) podemos usar la geometría geom_bar. Por defecto, R cuenta los casos en cada categoría y dibuja una barra. Aquí vemos el diagrama de barras para las regiones de Estados Unidos. murders %&gt;% ggplot(aes(region)) + geom_bar() A menudo ya tenemos una tabla con una distribución que queremos presentar como diagrama de barras. Aquí tenemos un ejemplo de tal tabla: data(murders) tab &lt;- murders %&gt;% count(region) %&gt;% mutate(proportion = n/sum(n)) tab #&gt; # A tibble: 4 x 3 #&gt; region n proportion #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Northeast 9 0.176 #&gt; 2 South 17 0.333 #&gt; 3 North Central 12 0.235 #&gt; 4 West 13 0.255 Ya no queremos que geom_bar cuente, sino simplemente grafique una barra a la altura proporcionada por la variable proportion. Para esto necesitamos proveer x (las categorías) y y (los valores) y usar la opción stat=&quot;identity&quot;. tab %&gt;% ggplot(aes(region, proportion)) + geom_bar(stat = &quot;identity&quot;) 8.16.2 Histogramas Para generar histogramas utilizamos geom_histogram. Al revisar la página de ayuda para esta función, vemos que el único argumento requerido es x, la variable para la cual construiremos un histograma. No usamos la x porque sabemos que es el primer argumento. El código se ve así: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_histogram() Si ejecutamos el código anterior, nos da un mensaje: stat_bin() utilizando bins = 30. Elija un mejor valor con binwidth. Anteriormente utilizamos un tamaño de compartimiento de 1 pulgada, por lo que el código se ve así: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_histogram(binwidth = 1) Finalmente, si por razones estéticas queremos agregar color, usamos los argumentos descritos en la página de ayuda. También añadimos etiquetas y un título: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_histogram(binwidth = 1, fill = &quot;blue&quot;, col = &quot;black&quot;) + xlab(&quot;Male heights in inches&quot;) + ggtitle(&quot;Histogram&quot;) 8.16.3 Gráficos de densidad Para crear una densidad suave, usamos geom_density. Para hacer un gráfico de densidad suave con los datos que anteriormente visualizamos como un histograma, podemos usar este código: heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_density() Para rellenar con color, podemos usar el argumento fill. heights %&gt;% filter(sex == &quot;Female&quot;) %&gt;% ggplot(aes(height)) + geom_density(fill=&quot;blue&quot;) Para cambiar la suavidad de la densidad, utilizamos el argumento adjust para multiplicar el valor por defecto por ese adjust. Por ejemplo, si queremos que el parámetro de suavizado sea el doble de grande, usamos: heights %&gt;% filter(sex == &quot;Female&quot;) + geom_density(fill=&quot;blue&quot;, adjust = 2) 8.16.4 Diagramas de caja La geometría para crear diagramas de caja es geom_boxplot. Como ya hemos discutido, los diagramas de caja son útiles para comparar distribuciones. Por ejemplo, a continuación vemos las alturas mostradas anteriormente para las mujeres, pero aquí en comparación con los hombres. Para esta geometría, necesitamos los argumentos x como las categorías y los argumentos y como los valores: 8.16.5 Gráficos Q-Q Para gráficos Q-Q usamos la geometría geom_qq. De la página de ayuda, aprendemos que necesitamos especificar el sample (aprenderemos sobre samples en un capítulo posterior). Aquí tenemos el gráfico Q-Q para alturas de varones: heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = height)) + geom_qq() Por defecto, la variable muestral se compara con una distribución normal con un promedio de 0 y una desviación estándar de 1. Para cambiar esto, utilizamos el argumento dparams según la página de ayuda. Para agregar una línea de identidad simplemente asignen otra capa. Para líneas rectas, usamos la función geom_abline. La línea por defecto es la línea de identidad (pendiente = 1, intercepto = 0). params &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% summarize(mean = mean(height), sd = sd(height)) heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = height)) + geom_qq(dparams = params) + geom_abline() Otra opción aquí es escalar los datos primero y luego hacer un gráfico Q-Q contra la distribución normal estándar. heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline() 8.16.6 Imágenes No hemos tenido que usar imágenes para los conceptos descritos en este capítulo, pero los usaremos en la Sección 10.14, así que presentamos las dos geometrías utilizadas para crear imágenes: geom_tile y geom_raster. Se comportan de manera similar; para ver cómo difieren, consulten la página de ayuda. Para crear una imagen en ggplot2, necesitamos un data frame con las coordenadas x e y, así como los valores asociados con cada uno de estos. Aquí tenemos un data frame: x &lt;- expand.grid(x = 1:12, y = 1:10) %&gt;% mutate(z = 1:120) Tengan en cuenta que esta es la versión tidy de una matriz, matrix(1:120, 12, 10). Para graficar la imagen, usamos el siguiente código: x %&gt;% ggplot(aes(x, y, fill = z)) + geom_raster() Con estas imágenes, a menudo querrán cambiar la escala de color. Esto se puede hacer a través de la capa scale_fill_gradientn. x %&gt;% ggplot(aes(x, y, fill = z)) + geom_raster() + scale_fill_gradientn(colors = terrain.colors(10)) 8.16.7 Gráficos rápidos En la sección 7.13 presentamos qplot como una función útil cuando necesitamos hacer un diagrama de dispersión rápido. También podemos usar qplot para hacer histogramas, diagramas de densidad, diagramas de caja, gráficos Q-Q y más. Aunque no provee el nivel de control de ggplot, qplot es definitivamente útil, ya que nos permite hacer un gráfico con un pequeño fragmento de código. Supongamos que tenemos las alturas femeninas en un objeto x: x &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% pull(height) Para hacer un histograma rápido podemos usar: qplot(x) La función adivina que queremos hacer un histograma porque solo proveemos una variable. En la sección 7.13 vimos que si le proporcionamos dos variables a qplot , automáticamente crea un diagrama de dispersión. Para hacer un gráfico Q-Q rápido, tienen que usar el argumento sample. Recuerden que podemos agregar capas tal como lo hacemos con ggplot. qplot(sample = scale(x)) + geom_abline() Si proveemos un factor y un vector numérico, obtenemos un gráfico como el que vemos abajo. Tengan en cuenta que en el código estamos utilizando el argumento data. Como el data frame no es el primer argumento en qplot, tenemos que usar el operador de punto. heights %&gt;% qplot(sex, height, data = .) También podemos seleccionar una geometría específica mediante el uso del argumento geom. Entonces, para convertir el diagrama anterior en un diagrama de caja, usamos el siguiente código: heights %&gt;% qplot(sex, height, data = ., geom = &quot;boxplot&quot;) También podemos usar el argumento geom para generar un gráfico de densidad en lugar de un histograma: qplot(x, geom = &quot;density&quot;) Aunque no tanto como con ggplot, tenemos cierta flexibilidad para mejorar los resultados de qplot. Mirando la página de ayuda, vemos varias formas en que podemos mejorar el aspecto del histograma anterior. Por ejemplo: qplot(x, bins=15, color = I(&quot;black&quot;), xlab = &quot;Population&quot;) Nota técnica: La razón por la que usamos I(&quot;black&quot;) es porque queremos que qplot trate &quot;black&quot; como un carácter en lugar de convertirlo en un factor. Este es el comportamiento por defecto dentro de aes, que se llama internamente aquí. En general, la función I se usa en R para decir “manténgalo como está”. 8.17 Ejercicios 1. Ahora vamos a usar la función geom_histogram para hacer un histograma de las alturas en el set de datos height. Al leer la documentación para esta función, vemos que requiere solo una asignación, los valores que se utilizarán para el histograma. Haz un histograma de todos los gráficos. ¿Cuál es la variable que contiene las alturas? sex heights height heights$height 2. Ahora cree un objeto ggplot usando el pipe para asignar los datos de altura a un objeto ggplot. Asigne height a los valores de x a través de la función aes. 3. Ahora estamos listos para agregar una capa para hacer el histograma. Utilice el objeto creado en el ejercicio anterior y la función geom_histogram para hacer el histograma. 4. Cuando ejecutamos el código en el ejercicio anterior recibimos la advertencia: stat_bin() utilizando bins = 30. Elija un mejor valor con binwidth. Utilice el argumento binwidth para cambiar el histograma creado en el ejercicio anterior para usar compartimientos de tamaño de 1 pulgada. 5. En lugar de un histograma, vamos a hacer un gráfico de densidad suave. En este caso, no crearemos un objeto, sino haremos y mostraremos el gráfico con una línea de código. Cambie la geometría en el código utilizado anteriormente para hacer una densidad suave en lugar de un histograma. 6. Ahora vamos a hacer un gráfico de densidad para varones y hembras por separado. Podemos hacer esto usando el argumento group. Asignamos grupos a través del mapeo estético, ya que cada punto necesita un grupo antes de hacer los cálculos necesarios para estimar una densidad. 7. También podemos asignar grupos a través del argumento color. Esto tiene el beneficio adicional de que utiliza el color para distinguir los grupos. Cambie el código anterior para usar color. 8. Además, podemos asignar grupos a través del argumento fill. Esto tiene el beneficio adicional de que usa colores para distinguir los grupos así: heights %&gt;% ggplot(aes(height, fill = sex)) + geom_density() Sin embargo, aquí la segunda densidad se traza sobre la primera. Podemos hacer que las curvas sean más visibles mediante el uso de alpha blending para agregar transparencia. Establezca el parámetro alfa a 0.2 en la función geom_density para hacer este cambio. "],
["gapminder.html", "Capítulo 9 Visualización de datos en la práctica 9.1 Estudio de caso: nuevas ideas sobre la pobreza 9.2 Diagrama de dispersión 9.3 Separar en facetas 9.4 Gráficos de series de tiempo 9.5 Transformaciones de datos 9.6 Cómo visualizar distribuciones multimodales 9.7 Cómo comparar múltiples distribuciones con diagramas de caja y gráficos ridge 9.8 La falacia ecológica y la importancia de mostrar los datos", " Capítulo 9 Visualización de datos en la práctica En este capítulo, demostraremos cómo el código relativamente sencillo de ggplot2 puede crear gráficos esclarecedores y estéticamente agradables. Como motivación, crearemos gráficos que nos ayudarán a comprender mejor las tendencias de la salud y la economía mundial. Implementaremos lo que aprendimos en los capítulos 7 y 8.16 y aprenderemos a expandir el código para perfeccionar los gráficos. A medida que avancemos en nuestro estudio de caso, describiremos los principios generales más relevantes a la visualización de datos y aprenderemos conceptos como facetas, gráficos de series de tiempo, transformaciones y gráficos ridge. 9.1 Estudio de caso: nuevas ideas sobre la pobreza Hans Rosling29 era el cofundador de la Fundación Gapminder30, una organización dedicada a educar al público mediante datos para disipar mitos comunes sobre el llamado mundo en desarrollo. La organización utiliza datos para mostrar cómo las tendencias actuales en los campos de salud y economía contradicen las narrativas que emanan de la cobertura sensacionalista de los medios de catástrofes, tragedias y otros eventos desafortunados. Como se indica en el sitio web de la Fundación Gapminder: Los periodistas y cabilderos cuentan historias dramáticas. Ese es su trabajo. Cuentan historias sobre eventos extraordinarios y personas inusuales. Las historias dramáticas se acumulan en las mentes de las personas en una visión del mundo demasiado dramática y con fuertes sentimientos de estrés negativo: “¡El mundo está empeorando!”, “¡Somos nosotros contra ellos!”, “¡Las demás personas son extrañas!”, &quot; ¡La población sigue creciendo! &quot; y “¡A nadie le importa!” Hans Rosling se dedicó, en su propia manera dramática, a educar al público sobre tendencias basadas en datos utilizando visualizaciones de datos eficaces. Esta sección se basa en dos charlas que ejemplifican esta perspectiva educativa: New Insights on Poverty31 y The Best Stats You’ve Ever Seen32. Específicamente, en esta sección usamos datos para intentar responder a las siguientes dos preguntas: ¿Es una caracterización justa del mundo actual decir que está dividido en naciones ricas occidentales y el mundo en desarrollo compuesto por África, Asia y América Latina? ¿Ha empeorado la desigualdad de ingresos en todos los países durante los últimos 40 años? Para responder a estas preguntas, utilizaremos el set de datos gapminder proveído por dslabs. Este set de datos se creó utilizando varias hojas de cálculo disponibles de la Fundación Gapminder. Pueden acceder a la tabla de esta manera: library(tidyverse) library(dslabs) data(gapminder) gapminder %&gt;% as_tibble() #&gt; # A tibble: 10,545 x 9 #&gt; country year infant_mortality life_expectancy fertility population #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Albania 1960 115. 62.9 6.19 1636054 #&gt; 2 Algeria 1960 148. 47.5 7.65 11124892 #&gt; 3 Angola 1960 208 36.0 7.32 5270844 #&gt; 4 Antigu… 1960 NA 63.0 4.43 54681 #&gt; 5 Argent… 1960 59.9 65.4 3.11 20619075 #&gt; # … with 10,540 more rows, and 3 more variables: gdp &lt;dbl&gt;, #&gt; # continent &lt;fct&gt;, region &lt;fct&gt; 9.1.1 La prueba de Hans Rosling Igual que en el video New Insights on Poverty, comenzamos probando nuestros conocimientos sobre las diferencias en la mortalidad infantil en diferentes países. Para cada uno de los cinco pares de países a continuación, ¿qué países creen que tuvieron las tasas de mortalidad infantil más altas en 2015? ¿Qué pares creen que son más similares? Sri Lanka o Turquía Polonia o Corea del Sur Malasia o Rusia Pakistán o Vietnam Tailandia o Sudáfrica Al responder a estas preguntas sin datos, los países no europeos suelen ser elegidos como los que tienen tasas de mortalidad infantil más altas: Sri Lanka sobre Turquía, Corea del Sur sobre Polonia y Malasia sobre Rusia. También es común suponer que los países considerados como parte del mundo en desarrollo: Pakistán, Vietnam, Tailandia y Sudáfrica, tienen tasas de mortalidad igualmente altas. Para responder a estas preguntas con datos, podemos usar dplyr. Por ejemplo, para la primera comparación vemos que: gapminder %&gt;% filter(year == 2015 &amp; country %in% c(&quot;Sri Lanka&quot;,&quot;Turkey&quot;)) %&gt;% select(country, infant_mortality) #&gt; country infant_mortality #&gt; 1 Sri Lanka 8.4 #&gt; 2 Turkey 11.6 Turquía tiene la mayor tasa de mortalidad infantil. Podemos usar este código en todas las comparaciones y descubrimos lo siguiente: country infant mortality country infant mortality Sri Lanka 8.4 Turkey 11.6 Poland 4.5 South Korea 2.9 Malaysia 6.0 Russia 8.2 Pakistan 65.8 Vietnam 17.3 Thailand 10.5 South Africa 33.6 9.2 Diagrama de dispersión La razón por las bajas puntuaciones se deriva de la noción preconcebida de que el mundo está dividido en dos grupos: el mundo occidental (Europa occidental y América del Norte), caracterizado por una larga vida y familias pequeñas, versus el mundo en desarrollo (África, Asia y América Latina), caracterizados por cortos períodos de vida y familias numerosas. ¿Pero los datos respaldan esta visión dicotómica? Los datos necesarios para responder a esta pregunta también están disponibles en nuestra tabla gapminder. Usando nuestras recién aprendidas habilidades de visualización de datos, podemos enfrentar este desafío. Para analizar esta visión del mundo, nuestro primer gráfico es un diagrama de dispersión de la esperanza de vida versus las tasas de fertilidad (número promedio de hijos por mujer). Comenzamos mirando los datos de hace unos 50 años, cuando quizás esta visión se consolidó por primera vez en nuestras mentes. filter(gapminder, year == 1962) %&gt;% ggplot(aes(fertility, life_expectancy)) + geom_point() La mayoría de puntos se dividen en dos categorías distintas: Esperanza de vida alrededor de 70 años y 3 o menos hijos por familia. Esperanza de vida inferior a 65 años y más de 5 niños por familia. Para confirmar que estos países son de las regiones que esperamos, podemos usar un color para representar un continente. filter(gapminder, year == 1962) %&gt;% ggplot( aes(fertility, life_expectancy, color = continent)) + geom_point() En 1962, la visión del “Occidente versus el mundo en desarrollo” se basaba en cierta realidad. ¿Sigue siendo así 50 años después? 9.3 Separar en facetas Podemos graficar fácilmente los datos de 2012 de la misma manera que lo hicimos para 1962. Sin embargo, para hacer comparaciones, es preferible graficar lado a lado. En ggplot2, logramos esto separando las variables en facetas (faceting en inglés): estratificamos los datos por alguna variable y hacemos el mismo gráfico para cada estrato. Para separar en facetas, agregamos una capa con la función facet_grid, que automáticamente separa los gráficos. Esta función les permite separar hasta dos variables en facetas usando columnas para representar una variable y filas para representar la otra. La función espera que las variables de fila y de columna estén separadas por un ~. Aquí vemos un ejemplo de un diagrama de dispersión donde agregramos facet_grid como la última capa: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(continent~year) Arriba vemos un gráfico para cada combinación de continente/año. Sin embargo, esto es solo un ejemplo y más de lo que queremos, que es simplemente comparar dos años: 1962 y 2012. En este caso, solo hay una variable y usamos . para que facet_grid sepa que no estamos usando una de las variables: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(. ~ year) Este gráfico muestra claramente que la mayoría de los países se han mudado del conjunto mundo en desarrollo al conjunto mundo occidental. En 2012, la visión del mundo occidental versus el mundo en desarrollo ya no tiene sentido. Esto es particularmente evidente cuando se compara Europa con Asia, este último ahora con varios países que han realizado grandes mejoras. 9.3.1 facet_wrap Para explorar cómo sucedió esta transformación a través de los años, podemos hacer el gráfico para varios años. Por ejemplo, podemos agregar los años 1970, 1980, 1990 y 2000. Sin embargo, si hacemos esto, no queremos a todos los gráficos en la misma fila, que es lo que hace facet_grid por defecto, ya que aparecerán demasiado estrechos para mostrar los datos. En cambio, queremos usar múltiples filas y columnas. La función facet_wrap nos permite hacer esto automáticamente acomodando la serie de gráficos para que cada imagen tenga dimensiones visibles: years &lt;- c(1962, 1980, 1990, 2000, 2012) continents &lt;- c(&quot;Europe&quot;, &quot;Asia&quot;) gapminder %&gt;% filter(year %in% years &amp; continent %in% continents) %&gt;% ggplot( aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(~year) Este gráfico muestra claramente cómo la mayoría de los países asiáticos han mejorado a un ritmo mucho más rápido que los europeos. 9.3.2 Escalas fijas para mejores comparaciones La elección por defecto del rango de los ejes es importante. Cuando no se usa facet, este rango está determinado por los datos que se muestran en el gráfico. Cuando usan facet, este rango está determinado por los datos que se muestran en todos los gráficos y, por lo tanto, se mantienen fijos en todos los gráficos. Esto hace que las comparaciones entre gráficos sean mucho más fáciles. Por ejemplo, en el gráfico anterior, podemos ver que la esperanza de vida ha aumentado y la fertilidad ha disminuido en la mayoría de los países. Vemos esto porque la nube de puntos se mueve. Este no es el caso si ajustamos las escalas: filter(gapminder, year%in%c(1962, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(. ~ year, scales = &quot;free&quot;) En el gráfico anterior, debemos prestar atención particular al rango para notar que el gráfico de la derecha tiene una mayor esperanza de vida. 9.4 Gráficos de series de tiempo Las visualizaciones anteriores ilustran efectivamente que los datos ya no son compatibles con la visión del mundo occidental frente al mundo en desarrollo. Al ver estos gráficos, surgen nuevas preguntas. Por ejemplo, ¿qué países están mejorando más y cuáles menos? ¿La mejora fue constante durante los últimos 50 años o se aceleró más durante ciertos períodos? Para una mirada más detenida que pueda ayudar a responder a estas preguntas, presentamos gráficos de series de tiempo (time series plots en inglés). Los gráficos de series de tiempo tienen tiempo en el eje-x y un resultado o medida de interés en el eje-y. Por ejemplo, aquí vemos un gráfico de la tendencia de las tasas de fertilidad de Estados Unidos: gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year, fertility)) + geom_point() Observamos que la tendencia no es lineal en absoluto, sino que durante los años sesenta y setenta se produce una fuerte caída por debajo de 2. Luego, la tendencia vuelve a 2 y se estabiliza durante los años noventa. Cuando los puntos están regular y densamente espaciados, como vemos arriba, creamos una curva que une los puntos con líneas, para transmitir que estos datos provienen de una sola serie, aquí un país. Para hacer esto, usamos la función geom_line en vez de geom_point. gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(year, fertility)) + geom_line() Esto es particularmente útil cuando comparamos dos países. Si creamos un subconjunto de los datos para incluir dos países, uno de Europa y uno de Asia, entonces adaptamos el código anterior: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year,fertility)) + geom_line() Claramente, este no es el gráfico que queremos. En lugar de una línea para cada país, se unen los puntos para ambos países porque no le hemos dicho a ggplot que queremos dos líneas independientes. Para que ggplot entienda que hay dos curvas que se deben hacer por separado, asignamos cada punto a un group, uno para cada país: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries &amp; !is.na(fertility)) %&gt;% ggplot(aes(year, fertility, group = country)) + geom_line() ¿Pero qué línea va con qué país? Podemos asignar colores para hacer esta distinción. Un efecto secundario útil de usar el argumento color para asignar diferentes colores a los diferentes países es que los datos se agrupan automáticamente: countries &lt;- c(&quot;South Korea&quot;,&quot;Germany&quot;) gapminder %&gt;% filter(country %in% countries &amp; !is.na(fertility)) %&gt;% ggplot(aes(year,fertility, col = country)) + geom_line() El gráfico muestra claramente cómo la tasa de fertilidad de Corea del Sur cayó drásticamente durante los años sesenta y setenta, y en 1990 tuvo una tasa similar a la de Alemania. 9.4.1 Etiquetas en lugar de leyendas Para los gráficos de tendencias, recomendamos etiquetar las líneas en lugar de usar leyendas, ya que el espectador puede ver rápidamente qué línea representa qué país. Esta sugerencia aplica a la mayoría de los gráficos: las etiquetas generalmente se prefieren a las leyendas. Demostramos cómo hacer esto usando los datos de esperanza de vida. Definimos una tabla de datos con las ubicaciones de las etiquetas y luego usamos una segunda asignación solo para estas etiquetas: labels &lt;- data.frame(country = countries, x = c(1975,1965), y = c(60,72)) gapminder %&gt;% filter(country %in% countries) %&gt;% ggplot(aes(year, life_expectancy, col = country)) + geom_line() + geom_text(data = labels, aes(x, y, label = country), size = 5) + theme(legend.position = &quot;none&quot;) El gráfico muestra claramente cómo una mejora en la esperanza de vida siguió a caídas en las tasas de fertilidad. En 1960, los alemanes vivieron 15 años más que los surcoreanos, aunque para 2010 la brecha está completamente cerrada. Ejemplifica la mejora que muchos países no occidentales han logrado en los últimos 40 años. 9.5 Transformaciones de datos Ahora cambiamos nuestra atención a la segunda pregunta relacionada con la idea común de que la distribución de la riqueza en todo el mundo ha empeorado durante las últimas décadas. Cuando se le pregunta al público en general si los países pobres se han vuelto más pobres y los países ricos se han vuelto más ricos, la mayoría responde que sí. Mediante el uso de estratificación, histogramas, densidades suaves y diagramas de caja, podremos ver si este realmente es el caso. Primero, aprenderemos cómo las transformaciones a veces pueden ayudar a proporcionar resúmenes y gráficos más informativos. La tabla de datos gapminder incluye una columna con el producto interno bruto de los países, o GDP por sus siglas en inglés. El GDP mide el valor de mercado de los bienes y servicios producidos por un país en un año. El GDP por persona a menudo se usa como un resumen aproximado de la riqueza de un país. Aquí dividimos esta cantidad por 365 para obtener la medida más interpretable de dólares por día. Utilizando los dólares estadounidenses actuales como una unidad, una persona que sobrevive con un ingreso de menos de $2 por día se define como viviendo en la “pobreza absoluta”. Agregamos esta variable a la tabla de datos: gapminder &lt;- gapminder %&gt;% mutate(dollars_per_day = gdp/population/365) Los valores del GDP se ajustan a la inflación y representan dólares estadounidenses actuales, por lo que estos valores deben ser comparables a lo largo de los años. Por supuesto, estos son promedios de país y dentro de cada país hay mucha variabilidad. Todos los gráficos y las ideas que se describen a continuación se refieren a los promedios de los países y no a los individuos dentro de estos. 9.5.1 Transformación logarítmica Abajo tenemos un histograma de ingresos diarios desde 1970: past_year &lt;- 1970 gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) Utilizamos el argumento color = &quot;black&quot;para dibujar un límite y distinguir claramente los compartimientos. En este gráfico, vemos que para la mayoría de los países, los promedios están por debajo de $10 por día. Sin embargo, la mayoría del eje-x está dedicado a 35 países con promedio de menos de $10. Por lo tanto, el gráfico no es muy informativo con respecto a países con valores inferiores a $10 por día. Sería más informativo poder ver rápidamente cuántos países tienen ingresos diarios promedio de aproximadamente $1 (extremadamente pobre), $2 (muy pobre), $4 (pobre), $8 (promedio), $16 (acomodado), $32 (rico), $64 (muy rico) por día. Estos cambios son multiplicativos y las transformaciones logarítmicas convierten los cambios multiplicativos en aditivos: cuando se usa la base 2, la duplicación de un valor se convierte en un aumento de 1. Aquí tenemos la distribución si aplicamos una transformación logarítmica base 2: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(log2(dollars_per_day))) + geom_histogram(binwidth = 1, color = &quot;black&quot;) Así logramos ver de cerca a los países de ingresos medios a ingresos bajos. 9.5.2 ¿Qué base? En el caso anterior, utilizamos la base 2 en las transformaciones logarítmicas. Otras opciones comunes son base \\(\\mathrm{e}\\) (el logaritmo natural) y base 10. En general, no recomendamos utilizar el logaritmo natural para la exploración y visualización de datos. La razón es porque mientras \\(2^2, 2^3, 2^4, \\dots\\) o \\(10^2, 10^3, \\dots\\) son fáciles de calcular en nuestras cabezas, lo mismo no es cierto para \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\), por lo que la escala no es intuitiva ni fácil de interpretar. En el ejemplo de dólares por día, utilizamos la base 2 en lugar de la base 10 porque el rango resultante es más fácil de interpretar. El rango de los valores que se trazan es 0.327, 48.885. En la base 10, esto se convierte en un rango que incluye muy pocos enteros: solo 0 y 1. Con la base dos, nuestro rango incluye -2, -1, 0, 1, 2, 3, 4 y 5. Es más fácil calcular \\(2^x\\) y \\(10^x\\) cuando \\(x\\) es un entero y entre -10 y 10, por lo que preferimos tener enteros más pequeños en la escala. Otra consecuencia de un rango limitado es que elegir el ancho del compartimiento (binwidth en inglés) es más difícil. Con log base 2, sabemos que un ancho de compartimiento de 1 se convertirá en un compartimiento con rango \\(x\\) a \\(2x\\). Para un ejemplo en el que la base 10 tiene más sentido, consideren los tamaños de poblaciones. Un logaritmo base 10 es preferible ya que el rango para estos es: filter(gapminder, year == past_year) %&gt;% summarize(min = min(population), max = max(population)) #&gt; min max #&gt; 1 46075 8.09e+08 Abajo tenemos el histograma de los valores transformados: gapminder %&gt;% filter(year == past_year) %&gt;% ggplot(aes(log10(population))) + geom_histogram(binwidth = 0.5, color = &quot;black&quot;) En el gráfico anterior, rápidamente vemos que las poblaciones de los países oscilan entre diez mil y diez mil millones. 9.5.3 ¿Transformar los valores o la escala? Hay dos formas en que podemos usar las transformaciones logarítmicas en los gráficos. Podemos tomar el logaritmo de los valores antes de graficarlos o usar escalas logarítmicas en los ejes. Ambos enfoques son útiles y tienen diferentes ventajas. Si tomamos el logaritmo de los datos, podemos interpretar más fácilmente los valores intermedios en la escala. Por ejemplo, si vemos: ----1----x----2--------3---- para datos transformados con el logaritmo, sabemos que el valor de \\(x\\) es de aproximadamente 1.5. Si usamos escalas logarítmicas: ----1----x----10------100--- entonces, para determinar x, necesitamos calcular \\(10^{1.5}\\), que no es fácil de hacer mentalmente. La ventaja de usar escalas logarítmicas es que vemos los valores originales en los ejes. Sin embargo, la ventaja de mostrar escalas logarítmicas es que los valores originales se muestran en el gráfico y son más fáciles de interpretar. Por ejemplo, veríamos “32 dólares por día” en lugar de “5 log base 2 dólares por día”. Como aprendimos anteriormente, si queremos escalar el eje con logaritmos, podemos usar la función scale_x_continuous. En vez de primero tomar el logaritmo de los valores, aplicamos esta capa: gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) Tengan en cuenta que la transformación logarítmica base 10 tiene su propia función: scale_x_log10(), pero actualmente la base 2 no tiene, aunque fácilmente podríamos definir una. Hay otras transformaciones disponibles a través del argumento trans. Como aprenderemos más adelante, la transformación de raíz cuadrada (sqrt) es útil cuando se consideran conteos. La transformación logística (logit) es útil cuando se grafican proporciones entre 0 y 1. La transformación reverse es útil cuando queremos que los valores más pequeños estén a la derecha o arriba. 9.6 Cómo visualizar distribuciones multimodales En el histograma anterior vemos dos protuberancias: una aproximadamente en 4 y otra aproximadamente en 32. En estadística, estas protuberancias a veces se denominan modas (modes en inglés). La moda de una distribución es el valor con la frecuencia más alta. La moda de distribución normal es el promedio. Cuando una distribución, como la anterior, no disminuye monotónicamente de la moda, llamamos a los lugares donde sube y baja de nuevo modas locales y decimos que la distribución tiene modas múltiples. El histograma anterior sugiere que la distribución de ingreso de los países en 1970 tiene dos modas: una de aproximadamente 2 dólares por día (1 en la escala log 2) y la otra de aproximadamente 32 dólares por día (5 en la escala log 2). Esta bimodalidad es consistente con un mundo dicotómico compuesto por países con ingresos promedio inferiores a $8 (3 en la escala log 2) por día y países por encima de eso. 9.7 Cómo comparar múltiples distribuciones con diagramas de caja y gráficos ridge De acuerdo con el histograma, los valores de distribución del ingreso de 1970 muestran una dicotomía. Sin embargo, el histograma no nos muestra si los dos grupos de países están en el oeste o forman parte del mundo en desarrollo. Comencemos examinando rápidamente los datos por región. Reordenamos las regiones por la mediana y usamos una escala logarítmica. gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% mutate(region = reorder(region, dollars_per_day, FUN = median)) %&gt;% ggplot(aes(dollars_per_day, region)) + geom_point() + scale_x_continuous(trans = &quot;log2&quot;) Ya podemos ver que efectivamente existe una dicotomía “el oeste versus el resto”: hay dos grupos claros, con el grupo rico compuesto por Norteamérica, Europa del Norte y Occidental, Nueva Zelanda y Australia. Definimos grupos basados en esta observación: gapminder &lt;- gapminder %&gt;% mutate(group = case_when( region %in% c(&quot;Western Europe&quot;, &quot;Northern Europe&quot;,&quot;Southern Europe&quot;, &quot;Northern America&quot;, &quot;Australia and New Zealand&quot;) ~ &quot;West&quot;, region %in% c(&quot;Eastern Asia&quot;, &quot;South-Eastern Asia&quot;) ~ &quot;East Asia&quot;, region %in% c(&quot;Caribbean&quot;, &quot;Central America&quot;, &quot;South America&quot;) ~ &quot;Latin America&quot;, continent == &quot;Africa&quot; &amp; region != &quot;Northern Africa&quot; ~ &quot;Sub-Saharan&quot;, TRUE ~ &quot;Others&quot;)) Convertimos esta variable group en un factor para controlar el orden de los niveles: gapminder &lt;- gapminder %&gt;% mutate(group = factor(group, levels = c(&quot;Others&quot;, &quot;Latin America&quot;, &quot;East Asia&quot;, &quot;Sub-Saharan&quot;, &quot;West&quot;))) En la siguiente sección mostramos cómo visualizar y comparar distribuciones entre grupos. 9.7.1 Diagrama de caja El anterior análisis exploratorio de datos reveló dos características sobre la distribución de ingreso promedio en 1970. Usando un histograma, encontramos una distribución bimodal con los modos relacionados con los países pobres y ricos. Ahora queremos comparar la distribución entre estos cinco grupos para confirmar la dicotomía “el oeste versus el resto”. El número de puntos en cada categoría es lo suficientemente grande como para que un gráfico de resumen pueda ser útil. Podríamos generar cinco histogramas o cinco gráficos de densidad, pero puede ser más práctico tener todos los resúmenes visuales en un gráfico. Por lo tanto, comenzamos apilando diagramas de caja uno al lado del otro. Tengan en cuenta que agregamos la capa theme(axis.text.x = element_text(angle = 90, hjust = 1)) para que las etiquetas de grupo sean verticales, ya que no encajan si las mostramos horizontalmente, y para quitar la etiqueta del eje a fin de hacer espacio. p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(gdp)) %&gt;% ggplot(aes(group, dollars_per_day)) + geom_boxplot() + scale_y_continuous(trans = &quot;log2&quot;) + xlab(&quot;&quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) p Los diagramas de caja tienen la limitación de que al resumir los datos en cinco números, se pueden perder características importantes de los datos. Una forma de evitar esto es mostrando los datos. p + geom_point(alpha = 0.5) 9.7.2 Gráficos ridge Mostrar cada punto individual no siempre revela características importantes de la distribución. Aunque no es el caso aquí, cuando el número de puntos de datos es demasiado grande acabamos sobregraficando y mostrar los datos puede ser contraproducente. Los diagramas de caja ayudan con esto al proporcionar un resumen de cinco números, pero esto también tiene limitaciones. Por ejemplo, los diagramas de caja no revelan distribuciones bimodales. Para ver esto, miren los dos gráficos abajo que resumen el mismo set de datos: En los casos en que nos preocupa que el resumen del diagrama de caja sea demasiado simplista, podemos mostrar densidades suaves o histogramas apilados utilizando gráficos ridge. Como estamos acostumbrados a visualizar densidades con valores en el eje-x, las apilamos verticalmente. Además, debido a que necesitamos más espacio en este enfoque, es conveniente superponerlos. El paquete ggridges incluye una función conveniente para hacer esto. Abajo vemos los datos de ingresos, que mostramos arriba con diagramas de caja, pero ahora visualizados con un gráfico ridge. library(ggridges) p &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(dollars_per_day)) %&gt;% ggplot(aes(dollars_per_day, group)) + scale_x_continuous(trans = &quot;log2&quot;) p + geom_density_ridges() Tengan en cuenta que tenemos que invertir el x y y que se usaron para el diagrama de caja. Un parametro útil de geom_density_ridges es scale, que les permite determinar cuánto superponer; por ejemplo, scale = 1 significa que no hay superposición. Valores mayores que 1 resultan en mayor superposición. Si el número de puntos de datos es lo suficientemente pequeño, podemos agregarlos al gráfico ridge usando el siguiente código: p + geom_density_ridges(jittered_points = TRUE) Por defecto, la altura de los puntos está jittered y no se debe interpretar de ninguna manera. Para mostrar puntos de datos, pero sin usar jitter, podemos usar el siguiente código para agregar lo que se conoce como una representación rug de los datos. p + geom_density_ridges(jittered_points = TRUE, position = position_points_jitter(height = 0), point_shape = &#39;|&#39;, point_size = 3, point_alpha = 1, alpha = 0.7) 9.7.3 Ejemplo: distribuciones de ingresos de 1970 versus 2010 La exploración de datos muestra claramente que en 1970 hubo una dicotomía del “oeste versus el resto”. ¿Pero persiste esta dicotomía? Vamos a usar facet_grid para ver cómo han cambiado las distribuciones. Para comenzar, nos enfocamos en dos grupos: el oeste y el resto. Hacemos cuatro histogramas. past_year &lt;- 1970 present_year &lt;- 2010 years &lt;- c(past_year, present_year) gapminder %&gt;% filter(year %in% years &amp; !is.na(gdp)) %&gt;% mutate(west = ifelse(group == &quot;West&quot;, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = &quot;black&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(year ~ west) Antes de interpretar los hallazgos de este gráfico, notamos que hay más países representados en los histogramas de 2010 que en 1970: los conteos totales son mayores. Una razón para esto es que varios países se fundaron después de 1970. Por ejemplo, la Unión Soviética se dividió en diferentes países durante la década de 1990. Otra razón es que hay mas datos disponibles para más países en 2010. Rehacemos los gráficos utilizando solo países con datos disponibles para ambos años. En la parte sobre data wrangling de este libro, aprenderemos a usar herramientas de tidyverse que nos permitará escribir código eficiente para esto, pero aquí podemos usar un código sencillo usando la función intersect: country_list_1 &lt;- gapminder %&gt;% filter(year == past_year &amp; !is.na(dollars_per_day)) %&gt;% pull(country) country_list_2 &lt;- gapminder %&gt;% filter(year == present_year &amp; !is.na(dollars_per_day)) %&gt;% pull(country) country_list &lt;- intersect(country_list_1, country_list_2) Estos 108 constituyen 86% de la población mundial, por lo que este subconjunto debe ser representativo. Vamos a rehacer el gráfico, pero solo para este subconjunto simplemente agregando country %in% country_list a la función filter: Ahora vemos que los países ricos se han vuelto un poco más ricos, pero en términos de porcentaje, los países pobres parecen haber mejorado más. En particular, vemos que la proporción de países en desarrollo que ganan más de $16 por día aumentó sustancialmente. Para ver qué regiones específicas mejoraron más, podemos rehacer los diagramas de caja que hicimos anteriormente, pero ahora agregamos el año 2010 y luego usamos facet para comparar los dos años. gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% ggplot(aes(group, dollars_per_day)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_y_continuous(trans = &quot;log2&quot;) + xlab(&quot;&quot;) + facet_grid(. ~ year) Aquí pausamos para presentar otra importante característica de ggplot2. Como queremos comparar cada región antes y después, sería conveniente tener el diagrama de caja de 1970 al lado del de 2010 para cada región. En general, las comparaciones son más fáciles cuando los datos se grafican uno al lado del otro. Entonces, en lugar de separar en facetas, mantenemos los datos de cada año juntos y pedimos colorearlos (o rellenarlos) según el año. Tengan en cuenta que los grupos se separan automáticamente por año y cada par de diagramas de caja se dibujan uno al lado del otro. Como el año es un número, lo convertimos en un factor ya que ggplot2 asigna automáticamente un color a cada categoría de un factor. Recuerden que tenemos que convertir la columnas year de numérica a factor. gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% mutate(year = factor(year)) %&gt;% ggplot(aes(group, dollars_per_day, fill = year)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_y_continuous(trans = &quot;log2&quot;) + xlab(&quot;&quot;) Finalmente, señalamos que si lo más que nos interesa es comparar los valores de antes y después, podría tener más sentido graficar los aumentos porcentuales. Todavía no estamos listos para aprender a codificar esto, pero así es como se vería el gráfico: La exploración de datos previa sugiere que la brecha de ingresos entre países ricos y pobres se ha reducido considerablemente durante los últimos 40 años. Usamos una serie de histogramas y diagramas de caja para ver esto. Sugerimos una forma sucinta de transmitir este mensaje con solo un gráfico. Empecemos observando que los gráficos de densidad para la distribución del ingreso en 1970 y 2010 transmiten el mensaje de que la brecha se está cerrando: gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% ggplot(aes(dollars_per_day)) + geom_density(fill = &quot;grey&quot;) + scale_x_continuous(trans = &quot;log2&quot;) + facet_grid(. ~ year) En el gráfico de 1970, vemos dos modas claras: países pobres y ricos. En 2010, parece que algunos de los países pobres se han desplazado hacia la derecha, cerrando la brecha. El próximo mensaje que debemos transmitir es que la razón de este cambio en distribución es que varios países pobres se hicieron más ricos, en lugar de que algunos países ricos se hicieron más pobres. Para hacer esto, podemos asignar un color a los grupos que identificamos durante la exploración de datos. Sin embargo, primero tenemos que aprender a hacer estas densidades suaves de una manera que conserve la información sobre el número de países en cada grupo. Para entender por qué necesitamos esto, recuerden la discrepancia en el tamaño de cada grupo: Developing West 87 21 Pero cuando superponemos dos densidades, el comportamiento por defecto es que el área representada por cada distribución sume a 1, independientemente del tamaño de cada grupo: gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% mutate(group = ifelse(group == &quot;West&quot;, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = &quot;log2&quot;) + geom_density(alpha = 0.2) + facet_grid(year ~ .) El gráfico de arriba hace que parezca que hay el mismo número de países en cada grupo. Para cambiar esto, necesitaremos aprender a acceder a las variables calculadas con la función geom_density. 9.7.4 Cómo obtener acceso a variables calculadas Para que las áreas de estas densidades sean proporcionales al tamaño de los grupos, simplemente multiplicamos los valores del eje-y por el tamaño del grupo. En el archivo de ayuda de geom_density, vemos que las funciones calculan una variable denominada count que hace exactamente esto. Queremos que esta variable, y no la densidad, esté en el eje-y. En ggplot2, obtenemos acceso a estas variables rodeando el nombre con dos puntos. Por lo tanto, utilizaremos el siguiente mapeo: aes(x = dollars_per_day, y = ..count..) Ahora podemos crear el diagrama deseado simplemente cambiando el mapeo en el fragmento del código anterior. También ampliaremos los límites del eje-x. p &lt;- gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% mutate(group = ifelse(group == &quot;West&quot;, &quot;West&quot;, &quot;Developing&quot;)) %&gt;% ggplot(aes(dollars_per_day, y = ..count.., fill = group)) + scale_x_continuous(trans = &quot;log2&quot;, limit = c(0.125, 300)) p + geom_density(alpha = 0.2) + facet_grid(year ~ .) Si queremos que las densidades sean más suaves, usamos el argumento bw para que se use el mismo parámetro de suavizado en cada densidad. Seleccionamos 0.75 después de probar varios valores. p + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .) Este gráfico ahora muestra lo que está sucediendo muy claramente. La distribución del mundo en desarrollo está cambiando. Aparece una tercera moda formada por los países que más redujeron la brecha. Para visualizar si alguno de los grupos definidos anteriormente son la causa principal de estos cambios, rápidamente podemos hacer un gráfico ridge: gapminder %&gt;% filter(year %in% years &amp; !is.na(dollars_per_day)) %&gt;% ggplot(aes(dollars_per_day, group)) + scale_x_continuous(trans = &quot;log2&quot;) + geom_density_ridges(adjust = 1.5) + facet_grid(. ~ year) Otra forma de lograr esto es apilando las densidades una encima de otra: gapminder %&gt;% filter(year %in% years &amp; country %in% country_list) %&gt;% group_by(year) %&gt;% mutate(weight = population/sum(population)*2) %&gt;% ungroup() %&gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = &quot;log2&quot;, limit = c(0.125, 300)) + geom_density(alpha = 0.2, bw = 0.75, position = &quot;stack&quot;) + facet_grid(year ~ .) Aquí podemos ver claramente cómo las distribuciones para Asia Oriental, América Latina y otros se desplazan notablemente hacia la derecha. Mientras que África subsahariana permanece estancada. Noten que ordenamos los niveles del grupo para que la densidad del Occidente se grafique primero, luego África subsahariana. Tener los dos extremos graficados primero nos permite ver mejor la bimodalidad restante. 9.7.5 Densidades ponderadas Como punto final, notamos que estas distribuciones ponderan cada país igual. Entonces, si la mayoría de la población está mejorando, pero viviendo en un país muy grande, como China, podríamos no apreciar esto. De hecho, podemos ponderar las densidades suaves usando el argumento de mapeo weight. El gráfico entonces se ve así: Esta figura en particular muestra muy claramente cómo se está cerrando la brecha de distribución de ingresos y que la mayoría de paises que siguen en la pobreza están en África subsahariana. 9.8 La falacia ecológica y la importancia de mostrar los datos A lo largo de esta sección, hemos estado comparando regiones del mundo. Hemos visto que, en promedio, algunas regiones obtienen mejores resultados que otras. En esta sección, nos enfocamos en describir la importancia de la variabilidad dentro de los grupos al examinar la relación entre las tasas de mortalidad infantil de un país y el ingreso promedio. Definimos algunas regiones más y comparamos los promedios entre regiones: La relación entre estas dos variables es casi perfectamente lineal y el gráfico muestra una diferencia dramática. Mientras que en el Occidente muere menos del 0.5% de los bebés, ¡en África subsahariana la tasa es superior al 6%! Observen que el gráfico utiliza una nueva transformación, la transformación logística. 9.8.1 Transformación logística La transformación logística o logit para una proporción o tasa \\(p\\) se define como: \\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\] Cuando \\(p\\) es una proporción o probabilidad, la cantidad que transformamos con el logaritmo, \\(p/(1-p)\\), se llama odds. En este caso \\(p\\) es la proporción de bebés que sobrevivieron. Los odds nos dicen cuántos más bebés se espera que sobrevivan a que mueran. La transformación logarítmica lo hace simétrico. Si las tasas son iguales, entonces el log odds es 0. Los aumentos multiplicativos se convierten en incrementos positivos o negativos, respectivamente. Esta escala es útil cuando queremos resaltar diferencias cercanas a 0 o 1. Para las tasas de supervivencia, esto es importante porque una tasa de supervivencia del 90% es inaceptable, mientras que una supervivencia del 99% es relativamente buena. Preferiríamos mucho una tasa de supervivencia más cercana al 99.9%. Queremos que nuestra escala resalte estas diferencias y el logit lo hace. Recuerden que 99.9/0.1 es aproximadamente 10 veces más grande que 99/1, que es aproximadamente 10 veces más grande que 90/10. Al usar el logaritmo, estos incrementos multiplicativos se convierten en aumentos constantes. 9.8.2 Mostrar los datos Ahora, de vuelta a nuestro gráfico. Basado en el gráfico anterior, ¿concluimos que un país con ingresos bajos está destinado a tener una tasa de supervivencia baja? Además, ¿concluímos que las tasas de supervivencia en el África subsahariana son más bajas que en el sur de Asia, que a su vez son más bajas que en las islas del Pacífico y así sucesivamente? Saltar a esta conclusión basada en un gráfico que muestra promedios se denomina una falacia ecológica. La relación casi perfecta entre las tasas de supervivencia y los ingresos solo se observa para los promedios a nivel regional. Una vez que mostramos todos los datos, vemos una historia más complicada: Específicamente, vemos que hay una gran cantidad de variabilidad. Vemos que los países de las mismas regiones pueden ser bastante diferentes y que los países con los mismos ingresos pueden tener diferentes tasas de supervivencia. Por ejemplo, mientras que, en promedio, África subsahariana tuvo los peores resultados económicos y de salud, existe una gran variabilidad dentro de ese grupo. Mauricio y Botswana están mejores que Angola y Sierra Leona, con Mauricio comparable a países occidentales. https://en.wikipedia.org/wiki/Hans_Rosling↩ http://www.gapminder.org/↩ https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩ https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩ "],
["principios-de-visualización-de-datos.html", "Capítulo 10 Principios de visualización de datos 10.1 Cómo codificar datos utilizando señales visuales 10.2 Sepa cuándo incluir 0 10.3 No distorsionar cantidades 10.4 Ordenar categorías por un valor significativo 10.5 Mostrar los datos 10.6 Cómo facilitar comparaciones 10.7 Piense en los daltónicos 10.8 Gráficos para dos variables 10.9 Cómo codificar una tercera variable 10.10 Evite los gráficos pseudo-tridimensionales 10.11 Evite demasiados dígitos significativos 10.12 Conozca a su audiencia 10.13 Ejercicios 10.14 Estudio de caso: las vacunas y las enfermedades infecciosas 10.15 Ejercicios", " Capítulo 10 Principios de visualización de datos Ya hemos presentado algunas reglas a seguir mientras creamos gráficos para nuestros ejemplos. Aquí, nuestro objetivo es ofrecer algunos principios generales que podemos usar como guía para una visualización de datos efectiva. Gran parte de esta sección se basa en una charla de Karl Broman33 titulada “Creating Effective Figures and Tables”34 e incluye algunas de las figuras que se hicieron con el código que Karl pone a disposición en su repositorio de GitHub35, así como las notas de la clase “Introduction to Data Visualization” de Peter Aldhous36. Siguiendo el enfoque de Karl, mostramos algunos ejemplos de estilos de gráficos que debemos evitar, explicamos cómo mejorarlos y entonces los usamos como motivación para una lista de principios. Además, comparamos y contrastamos los gráficos que siguen estos principios con otros que los ignoran. Los principios se basan principalmente en investigaciones relacionadas a la manera en que los humanos detectan patrones y hacen comparaciones visuales. Los enfoques preferidos son aquellos que mejor se adaptan a la forma en que nuestros cerebros procesan la información visual. Al escoger las herramientas de visualización, es importante tener en cuenta nuestro objetivo. Podemos estar comparando una cantidad de números suficientemente pequeña que se pueden distinguir, describiendo distribuciones de datos categóricos o valores numéricos, comparando los datos de dos grupos o describiendo la relación entre dos variables y esto afecta la presentación que escojeremos. Como nota final, queremos enfatizar que para los científicos de datos es importante adaptar y optimizar los gráficos para la audiencia. Por ejemplo, un gráfico exploratorio hecho para nosotros será diferente a una tabla destinada a comunicar un hallazgo a una audiencia general. Utilizaremos estos paquetes: library(tidyverse) library(dslabs) library(gridExtra) 10.1 Cómo codificar datos utilizando señales visuales Comenzamos describiendo algunos principios para codificar datos. Hay varios acercamientos a nuestra disposición que incluyen posición, largo, ángulos, área, brillo y tono de color. Para ilustrar cómo se comparan algunas de estas estrategias, supongamos que queremos informar los resultados de dos encuestas hipotéticas, tomadas en 2000 y luego en 2015, con respecto a la preferencia de navegador. Para cada año, simplemente estamos comparando cinco cantidades: los cinco porcentajes. Una representación gráfica de porcentajes ampliamente utilizada, y popularizada por Microsoft Excel, es el gráfico circular: Aquí estamos representando cantidades con áreas y ángulos, ya que tanto el ángulo como el área de cada sección del gráfico son proporcionales a la cantidad que representa el sector. Esto resulta ser una opción subóptima dado que, como lo demuestran los estudios de percepción, los humanos no son buenos para cuantificar ángulos con precisión y son aún peores cuando el área es la única señal visual disponible. El gráfico de anillo es un ejemplo de un gráfico que usa solo área: Para ver cuán difícil es cuantificar los ángulos y el área, recuerden que las clasificaciones y todos los porcentajes en los gráficos anteriores cambiaron de 2000 a 2015. ¿Pueden determinar los porcentajes reales y clasificar la popularidad de los navegadores? ¿Pueden ver cómo los porcentajes cambiaron de 2000 a 2015? No es fácil distinguirlo del gráfico. De hecho, la función pie de la página de ayuda de R señala que: Los gráficos circulares son una forma muy mala de mostrar información. El ojo es bueno juzgando medidas lineales y malo juzagando áreas relativas. Un diagrama de barras o de puntos es una forma preferible de mostrar este tipo de datos. En este caso, simplemente mostrar los números no solo es más claro, sino que también ahorraría costos de impresión si imprime una copia en papel: Browser 2000 2015 Opera 3 2 Safari 21 22 Firefox 23 21 Chrome 26 29 IE 28 27 La forma preferida de graficar estas cantidades es usar la longitud y la posición como señales visuales, ya que los humanos son mucho mejores juzgando medidas lineales. El diagrama de barras usa este enfoque al usar barras de longitud proporcionales a las cantidades de interés. Al agregar líneas horizontales a valores estratégicamente elegidos, en este caso en cada múltiplo de 10, aliviamos la carga visual de cuantificar a través de la posición de la parte superior de las barras. Comparen y contrasten la información que podemos extraer de los siguientes dos pares de gráficos. Observen lo fácil que es ver las diferencias en el diagrama de barras. De hecho, ahora podemos determinar los porcentajes reales siguiendo una línea horizontal hasta el eje-x. Si, por alguna razón, tienen que hacer un gráfico circular, etiqueten cada sección del círculo con su porcentaje respectivo para que la audiencia no tenga que inferirlos de los ángulos o del área: En general, cuando se muestran cantidades, se prefieren la posición y la longitud sobre los ángulos y/o el área. El brillo y el color son aún más difíciles de cuantificar que los ángulos. Pero, como veremos más adelante, a veces son útiles cuando se deben mostrar más de dos dimensiones a la vez. 10.2 Sepa cuándo incluir 0 Cuando se usan diagramas de barras, es erróneo no comenzar las barras en 0. Esto se debe a que, al usar un diagrama de barras, estamos implicando que la longitud es proporcional a las cantidades que se muestran. Al evitar 0, se pueden hacer diferencias relativamente pequeñas verse mucho más grandes de lo que realmente son. Este acercamiento a menudo es utilizado por políticos o medios de comunicación que intentan exagerar la diferencia. A continuación se muestra un ejemplo ilustrativo utilizado por Peter Aldhous en esta conferencia: http://paldhous.github.io/ucb/2016/dataviz/week2.htmlfont37. (Fuente: Fox News, vía Media Matters38.) En el gráfico anterior, las detenciones parecen haber casi triplicado cuando, de hecho, solo han aumentado aproximadamente un 16%. Comenzar el gráfico en 0 ilustra esto claramente: Abajo vemos otro ejemplo, que se describe en detalle en un artículo del blog “Flowing Data”: (Fuente: Fox News, a través de Flowing Data39) Este gráfico hace que un aumento del 13% parezca cinco veces más grande. Aquí tenemos el gráfico apropiado: Finalmente, aquí tenemos un ejemplo extremo que hace que una diferencia muy pequeña, de menos de 2%, parezca 10 a 100 veces más grande: (Fuente: Venezolana de Televisión vía Pakistan Today40 y Diego Mariano) Aquí está el gráfico apropiado: Cuando se usa posición en lugar de longitud, no es necesario incluir 0. Este es el caso en particular cuando queremos comparar las diferencias entre los grupos en relación con la variabilidad dentro de un grupo. Aquí tenemos un ejemplo ilustrativo que muestra la esperanza de vida promedio de cada país estratificada por continente en 2012: Tengan en cuenta que en el gráfico de la izquierda, que incluye 0, el espacio entre 0 y 43 no agrega información y hace que sea más difícil comparar la variabilidad entre y dentro del grupo. 10.3 No distorsionar cantidades Durante el discurso del Estado de la Unión de 2011 del Presidente Barack Obama, se utilizó el siguiente gráfico para comparar el PIB de EE. UU. con el PIB de cuatro naciones competidoras: (Fuente: The 2011 State of the Union Address41) Si juzgamos por el área de los círculos, Estados Unidos parece tener una economía cinco veces más grande que la de China y más de 30 veces más grande que la de Francia. Sin embargo, si nos fijamos en los números actuales, vemos que este no es el caso. Las proporciones son 2.6 y 5.8 veces mayores que las de China y Francia, respectivamente. La razón de esta distorsión es que el radio del círculo, en lugar del área, se hizo proporcional a la cantidad, lo que implica que la proporción entre las áreas es cuadrada: 2.6 se convierte en 6.5 y 5.8 se convierte en 34.1. Aquí hay una comparación de los círculos que obtenemos si hacemos que el valor sea proporcional al radio y al área: No sorprende entonces que por defecto ggplot2 use el área en lugar del radio. Sin embargo, en este caso, realmente no deberíamos usar el área, ya que podemos usar la posición y la longitud: 10.4 Ordenar categorías por un valor significativo Cuando uno de los ejes se usa para mostrar categorías, como se hace en los diagramas de barras, el comportamiento por defecto de ggplot2 es ordenar las categorías alfabéticamente cuando se definen por cadenas de caracteres. Si están definidas por factores, se ordenan según los niveles de factores. Raras veces queremos usar el orden alfabético. En cambio, debemos ordenar por una cantidad significativa. En todos los casos anteriores, los diagramas de barras se ordenaron según los valores que mostraban. La excepción fueron los diagramas de barras comparando navegadores. En ese caso, mantuvimos el orden igual en todos los diagramas de barras para facilitar la comparación. Específicamente, en vez de ordenar los navegadores por separado en los dos años, ordenamos ambos años por el valor promedio de 2000 y 2015. Anteriormente aprendimos a usar la función reorder, que nos ayuda a lograr este objetivo. Para apreciar cómo el orden correcto puede ayudar a transmitir un mensaje, supongamos que queremos crear un gráfico para comparar la tasa de homicidios en todos los estados de EE.UU.. Estamos particularmente interesados en los estados más peligrosos y los más seguros. Tengan en cuenta la diferencia cuando ordenamos alfabéticamente, la accion por defecto, versus cuando ordenamos por la tasa real: Podemos hacer el segundo gráfico así: data(murders) murders %&gt;% mutate(murder_rate = total/ population * 100000) %&gt;% mutate(state = reorder(state, murder_rate)) %&gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + theme(axis.text.y = element_text(size = 6)) + xlab(&quot;&quot;) La función reorder también nos permite reordenar grupos. Anteriormente vimos un ejemplo relacionado con la distribución de ingresos entre regiones. Aquí vemos las dos versiones graficadas una contra la otra: El primer gráfico ordena las regiones alfabéticamente, mientras que el segundo las ordena por la mediana del grupo. 10.5 Mostrar los datos Nos hemos enfocado en mostrar cantidades únicas en todas las categorías. Ahora cambiamos nuestra atención a la visualización de datos, con un enfoque en la comparación de grupos. Para motivar nuestro primer principio, “mostrar los datos”, volvemos a nuestro ejemplo artificial de describir alturas a ET, un extraterrestre. Esta vez supongamos que ET está interesado en la diferencia de alturas entre hombres y mujeres. Un gráfico comúnmente utilizado para comparaciones entre grupos y popularizado por software como Microsoft Excel, es el dynamite plot, que muestra el promedio y los errores estándar (los errores estándar se definen en un capítulo posterior, pero no los confundan con la desviación estándar de los datos). El gráfico se ve así: El promedio de cada grupo está representado por la parte superior de cada barra y las antenas se extienden desde el promedio al promedio más dos errores estándar. Si todo lo que ET recibe es este gráfico, tendrá poca información sobre qué esperar si se encuentra con un grupo de hombres y mujeres. Las barras van a 0: ¿esto significa que hay humanos pequeños que miden menos de un pie? ¿Todos los varones son más altos que las hembras más altas? ¿Hay una rango de alturas? ET no puede responder a estas preguntas ya que casi no le hemos dado información sobre la distribución de altura. Esto nos lleva a nuestro primer principio: mostrar los datos. Este código sencillo de ggplot2 ya genera un gráfico más informativo que el diagrama de barras al simplemente mostrar todos los puntos de datos: heights %&gt;% ggplot(aes(sex, height)) + geom_point() El gráfico anterior nos da una idea del rango de los datos. Sin embargo, este gráfico también tiene limitaciones, ya que realmente no podemos ver todos los 238 y 812 puntos graficados para hembras y varones, respectivamente, y muchos puntos están graficados uno encima del otro. Como hemos descrito anteriormente, visualizar la distribución es mucho más informativo. Pero antes de hacer esto, señalamos dos formas en que podemos mejorar un gráfico que muestra todos los puntos. El primero es agregar jitter, que añade un pequeño desplazamiento aleatorio a cada punto. En este caso, agregar jitter horizontal no cambia la interpretación, ya que las alturas de los puntos no cambian, pero minimizamos el número de puntos que se superponen y, por lo tanto, tenemos una mejor idea visual de cómo se distribuyen los datos. Una segunda mejora proviene del uso de alpha blending, que hace que los puntos sean algo transparentes. Entre más puntos se superponen, más oscuro será el gráfico, que también nos ayuda tener una idea de cómo se distribuyen los puntos. Aquí está el mismo gráfico con jitter y alpha blending: heights %&gt;% ggplot(aes(sex, height)) + geom_jitter(width = 0.1, alpha = 0.2) Ahora comenzamos a sentir que, en promedio, los varones son más altos que las hembras. También observamos bandas horizontales oscuras de puntos, que demuestra que muchos estudiantes indican valores que se redondean al entero más cercano. 10.6 Cómo facilitar comparaciones 10.6.1 Use ejes comunes Como hay tantos puntos, es más efectivo mostrar distribuciones que puntos individuales. Por lo tanto, mostramos histogramas para cada grupo: Sin embargo, mirando el gráfico arriba, no es inmediatamente obvio que los varones son, en promedio, más altos que las hembras. Tenemos que mirar cuidadosamente para notar que el eje-x tiene un rango más alto de valores en el histograma masculino. Un principio importante aquí es mantener los ejes iguales cuando se comparan datos en dos gráficos. A continuación, vemos cómo la comparación se vuelve más fácil: 10.6.2 Alinee gráficos verticalmente para ver cambios horizontales y horizontalmente para ver cambios verticales En estos histogramas, la señal visual relacionada con las disminuciones o los aumentos de altura son los cambios hacia la izquierda o hacia la derecha, respectivamente: los cambios horizontales. Alinear los gráficos verticalmente nos ayuda a ver este cambio cuando los ejes son fijos: heights %&gt;% ggplot(aes(height, ..density..)) + geom_histogram(binwidth = 1, color=&quot;black&quot;) + facet_grid(sex~.) El gráfico anterior hace que sea mucho más fácil notar que los varones son, en promedio, más altos. Si queremos obtener el resumen compacto que ofrecen los diagramas de caja, tenemos que alinearlos horizontalmente ya que, por defecto, los diagramas de caja se mueven hacia arriba y hacia abajo según los cambios de altura. Siguiendo nuestro principio de “mostrar los datos”, superponemos todos los puntos de datos: heights %&gt;% ggplot(aes(sex, height)) + geom_boxplot(coef=3) + geom_jitter(width = 0.1, alpha = 0.2) + ylab(&quot;Height in inches&quot;) Ahora comparen y contrasten estos tres gráficos, basados exactamente en los mismos datos: Observen cuánto más aprendemos de los dos gráficos a la derecha. Los diagramas de barras son útiles para mostrar un número, pero no son muy útiles cuando queremos describir distribuciones. 10.6.3 Considere transformaciones Hemos motivado el uso de la transformación logarítmica en los casos en que los cambios son multiplicativos. El tamaño de la población fue un ejemplo en el que encontramos que una transformación logarítmica produjo una transformación más informativa. La combinación de un diagrama de barras elegido incorrectamente y no usar una transformación logarítimica cuando sea necesaria puede ser particularmente distorsionante. Como ejemplo, considere este diagrama de barras que muestra los tamaños de población promedio para cada continente en 2015: Mirando el gráfico anterior, uno concluiría que los países de Asia son mucho más poblados que los de otros continentes. Siguiendo el principio de “mostrar los datos”, notamos rápidamente que esto se debe a dos países muy grandes, que suponemos son India y China: El uso de una transformación logarítmica aquí produce un gráfico mucho más informativo. Comparamos el diagrama de barras original con un diagrama de caja usando la transformación de escala logarítmica para el eje-y: Con el nuevo gráfico, nos damos cuenta de que los países de África en realidad tienen una población mediana mayor que los de Asia. Otras transformaciones que deben considerar son la transformación logística (logit), que es útil para ver mejor los cambios en las probabilidades, y la transformación de la raíz cuadrada (sqrt), que es útil para conteos. 10.6.4 Señales visuales comparadas deben estar adyacentes Para cada continente, comparemos los ingresos en 1970 versus 2010. Al comparar los datos de ingresos entre regiones entre 1970 y 2010, hicimos un gráfico similar al siguiente, pero esta vez investigamos continentes en lugar de regiones. El comportamiento por defecto de ggplot2 es ordenar las etiquetas alfabéticamente para que las etiquetas con 1970 aparezcan antes que las etiquetas con 2010. Esto dificulta las comparaciones porque la distribución de un continente en 1970 está visualmente lejos de su distribución en 2010. Es mucho más fácil hacer la comparación entre 1970 y 2010 para cada continente cuando los diagramas de caja para ese continente están uno al lado del otro: 10.6.5 Use color La comparación se hace aún más fácil si usamos color para denotar las dos cosas que queremos comparar: 10.7 Piense en los daltónicos Alrededor de 10% de la población es daltónica. Desafortunadamente, los colores por defecto utilizados en ggplot2 no son óptimos para este grupo. Sin embargo, ggplot2 hace fácil cambiar la paleta de colores utilizada en los gráficos. Aquí se describe un ejemplo de cómo podemos usar una paleta que considera los daltónicos: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palettefont: color_blind_friendly_cols &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) Aquí están los colores: Además, hay varios recursos que pueden ayudarlos a seleccionar colores, por ejemplo este: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/font. 10.8 Gráficos para dos variables En general, deben usar diagramas de dispersión para visualizar la relación entre dos variables. Para cada caso en el que hemos examinado la relación entre dos variables, incluyendo asesinatos totales versus tamaño de población, esperanza de vida versus tasas de fertilidad y mortalidad infantil versus ingresos, hemos utilizado diagramas de dispersión y ese es el gráfico que generalmente recomendamos. Sin embargo, hay algunas excepciones y aquí describimos dos gráficos alternativos: el slope chart y el gráfico Bland-Altman. 10.8.1 Slope charts Una excepción en la que otro tipo de gráfico puede ser más informativo es cuando se comparan variables del mismo tipo, pero en diferentes momentos y para un número relativamente pequeño de comparaciones. Por ejemplo, si estamos comparando la esperanza de vida entre 2010 y 2015. En ese caso, podríamos recomendar un slope chart. No hay geometría para los slope charts en ggplot2, pero podemos construir una usando geom_line. Necesitamos hacer algunos ajustes para agregar etiquetas. A continuación, mostramos un ejemplo que compara 2010 a 2015 para los grandes países occidentales: west &lt;- c(&quot;Western Europe&quot;,&quot;Northern Europe&quot;,&quot;Southern Europe&quot;, &quot;Northern America&quot;,&quot;Australia and New Zealand&quot;) dat &lt;- gapminder %&gt;% filter(year%in% c(2010, 2015) &amp; region %in% west &amp; !is.na(life_expectancy) &amp; population &gt; 10^7) dat %&gt;% mutate(location = ifelse(year == 2010, 1, 2), location = ifelse(year == 2015 &amp; country %in% c(&quot;United Kingdom&quot;, &quot;Portugal&quot;), location+0.22, location), hjust = ifelse(year == 2010, 1, 0)) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot(aes(year, life_expectancy, group = country)) + geom_line(aes(color = country), show.legend = FALSE) + geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) + xlab(&quot;&quot;) + ylab(&quot;Life Expectancy&quot;) Una ventaja del slope chart es que rápidamente nos permite tener una idea de los cambios basados en la pendiente de las líneas. Aunque estamos usando el ángulo como señal visual, también posición para determinar los valores exactos. Comparar las mejoras es un poco más difícil con un diagrama de dispersión: En el diagrama de dispersión, hemos seguido el principio de usar ejes comunes porque estamos comparando estos antes y después. Sin embargo, si tenemos muchos puntos, los slope charts dejan de ser útiles ya que se hace difícil ver todas las líneas. 10.8.2 Gráfico Bland-Altman Como estamos interesados principalmente en la diferencia, tiene sentido dedicarle uno de nuestros ejes. El gráfico Bland-Altman, también conocido como el gráfico de diferencia de medias de Tukey y como el MA plot, muestra la diferencia versus el promedio: library(ggrepel) dat %&gt;% mutate(year = paste0(&quot;life_expectancy_&quot;, year)) %&gt;% select(country, year, life_expectancy) %&gt;% spread(year, life_expectancy) %&gt;% mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2, difference = life_expectancy_2015 - life_expectancy_2010) %&gt;% ggplot(aes(average, difference, label = country)) + geom_point() + geom_text_repel() + geom_abline(lty = 2) + xlab(&quot;Average of 2010 and 2015&quot;) + ylab(&quot;Difference between 2015 and 2010&quot;) Aquí, al simplemente mirar el eje-y, rápidamente vemos qué países han mostrado la mayor mejora. También, obtenemos una idea del valor general del eje-x. 10.9 Cómo codificar una tercera variable Un diagrama de dispersión anterior mostró la relación entre la supervivencia infantil y el ingreso promedio. A continuación se muestra una versión de este gráfico que codifica tres variables: pertenencia a OPEC, región y población. Codificamos variables categóricas con color y forma. Estas formas se pueden controlar con el argumento shape. Abajo mostramos las formas disponibles para su uso en R. Para los últimos cinco, el color rellena la forma. Para variables continuas, podemos usar color, intensidad o tamaño. A continuación ofrecemos un ejemplo de cómo hacer esto con un estudio de caso. Al seleccionar colores para cuantificar una variable numérica, elegimos entre dos opciones: secuenciales y divergentes. Los colores secuenciales son adecuados para datos que van de mayor a menor. Los valores altos se distinguen claramente de los valores bajos. Aquí tenemos algunos ejemplos ofrecidos por el paquete RColorBrewer: library(RColorBrewer) display.brewer.all(type=&quot;seq&quot;) Los colores divergentes se utilizan para representar valores que divergen de un centro. Ponemos igual énfasis en ambos extremos del rango de datos: más alto que el centro y más bajo que el centro. Un ejemplo de cuándo usaríamos un patrón divergente sería cuando mostramos la altura en cuántas desviaciones estándar están del promedio. Aquí hay algunos ejemplos de patrones divergentes: library(RColorBrewer) display.brewer.all(type=&quot;div&quot;) 10.10 Evite los gráficos pseudo-tridimensionales La siguiente figura, tomada de la literatura científica42, muestra tres variables: dosis, tipo de fármaco y supervivencia. Aunque sus pantallas o páginas de libros son planas y bidimensionales, el gráfico intenta imitar tres dimensiones y asigna una dimensión a cada variable. (Imagen cortesía de Karl Broman) Los humanos no son buenos para ver en tres dimensiones (que explica por qué es difícil estacionar en paralelo) y nuestra limitación es aún peor con respecto a las pseudo-tres dimensiones. Para ver esto, intenten determinar los valores de la variable de supervivencia en el gráfico anterior. ¿Pueden decir cuándo la cinta púrpura se cruza con la roja? Este es un ejemplo en el que podemos fácilmente usar color para representar la variable categórica en lugar de un pseudo-3D: Observe cuánto más fácil es determinar los valores de supervivencia. Pseudo-3D a veces se usa de forma totalmente gratuita: los gráficos se hacen para que se vean en 3D incluso cuando la tercera dimensión no representa una cantidad. Esto solo agrega confusión y hace que sea más difícil transmitir su mensaje. Aquí hay dos ejemplos: (Imágenes cortesía de Karl Broman) 10.11 Evite demasiados dígitos significativos Por defecto, el software estadístico como R devuelve muchos dígitos significativos. El comportamiento por defecto en R es mostrar 7 dígitos significativos. Esa cantidad de dígitos a menudo no añade información y el desorden visual agregado puede dificultar que se entienda el mensaje. Como ejemplo, aquí están las tasas de enfermedades por cada 10,000 para California en cinco décadas, calculadas de los totales y la población con R: state year Measles Pertussis Polio California 1940 37.8826320 18.3397861 0.8266512 California 1950 13.9124205 4.7467350 1.9742639 California 1960 14.1386471 NA 0.2640419 California 1970 0.9767889 NA NA California 1980 0.3743467 0.0515466 NA Estamos reportando precisión de hasta 0.00001 casos por 10,000, un valor muy pequeño en el contexto de los cambios que ocurren a través del tiempo. En este caso, dos cifras significativas son más que suficientes y claramente indican que las tasas están disminuyendo: state year Measles Pertussis Polio California 1940 37.9 18.3 0.8 California 1950 13.9 4.7 2.0 California 1960 14.1 NA 0.3 California 1970 1.0 NA NA California 1980 0.4 0.1 NA Para cambiar el número de dígitos significativos o redondear números usamos signif y round. Pueden definir el número de dígitos significativos a nivel mundial configurando opciones como esta: options(digits = 3). Otro principio relacionado con la visualización de tablas es colocar los valores que se comparan en columnas en lugar de filas. Observen que nuestra tabla anterior es más fácil de leer que esta: state disease 1940 1950 1960 1970 1980 California Measles 37.9 13.9 14.1 1 0.4 California Pertussis 18.3 4.7 NA NA 0.1 California Polio 0.8 2.0 0.3 NA NA 10.12 Conozca a su audiencia Los gráficos se pueden usar para 1) nuestros propios análisis exploratorios de datos, 2) para transmitir un mensaje a los expertos o 3) para ayudar a contar una historia a una audiencia general. Asegúrense de que el público destinario comprenda cada elemento del gráfico. Como un ejemplo sencillo, considere que para su propia exploración puede ser más útil transformar logarítmicamente los datos y luego graficarlos. Sin embargo, para una audiencia general que no está familiarizada con la conversión de valores logarítmicos a las medidas originales, será mucho más fácil entender el uso de una escala logarítmica para el eje en lugar de los valores transformados logarítmicamente. 10.13 Ejercicios Para estos ejercicios, utilizaremos los datos de vacunas en el paquete dslabs: library(dslabs) data(us_contagious_diseases) 1. Los gráficos circulares son apropiados: Cuando queremos mostrar porcentajes. Cuando ggplot2 no está disponible. Cuando estoy jugando frisbee. Nunca. Los diagramas de barras y las tablas siempre son mejores. 2. ¿Cuál es el problema con el siguiente gráfico? Los valores están mal. La votación final fue de 306 a 232. El eje no comienza en 0. Juzgando por la longitud, parece que Trump recibió 3 veces más votos cuando, de hecho, fue aproximadamente 30% más. Los colores deben ser iguales. Los porcentajes deben mostrarse como un gráfico circular. 3. Mire a los siguientes dos gráficos. Muestran la misma información: tasas de sarampión de 1928 en los 50 estados. ¿Qué gráfico es más fácil de leer si quieren determinar cuáles son los mejores y peores estados en términos de tasas, y por qué? Dan la misma información, por lo que ambos son igual de buenos. El gráfico de la izquierda es mejor porque ordena los estados alfabéticamente. El gráfico de la derecha es mejor porque el orden alfabético no tiene nada que ver con la enfermedad y al ordenar según la tasa real, rápidamente vemos los estados con las tasas más altas y más bajas. Ambos gráficos deben ser un gráfico circular. 4. Para hacer el gráfico de la izquierda, tenemos que reordenar los niveles de las variables de los estados. dat &lt;- us_contagious_diseases %&gt;% filter(year == 1967 &amp; disease==&quot;Measles&quot; &amp; !is.na(population)) %&gt;% mutate(rate = count/ population * 10000 * 52/ weeks_reporting) Recuerde lo que sucede cuando hacemos un diagrama de barras: dat %&gt;% ggplot(aes(state, rate)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() Defina estos objetos: state &lt;- dat$state rate &lt;- dat$count/dat$population*10000*52/dat$weeks_reporting Redefina el objeto state para que los niveles se reordenen. Imprima el nuevo objeto state y sus niveles para que pueda ver que los niveles no reordenan el vector. 5. Ahora edite el código de arriba para redefinir dat para que los niveles de la variable state se reordenen por la variable rate. Luego haga un diagrama de barras usando el código anterior, pero para este nuevo dat. 6. Digamos que está interesado en comparar las tasas de homicidios con armas de fuego en todas las regiones de los EE. UU. Ve este gráfico: library(dslabs) data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) %&gt;% group_by(region) %&gt;% summarize(avg = mean(rate)) %&gt;% mutate(region = factor(region)) %&gt;% ggplot(aes(region, avg)) + geom_bar(stat=&quot;identity&quot;) + ylab(&quot;Murder Rate Average&quot;) y decide mudarse a un estado en la región occidental. ¿Cuál es el problema principal con esta interpretación? Las categorías están ordenadas alfabéticamente. El gráfico no muestra errores estándar. El gráfico no muestra todos los datos. No vemos la variabilidad dentro de una región y es posible que los estados más seguros no estén en el occidente. El noreste tiene el promedio más bajo. 7. Haga un diagrama de caja de las tasas de asesinatos que se definen como: data(&quot;murders&quot;) murders %&gt;% mutate(rate = total/population*100000) por región, mostrando todos los puntos y ordenando las regiones por su tasa mediana. 8. Los gráficos a continuación muestran tres variables continuas. La línea \\(x=2\\) parece separar los puntos. Pero en realidad no es el caso, como vemos cuando graficamos los datos en un par de puntos bidimensionales. ¿Por qué pasa esto? Los humanos no son buenos para leer gráficos pseudo-3D. Debe haber un error en el código. Los colores nos confunden. Los diagramas de dispersión no se deben usar para comparar dos variables cuando tenemos acceso a tres variables. 9. Reproduzca el gráfico de imagen que hicimos anteriormente pero para la viruela. Para este gráfico, no incluya años en los que no se indicaron casos en 10 o más semanas. 10. Ahora reproduzca el gráfico de series de tiempo que hicimos anteriormente, pero esta vez siguiendo las instrucciones de la pregunta anterior. 11. Para el estado de California, haga gráficos de series de tiempo que muestren las tasas de todas las enfermedades. Incluya solo años en los cuales se proveen datos en 10 o más semanas. Use un color diferente para cada enfermedad. 12. Ahora haga lo mismo con las tasas para EE. UU.. Sugerencia: calcule la tasa de EE. UU. usando summarize, el total de casos dividido por la población total. 10.14 Estudio de caso: las vacunas y las enfermedades infecciosas Las vacunas han ayudado a salvar millones de vidas. En el siglo XIX, antes de que se lograra la inmunización de grupo a través de programas de vacunación, las muertes por enfermedades infecciosas, como la viruela y la poliomielitis, eran comunes. Sin embargo, hoy los programas de vacunación se han vuelto algo controversiales a pesar de toda la evidencia científica de su importancia. La controversia comenzó con un artículo43 publicado en 1988 y liderado por Andrew Wakefield, que afirmó la existencia de un vínculo entre la administración de la vacuna contra el sarampión, las paperas y la rubéola, y el autismo y las enfermedades intestinales. A pesar de la gran cantidad de evidencia científica que contradice este hallazgo, los informes sensacionalistas de los medios de comunicación y el alarmismo de los que creen en teorías de conspiración llevaron a partes del público a creer que las vacunas eran perjudiciales. Como resultado, muchos padres dejaron de vacunar a sus hijos. Esta práctica peligrosa puede ser potencialmente desastrosa dado que los Centros para el Control de Enfermedades de EE.UU., o CDC por sus siglas en inglés, estiman que las vacunas evitarán más de 21 millones de hospitalizaciones y 732,000 muertes entre los niños nacidos en los últimos 20 años (ver “Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR”44). Desde entonces, “The Lancet” ha retractado el artículo y Andrew Wakefield finalmente fue “excluido del registro médico en mayo de 2010 con una observación que indica la falsificación fraudulenta en que incurrió y se le revocó la licencia para ejercer la medicina en el Reino Unido” (Fuente: Wikipedia45). Sin embargo, los conceptos erróneos perduran, en parte debido a activistas autoproclamados que continúan diseminando información errónea sobre las vacunas. La comunicación efectiva de datos es un fuerte antídoto contra la información errónea y el miedo. Anteriormente enseñamos un ejemplo de un artículo del Wall Street Journal46 que muestra datos relacionados con el impacto de las vacunas en la lucha contra las enfermedades infecciosas. A continuación reconstruiremos ese ejemplo. Los datos utilizados para estos gráficos fueron recopilados, organizados y distribuidos por el Proyecto Tycho47 e incluyen conteos reportados semanalmente para siete enfermedades desde 1928 hasta 2011 en los cincuenta estados de EE.UU.. Incluimos los totales anuales en el paquete dslabs: library(tidyverse) library(RColorBrewer) library(dslabs) data(us_contagious_diseases) names(us_contagious_diseases) #&gt; [1] &quot;disease&quot; &quot;state&quot; &quot;year&quot; #&gt; [4] &quot;weeks_reporting&quot; &quot;count&quot; &quot;population&quot; Creamos un objeto temporero dat que almacena solo los datos de sarampión, incluye la tasa por 100,000, ordena a los estados según el valor promedio de enfermedad y elimina Alaska y Hawai ya que estos dos se convirtieron en estados a fines de la década de 1950. Tengan en cuenta que hay una columna weeks_reporting que nos dice cuántas semanas del año se reportaron datos. Tenemos que ajustar ese valor al calcular la tasa: the_disease &lt;- &quot;Measles&quot; dat &lt;- us_contagious_diseases %&gt;% filter(!state%in%c(&quot;Hawaii&quot;,&quot;Alaska&quot;) &amp; disease == the_disease) %&gt;% mutate(rate = count/ population * 10000 * 52/ weeks_reporting) %&gt;% mutate(state = reorder(state, rate)) Ahora podemos graficar fácilmente las tasas de enfermedad por año. Aquí están los datos de sarampión de California: dat %&gt;% filter(state == &quot;California&quot; &amp; !is.na(rate)) %&gt;% ggplot(aes(year, rate)) + geom_line() + ylab(&quot;Cases per 10,000&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) Agregamos una línea vertical en 1963, ya que es cuando se introdujo la vacuna48. ¿Ahora podemos mostrar datos para todos los estados en un gráfico? Tenemos tres variables para incluir: año, estado y tasa. En la figura del WSJ, usan el eje-x para el año, el eje-y para el estado y el tono de color para representar las tasas. Sin embargo, la escala de colores que utilizan, que va del amarillo al azul al verde al naranja al rojo, se puede mejorar. En nuestro ejemplo queremos usar una paleta secuencial ya que no hay un centro significativo, solo tasas bajas y altas. Usamos la geometría geom_tile para tejar la región con colores que representan las tasas de enfermedad. Usamos una transformación de raíz cuadrada para evitar que los conteos particularmente altos dominen el gráfico. Observen que los valores faltantes se muestran en gris. Además, noten que tan pronto una enfermedad fue prácticamente erradicada, algunos estados dejaron de informar casos por completo. Es por esa razón que vemos tanto gris después de 1980. dat %&gt;% ggplot(aes(year, state, fill = rate)) + geom_tile(color = &quot;grey50&quot;) + scale_x_continuous(expand=c(0,0)) + scale_fill_gradientn(colors = brewer.pal(9, &quot;Reds&quot;), trans = &quot;sqrt&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) + theme_minimal() + theme(panel.grid = element_blank(), legend.position=&quot;bottom&quot;, text = element_text(size = 8)) + ggtitle(the_disease) + ylab(&quot;&quot;) + xlab(&quot;&quot;) Este gráfico ofrece evidencia preponderante a favor de la contribución de las vacunas. Sin embargo, una limitación es que usa el color para representar la cantidad, que, como explicamos anteriormente, dificulta saber exactamente cuán altos llegan los valores. La posición y la longitud son mejores señales. Si estamos dispuestos a perder información de estado, podemos hacer una versión del gráfico que muestre los valores con posición. También podemos mostrar el promedio de EE. UU., que calculamos así: avg &lt;- us_contagious_diseases %&gt;% filter(disease==the_disease) %&gt;% group_by(year) %&gt;% summarize(us_rate = sum(count, na.rm = TRUE)/ sum(population, na.rm = TRUE) * 10000) Ahora para hacer el gráfico simplemente usamos la geometría geom_line: dat %&gt;% filter(!is.na(rate)) %&gt;% ggplot() + geom_line(aes(year, rate, group = state), color = &quot;grey50&quot;, show.legend = FALSE, alpha = 0.2, size = 1) + geom_line(mapping = aes(year, us_rate), data = avg, size = 1) + scale_y_continuous(trans = &quot;sqrt&quot;, breaks = c(5, 25, 125, 300)) + ggtitle(&quot;Cases per 10,000 by state&quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) + geom_text(data = data.frame(x = 1955, y = 50), mapping = aes(x, y, label=&quot;US average&quot;), color=&quot;black&quot;) + geom_vline(xintercept=1963, col = &quot;blue&quot;) En teoría, podríamos usar el color para representar los estados, que son una variable categórica, pero es difícil elegir 50 colores distintos. 10.15 Ejercicios Reproduzca el mapa de matriz que hicimos anteriormente pero para la viruela. Para este gráfico, no incluya los años en que no se reportaron casos por 10 o más semanas. Ahora reproduzca el gráfico de series de tiempo que hicimos anteriormente, pero esta vez siguiendo las instrucciones de la pregunta anterior para la viruela. Para el estado de California, haga un gráfico de series de tiempo que muestre las tasas de todas las enfermedades. Incluya solo años con informes de 10 o más semanas. Use un color diferente para cada enfermedad. Ahora haga lo mismo con las tasas para Estados Unidos. Sugerencia: calcule la tasa de EE. UU. usando summarize: total dividido por población total. http://kbroman.org/↩ https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf↩ https://github.com/kbroman/Talk_Graphs↩ http://paldhous.github.io/ucb/2016/dataviz/index.html↩ http://paldhous.github.io/ucb/2016/dataviz/week2.html↩ http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩ http://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩ https://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩ https://www.youtube.com/watch?v=kl2g40GoRxg↩ https://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩ http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract↩ https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm↩ https://es.wikipedia.org/wiki/Andrew_Wakefield↩ http://graphics.wsj.com/infectious-diseases-and-vaccines/↩ http://www.tycho.pitt.edu/↩ Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505↩ "],
["robust-summaries.html", "Capítulo 11 Resúmenes robustos 11.1 Valores atípicos 11.2 Mediana 11.3 El rango intercuartil (IQR) 11.4 La definición de Tukey de un valor atípico 11.5 Desviación absoluta mediana 11.6 Ejercicios 11.7 Estudio de caso: alturas autoreportadas de estudiantes", " Capítulo 11 Resúmenes robustos 11.1 Valores atípicos Anteriormente describimos cómo los diagramas de caja muestran valores atípicos (outliers en inglés), pero no ofrecimos una definición precisa. Aquí discutimos los valores atípicos, los acercamientos que pueden ayudar para detectarlos y los resúmenes que toman en cuenta su presencia. Los valores atípicos son muy comunes en la ciencia de datos. La recopilación de datos puede ser compleja y es común observar puntos de datos generados por error. Por ejemplo, un viejo dispositivo de monitoreo puede leer mediciones sin sentido antes de fallar por completo. El error humano también es una fuente de valores atípicos, en particular cuando la entrada de datos se realiza manualmente. Un individuo, por ejemplo, puede ingresar erróneamente su altura en centímetros en lugar de pulgadas o colocar el decimal en el lugar equivocado. ¿Cómo distinguimos un valor atípico de mediciones que son demasiado grandes o pequeñas simplemente debido a la variabilidad esperada? Esta no siempre es una pregunta fácil de contestar, pero intentaremos ofrecer alguna orientación. Comencemos con un caso sencillo. Supongamos que un colega se encarga de recopilar datos demográficos para un grupo de varones. Los datos indican la altura en pies y se almacenan en el objeto: library(tidyverse) library(dslabs) data(outlier_example) str(outlier_example) #&gt; num [1:500] 5.59 5.8 5.54 6.15 5.83 5.54 5.87 5.93 5.89 5.67 ... Nuestro colega utiliza el hecho de que las alturas suelen estar bien aproximadas por una distribución normal y resume los datos con el promedio y la desviación estándar: mean(outlier_example) #&gt; [1] 6.1 sd(outlier_example) #&gt; [1] 7.8 y escribe un informe sobre el hecho interesante de que este grupo de varones es mucho más alto de lo normal. ¡La altura promedio es más de seis pies! Sin embargo, al usar sus conocimientos de ciencia de datos, notan algo más que es inesperado: la desviación estándar es de más de 7 pies. Al sumar y restar dos desviaciones estándar, observan que el 95% de esta población parece tener alturas entre -9.489, 21.697 pies, que no tiene sentido. Un gráfico rápido muestra el problema: boxplot(outlier_example) Parece que hay al menos un valor que no tiene sentido, ya que sabemos que una altura de 180 pies es imposible. El diagrama de caja detecta este punto como un valor atípico. 11.2 Mediana Cuando tenemos un valor atípico como este, el promedio puede llegar a ser muy grande. Matemáticamente, podemos hacer que el promedio sea tan grande como queramos simplemente cambiando un número: con 500 puntos de datos, podemos aumentar el promedio en cualquier cantidad \\(\\Delta\\) añadiendo \\(\\Delta \\times\\) 500 a un solo número. La mediana, definida como el valor para el cual la mitad de los valores son más pequeños y la otra mitad son más grandes, es robusta para tales valores atípicos. No importa cuán grande hagamos el punto más grande, la mediana sigue siendo la misma. Con estos datos, la mediana es: median(outlier_example) #&gt; [1] 5.74 lo cual es aproximadamente 5 pies y 9 pulgadas. La mediana es lo que los diagramas de caja muestran como una línea horizontal. 11.3 El rango intercuartil (IQR) La caja en un diagrama de caja se define por el primer y tercer cuartil. Estos están destinados a proveer una idea de la variabilidad en los datos: el 50% de los datos están dentro de este rango. La diferencia entre el 3er y 1er cuartil (o los percentiles 75 y 25) se conoce como el rango intercuartil, o IQR por sus siglas en inglés. Como sucede con la mediana, esta cantidad será robusta para los valores atípicos ya que los valores grandes no la afectan. Podemos hacer algunos cálculos y ver que para los datos que siguen la distribución normal, el IQR / 1.349 se aproxima a la desviación estándar de los datos si un valor atípico no hubiera estado presente. Podemos ver que esto funciona bien en nuestro ejemplo, ya que obtenemos una estimado de la desviación estándar de: IQR(outlier_example)/ 1.349 #&gt; [1] 0.245 lo cual es cerca de 3 pulgadas. 11.4 La definición de Tukey de un valor atípico En R, los puntos que caen fuera de los bigotes del diagrama de caja y se denominan valores atípicos, una definición que Tukey introdujo. El bigote superior termina en el percentil 75 más 1.5 \\(\\times\\) IQR, mientras que el bigote inferior termina en el percentil 25 menos 1.5 \\(\\times\\) IQR. Si definimos el primer y el tercer cuartil como \\(Q_1\\) y \\(Q_3\\), respectivamente, entonces un valor atípico es cualquier valor fuera del rango: \\[[Q_1 - 1.5 \\times (Q_3 - Q1), Q_3 + 1.5 \\times (Q_3 - Q1)].\\] Cuando los datos se distribuyen normalmente, las unidades estándar de estos valores son: q3 &lt;- qnorm(0.75) q1 &lt;- qnorm(0.25) iqr &lt;- q3 - q1 r &lt;- c(q1 - 1.5*iqr, q3 + 1.5*iqr) r #&gt; [1] -2.7 2.7 Utilizando la función pnorm, vemos que 99.3% de los datos caen en este intervalo. Tengan en cuenta que este no es un evento tan extremo: si tenemos 1000 puntos de datos que se distribuyen normalmente, esperamos ver unos 7 fuera de este rango. Pero estos no serían valores atípicos ya que esperamos verlos bajo la variación típica. Si queremos que un valor atípico sea más raro, podemos cambiar el 1.5 a un número mas grande. Tukey también usó 3 y los denominó far out outliers o valores atípicos extremos. Con una distribución normal, 100% de los datos caen en este intervalo. Esto se traduce en aproximadamente 2 en un millón de posibilidades de estar fuera del rango. En la función geom_boxplot, esto se puede controlar usando el argumento outlier.size, que por defecto es 1.5. La medida de 180 pulgadas está más allá del rango de los datos de altura: max_height &lt;- quantile(outlier_example, 0.75) + 3*IQR(outlier_example) max_height #&gt; 75% #&gt; 6.91 Si sacamos este valor, podemos ver que los datos se distribuyen normalmente como se espera: x &lt;- outlier_example[outlier_example &lt; max_height] qqnorm(x) qqline(x) 11.5 Desviación absoluta mediana Otra opción para estimar la desviación estándar de manera robusta en presencia de valores atípicos es usar la desviación absoluta mediana, o MAD por sus siglas en inglés. Para calcular el MAD, primero calculamos la mediana y luego, para cada valor, calculamos la distancia entre ese valor y la mediana. El MAD se define como la mediana de estas distancias. Por razones técnicas que no discutimos aquí, esta cantidad debe multiplicarse por 1.4826 para asegurar que se aproxime a la desviación estándar real. La función mad ya incorpora esta corrección. Para los datos de altura, obtenemos una MAD de: mad(outlier_example) #&gt; [1] 0.237 lo cual es cerca de 3 pulgadas. 11.6 Ejercicios Vamos a usar el paquete HistData. Si no lo ha instalando, puede hacerlo así: install.packages(&quot;HistData&quot;) Cargue el set de datos de altura y cree un vector x que contiene solo las alturas masculinas de los datos de Galton de los padres y sus hijos de su investigación histórica sobre la herencia. library(HistData) data(Galton) x &lt;- Galton$child 1. Calcule el promedio y la mediana de estos datos. 2. Calcule la mediana y el MAD de estos datos. 3. Ahora supongamos que Galton cometió un error al ingresar el primer valor y olvidó usar el punto decimal. Puede imitar este error escribriendo: x_with_error &lt;- x x_with_error[1] &lt;- x_with_error[1]*10 ¿Cuántas pulgadas crece el promedio como resultado de este error? 4. ¿Cuántas pulgadas crece la SD como resultado de este error? 5. ¿Cuántas pulgadas crece la mediana como resultado de este error? 6. ¿Cuántas pulgadas crece el MAD como resultado de este error? 7. ¿Cómo podríamos utilizar el análisis exploratorio de datos para detectar que se cometió un error? Dado que es solo un valor entre muchos, no se puede detectar esto. Veríamos un cambio obvio en la distribución. Un diagrama de caja, histograma o gráfico Q-Q revelarían un valor atípico obvio. Un diagrama de dispersión mostraría altos niveles de error de medición. 8. ¿Cuánto puede crecer accidentalmente el promedio con errores como este? Escribe una función llamada error_avg que toma un valor k y devuelve el promedio del vector x después de que la primera entrada cambie a k. Muestre los resultados para k=10000 y k=-10000. 11.7 Estudio de caso: alturas autoreportadas de estudiantes Las alturas que hemos estado estudiando no son las alturas originales reportadas por los estudiantes. Las alturas originales también se incluyen en el paquete dslabs y se pueden cargar así: library(dslabs) data(&quot;reported_heights&quot;) La altura es un vector de caracteres, por lo que creamos una nueva columna con la versión numérica: reported_heights &lt;- reported_heights %&gt;% mutate(original_heights = height, height = as.numeric(height)) #&gt; Warning: NAs introducidos por coerción Tengan en cuenta que recibimos una advertencia sobre los NAs. Esto se debe a que algunas de las alturas autoreportadas no eran números. Podemos ver por qué obtenemos estos NAs: reported_heights %&gt;% filter(is.na(height)) %&gt;% head() #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-02 15:16:28 Male NA 5&#39; 4&quot; #&gt; 2 2014-09-02 15:16:37 Female NA 165cm #&gt; 3 2014-09-02 15:16:52 Male NA 5&#39;7 #&gt; 4 2014-09-02 15:16:56 Male NA &gt;9000 #&gt; 5 2014-09-02 15:16:56 Male NA 5&#39;7&quot; #&gt; 6 2014-09-02 15:17:09 Female NA 5&#39;3&quot; Algunos estudiantes indicaron sus alturas usando pies y pulgadas en lugar de solo pulgadas. Otros usaron centímetros y otros solo estaban trolleando. Por ahora eliminaremos estas entradas: reported_heights &lt;- filter(reported_heights, !is.na(height)) Si calculamos el promedio y la desviación estándar, observamos que obtenemos resultados extraños. El promedio y la desviación estándar son diferentes de la mediana y del MAD: reported_heights %&gt;% group_by(sex) %&gt;% summarize(average = mean(height), sd = sd(height), median = median(height), MAD = mad(height)) #&gt; # A tibble: 2 x 5 #&gt; sex average sd median MAD #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Female 63.4 27.9 64.2 4.05 #&gt; 2 Male 103. 530. 70 4.45 Esto sugiere que tenemos valores atípicos, lo que se confirma creando un diagrama de caja: Vemos algunos valores bastante extremos. Para ver cuáles son estos valores, podemos rápidamente mirar los valores más grandes utilizando la función arrange: reported_heights %&gt;% arrange(desc(height)) %&gt;% top_n(10, height) #&gt; time_stamp sex height original_heights #&gt; 1 2014-09-03 23:55:37 Male 11111 11111 #&gt; 2 2016-04-10 22:45:49 Male 10000 10000 #&gt; 3 2015-08-10 03:10:01 Male 684 684 #&gt; 4 2015-02-27 18:05:06 Male 612 612 #&gt; 5 2014-09-02 15:16:41 Male 511 511 #&gt; 6 2014-09-07 20:53:43 Male 300 300 #&gt; 7 2014-11-28 12:18:40 Male 214 214 #&gt; 8 2017-04-03 16:16:57 Male 210 210 #&gt; 9 2015-11-24 10:39:45 Male 192 192 #&gt; 10 2014-12-26 10:00:12 Male 190 190 #&gt; 11 2016-11-06 10:21:02 Female 190 190 Las primeras siete entradas parecen errores extraños. Sin embargo, las siguientes entradas parecen haber sido ingresadas en centímetros en lugar de pulgadas. Dado que 184 cm es equivalente a seis pies de altura, sospechamos que 184 en realidad significa 72 pulgadas. Podemos revisar todas las respuestas sin sentido examinando los datos que Tukey considera far out o extremos: whisker &lt;- 3*IQR(reported_heights$height) max_height &lt;- quantile(reported_heights$height, .75) + whisker min_height &lt;- quantile(reported_heights$height, .25) - whisker reported_heights %&gt;% filter(!between(height, min_height, max_height)) %&gt;% select(original_heights) %&gt;% head(n=10) %&gt;% pull(original_heights) #&gt; [1] &quot;6&quot; &quot;5.3&quot; &quot;511&quot; &quot;6&quot; &quot;2&quot; &quot;5.25&quot; &quot;5.5&quot; &quot;11111&quot; #&gt; [9] &quot;6&quot; &quot;6.5&quot; Revisando estas alturas cuidadosamente, vemos dos errores comunes: entradas en centímetros, que resultan ser demasiado grandes, y entradas del tipo x.y con x y y representando pies y pulgadas respectivamente, que resultan ser demasiado pequeñas. Algunos de los valores aún más pequeños, como 1.6, podrían ser entradas en metros. En la parte de data wrangling de este libro, aprenderemos técnicas para corregir estos valores y convertirlos en pulgadas. Aquí pudimos detectar este problema mediante una cuidadosa exploración de los datos para descubrir problemas con ellos: el primer paso en la gran mayoría de los proyectos de ciencia de datos. "],
["introducción-a-las-estadísticas-con-r.html", "Capítulo 12 Introducción a las estadísticas con R", " Capítulo 12 Introducción a las estadísticas con R El análisis de datos es uno de los enfoques principales de este libro. Si bien las herramientas informáticas que hemos introducido son desarrollos relativamente recientes, el análisis de datos ha existido durante más de un siglo. A lo largo de los años, los analistas de datos que trabajan en proyectos específicos han presentado ideas y conceptos que se generalizan a muchas otras aplicaciones. También han identificado maneras comunes en que patrones aparentes en los datos y realidades matemáticas importantes que no son inmediatamente obvias nos pueden engañar. La acumulación de estas ideas y perspectivas ha dado lugar a la disciplina de la estadística, que proporciona un marco matemático que facilita enormemente la descripción y la evaluación formal de estas ideas. Para evitar repetir errores comunes y perder el tiempo reinventando la rueda, es importante que los analistas de datos tengan una comprensión profunda de las estadísticas. Debido a la madurez de la disciplina, hay docenas de libros excelentes ya publicados sobre este tema y, por lo tanto, no nos enfocamos en describir el marco matemático aquí. En cambio, presentamos conceptos brevemente y luego proporcionamos estudios de caso detallados que demuestran cómo se utilizan las estadísticas en el análisis de datos junto con el código R que implementa estas ideas. También usamos el código R para ayudar a aclarar algunos de los principales conceptos estadísticos que generalmente se describen usando las matemáticas. Recomendamos complementar este capítulo con un libro de texto de estadísticas. Dos ejemplos son Statistics de Freedman, Pisani y Purves y Statistical Inference de Casella y Berger. Los conceptos específicos cubiertos en esta parte del libro son Probabilidad, Inferencia estadística, Modelos estadísticos, Regresión y Modelos lineales, que son los principales temas cubiertos en un curso de estadística. Los estudios de caso que presentamos se relacionan con la crisis financiera, el pronóstico de los resultados de las elecciones, cómo entender la herencia y cómo formar un equipo de béisbol. "],
["probabilidad.html", "Capítulo 13 Probabilidad 13.1 Probabilidad discreta 13.2 Simulaciones Monte Carlo para datos categóricos 13.3 Independencia 13.4 Probabilidades condicionales 13.5 Reglas de la adición y de la multiplicación 13.6 Combinaciones y permutaciones 13.7 Ejemplos 13.8 [fix] Infinito en la práctica 13.9 Ejercicios 13.10 Probabilidad continua 13.11 Distribuciones teóricas continuas 13.12 Simulaciones Monte Carlo para variables continuas 13.13 Distribuciones continuas 13.14 Ejercicios", " Capítulo 13 Probabilidad En los juegos de azar, la probabilidad tiene una definición muy intuitiva. Por ejemplo, sabemos lo que significa cuando decimos que la probabilidad de que un par de dados salga siete es 1 en 6. Sin embargo, este no es el caso en otros contextos. Hoy la teoría de probabilidad se utiliza de manera mucho más amplia con la palabra probabilidad ya formando parte del lenguaje cotidiano. Si escribimos “¿Cuáles son las probabilidades de” en Google, la función de autocompletar nos da: “tener mellizos”, “tener gemelos” y “ganar la lotería”. Uno de los objetivos de esta parte del libro es ayudarnos a comprender cómo la probabilidad es útil para entender y describir eventos del mundo real cuando realizamos análisis de datos. Dado que saber cómo calcular las probabilidades nos da una ventaja en los juegos de azar, a lo largo de la historia muchas personas inteligentes, incluyendo matemáticos famosos como Cardano, Fermat y Pascal, le han dedicado tiempo y energía a pensar en las matemáticas de estos juegos. Como resultado, nació la teoría de la probabilidad. La probabilidad sigue siendo muy útil en los juegos de azar modernos. Por ejemplo, en póker, podemos calcular la probabilidad de ganar una mano basado en las cartas en la mesa. Además, los casinos se basan en la teoría de la probabilidad para desarrollar juegos que casi siempre les garantizan ganancias. La teoría de la probabilidad es útil en muchos otros contextos y, en particular, en áreas que de alguna manera dependen de los datos afectados por el azar. Todos los otros capítulos de esta parte se basan en la teoría de la probabilidad. El conocimiento de la probabilidad es, por lo tanto, indispensable para la ciencia de datos. 13.1 Probabilidad discreta Comenzamos explorando algunos principios básicos relacionados con datos categóricos. Esta parte de la probabilidad se conoce como probabilidad discreta. Más tarde, tal nos ayudará a comprender la teoría de la probabilidad que luego presentaremos para datos numéricos y continuos, que es mucho más común en las aplicaciones de ciencia de datos. La probabilidad discreta es más útil en los juegos de cartas y, por ende, usamos estos como ejemplos. 13.1.1 Frecuencia relativa La palabra probabilidad se usa en el lenguaje cotidiano. Sin embargo, responder a preguntas sobre la probabilidad es difícil, si no imposible. Aquí discutimos una definición matemática de probabilidad que nos permite dar respuestas precisas a ciertas preguntas. Por ejemplo, si tengo 2 canicas rojas y 3 canicas azules dentro de una bolsa49 y escojo una al azar, ¿cuál es la probabilidad de elegir una roja? Nuestra intuición nos dice que la respuesta es 2/5 o 40%. Se puede dar una definición precisa al señalar que hay cinco resultados posibles de los cuales dos satisfacen la condición necesaria para el evento “escoger una canica roja”. Dado que cada uno de los cinco resultados tiene la misma posibilidad de ocurrir, concluimos que la probabilidad es .4 para rojo y .6 para azul. Una forma más tangible de pensar en la probabilidad de un evento es la proporción de veces que ocurre el evento cuando repetimos el experimento un número infinito de veces, independientemente y bajo las mismas condiciones. 13.1.2 Notación Usamos la notación \\(\\mbox{Pr}(A)\\) para denotar la probabilidad de que suceda evento \\(A\\). Usamos el término general evento para referirnos a cosas que pueden suceder cuando algo ocurre por casualidad. En nuestro ejemplo anterior, el evento fue “escoger una canica roja”. En una encuesta política en la que llamamos al azar a 100 probables votantes estadounidenses, un ejemplo de un evento es “llamar a 48 demócratas y 52 republicanos”. En aplicaciones de ciencia de datos, frecuentemente trabajaremos con variables continuas. Estos eventos a menudo serán cosas como “es esta persona más alta que 6 pies”. En ese caso, escribimos eventos en una forma más matemática: \\(X \\geq 6\\). Veremos más de estos ejemplos a continuación. Aquí nos enfocamos en datos categóricos. 13.1.3 Distribuciones de probabilidad Si conocemos la frecuencia relativa de las diferentes categorías, definir una distribución para resultados categóricos es relativamente sencillo. Simplemente asignamos una probabilidad a cada categoría. En los casos que pueden considerarse como canicas en una bolsa, para cada tipo de canica, su proporción define la distribución. Si estamos llamando al azar a votantes probables de una población que es 44% demócratas, 44% republicanos, 10% indecisos y 2% del partido verde, estas proporciones definen la probabilidad para cada grupo. La distribución de probabilidad es: Pr (elegir un republicano) = 0.44 Pr (elegir un demócrata) = 0.44 Pr (elegir un indeciso) = 0.10 Pr (elegir un verde) = 0.02 13.2 Simulaciones Monte Carlo para datos categóricos Las computadoras ofrecen una forma de realizar el experimento aleatorio sencillo descrito anteriormente: elegir una canica al azar de una bolsa que contiene tres canicas azules y dos rojas. Los generadores de números aleatorios nos permiten imitar el proceso de selección al azar. Un ejemplo es la función sample en R. Demostramos su uso en el código de abajo. Primero, usamos la función rep para generar la bolsa: beads &lt;- rep(c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) beads #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; y luego usamos sample para elegir una canica al azar: sample(beads, 1) #&gt; [1] &quot;blue&quot; Esta línea de código produce un resultado aleatorio. Queremos repetir este experimento un número infinito de veces, pero es imposible repetirlo para siempre. En cambio, repetimos el experimento un número suficientemente grande de veces para que los resultados sean prácticamente equivalentes a repetirlo para siempre. Este es un ejemplo de una simulación Monte Carlo. Gran parte de lo que estudian los estadísticos matemáticos y teóricos, que no discutimos en este libro, se relaciona con proveer definiciones rigurosas de “prácticamente equivalente”, así como estudiar cuán cerca nos llevan un gran número de experimentos a lo que sucede en el límite. Más adelante en esta sección, ofrecemeos un acercamiento práctico para decidir qué es “lo suficientemente grande”. Para realizar nuestra primera simulación Monte Carlo, utilizamos la función replicate, que nos permite repetir la misma tarea varias veces. Aquí, repetimos el evento aleatorio \\(B =\\) 10,000 veces: B &lt;- 10000 events &lt;- replicate(B, sample(beads, 1)) Ahora podemos ver si nuestra definición realmente está de acuerdo con esta aproximación de simulación Monte Carlo. Nosotros podemos usar table para ver la distribución: tab &lt;- table(events) tab #&gt; events #&gt; blue red #&gt; 5995 4005 y prop.table nos da las proporciones: prop.table(tab) #&gt; events #&gt; blue red #&gt; 0.600 0.401 Los números anteriores son probabilidades estimadas proveídas por esta simulación Monte Carlo. La teoría estadística, que no discutimos aquí, nos dice que en lo que \\(B\\) se hace más grande, las estimaciones se acercan a 3/5 = .6 y 2/5 = .4. Aunque este es un ejemplo sencillo y no muy útil, utilizaremos las simulaciones Monte Carlo para estimar las probabilidades en los casos [fix] en que es más difícil calcular las exactas. Antes de profundizar en ejemplos más complejos, usaremos algunos sencillos para demostrar las herramientas informáticas disponibles en R. 13.2.1 [fix] Configurar/Establecer/Fijar de la semilla aleatoria Antes de continuar, explicaremos brevemente la siguiente línea de código importante: set.seed(1986) A lo largo de este libro, utilizamos generadores de números aleatorios. Esto implica que muchos de los resultados presentados pueden cambiar por casualidad, lo que sugiere que una versión congelada del libro puede mostrar un resultado diferente al que obtienen cuando intenten codificar como observan en el libro. Esto está bien, ya que los resultados son aleatorios y pueden cambiar. Sin embargo, si quieren asegurarse de que los resultados son exactamente los mismos cada vez que los ejecuten, pueden [fix] fijar la semilla (seed en inglés) de generación de números aleatorios de R en un número específico. Arriba la [fix] fijamos en 1986. Queremos evitar usar la misma semilla cada vez. Una forma popular de escoger la semilla es el año, mes y día. Por ejemplo, elegimos 1986 el 20 de diciembre de 2018: \\(2018 - 12 - 20 = 1986\\). Pueden obtener más información sobre cómo [fix] configurar/establecer/fijar la semilla mirando la documentación: ?set.seed En los ejercicios, es posible que les pidamos que [fix] fijen la semilla para asegurar que los resultados que obtengan sean exactamente lo que esperamos. 13.2.2 Con y sin reemplazo La función sample tiene un argumento que nos permite elegir más de un elemento de la bolsa. Sin embargo, por defecto, esta selección ocurre sin reemplazo: después de seleccionar una canica, no se vuelve a colocar en la bolsa. Observen lo que sucede cuando pedimos seleccionar al azar cinco canicas: sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; sample(beads, 5) #&gt; [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; sample(beads, 5) #&gt; [1] &quot;blue&quot; &quot;red&quot; &quot;blue&quot; &quot;red&quot; &quot;blue&quot; Esto resulta en reordenamientos que siempre tienen tres canicas azules y dos rojas. Si pedimos que se seleccionen seis canicas, obtenemos un error: sample(beads, 6) Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE' Sin embargo, la función sample se puede usar directamente, sin el uso de replicate, para repetir el mismo experimento de elegir 1 de las 5 canicas, continuamente, en las mismas condiciones. Para hacer esto, muestreamos con reemplazo: se devuelve la canica a la bolsa después de seleccionarla. Podemos decirle a sample que haga esto cambiando el argumento replace, que por defecto es FALSE, a replace = TRUE: events &lt;- sample(beads, B, replace = TRUE) prop.table(table(events)) #&gt; events #&gt; blue red #&gt; 0.602 0.398 No sorprende que obtengamos resultados muy similares a los obtenidos previamente con replicate. 13.3 Independencia Decimos que dos eventos son independientes si el resultado de uno no afecta al otro. El ejemplo clásico es el lanzamiento de monedas. Cada vez que lanzamos una moneda, la probabilidad de ver cara es 1/2, independientemente de los resultados de lanzamientos anteriores. Lo mismo es cierto cuando recogemos canicas de una bolsa con reemplazo. En el ejemplo anterior, la probabilidad de rojo es 0.40 [fix] independientemente de los sorteos anteriores. Muchos ejemplos de eventos que no son independientes provienen de juegos de cartas. Cuando repartimos la primera carta, la probabilidad de obtener una K es 1/13 ya que hay trece posibilidades: Dos, Tres, \\(\\dots\\), Diez, J, Q, K y As. Ahora, si repartimos una K como la primera carta, y no la reemplazamos en la baraja, las probabilidades de que una segunda carta sea K es menor porque solo quedan tres Ks: la probabilidad es 3 de 51. Estos eventos por lo tanto no son independientes: el primer resultado afecta al siguiente. Para ver un caso extremo de eventos no independientes, consideren nuestro ejemplo de escoger cinco canicas al azar sin reemplazo: x &lt;- sample(beads, 5) Si tienen que adivinar el color de la primera canica, predecirán el azul ya que el azul tiene un 60% de posibilidad. Pero si les mostramos el resultado de los últimos cuatro resultados: x[2:5] #&gt; [1] &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; ¿aún adivinarían azul? Por supuesto que no. Ahora saben que la probabilidad de rojo es 1 ya que la única canica que queda es roja. Los eventos no son independientes, por lo que las probabilidades cambian. 13.4 Probabilidades condicionales Cuando los eventos no son independientes, las probabilidades condicionales son útiles. Ya vimos un ejemplo de una probabilidad condicional: calculamos la probabilidad de que una segunda carta repartida sea K dado que la primera fue K. En probabilidad, usamos la siguiente notación: \\[ \\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51 \\] Utilizamos el \\(\\mid\\) como abreviatura de “dado eso” o “condicional en”. Cuando dos eventos, digamos \\(A\\) y \\(B\\), somos independientes, tenemos: \\[ \\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A) \\] Esta es la forma matemática de decir: el hecho de que \\(B\\) sucedió no afecta la probabilidad de que \\(A\\) suceda. De hecho, esto puede considerarse la definición matemática de independencia. 13.5 Reglas de la adición y de la multiplicación 13.5.1 Regla de la multiplicación Si queremos saber la probabilidad de dos eventos, digamos \\(A\\) y \\(B\\), ocurriendo, podemos usar la regla de la multiplicación: \\[ \\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A) \\] Usemos el juego de cartas Blackjack como ejemplo. En Blackjack, se les asignan dos cartas al azar. Después de ver lo que tienen, pueden pedir más. El objetivo es acercarse más a 21 que el croupier, sin pasar. Las cartas con figuras o figuras (face cards en inglés) valen 10 puntos y las A ses valen 11 o 1 (uno elige). Entonces, en un juego de Blackjack, para calcular las posibilidades de obtener un 21 sacando un As y luego una carta de figura, calculamos la probabilidad de que el primero sea un As y multiplicamos por la probabilidad de sacar una carta de figura o un 10 dado que la primera fue un As: \\(1/13 \\times 16/51 \\approx 0.025\\). La regla de la multiplicación también se aplica a más de dos eventos. Podemos usar la inducción para incluir más eventos: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B) \\] 13.5.2 Regla de la multiplicación bajo independencia Cuando tenemos eventos independientes, la regla de la multiplicación se vuelve más sencilla: \\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C) \\] Pero debemos tener mucho cuidado antes de usar esto, ya que suponer independencia puede dar como resultado cálculos de probabilidad muy diferentes e incorrectos cuando en realidad no tenemos independencia. Como ejemplo, imaginen un caso judicial en el que se describió al sospechoso como teniendo bigote y barba. El acusado tiene bigote y barba y la fiscalía trae a un “experto” que testifica que 1/10 hombres tienen barba y 1/5 tienen bigote, así que usando la regla de la multiplicación concluimos que solo \\(1/10 \\times 1/5\\) o 0.02 tienen ambos. ¡Pero para multiplicar así necesitamos asumir independencia! Digamos que la probabilidad condicional de que un hombre tenga un bigote condicionado en que tenga barba es .95. Entonces el cálculo correcto de la probabilidad resulta en un número mucho mayor: \\(1/10 \\times 95/100 = 0.095\\). La regla de la multiplicación también nos da una fórmula general para calcular probabilidades condicionales: \\[ \\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] Para ilustrar cómo usamos estas fórmulas y conceptos en la práctica, utilizaremos varios ejemplos relacionados con los juegos de cartas. 13.5.3 Regla de la adición La regla de la adición nos dice que: \\[ \\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B) \\] Esta regla es intuitiva: piense en un diagrama de Venn. Si simplemente sumamos las probabilidades, contamos la intersección dos veces, por lo que debemos restar una instancia. 13.6 Combinaciones y permutaciones En nuestro primer ejemplo, imaginamos una bolsa con cinco canicas. Recuerden que para calcular la distribución de probabilidad de un sorteo, simplemente enumeramos todas las posibilidades. Hubo 5 y entonces, para cada evento, contamos cuántas de estas posibilidades estaban asociadas con el evento. Entonces la probabilidad de elegir una canica azul es 3/5 porque de los cinco resultados posibles, tres fueron azules. Para casos más complicados, los cálculos no son tan sencillos. Por ejemplo, ¿cuál es la probabilidad de que si escojo cinco cartas sin reemplazo, obtengo todas las cartas del mismo palo (suit en inglés), lo que se conoce como “flush” en el póker? En un curso de probabilidad discreta, se aprende la teoría sobre cómo hacer estos cálculos. Aquí nos enfocamos en cómo usar el código R para calcular las respuestas. Primero, construyamos una baraja de cartas. Para esto, usaremos las funciones expand.grid y paste. Usamos paste para crear cadenas uniendo cadenas más pequeñas. Para hacer esto, tomamos el número y el tipo de una carta y creamos el nombre de la carta de esta manera: number &lt;- &quot;Three&quot; suit &lt;- &quot;Hearts&quot; paste(number, suit) #&gt; [1] &quot;Three Hearts&quot; paste también funciona en pares de vectores que realizan la operación elemento por elemento: paste(letters[1:5], as.character(1:5)) #&gt; [1] &quot;a 1&quot; &quot;b 2&quot; &quot;c 3&quot; &quot;d 4&quot; &quot;e 5&quot; La función expand.grid nos da todas las combinaciones de entradas de dos vectores. Por ejemplo, si tienen pantalones azules y negros y camisas blancas, grises y a cuadros (plaid en inglés), todas sus combinaciones son: expand.grid(pants = c(&quot;blue&quot;, &quot;black&quot;), shirt = c(&quot;white&quot;, &quot;grey&quot;, &quot;plaid&quot;)) #&gt; pants shirt #&gt; 1 blue white #&gt; 2 black white #&gt; 3 blue grey #&gt; 4 black grey #&gt; 5 blue plaid #&gt; 6 black plaid Así es como generamos una baraja de cartas: suits &lt;- c(&quot;Diamonds&quot;, &quot;Clubs&quot;, &quot;Hearts&quot;, &quot;Spades&quot;) numbers &lt;- c(&quot;Ace&quot;, &quot;Deuce&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;, &quot;Six&quot;, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;, &quot;Ten&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;) deck &lt;- expand.grid(number=numbers, suit=suits) deck &lt;- paste(deck$number, deck$suit) Con la baraja construida, podemos verificar que la probabilidad de una K en la primera carta es 1/13 calculando la proporción de posibles resultados que satisfagan nuestra condición: kings &lt;- paste(&quot;King&quot;, suits) mean(deck %in% kings) #&gt; [1] 0.0769 Ahora, ¿qué tal la probabilidad condicional de que la segunda carta sea una K dado que la primera era una K? Anteriormente, dedujimos que si una K ya está fuera de la baraja y quedan 51 cartas, entonces la probabilidad es 3/51. Confirmemos enumerando todos los resultados posibles. Para hacer esto, podemos usar la función permutations del paquete gtools. Para cualquier lista de tamaño n, esta función calcula todas las diferentes combinaciones que podemos obtener cuando seleccionamos r artículos. Aquí están todas las formas en que podemos elegir dos números de una lista que consiste en 1,2,3: library(gtools) permutations(3, 2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 1 #&gt; [4,] 2 3 #&gt; [5,] 3 1 #&gt; [6,] 3 2 Observen que el orden importa aquí: 3,1 es diferente de 1,3. Además, tengan en cuenta que (1,1), (2,2) y (3,3) no aparecen porque una vez que elegimos un número, no puede volver a aparecer. Opcionalmente, podemos agregar un vector. Si desean ver cinco números de teléfono aleatorios de siete dígitos de todos los números de teléfono posibles (sin repeticiones), pueden escribir: all_phone_numbers &lt;- permutations(10, 7, v = 0:9) n &lt;- nrow(all_phone_numbers) index &lt;- sample(n, 5) all_phone_numbers[index,] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] #&gt; [1,] 1 3 8 0 6 7 5 #&gt; [2,] 2 9 1 6 4 8 0 #&gt; [3,] 5 1 6 0 9 8 2 #&gt; [4,] 7 4 6 0 2 8 1 #&gt; [5,] 4 6 5 9 2 8 0 En lugar de usar los números del 1 al 10, el valor predeterminado, R usa lo que proveemos a través de v: los dígitos de 0 a 9. Para calcular todas las formas posibles en que podemos elegir dos cartas cuando el orden importa, escribimos: hands &lt;- permutations(52, 2, v = deck) Esta es una matriz con dos columnas y 2652 filas. Con una matriz podemos obtener la primera y la segunda carta así: first_card &lt;- hands[,1] second_card &lt;- hands[,2] Ahora los casos para los cuales la primera mano era una K se pueden calcular así: kings &lt;- paste(&quot;King&quot;, suits) sum(first_card %in% kings) #&gt; [1] 204 Para obtener la probabilidad condicional, calculamos qué fracción de estos tiene una K como la segunda carta: sum(first_card%in%kings &amp; second_card%in%kings)/ sum(first_card%in%kings) #&gt; [1] 0.0588 que es exactamente 3/51, como ya habíamos deducido. Tengan en cuenta que el código anterior es equivalente a: mean(first_card%in%kings &amp; second_card%in%kings)/ mean(first_card%in%kings) #&gt; [1] 0.0588 que usa mean en lugar de sum y es una versión R de: \\[ \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\] ¿Y qué tal si el orden no importa? Por ejemplo, en Blackjack si le dan un As y una carta de figura como su primera mano, se llama un Natural 21 y ganan automáticamente. Si quisiéramos calcular la probabilidad de que esto suceda, enumeraríamos las combinaciones, no las permutaciones, ya que el orden no importa. combinations(3,2) #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 1 3 #&gt; [3,] 2 3 En la segunda línea, el resultado no incluye (2,1) porque (1,2) ya se enumeró. Lo mismo aplica a (3,1) y (3,2). Entonces, para calcular la probabilidad de un Natural 21 en Blackjack, podemos hacer esto: aces &lt;- paste(&quot;Ace&quot;, suits) facecard &lt;- c(&quot;King&quot;, &quot;Queen&quot;, &quot;Jack&quot;, &quot;Ten&quot;) facecard &lt;- expand.grid(number = facecard, suit = suits) facecard &lt;- paste(facecard$number, facecard$suit) hands &lt;- combinations(52, 2, v = deck) mean(hands[,1] %in% aces &amp; hands[,2] %in% facecard) #&gt; [1] 0.0483 [fix check para] En la última línea, suponemos que el As es la primera carta que nos dan. Esto es solo porque sabemos cómo combination enumera las posibilidades y enumerará este caso primero. Pero para estar seguros, podríamos haber escrito esto y producido la misma respuesta: mean((hands[,1] %in% aces &amp; hands[,2] %in% facecard) | (hands[,2] %in% aces &amp; hands[,1] %in% facecard)) #&gt; [1] 0.0483 13.6.1 Ejemplo Monte Carlo En lugar de usar combinations para deducir la probabilidad exacta de un Natural 21, podemos usar un Monte Carlo para estimar esta probabilidad. En este caso, escogemos dos cartas una y otra vez y notamos cuántos 21s tenemos. Podemos usar la función sample para escoger dos cartas sin reemplazos: hand &lt;- sample(deck, 2) hand #&gt; [1] &quot;Queen Clubs&quot; &quot;Seven Spades&quot; Y luego verificar si una carta es un As y la otra una figura o un 10. De ahora en adelante, incluimos 10 cuando decimos carta de figura o figura. Ahora necesitamos verificar ambas posibilidades: (hands[1] %in% aces &amp; hands[2] %in% facecard) | (hands[2] %in% aces &amp; hands[1] %in% facecard) #&gt; [1] FALSE Si repetimos esto 10,000 veces, obtenemos una muy buena aproximación de la probabilidad de un Natural 21. Comencemos escribiendo una función que dibuje una mano y devuelva TRUE si obtenemos un 21. La función no necesita argumentos porque usa objetos definidos en el entorno global. blackjack &lt;- function(){ hand &lt;- sample(deck, 2) (hand[1] %in% aces &amp; hand[2] %in% facecard) | (hand[2] %in% aces &amp; hand[1] %in% facecard) } Aquí tenemos que verificar ambas posibilidades: As primero o As segundo porque no estamos usando la función combinations. La función devuelve TRUE si obtenemos un 21 y FALSE [fix] en caso contrario/de otra manera: blackjack() #&gt; [1] FALSE Ahora podemos jugar este juego, digamos, 10,000 veces: B &lt;- 10000 results &lt;- replicate(B, blackjack()) mean(results) #&gt; [1] 0.0475 13.7 Ejemplos En esta sección, describimos dos ejemplos populares de probabilidad discreta: el problema Monty Hall y el problema del cumpleaños. Usamos R para ayudar a ilustrar los conceptos matemáticos. 13.7.1 Problema Monty Hall [fix check all] En la década de 1970 en EE.UU, hubo un juego televiso llamado “Let’s Make a Deal” y Monty Hall era el anfitrión. En algún momento del juego, se le pedía al concursante que eligiera una de tres puertas. Detrás de una puerta había un premio, mientras detrás de las otras puertas tenían una cabra que señalaba que el concursante que había perdido. Después de que el concursante eligiera una puerta y antes de revelar si esa puerta contenía un premio, Monty Hall abría una de las dos puertas restantes y le mostraba al concursante que no había ningún premio detrás de esa puerta. Luego le preguntaba al concursante: “¿Quieres cambiar de puerta?” ¿Qué harían Uds.? Podemos usar la probabilidad para mostrar que si se quedan con la opción de la puerta original, sus posibilidades de ganar un premio siguen siendo 1 en 3. [fix check] Sin embargo, si cambian a la otra puerta, ¡sus posibilidades de ganar duplican a 2 en 3! Esto parece contradictorio. Muchas personas piensan incorrectamente que ambas posibilidades son 1 en 2 ya que uno elige entre 2 opciones. Pueden ver una explicación matemática detallada en Khan Academy50 o leer una en Wikipedia51. A continuación, usamos una simulación Monte Carlo para ver qué estrategia es mejor. [fix] Tengan en cuenta que este código se escribe en más detalle que lo necesario para fines pedagógicos. [fix] Comencemos con la estrategia de palo/“stick strategy”??: B &lt;- 10000 monty_hall &lt;- function(strategy){ doors &lt;- as.character(1:3) prize &lt;- sample(c(&quot;car&quot;, &quot;goat&quot;, &quot;goat&quot;)) prize_door &lt;- doors[prize == &quot;car&quot;] my_pick &lt;- sample(doors, 1) show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1) stick &lt;- my_pick stick == prize_door switch &lt;- doors[!doors%in%c(my_pick, show)] choice &lt;- ifelse(strategy == &quot;stick&quot;, stick, switch) choice == prize_door } stick &lt;- replicate(B, monty_hall(&quot;stick&quot;)) mean(stick) #&gt; [1] 0.342 switch &lt;- replicate(B, monty_hall(&quot;switch&quot;)) mean(switch) #&gt; [1] 0.668 [fix check] Mientras escribimos el código, notamos que las líneas que comienzan con my_pick y show no afectan la última operación lógica cuando nos atenemos a nuestra elección original. De esto, debemos darnos cuenta de que la probabilidad es de 1 en 3, con lo que comenzamos. Cuando cambiamos, la estimación Monte Carlo confirma el cálculo de 2/3. Esto nos ayuda entender el problema mejor al mostrar que estamos quitando una puerta, show, que definitivamente no esconde un premio de nuestras opciones. También vemos que, a menos que lo hagamos bien cuando elegimos por primera vez, ustedes ganan: 1 - 1/3 = 2/3. 13.7.2 Problema de cumpleaños Supongamos que están en un salón de clases con 50 personas. Si suponemos que este es un grupo de 50 personas seleccionadas al azar, ¿cuál es la probabilidad de que al menos dos personas tengan el mismo cumpleaños? Aunque es algo avanzado, podemos deducir esto matemáticamente. Haremos esto más tarde, pero aquí usamos una simulación Monte Carlo. Para simplificar, suponemos que nadie nació el 29 de febrero. En realidad esto no cambia mucho la respuesta. Primero, tengan en cuenta que los cumpleaños se pueden representar como números entre 1 y 365, por lo que se puede obtener una muestra de 50 cumpleaños de esta manera: n &lt;- 50 bdays &lt;- sample(1:365, n, replace = TRUE) Para verificar si en este set particular de 50 personas tenemos al menos dos con el mismo cumpleaños, podemos usar la función duplicated, que devuelve TRUE siempre que un elemento de un vector es un duplicado. Aquí hay un ejemplo: duplicated(c(1,2,3,1,4,3,5)) #&gt; [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE La segunda vez que aparecen 1 y 3, obtenemos un TRUE. Entonces, para verificar si dos cumpleaños fueron iguales, simplemente usamos las funciones any y duplicated así: any(duplicated(bdays)) #&gt; [1] TRUE En este caso, vemos que sucedió. Al menos dos personas tuvieron el mismo cumpleaños. Para estimar la probabilidad de un cumpleaños compartido en el grupo, repetimos este experimento muestreando sets de 50 cumpleaños una y otra vez: B &lt;- 10000 same_birthday &lt;- function(n){ bdays &lt;- sample(1:365, n, replace=TRUE) any(duplicated(bdays)) } results &lt;- replicate(B, same_birthday(50)) mean(results) #&gt; [1] 0.969 ¿Esperaban que la probabilidad fuera tan alta? Las personas tienden a subestimar estas probabilidades. Para tener una idea de por qué es tan alto, piense en lo que sucede cuando el tamaño del grupo es cercano a 365. En esta etapa, se nos acaban los días y la probabilidad es una. [fix check all paragraph] Digamos que queremos usar este conocimiento para apostar con amigos sobre si dos personas en un grupo de personas tienen el mismo cumpleaños. ¿Cuándo son las posibilidades superiores a 50%? ¿Superiores a 75%? Empecemos creando una tabla de consulta. Rápidamente podemos crear una función para calcular esto para cualquier tamaño de grupo: compute_prob &lt;- function(n, B=10000){ results &lt;- replicate(B, same_birthday(n)) mean(results) } [check] Usando la función sapply, podemos realizar operaciones elemento por elemento en cualquier función: n &lt;- seq(1,60) prob &lt;- sapply(n, compute_prob) Ahora podemos hacer una gráfica de las probabilidades estimadas de dos personas tener el mismo cumpleaños en un grupo de tamaño \\(n\\): library(tidyverse) prob &lt;- sapply(n, compute_prob) qplot(n, prob) Ahora calculemos las probabilidades exactas en lugar de usar aproximaciones Monte Carlo. No solo obtenemos la respuesta exacta usando matemáticas, sino que los cálculos son mucho más rápidos ya que no tenemos que generar experimentos. Para simplificar las matemáticas, en lugar de calcular la probabilidad de que ocurra, calcularemos la probabilidad de que no ocurra. Para esto, usamos la regla de la multiplicación. [fix check all] Comencemos con la primera persona. La probabilidad de que persona 1 tenga un cumpleaños único es 1. La probabilidad de que persona 2 tenga un cumpleaños único, dado que persona 1 ya cumplió uno, es 364/365. Luego, dado que las dos primeras personas tienen cumpleaños únicos, persona 3 tiene 363 días para elegir. Continuamos de esta manera y encontramos que las posibilidades de que todas las 50 personas tengan un cumpleaños único son: \\[ 1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365} \\] Podemos escribir una función que haga esto para cualquier número: exact_prob &lt;- function(n){ prob_unique &lt;- seq(365,365-n+1)/365 1 - prod( prob_unique) } eprob &lt;- sapply(n, exact_prob) qplot(n, prob) + geom_line(aes(n, eprob), col = &quot;red&quot;) Este gráfico muestra que la simulación Monte Carlo ofrece una muy buena estimación de la probabilidad exacta. Si no hubiera sido posible calcular las probabilidades exactas, aún habríamos podido estimar con precisión las probabilidades. 13.8 [fix] Infinito en la práctica La teoría descrita aquí requiere repetir experimentos una y otra vez para siempre. En la práctica no podemos hacer esto. En los ejemplos anteriores, utilizamos \\(B=10,000\\) experimentos Monte Carlo y resultó que esto nos dio estimaciones precisas. Cuanto mayor sea este número, más precisa será la estimación hasta que la aproximación sea tan buena que sus computadoras no puedan notar la diferencia. Pero en cálculos más complejos, 10,000 puede ser insuficiente. Además, para algunos cálculos, 10,000 experimentos podrían ser computacionalmente infactibles. En la práctica, no sabremos cuál es la respuesta, por lo que no sabremos si nuestra estimación Monte Carlo es precisa. [fix] Sabemos que entre más grande sea \\(B\\), mejor será la aproximación. ¿Pero cuán grande necesitamos que sea? Esta es realmente una pregunta desafiante y contestarla frecuentemente requiere una formación avanzada en estadística teórica. Un enfoque práctico que describiremos aquí es verificar la estabilidad de la estimación. A continuación ofrecemos un ejemplo con el problema de cumpleaños para un grupo de 25 personas. B &lt;- 10^seq(1, 5, len = 100) compute_prob &lt;- function(B, n=25){ same_day &lt;- replicate(B, same_birthday(n)) mean(same_day) } prob &lt;- sapply(B, compute_prob) qplot(log10(B), prob, geom = &quot;line&quot;) En este gráfico, podemos ver que los valores comienzan a estabilizarse (es decir, varían menos de .01) alrededor de 1000. Noten que la probabilidad exacta, que en este caso sabemos, es 0.569. 13.9 Ejercicios 1. Se escoge una canica al azar de una bolsa que contiene: 3 canicas cian, 5 canicas magenta y 7 canicas amarillas. ¿Cuál es la probabilidad de que la canica sea cian? 2. ¿Cuál es la probabilidad de que la canica no sea cian? 3. En lugar de escoger solo una vez, considere escoger dos canicas. Usted saca la primera canica sin devolverla a la bolsa. Este es un muestreo sin reemplazo. ¿Cuál es la probabilidad de que la primera canica sea cian y la segunda no sea cian? 4. Ahora repita el experimento, pero esta vez, después de sacar la primera canica y regsitrar el color, devuélvala a la bolsa y agite la bolsa. Este es un muestreo con reemplazo. ¿Cuál es la probabilidad de que la primera canica sea cian y la segunda canica no sea cian? 5. [fix] Dos eventos \\(A\\) y \\(B\\) son independientes si \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). ¿Bajo qué situación son independientes la seleccion? No reemplazas el articulo seleccionado. Reemplazas el articulo seleccionado. Ninguno. Ambos. 6. Digamos que ha sacado 5 canicas de la bolsa, con reemplazo, y todas han sido amarillas. ¿Cuál es la probabilidad de que la próxima sea amarilla? 7. Si lanzas un dado de 6 lados seis veces, ¿cuál es la probabilidad de no ver un 6? 8. [fix] Dos equipos de baloncesto, digamos los Celtics y los Cavs, están jugando una serie de siete juegos. Los Cavs son un mejor equipo y tienen un 60% de posibilidad de ganar cada juego. ¿Cuál es la probabilidad de que los Celtics ganen al menos un juego? 9. Cree una simulación Monte Carlo para confirmar su respuesta al problema anterior. Utilizar B &lt;- 10000 simulaciones. Sugerencia: use el siguiente código para generar los resultados de los primeros cuatro juegos: celtic_wins &lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4)) Los Celtics deben ganar uno de estos 4 juegos. 10. Dos equipos de baloncesto, digamos los Cavs y los Warriors, están jugando una serie de campeonato de siete juegos. El primero en ganar cuatro juegos, por ende, gana la serie. Los equipos son igualmente buenos, por lo que cada uno tiene una probabilidad de 50-50 de ganar cada juego. Si los Cavs pierden el primer juego, ¿cuál es la probabilidad de que ganen la serie? 11. Confirme los resultados de la pregunta anterior con una simulación Monte Carlo. 12. Dos equipos, \\(A\\) y \\(B\\), están jugando una serie de siete juegos. Equipo \\(A\\) es mejor que equipo \\(B\\) y tiene un \\(p&gt;0.5\\) posibilidad de ganar cada juego. Dado un valor \\(p\\), la probabilidad de ganar la serie para el equipo de abajo \\(B\\) se puede calcular con la siguiente función basada en una simulación Monte Carlo: prob_win &lt;- function(p){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=4 }) mean(result) } Usa la función sapply para calcular la probabilidad, llámela Pr, de ganar para p &lt;- seq(0.5, 0.95, 0.025). Luego grafica el resultado. 13. Repita el ejercicio anterior, pero ahora mantenga la probabilidad fija en p &lt;- 0.75 y calcule la probabilidad de diferentes longitudes de serie: lo mejor de 1 juego, 3 juegos, 5 juegos, … Específicamente, N &lt;- seq(1, 25, 2). Sugerencia: use esta función: prob_win &lt;- function(N, p=0.75){ B &lt;- 10000 result &lt;- replicate(B, { b_win &lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p)) sum(b_win)&gt;=(N+1)/2 }) mean(result) } 13.10 Probabilidad continua En la sección 8.4, explicamos por qué al resumir una lista de valores numéricos, como las alturas, no es útil construir una distribución que defina una proporción para cada resultado posible. Por ejemplo, imagínemos que medimos a cada persona en una población grande, digamos de tamaño \\(n\\), con una precisión extremadamente alta. Como no hay dos personas con exactamente la misma altura, debemos asignar la proporción \\(1/n\\) a cada valor observado y como consecuencia no se obtiene ningún resumen útil. Del mismo modo, al definir distribuciones de probabilidad, no es útil asignar una probabilidad muy pequeña a cada altura. Al igual que cuando se usan distribuciones para resumir datos numéricos, es mucho más práctico definir una función que opere en intervalos en lugar de valores individuales. La forma estándar de hacerlo es utilizando la función de distribución acumulada, o CDF por sus siglas en inglés. Describimos la función de distribución acumulada empírica, o eCDF, en la Sección 8.4 como un resumen básico de una lista de valores numéricos. Como ejemplo, anteriormente definimos la distribución de altura para los estudiantes varones adultos. Aquí definimos el vector \\(x\\) para contener estas alturas: library(tidyverse) library(dslabs) data(heights) x &lt;- heights %&gt;% filter(sex==&quot;Male&quot;) %&gt;% pull(height) Definimos la función de distribución acumulada empírica como: F &lt;- function(a) mean(x&lt;=a) que, por cualquier valor a, da la proporción de valores en la lista x que son más pequeños o iguales que a. Tengan en cuenta que todavía no hemos introducido la probabilidad en el contexto de los CDF. Hagamos esto preguntando lo siguiente: si elijo a uno de los estudiantes varones al azar, ¿cuál es la probabilidad de que sea más alto que 70.5 pulgadas? Debido a que cada estudiante tiene la misma posibilidad de ser elegido, la respuesta a esto es equivalente a la proporción de estudiantes que son más altos que 70.5 pulgadas. Usando el CDF obtenemos una respuesta escribiendo: 1 - F(70) #&gt; [1] 0.377 Una vez que se define un CDF, podemos usar esto para calcular la probabilidad de cualquier subconjunto. Por ejemplo, la probabilidad de que un estudiante esté entre altura a y altura b es: F(b)-F(a) Como podemos calcular la probabilidad de cualquier evento posible de esta manera, la función de probabilidad acumulada define la distribución de probabilidad para elegir una altura al azar de nuestro vector de alturas x. 13.11 Distribuciones teóricas continuas En la sección 8.8 introdujimos la distribución normal como una aproximación útil a muchas distribuciones naturales, incluyendo la altura. La distribución acumulada para la distribución normal se define mediante una fórmula matemática que en R se puede obtener con la función pnorm. Decimos que una cantidad aleatoria se distribuye normalmente con un promedio m y desviación estándar s si su distribución de probabilidad se define por: F(a) = pnorm(a, m, s) Esto es útil porque si estamos dispuestos a usar la aproximación normal para, por ejemplo, la altura, no necesitamos todo el conjunto de datos para responder a preguntas como: ¿cuál es la probabilidad de que un estudiante seleccionado al azar sea más alto que 70 pulgadas? Solo necesitamos la altura promedio y la desviación estándar: m &lt;- mean(x) s &lt;- sd(x) 1 - pnorm(70.5, m, s) #&gt; [1] 0.371 13.11.1 Distribuciones teóricas como aproximaciones La distribución normal se deriva matemáticamente: no necesitamos datos para definirla. Para los científicos de datos, [fix] casi todo lo que hacemos en la práctica involucra datos. Los datos son siempre, desde un punto de vista técnico, discretos. Por ejemplo, podríamos considerar nuestros datos de altura categóricos con cada altura específica como una categoría única. La distribución de probabilidad se define por la proporción de estudiantes que indican cada altura. Aquí hay una gráfica de esa distribución de probabilidad: Mientras que la mayoría de los estudiantes redondearon sus alturas a la pulgada más cercana, otros informaron valores con más precisión. Un estudiante informó que su altura era 69.6850393700787 pulgadas, que equivale 177 centímetros. La probabilidad asignada a esta altura es 0.001 o 1 en 812. La probabilidad de 70 pulgadas es mucho mayor en 0.106, pero ¿tiene sentido pensar que la probabilidad de tener exactamente 70 pulgadas es diferente de 69.6850393700787? Claramente, es mucho más útil para fines de análisis de datos tratar este resultado como una variable numérica continua, teniendo en cuenta que muy pocas personas, o tal vez ninguna, son exactamente 70 pulgadas, y que la razón por la que obtenemos más valores en 70 es porque las personas redondean a la pulgada más cercana. [fix] Con distribuciones continuas, la probabilidad de un valor singular ni siquiera se define. Por ejemplo, no tiene sentido preguntar cuál es la probabilidad de que un valor distribuido normalmente sea 70. En cambio, definimos probabilidades para intervalos. Por lo tanto, podríamos preguntar cuál es la probabilidad de que alguien mida entre 69.5 y 70.5. En casos como la altura, en los que los datos se redondean, la aproximación normal es particularmente útil si estamos trabajando con intervalos que incluyen exactamente un número redondo. Por ejemplo, la distribución normal es útil para aproximar la proporción de estudiantes que indican valores en intervalos como los tres siguientes: mean(x &lt;= 68.5) - mean(x &lt;= 67.5) #&gt; [1] 0.115 mean(x &lt;= 69.5) - mean(x &lt;= 68.5) #&gt; [1] 0.119 mean(x &lt;= 70.5) - mean(x &lt;= 69.5) #&gt; [1] 0.122 Tengan en cuenta lo cerca que nos acercamos con la aproximación normal: pnorm(68.5, m, s) - pnorm(67.5, m, s) #&gt; [1] 0.103 pnorm(69.5, m, s) - pnorm(68.5, m, s) #&gt; [1] 0.11 pnorm(70.5, m, s) - pnorm(69.5, m, s) #&gt; [1] 0.108 Sin embargo, la aproximación no es tan útil para otros intervalos. Por ejemplo, observen cómo se descompone la aproximación cuando intentamos estimar: mean(x &lt;= 70.9) - mean(x&lt;=70.1) #&gt; [1] 0.0222 con: pnorm(70.9, m, s) - pnorm(70.1, m, s) #&gt; [1] 0.0836 En general, llamamos a esta situación discretización. Aunque la distribución de altura real es continua, las alturas reportadas tienden a ser más comunes en valores discretos, en este caso, debido al redondeo.Con tal que sepamos cómo lidiar con esta realidad, la aproximación normal puede ser una herramienta muy útil. 13.11.2 La densidad de probabilidad Para distribuciones categóricas, podemos definir la probabilidad de una categoría. Por ejemplo, un lanzamiento de dado, llamémoslo \\(X\\), puede ser 1,2,3,4,5 o 6. La probabilidad de 4 se define como: \\[ \\mbox{Pr}(X=4) = 1/6 \\] El CDF entonces se puede definir fácilmente: \\[ F(4) = \\mbox{Pr}(X\\leq 4) = \\mbox{Pr}(X = 4) + \\mbox{Pr}(X = 3) + \\mbox{Pr}(X = 2) + \\mbox{Pr}(X = 1) \\] Aunque para distribuciones continuas la probabilidad de un solo valor \\(\\mbox{Pr}(X=x)\\) no se define, hay una definición teórica que tiene una interpretación similar. La densidad de probabilidad en \\(x\\) se define como la función \\(f(a)\\) tal que: \\[ F(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx \\] Para aquellos que conocen el cálculo, recuerden que la integral está relacionada con una suma: es la suma de las barras con anchos que se aproximan a 0. Si no conocen el cálculo, pueden pensar en \\(f(x)\\) como una curva para la cual el área debajo de esa curva hasta el valor \\(a\\), les da la probabilidad \\(\\mbox{Pr}(X\\leq a)\\). Por ejemplo, para usar la aproximación normal para estimar la probabilidad de que alguien sea más alto que 76 pulgadas, usamos: 1 - pnorm(76, m, s) #&gt; [1] 0.0321 que matemáticamente es el área gris a continuación: La curva que ven es la densidad de probabilidad para la distribución normal. En R, obtenemos esto usando la función dnorm. Aunque puede no ser inmediatamente obvio por qué es útil conocer las densidades de probabilidad, comprender este concepto será esencial para aquellos que quieran ajustar modelos a datos para los que no hay funciones predefinidas disponibles. 13.12 Simulaciones Monte Carlo para variables continuas R provee funciones para generar resultados normalmente distribuidos. Específicamente, la función rnorm toma tres argumentos: tamaño, promedio (predeterminado a 0) y desviación estándar (predeterminado a 1) y produce números aleatorios. Aquí hay un ejemplo de cómo podríamos generar datos que se parezcan a nuestras alturas informadas: n &lt;- length(x) m &lt;- mean(x) s &lt;- sd(x) simulated_heights &lt;- rnorm(n, m, s) No es sorprendente que la distribución se vea normal: Esta es una de las funciones más útiles en R, ya que nos permite generar datos que imitan eventos naturales y responder a preguntas relacionadas con lo que podría suceder por casualidad al ejecutar simulaciones Monte Carlo. Si, por ejemplo, elegimos 800 hombres al azar, ¿cuál es la distribución de la persona más alta? ¿Cuán raro es un hombre de 7 pies o seven footer en un grupo de 800 hombres? La siguiente simulación Monte Carlo nos ayuda a responder esa pregunta: B &lt;- 10000 tallest &lt;- replicate(B, { simulated_data &lt;- rnorm(800, m, s) max(simulated_data) }) Tener un seven footer es bastante raro: mean(tallest &gt;= 7*12) #&gt; [1] 0.0196 Aquí está la distribución resultante: Tengan en cuenta que no parece normal. 13.13 Distribuciones continuas Introdujimos la distribución normal en la Sección 8.8 y se usó como ejemplo arriba. La distribución normal no es la única distribución teórica útil. Otras distribuciones continuas que podemos encontrar son student-t, chi-cuadrada, exponencial, gamma, beta y beta-binomial. R provee funciones para calcular la densidad, los cuantiles, las funciones de distribución acumulada y para generar simulaciones Monte Carlo. R usa una convención que nos ayuda recordar los nombres: usar las letras d, q, p y r delante de una [fix] taquigrafía/ “shorthand” para la distribución. Ya hemos visto las funciones dnorm, pnorm y rnorm para la distribución normal. La función qnorm nos da los cuantiles. Por lo tanto, podemos dibujar una distribución como esta: x &lt;- seq(-4, 4, length.out = 100) qplot(x, f, geom = &quot;line&quot;, data = data.frame(x, f = dnorm(x))) [fix check] Para student-t, descrito más adelante en la Sección ??, [fix] la taquigrafía t se usa para que las funciones sean dt para la densidad, qt para los cuantiles, pt para la función de distribución acumulada y rt para la simulación Monte Carlo. 13.14 Ejercicios 1. Suponga que la distribución de las alturas femeninas se aproxima por una distribución normal con una media de 64 pulgadas y una desviación estándar de 3 pulgadas. Si elegimos una hembra al azar, ¿cuál es la probabilidad de que mida 5 pies o menos? 2. Suponga que la distribución de las alturas femeninas se aproxima por una distribución normal con una media de 64 pulgadas y una desviación estándar de 3 pulgadas. Si elegimos una hembra al azar, ¿cuál es la probabilidad de que mida 6 pies o más? 3. Suponga que la distribución de las alturas femeninas se aproxima por una distribución normal con una media de 64 pulgadas y una desviación estándar de 3 pulgadas. Si elegimos una hembra al azar, ¿cuál es la probabilidad de que mida entre 61 y 67 pulgadas? 4. Repita el ejercicio anterior, pero convierta todo a centímetros. Es decir, multiplique cada altura, incluyendo la desviación estándar, por 2.54. ¿Cuál es la respuesta ahora? 5. Tenga en cuenta que la respuesta a la pregunta no cambia cuando cambian las unidades. Esto tiene sentido ya que la respuesta a la pregunta se debe ser afectada por las unidades que usamos. De hecho, si se fija bien, notará que 61 y 64 están a 1 SD de distancia del promedio. Calcule la probabilidad de que una variable aleatoria distribuida normal y aleatoriamente esté dentro de 1 SD del promedio. 6. Para ver las matemáticas que explican por qué las respuestas a las preguntas 3, 4 y 5 son las mismas, supongamos que tenemos una variable aleatoria con promedio \\(m\\) y error estándar \\(s\\). Supongamos que preguntamos la probabilidad de \\(X\\) ser más pequeño o igual a \\(a\\). Recuerda que, por definición, \\(a\\) es \\((a - m)/s\\) desviaciones estandar \\(s\\) lejos del promedio \\(m\\). La probabilidad es: \\[ \\mbox{Pr}(X \\leq a) \\] Ahora restamos \\(\\mu\\) a ambos lados y luego dividimos ambos lados por \\(\\sigma\\): \\[ \\mbox{Pr}\\left(\\frac{X-m}{s} \\leq \\frac{a-m}{s} \\right) \\] La cantidad a la izquierda es una variable aleatoria normal estándar. Tiene un promedio de 0 y un error estándar de 1. Lo llamaremos \\(Z\\): \\[ \\mbox{Pr}\\left(Z \\leq \\frac{a-m}{s} \\right) \\] Entonces, sin importar las unidades, la probabilidad de \\(X\\leq a\\) es igual a la probabilidad de que una variable normal estándar sea menor que \\((a - m)/s\\). ¿Si mu es el promedio y sigma el error estándar, cuál de los siguientes códigos de R nos dará la respuesta correcta en cada situación? mean(X&lt;=a) pnorm((a - m)/s) pnorm((a - m)/s, m, s) pnorm(a) 7. Imagine que la distribución de los hombres adultos es aproximadamente normal con un valor esperado de 69 y una desviación estándar de 3. ¿Cuán alto es el hombre en el percentil 99? Sugerencia: use qnorm. 8. [fix read all] La distribución de las puntuaciones de CI (IQ en inglés) se distribuye aproximadamente de manera normal. El promedio es 100 y la desviación estándar es 15. Suponga que desea conocer la distribución de los coeficientes intelectuales más altos en todas las clases de graduación si nacen 10,000 personas cada una en su distrito escolar. Ejecute una simulación Monte Carlo con B=1000 generando 10,000 puntuaciones de CI y manteniendo/ los más altos. Haga un histograma. https://en.wikipedia.org/wiki/Urn_problem↩ https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem↩ https://en.wikipedia.org/wiki/Monty_Hall_problem↩ "],
["variables-aleatorias.html", "Capítulo 14 Variables aleatorias 14.1 Variables aleatorias 14.2 Modelos de muestreo 14.3 La distribución de probabilidad de una variable aleatoria 14.4 Distribuciones versus distribuciones de probabilidad 14.5 Notación para variables aleatorias 14.6 El valor esperado y el error estándar 14.7 Teorema del límite central 14.8 Propiedades estadísticas de promedios 14.9 Ley de grandes números 14.10 Ejercicios 14.11 Estudio de caso: The Big Short 14.12 Ejercicios", " Capítulo 14 Variables aleatorias [fix check first 2 sent] En la ciencia de datos, a menudo trabajamos con datos que se ven afectados de alguna manera por la casualidad. Algunos ejemplos son datos que provienen de una muestra aleatoria, datos afectados por un error de medición o datos que miden algún resultado que es de naturaleza aleatoria. Ser capaz de cuantificar la incertidumbre introducida por la aleatoriedad es uno de los trabajos más importantes de los analistas de datos. La inferencia estadística ofrece un marco, así como varias herramientas prácticas para hacerlo. El primer paso es aprender a describir matemáticamente variables aleatorias. En este capítulo, presentamos variables aleatorias y sus propiedades comenzando con su aplicación a juegos de azar. Luego describimos algunos de los eventos que rodearon la crisis financiera de 2007-200852 usando la teoría de la probabilidad. Esta crisis financiera fue causada en parte por subestimar el riesgo de ciertos valores53 vendidos por instituciones financieras. Específicamente, los riesgos de los valores respaldados por hipotecas (MBS) y las obligaciones de deuda garantizadas (CDO) se subestimaron enormemente. Estos activos se vendieron a precios que suponían que la mayoría de los propietarios harían sus pagos mensuales y la probabilidad de que esto no ocurriera se calculó como baja. Una combinación de factores resultó en muchos más incumplimientos de lo esperado, lo que condujo a una caída de los precios de estos valores. Como consecuencia, los bancos perdieron tanto dinero que necesitaron rescates del gobierno para evitar cerrar por completo. 14.1 Variables aleatorias Las variables aleatorias son los resultados numéricos de procesos aleatorios. Podemos generar fácilmente variables aleatorias utilizando algunos de los ejemplos simples que ya hemos mostrado. [fix]Por ejemplo, definir X a ser 1 si una cuenta (bead en inglés) es azul y roja de lo contrario: beads &lt;- rep( c(&quot;red&quot;, &quot;blue&quot;), times = c(2,3)) X &lt;- ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) Aquí X es una variable aleatoria: cada vez que seleccionamos una nueva cuenta, el resultado cambia al azar. Vea abajo: ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 1 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 0 ifelse(sample(beads, 1) == &quot;blue&quot;, 1, 0) #&gt; [1] 0 A veces es 1 y a veces es 0. 14.2 Modelos de muestreo Muchos procedimientos de generación de datos, aquellos que producen los datos que estudiamos, pueden modelarse bastante bien como extraídos de una urna. Por ejemplo, podemos modelar el proceso de sondeo de votantes probables como sacar 0s (republicanos) y 1s (demócratas) de una urna que contiene el código 0 y 1 para todos los votantes probables. En los estudios epidemiológicos, a menudo suponemos que los sujetos de nuestro estudio son una muestra aleatoria de la población de interés. Los datos relacionados con un resultado específico se pueden modelar como una muestra aleatoria de una urna que contiene el resultado para toda la población de interés. De manera similar, en la investigación experimental, a menudo asumimos que los organismos individuales que estamos estudiando, por ejemplo, gusanos, moscas o ratones, son una muestra aleatoria de una población más grande. Los experimentos aleatorios también se pueden modelar mediante sorteos de una urna dada la forma en que los individuos se asignan en grupos: cuando se les asigna, dibujan su grupo al azar. Los modelos de muestreo son, por lo tanto, omnipresentes en la ciencia de datos. Los juegos de casino ofrecen una gran cantidad de ejemplos de situaciones del mundo real en las que se utilizan modelos de muestreo para responder preguntas específicas. Por lo tanto, comenzaremos con tales ejemplos. Supongamos que un casino muy pequeño lo contrata para consultar si deben configurar las ruedas de la ruleta. Para simplificar el ejemplo, asumiremos que jugarán 1,000 personas y que el único juego que puedes jugar en la ruleta es apostar en rojo o negro. El casino quiere que prediga cuánto dinero ganarán o perderán. Quieren una gama de valores y, en particular, quieren saber cuál es la posibilidad de perder dinero. Si esta probabilidad es demasiado alta, seguirán instalando ruedas de ruleta. Vamos a definir una variable aleatoria \\(S\\) eso representará las ganancias totales del casino. Comencemos por construir la urna. Una rueda de ruleta tiene 18 bolsillos rojos, 18 bolsillos negros y 2 verdes. Entonces, jugar un color en un juego de ruleta es equivalente a dibujar de esta urna: color &lt;- rep(c(&quot;Black&quot;, &quot;Red&quot;, &quot;Green&quot;), c(18, 18, 2)) Los 1,000 resultados de 1,000 personas jugando son sorteos independientes de esta urna. Si aparece el rojo, el jugador gana y el casino pierde un dólar, por lo que sacamos un - $1. Otherwise, the casino wins a dollar and we draw a $ 1. Para construir nuestra variable aleatoria \\(S\\), podemos usar este código: n &lt;- 1000 X &lt;- sample(ifelse(color == &quot;Red&quot;, -1, 1), n, replace = TRUE) X[1:10] #&gt; [1] -1 1 1 -1 -1 -1 1 1 1 1 Como conocemos las proporciones de 1s y -1s, podemos generar los sorteos con una línea de código, sin definir color: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) Llamamos a esto un modelo de muestreo ya que estamos modelando el comportamiento aleatorio de la ruleta con el muestreo de sorteos de una urna. Las ganancias totales \\(S\\) es simplemente la suma de estos 1,000 sorteos independientes: X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) S &lt;- sum(X) S #&gt; [1] 22 14.3 La distribución de probabilidad de una variable aleatoria Si ejecuta el código anterior, verá que \\(S\\) cambia cada vez. Esto es, por supuesto, porque \\(S\\) es una variable aleatoria. La distribución de probabilidad de una variable aleatoria nos dice la probabilidad de que el valor observado caiga en cualquier intervalo dado. Entonces, por ejemplo, si queremos saber la probabilidad de que perdamos dinero, estamos preguntando la probabilidad de que \\(S\\) está en el intervalo \\(S&lt;0\\). Tenga en cuenta que si podemos definir una función de distribución acumulativa \\(F(a) = \\mbox{Pr}(S\\leq a)\\), entonces podremos responder cualquier pregunta relacionada con la probabilidad de eventos definidos por nuestra variable aleatoria \\(S\\), incluido el evento \\(S&lt;0\\). A esto le llamamos \\(F\\) la función de distribución de la variable aleatoria. Podemos estimar la función de distribución para la variable aleatoria \\(S\\) mediante el uso de una simulación de Monte Carlo para generar muchas realizaciones de la variable aleatoria. Con este código, ejecutamos el experimento de tener 1,000 personas jugando a la ruleta, una y otra vez, específicamente \\(B = 10,000\\) veces: n &lt;- 1000 B &lt;- 10000 roulette_winnings &lt;- function(n){ X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S &lt;- replicate(B, roulette_winnings(n)) Ahora podemos preguntar lo siguiente: en nuestras simulaciones, con qué frecuencia obtuvimos sumas menores o iguales a a? mean(S &lt;= a) Esta será una muy buena aproximación de \\(F(a)\\) y podemos responder fácilmente a la pregunta del casino: ¿qué tan probable es que perdamos dinero? Podemos ver que es bastante bajo: mean(S&lt;0) #&gt; [1] 0.0456 Podemos visualizar la distribución de \\(S\\) creando un histograma que muestra la probabilidad \\(F(b)-F(a)\\) por varios intervalos \\((a,b]\\): Vemos que la distribución parece ser aproximadamente normal. Un diagrama qq confirmará que la aproximación normal está cerca de una aproximación perfecta para esta distribución. Si, de hecho, la distribución es normal, entonces todo lo que necesitamos para definir la distribución es el promedio y la desviación estándar. Debido a que tenemos los valores originales a partir de los cuales se crea la distribución, podemos calcularlos fácilmente con mean(S) y sd(S). La curva azul que ve agregada al histograma anterior es una densidad normal con esta desviación promedio y estándar. Este promedio y esta desviación estándar tienen nombres especiales. Se les conoce como valor esperado_y error estándar_ de la variable aleatoria \\(S\\). Diremos más sobre esto en la siguiente sección. La teoría estadística proporciona una forma de derivar la distribución de variables aleatorias definidas como extracciones aleatorias independientes de una urna. Específicamente, en nuestro ejemplo anterior, podemos mostrar que \\((S+n)/2\\) sigue una distribución binomial. Por lo tanto, no necesitamos correr para simulaciones de Monte Carlo para conocer la distribución de probabilidad de \\(S\\). Lo hicimos con fines ilustrativos. Podemos usar la función dbinom y pbinom para calcular las probabilidades exactamente. Por ejemplo, para calcular \\(\\mbox{Pr}(S &lt; 0)\\) notamos eso: \\[\\mbox{Pr}(S &lt; 0) = \\mbox{Pr}((S+n)/2 &lt; (0+n)/2)\\] y podemos usar el pbinom computar \\[\\mbox {Pr}(S \\leq 0) \\] n &lt;- 1000 pbinom(n/2, size = n, prob = 10/19) #&gt; [1] 0.0511 Debido a que esta es una función de probabilidad discreta, para obtener \\(\\mbox{Pr}(S &lt; 0)\\) más bien que \\(\\mbox{Pr}(S \\leq 0)\\), nosotros escribimos: pbinom(n/2-1, size = n, prob = 10/19) #&gt; [1] 0.0448 Para obtener detalles sobre la distribución binomial, puede consultar cualquier libro de probabilidad básico o incluso Wikipedia54. Aquí no cubrimos estos detalles. En cambio, discutiremos una aproximación increíblemente útil proporcionada por la teoría matemática que se aplica generalmente a sumas y promedios de sorteos de cualquier urna: el Teorema del límite central (CLT). 14.4 Distribuciones versus distribuciones de probabilidad Antes de continuar, hagamos una distinción y conexión importantes entre la distribución de una lista de números y una distribución de probabilidad. En el capítulo de visualización, describimos cómo cualquier lista de números \\(x_1,\\dots,x_n\\) tiene una distribución. La definición es bastante sencilla. Definimos \\(F(a)\\) como la función que nos dice qué proporción de la lista es menor o igual a \\(a\\). Debido a que son resúmenes útiles cuando la distribución es aproximadamente normal, definimos la desviación promedio y estándar. Estos se definen con una operación directa del vector que contiene la lista de números x: m &lt;- sum(x)/length(x) s &lt;- sqrt(sum((x - m)^2)/ length(x)) Una variable aleatoria \\(X\\) tiene una función de distribución. Para definir esto, no necesitamos una lista de números. Es un concepto teórico. En este caso, definimos la distribución como \\(F(a)\\) que responde a la pregunta: ¿cuál es la probabilidad de que \\(X\\) es menor o igual que \\(a\\)? No hay una lista de números. Sin embargo, si \\(X\\) se define dibujando desde una urna con números en ella, luego hay una lista: la lista de números dentro de la urna. En este caso, la distribución de esa lista es la distribución de probabilidad de \\(X\\) y la desviación promedio y estándar de esa lista son el valor esperado y el error estándar de la variable aleatoria. Otra forma de pensar que no involucra una urna es ejecutar una simulación de Monte Carlo y generar una lista muy grande de resultados de \\(X\\). Estos resultados son una lista de números. La distribución de esta lista será una muy buena aproximación de la distribución de probabilidad de \\(X\\). Cuanto más larga sea la lista, mejor será la aproximación. La desviación promedio y estándar de esta lista se aproximará al valor esperado y al error estándar de la variable aleatoria. 14.5 Notación para variables aleatorias En los libros de texto estadísticos, las letras mayúsculas se usan para denotar variables aleatorias y seguimos esta convención aquí. Se usan letras minúsculas para los valores observados. Verá alguna notación que incluye ambos. Por ejemplo, verá eventos definidos como \\(X \\leq x\\). Aquí \\(X\\) es una variable aleatoria, por lo que es un evento aleatorio, y \\(x\\) es un valor arbitrario y no aleatorio. Así por ejemplo, \\(X\\) podría representar el número en un dado y \\(x\\) representará un valor real que vemos 1, 2, 3, 4, 5 o 6. Entonces, en este caso, la probabilidad de \\(X=x\\) es 1/6 independientemente del valor observado \\(x\\). Esta notación es un poco extraña porque, cuando hacemos preguntas sobre la probabilidad, \\(X\\) no es una cantidad observada. En cambio, es una cantidad aleatoria que veremos en el futuro. Podemos hablar sobre lo que esperamos que sea, qué valores son probables, pero no qué es. Pero una vez que tenemos datos, vemos una realización de \\(X\\). Entonces, los científicos de datos hablan de lo que pudo haber sido después de ver lo que realmente sucedió. 14.6 El valor esperado y el error estándar Hemos descrito modelos de muestreo para sorteos. Ahora repasaremos la teoría matemática que nos permite aproximar las distribuciones de probabilidad para la suma de los sorteos. Una vez que hagamos esto, podremos ayudar al casino a predecir cuánto dinero ganarán. El mismo enfoque que usamos para la suma de los sorteos será útil para describir la distribución de promedios y la proporción que necesitaremos para entender cómo funcionan las encuestas. El primer concepto importante para aprender es el valor esperado. En los libros de estadísticas, es común usar letras \\(\\mbox{E}\\) me gusta esto: \\[\\mbox{E}[X]\\] para denotar el valor esperado de la variable aleatoria \\(X\\). Una variable aleatoria variará alrededor de su valor esperado de una manera que si toma el promedio de muchos, muchos sorteos, el promedio de los sorteos se aproximará al valor esperado, acercándose cada vez más a los sorteos que tome. La estadística teórica proporciona técnicas que facilitan el cálculo de los valores esperados en diferentes circunstancias. Por ejemplo, una fórmula útil nos dice que el * valor esperado de una variable aleatoria definida por un sorteo es el promedio de los números en la urna *. En la urna utilizada para modelar las apuestas al rojo en la ruleta, tenemos 20 dólares y 18 dólares negativos. El valor esperado es así: \\[ \\mbox{E}[X] = (20 + -18)/38 \\] que es de unos 5 centavos. Es un poco contradictorio decir que \\(X\\) varía alrededor de 0.05, cuando los únicos valores que toma son 1 y -1. Una manera de dar sentido al valor esperado en este contexto es darse cuenta de que si jugamos el juego una y otra vez, el casino gana, en promedio, 5 centavos por juego. Una simulación de Monte Carlo confirma esto: B &lt;- 10^6 x &lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19)) mean(x) #&gt; [1] 0.0517 En general, si la urna tiene dos resultados posibles, digamos \\(a\\) y \\(b\\), con proporciones \\(p\\) y \\(1-p\\) respectivamente, el promedio es: \\[\\mbox{E}[X] = ap + b(1-p)\\] Para ver esto, observe que si hay \\(n\\) cuentas en la urna, entonces tenemos \\(np\\) \\(a\\) s y \\(n(1-p)\\) \\(b\\) sy porque el promedio es la suma, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), dividido por el total \\(n\\), obtenemos que el promedio es \\(ap + b(1-p)\\). Ahora, la razón por la que definimos el valor esperado es porque esta definición matemática resulta útil para aproximar las distribuciones de probabilidad de suma, que luego es útil para describir la distribución de promedios y proporciones. El primer hecho útil es que el * valor esperado de la suma de los sorteos * es: \\[ \\mbox{}\\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn} \\] Entonces, si 1,000 personas juegan a la ruleta, el casino espera ganar, en promedio, alrededor de 1,000 \\(\\times\\) $0.05 = $ 50. Pero este es un valor esperado. ¿Cuán diferente puede ser una observación del valor esperado? El casino realmente necesita saber esto. ¿Cuál es el rango de posibilidades? Si los números negativos son muy probables, no instalarán las ruedas de ruleta. La teoría estadística una vez más responde a esta pregunta. El error estándar (SE) nos da una idea del tamaño de la variación alrededor del valor esperado. En los libros de estadísticas, es común usar: \\[\\mbox{SE}[X]\\] para denotar el error estándar de una variable aleatoria. Si nuestros sorteos son independientes, entonces el * error estándar de la suma * viene dado por la ecuación: \\[ \\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn} \\] Usando la definición de desviación estándar, podemos derivar, con un poco de matemática, que si una urna contiene dos valores \\(a\\) y \\(b\\) con proporciones \\(p\\) y \\((1-p)\\), respectivamente, la desviación estándar es: \\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\] Entonces, en nuestro ejemplo de ruleta, la desviación estándar de los valores dentro de la urna es: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) o: 2 * sqrt(90)/19 #&gt; [1] 0.999 El error estándar nos dice la diferencia típica entre una variable aleatoria y su expectativa. Dado que un sorteo es obviamente la suma de un solo sorteo, podemos usar la fórmula anterior para calcular que la variable aleatoria definida por un sorteo tiene un valor esperado de 0.05 y un error estándar de aproximadamente 1. Esto tiene sentido ya que obtenemos 1 o -1, con 1 ligeramente favorecido sobre -1. Usando la fórmula anterior, la suma de 1,000 personas jugando tiene un error estándar de aproximadamente $ 32: n &lt;- 1000 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 Como resultado, cuando 1,000 personas apuestan por el rojo, se espera que el casino gane $50 with a standard error of $ 32. Por lo tanto, parece una apuesta segura. Pero aún no hemos respondido la pregunta: ¿qué posibilidades hay de perder dinero? Aquí el CLT te ayudará. Nota avanzada: Antes de continuar, debemos señalar que los cálculos exactos de probabilidad de las ganancias del casino se pueden realizar con la distribución binomial. Sin embargo, aquí nos centramos en el CLT, que generalmente se puede aplicar a sumas de variables aleatorias de una manera que la distribución binomial no puede. 14.6.1 Población SD versus la muestra SD La desviación estándar de una lista. x (a continuación usamos alturas como ejemplo) se define como la raíz cuadrada del promedio de las diferencias cuadradas: library(dslabs) x &lt;- heights$height m &lt;- mean(x) s &lt;- sqrt(mean((x-m)^2)) Usando notación matemática escribimos: \\[ \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\] Sin embargo, tenga en cuenta que el sd la función devuelve un resultado ligeramente diferente: identical(s, sd(x)) #&gt; [1] FALSE s-sd(x) #&gt; [1] -0.00194 Esto es porque el sd la función R no devuelve el sd de la lista, sino que utiliza una fórmula que estima las desviaciones estándar de una población de una muestra aleatoria \\(X_1, \\dots, X_N\\) que, por razones no discutidas aquí, divide la suma de cuadrados por \\(N-1\\). \\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i, \\,\\,\\,\\, s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2} \\] Puede ver que este es el caso escribiendo: n &lt;- length(x) s-sd(x)*sqrt((n-1)/ n) #&gt; [1] 0 Para toda la teoría discutida aquí, debe calcular la desviación estándar real como se define: sqrt(mean((x-m)^2)) Así que tenga cuidado al usar el sd funcionan en R. Sin embargo, tenga en cuenta que a lo largo del libro a veces usamos el sd funcionar cuando realmente queremos la SD real. Esto se debe a que cuando el tamaño de la lista es grande, estos dos son prácticamente equivalentes ya que \\(\\sqrt{(N-1)/N} \\approx 1\\). 14.7 Teorema del límite central El Teorema del límite central (CLT) nos dice que cuando el número de sorteos, también llamado tamaño de muestra, es grande, la distribución de probabilidad de la suma de los sorteos independientes es aproximadamente normal. Debido a que los modelos de muestreo se utilizan para tantos procesos de generación de datos, el CLT se considera una de las ideas matemáticas más importantes de la historia. Anteriormente, discutimos que si sabemos que la distribución de una lista de números se aproxima a la distribución normal, todo lo que necesitamos para describir la lista es la desviación promedio y estándar. También sabemos que lo mismo se aplica a las distribuciones de probabilidad. Si una variable aleatoria tiene una distribución de probabilidad que se aproxima a la distribución normal, entonces todo lo que necesitamos para describir la distribución de probabilidad es el promedio y la desviación estándar, lo que se conoce como el valor esperado y el error estándar. Anteriormente ejecutamos esta simulación de Monte Carlo: n &lt;- 1000 B &lt;- 10000 roulette_winnings &lt;- function(n){ X &lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S &lt;- replicate(B, roulette_winnings(n)) El Teorema del límite central (CLT) nos dice que la suma \\(S\\) se aproxima por una distribución normal. Usando las fórmulas anteriores, sabemos que el valor esperado y el error estándar son: n * (20-18)/38 #&gt; [1] 52.6 sqrt(n) * 2 * sqrt(90)/19 #&gt; [1] 31.6 Los valores teóricos anteriores coinciden con los obtenidos con la simulación de Monte Carlo: mean(S) #&gt; [1] 52.2 sd(S) #&gt; [1] 31.7 Usando el CLT, podemos omitir la simulación de Monte Carlo y en su lugar calcular la probabilidad de que el casino pierda dinero usando esta aproximación: mu &lt;- n * (20-18)/38 se &lt;- sqrt(n) * 2 * sqrt(90)/19 pnorm(0, mu, se) #&gt; [1] 0.0478 que también está muy de acuerdo con nuestro resultado de Monte Carlo: mean(S &lt; 0) #&gt; [1] 0.0458 14.7.1 ¿Qué tan grande es grande en el Teorema del límite central? El CLT funciona cuando el número de sorteos es grande. Pero grande es un término relativo. En muchas circunstancias, solo 30 sorteos son suficientes para que el CLT sea útil. En algunos casos específicos, solo 10 son suficientes. Sin embargo, estas no deben considerarse reglas generales. Tenga en cuenta, por ejemplo, que cuando la probabilidad de éxito es muy pequeña, necesitamos tamaños de muestra mucho mayores. A modo de ilustración, consideremos la lotería. En la lotería, las posibilidades de ganar son menos de 1 en un millón. Miles de personas juegan, por lo que el número de sorteos es muy grande. Sin embargo, el número de ganadores, la suma de los sorteos, oscila entre 0 y 4. Esta suma ciertamente no se aproxima bien por una distribución normal, por lo que el CLT no se aplica, incluso con el tamaño de muestra muy grande. Esto es generalmente cierto cuando la probabilidad de éxito es muy baja. En estos casos, la distribución de Poisson es más apropiada. Puede examinar las propiedades de la distribución de Poisson usando dpois y ppois. Puede generar variables aleatorias siguiendo esta distribución con rpois. Sin embargo, no cubrimos la teoría aquí. Puede aprender sobre la distribución de Poisson en cualquier libro de texto de probabilidad e incluso Wikipedia55 14.8 Propiedades estadísticas de promedios Hay varios resultados matemáticos útiles que utilizamos anteriormente y que a menudo empleamos al trabajar con datos. Los enumeramos a continuación. 1. El valor esperado de la suma de variables aleatorias es la suma del valor esperado de cada variable aleatoria. Podemos escribirlo así: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n] = \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n] \\] Si el \\(X\\) son sorteos independientes de la urna, entonces todos tienen el mismo valor esperado. Vamos a llamarlo \\(\\mu\\) y por lo tanto: \\[ \\mbox{E}[X_1+X_2+\\dots+X_n]= n\\mu \\] que es otra forma de escribir el resultado que mostramos arriba para la suma de los sorteos. 2. El valor esperado de una constante no aleatoria multiplicada por una variable aleatoria es la constante no aleatoria multiplicada por el valor esperado de una variable aleatoria. Esto es más fácil de explicar con símbolos: \\[ \\mbox{E}[aX] = a\\times\\mbox{E}[X] \\] Para ver por qué esto es intuitivo, considere el cambio de unidades. Si cambiamos las unidades de una variable aleatoria, digamos de dólares a centavos, la expectativa debería cambiar de la misma manera. Una consecuencia de los dos hechos anteriores es que el valor esperado del promedio de extracciones independientes de la misma urna es el valor esperado de la urna, llámelo \\(\\mu\\) de nuevo: \\[ \\mbox{E}[(X_1+X_2+\\dots+X_n)/ n]= \\mbox{E}[X_1+X_2+\\dots+X_n]/ n = n\\mu/n = \\mu \\] 3. El cuadrado del error estándar de la suma de variables aleatorias independientes es la suma del cuadrado del error estándar de cada variable aleatoria. Este es más fácil de entender en forma matemática: \\[ \\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2 } \\] El cuadrado del error estándar se denomina varianza en los libros de texto estadísticos. Tenga en cuenta que esta propiedad en particular no es tan intuitiva como las tres y más explicaciones en profundidad que se pueden encontrar en los libros de texto de estadísticas. 4. El error estándar de una constante no aleatoria multiplicada por una variable aleatoria es la constante no aleatoria multiplicada por el error estándar de la variable aleatoria. Como con la expectativa: \\[ \\mbox{SE}[aX] = a \\times \\mbox{SE}[X] \\] Para ver por qué esto es intuitivo, piense nuevamente en las unidades. Una consecuencia de 3 y 4 es que el error estándar del promedio de extracciones independientes de la misma urna es la desviación estándar de la urna dividida por la raíz cuadrada de \\(n\\) (el número de sorteos), llámalo \\(\\sigma\\): \\[ \\begin{aligned} \\mbox{SE}[(X_1+X_2+\\dots+X_n)/ n] &amp;= \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\ &amp;= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\ &amp;= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\ &amp;= \\sqrt{n\\sigma^2}/n\\\\ &amp;= \\sigma/ \\sqrt{n} \\end{aligned} \\] 5. Si \\(X\\) es una variable aleatoria normalmente distribuida, entonces si \\(a\\) y \\(b\\) son constantes no aleatorias, \\(aX + b\\) también es una variable aleatoria normalmente distribuida. Todo lo que estamos haciendo es cambiar las unidades de la variable aleatoria multiplicando por \\(a\\), luego desplazando el centro por \\(b\\). Tenga en cuenta que los libros de texto estadísticos usan las letras griegas \\(\\mu\\) y \\(\\sigma\\) para denotar el valor esperado y el error estándar, respectivamente. Esto es porque \\(\\mu\\) es la letra griega para \\(m\\), la primera letra de mean, que es otro término utilizado para el valor esperado. Similar, \\(\\sigma\\) es la letra griega para \\(s\\), la primera letra de error estándar. 14.9 Ley de grandes números Una implicación importante del resultado final es que el error estándar del promedio se vuelve más y más pequeño a medida que \\(n\\) se hace más grande Cuando \\(n\\) es muy grande, entonces el error estándar es prácticamente 0 y el promedio de los sorteos converge al promedio de la urna. Esto se conoce en los libros de texto estadísticos como la ley de los grandes números o la ley de los promedios. 14.9.1 Interpretación errónea de la ley de promedios La ley de los promedios a veces se malinterpreta. Por ejemplo, si arroja una moneda 5 veces y ve una cara cada vez, es posible que escuche a alguien argumentar que el próximo lanzamiento probablemente sea una cola debido a la ley de los promedios: en promedio, deberíamos ver 50 % de caras y 50 % cruz. Un argumento similar sería decir que el rojo “se debe” en la rueda de la ruleta después de ver que el negro aparece cinco veces seguidas. Estos eventos son independientes, por lo que la probabilidad de que una moneda caiga cara es del 50 % independientemente de los 5 anteriores. Este también es el caso del resultado de la ruleta. La ley de promedios se aplica solo cuando el número de sorteos es muy grande y no en muestras pequeñas. Después de un millón de lanzamientos, definitivamente verá alrededor del 50 % de cabezas independientemente del resultado de los primeros cinco lanzamientos. Otro mal uso divertido de la ley de los promedios es en los deportes cuando los presentadores de televisión predicen que un jugador está a punto de triunfar porque han fallado varias veces seguidas. 14.10 Ejercicios 1. En la ruleta americana también puedes apostar por el verde. Hay 18 rojos, 18 negros y 2 verdes (0 y 00). ¿Cuáles son las posibilidades de que salga el verde? 2. El pago por ganar en verde es $17 dollars. This means that if you bet a dollar and it lands on green, you get $ 17. Cree un modelo de muestreo usando una muestra para simular la variable aleatoria \\(X\\) por tus ganancias Sugerencia: vea el ejemplo a continuación para ver cómo debería ser al apostar en rojo. x &lt;- sample(c(1,-1), 1, prob = c(9/19, 10/19)) 3. Calcule el valor esperado de \\(X\\). 4. Calcule el error estándar de \\(X\\). 5. Ahora crea una variable aleatoria \\(S\\) esa es la suma de sus ganancias después de apostar en green 1000 veces Sugerencia: cambie el argumento size y replace en su respuesta a la pregunta 2. Comience su código estableciendo la semilla en 1 con set.seed(1). 6. ¿Cuál es el valor esperado de \\(S\\)? 7. ¿Cuál es el error estándar de \\(S\\)? 8. ¿Cuál es la probabilidad de que termines ganando dinero? Sugerencia: use el CLT. 9. Cree una simulación de Monte Carlo que genere 1,000 resultados de \\(S\\). Calcule la desviación promedio y estándar de la lista resultante para confirmar los resultados de 6 y 7. Comience su código estableciendo la semilla en 1 con set.seed(1). 10. Ahora verifique su respuesta a 8 usando el resultado de Monte Carlo. 11. El resultado de Monte Carlo y la aproximación CLT están cerca, pero no tan cerca. ¿Qué podría explicar esto? 1,000 simulaciones no son suficientes. Si hacemos más, coinciden. si. El CLT no funciona tan bien cuando la probabilidad de éxito es pequeña. En este caso, fue 1/19. Si hacemos que el número de juegos de ruleta sea mayor, coincidirán mejor. La diferencia está dentro del error de redondeo. re. El CLT solo funciona para promedios. 12. Ahora crea una variable aleatoria \\(Y\\) ese es su promedio de ganancias por apuesta después de jugar sus ganancias después de apostar en green 1,000 veces. 13. ¿Cuál es el valor esperado de \\(Y\\)? 14. ¿Cuál es el error estándar de \\(Y\\)? 15. ¿Cuál es la probabilidad de que termines con ganancias por juego que sean positivas? Sugerencia: use el CLT. Dieciséis. Cree una simulación de Monte Carlo que genere 2.500 resultados de \\(Y\\). Calcule la desviación promedio y estándar de la lista resultante para confirmar los resultados de 6 y 7. Comience su código estableciendo la semilla en 1 con set.seed(1). 17. Ahora verifique su respuesta a 8 usando el resultado de Monte Carlo. 18. El resultado de Monte Carlo y la aproximación CLT ahora están mucho más cerca. ¿Qué podría explicar esto? Ahora estamos calculando promedios en lugar de sumas. si. 2.500 simulaciones de Monte Carlo no son mejores que 1.000. El CLT funciona mejor cuando el tamaño de la muestra es mayor. Aumentamos de 1,000 a 2,500. re. No está más cerca. La diferencia está dentro del error de redondeo. 14.11 Estudio de caso: The Big Short 14.11.1 Tasas de interés explicadas con modelo de oportunidad Los bancos también usan versiones más complejas de los modelos de muestreo que hemos discutido para decidir las tasas de interés. Supongamos que compran un banco pequeño que tiene un historial de identificación de posibles propietarios de viviendas en los que se puede confiar para realizar pagos. De hecho, históricamente, en un año determinado, solo el 2% de sus clientes no pagan el dinero que se les prestó. Sin embargo, el banco sabe que si simplemente le presta dinero a todos sus clientes sin intereses, terminará perdiendo dinero debido a este 2%. Aunque el banco sabe que el 2% de sus clientes probablemente no pagarán, no sabe cuáles son esos. Sin embargo, al cobrarles a todos un poco más en intereses, pueden compensar las pérdidas incurridas debido a ese 2% y también cubrir sus costos operativos. También pueden obtener ganancias, pero si establecen tasas de interés demasiado altas, los clientes se irán a otro banco. Utilizaremos todos estos hechos y un poco de teoría de probabilidad para decidir qué tasa de interés deben cobrar. Supongamos que su banco otorgará 1,000 préstamos por $180,000 este año. Además, tras sumar todos los costos, supongamos que su banco pierde $ 200,000 por ejecución hipotecaria. Para simplificar, asumimos que esto incluye todos los costos operativos. Un modelo de muestreo para este escenario puede codificarse así: n &lt;- 1000 loss_per_foreclosure &lt;- -200000 p &lt;- 0.02 defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) #&gt; [1] -4e+06 Tengan en cuenta que la pérdida total definida por la suma final es una variable aleatoria. Cada vez que ejecutan el código anterior, obtienen una respuesta diferente. Podemos construir fácilmente una simulación Monte Carlo para tener una idea de la distribución de esta variable aleatoria. B &lt;- 10000 losses &lt;- replicate(B, { defaults &lt;- sample( c(0,1), n, prob=c(1-p, p), replace = TRUE) sum(defaults * loss_per_foreclosure) }) Sin embargo, realmente no necesitamos una simulación Monte Carlo. Usando lo que hemos aprendido, el CLT nos dice que, debido a que nuestras pérdidas son una suma de sorteos independientes, su distribución es aproximadamente normal con el valor esperado y los errores estándar dados por: n*(p*loss_per_foreclosure + (1-p)*0) #&gt; [1] -4e+06 sqrt(n)*abs(loss_per_foreclosure)*sqrt(p*(1-p)) #&gt; [1] 885438 Ahora podemos establecer una tasa de interés para garantizar que, como promedio, lleguemos a un punto de equilibrio. Básicamente, necesitamos agregar una cantidad \\(x\\) a cada préstamo, que en este caso están representados por [fix] sorteos/draws, de modo que el valor esperado es 0. Si definimos \\(l\\) para ser la pérdida por ejecución hipotecaria, necesitamos: \\[ lp + x(1-p) = 0 \\] que implica \\(x\\) es: - loss_per_foreclosure*p/(1-p) #&gt; [1] 4082 o una tasa de interés de 0.023. Sin embargo, todavía tenemos un problema. Aunque esta tasa de interés garantiza que, como promedio, lleguen a un punto de equilibrio, existe una probabilidad del 50% de que pierdan dinero. Si su banco pierde dinero, tendrán que cerrarlo. Por lo tanto, deben elegir una tasa de interés que los protega de esto. Al mismo tiempo, si la tasa de interés es demasiado alta, sus clientes se irán a otro banco, por lo que debens estar dispuestos a asumir algunos riesgos. Entonces, digamos que quieren que sus posibilidades de perder dinero sean de 1 en 100, entonces ¿qué cantidad debe ser \\(x\\) ahora? Esto es un poco más difícil. Queremos que la suma \\(S\\) tenga: \\[\\mbox{Pr}(S&lt;0) = 0.01\\] Sabemos que \\(S\\) es aproximadamente normal. El valor esperado de \\(S\\) es \\[\\mbox{E}[S] = \\{ lp + x(1-p)\\}n\\] con \\(n\\) el número de [fix] sorteos/elecciones, que en este caso representa préstamos. El error estándar es: \\[\\mbox{SD}[S] = |x-l| \\sqrt{np(1-p)}.\\] Porque \\(x\\) es positivo y \\(l\\) negativo \\(|x-l|=x-l\\). Tengan en cuenta que estas son solo una aplicación de las fórmulas mostradas anteriormente, pero que usan símbolos más compactos. Ahora vamos a utilizar un “truco” matemático que es muy común en las estadísticas. Sumamos y restamos las mismas cantidades a ambos lados del evento. \\(S&lt;0\\) para que la probabilidad no cambie y terminemos con una variable aleatoria normal estándar a la izquierda, que luego nos permitirá escribir una ecuación con solo \\(x\\) como un desconocido. Este “truco” es el siguiente: Si \\(\\mbox{Pr}(S&lt;0) = 0.01\\) luego: \\[ \\mbox{Pr}\\left(\\frac{S - \\mbox{E}[S]}{\\mbox{SE}[S]} &lt; \\frac{ - \\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] [fix] Y recuerden \\(\\mbox{E}[S]\\) y \\(\\mbox{SE}[S]\\) son el valor esperado y el error estándar de \\(S\\), respectivamente. Todo lo que hicimos arriba fue sumar y dividir por la misma cantidad en ambos lados. Hicimos esto porque ahora el término de la izquierda es una variable aleatoria normal estándar, a la que cambiaremos el nombre \\(Z\\). Ahora completamos los espacios en blanco con la fórmula real para el valor esperado y el error estándar: \\[ \\mbox{Pr}\\left(Z &lt; \\frac{- \\{ lp + x(1-p)\\}n}{(x-l) \\sqrt{np(1-p)}}\\right) = 0.01 \\] Puede parecer complicado, pero recuerden que \\(l\\), \\(p\\) y \\(n\\) son todas cantidades conocidas, por lo que eventualmente las reemplazaremos con números. Ahora, debido a que Z es un [fix] azar normal con valor esperado 0 y error estándar 1, significa que la cantidad en el lado derecho del [fix] signo &lt; debe ser igual a: qnorm(0.01) #&gt; [1] -2.33 para que la ecuación sea [fix] verdadera/cierta. Recuérdalo \\(z=\\) qnorm(0.01) nos da el valor de \\(z\\) para cual: \\[ \\mbox{Pr}(Z \\leq z) = 0.01 \\] Esto significa que el lado derecho de la ecuación complicada debe ser \\(z\\)=qnorm(0.01). \\[ \\frac{- \\{ lp + x(1-p)\\}n} {(x-l) \\sqrt{n p (1-p)}} = z \\] El truco funciona porque terminamos con una expresión que contiene \\(x\\), que sabemos tiene que ser igual a una cantidad conocida \\(z\\). Resolviendo para \\(x\\) ahora es simplemente álgebra: \\[ x = - l \\frac{ np - z \\sqrt{np(1-p)}}{n(1-p) + z \\sqrt{np(1-p)}}\\] cual es: l &lt;- loss_per_foreclosure z &lt;- qnorm(0.01) x &lt;- -l*( n*p - z*sqrt(n*p*(1-p)))/ ( n*(1-p) + z*sqrt(n*p*(1-p))) x #&gt; [1] 6249 Nuestra tasa de interés ahora sube a 0.035. Esta sigue siendo una tasa de interés muy competitiva. Al elegir esta tasa de interés, ahora tenemos una ganancia esperada por préstamo de: loss_per_foreclosure*p + x*(1-p) #&gt; [1] 2124 que es un beneficio total esperado de aproximadamente: n*(loss_per_foreclosure*p + x*(1-p)) #&gt; [1] 2124198 dolares! Podemos ejecutar una simulación Monte Carlo para verificar nuestras aproximaciones teóricas: B &lt;- 100000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit) #&gt; [1] 2125109 mean(profit&lt;0) #&gt; [1] 0.0121 14.11.2 The Big Short Uno de sus empleados señala que, dado que el banco está haciendo 2,124 dólares por préstamo, ¡el banco debería otorgar más préstamos! ¿Por qué solo \\(n\\)? Usted explica que encontrar esos \\(n\\) clientes fue difícil. Necesita un grupo que sea predecible y que mantenga bajas las posibilidades de incumplimiento. Luego señala que incluso si la probabilidad de incumplimiento es mayor, siempre que nuestro valor esperado sea positivo, puede minimizar sus posibilidades de pérdidas al aumentar \\(n\\) y confiando en la ley de grandes números. Afirma que incluso si la tasa predeterminada es el doble, digamos 4%, si establecemos la tasa un poco más alta que este valor: p &lt;- 0.04 r &lt;- (- loss_per_foreclosure*p/(1-p))/ 180000 r #&gt; [1] 0.0463 [fix] nos beneficiaremos. Al 5%, tenemos garantizado un valor positivo esperado de: r &lt;- 0.05 x &lt;- r*180000 loss_per_foreclosure*p + x * (1-p) #&gt; [1] 640 y puede minimizar nuestras posibilidades de perder dinero simplemente aumentando \\(n\\) ya que: \\[ \\mbox{Pr}(S &lt; 0) = \\mbox{Pr}\\left(Z &lt; - \\frac{\\mbox{E}[S]}{\\mbox{SE}[S]}\\right) \\] con \\(Z\\) una variable aleatoria normal estándar como se muestra anteriormente. Si definimos \\(\\mu\\) y \\(\\sigma\\) para ser el valor esperado y la desviación estándar de la urna, respectivamente (es decir, de un solo préstamo), usando las fórmulas anteriores tenemos: \\(\\mbox{E}[S]= n\\mu\\) y \\(\\mbox{SE}[S]= \\sqrt{n}\\sigma\\). Entonces si definimos \\(z\\)=qnorm(0.01), tenemos: \\[ - \\frac{n\\mu}{\\sqrt{n}\\sigma} = - \\frac{\\sqrt{n}\\mu}{\\sigma} = z \\] lo que implica que si dejamos: \\[ n \\geq z^2 \\sigma^2/ \\mu^2 \\] tenemos garantizada una probabilidad de menos de 0.01. La implicación es que, siempre y cuando \\(\\mu\\) es positivo, podemos encontrar un \\(n\\) eso minimiza la probabilidad de una pérdida. Esta es una forma de la ley de los grandes números: cuando \\(n\\) es grande, nuestras ganancias promedio por préstamo convergen a la ganancia esperada \\(\\mu\\). Con \\(x\\) arreglado, ahora podemos preguntar qué \\(n\\) ¿necesitamos que la probabilidad sea 0.01? En nuestro ejemplo, si damos a conocer: z &lt;- qnorm(0.01) n &lt;- ceiling((z^2*(x-l)^2*p*(1-p))/(l*p + x*(1-p))^2) n #&gt; [1] 22163 préstamos, la probabilidad de perder es de aproximadamente 0.01 y se espera que ganemos un total de n*(loss_per_foreclosure*p + x * (1-p)) #&gt; [1] 14184320 dolares! Podemos confirmar esto con una simulación de Monte Carlo: p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-p, p), replace = TRUE) sum(draws) }) mean(profit) #&gt; [1] 14188977 Esto parece una obviedad. Como resultado, su colega decide abandonar su banco y comenzar su propia compañía hipotecaria de alto riesgo. Unos meses después, el banco de su colega se declaró en quiebra. Se escribe un libro y finalmente se hace una película relatando el error que cometieron su amigo y muchos otros. ¿Que pasó? El esquema de su colega se basó principalmente en esta fórmula matemática: \\[ \\mbox{SE}[(X_1+X_2+\\dots+X_n)/ n] = \\sigma/ \\sqrt{n} \\] Haciendo \\(n\\) grande, minimizamos el error estándar de nuestro beneficio por préstamo. Sin embargo, para que esta regla se cumpla, el \\(X\\) s deben ser sorteos independientes: el incumplimiento de una persona debe ser independiente del incumplimiento de otros. Tenga en cuenta que en el caso de promediar el mismo evento una y otra vez, un ejemplo extremo de eventos que no son independientes, obtenemos un error estándar que es \\(\\sqrt{n}\\) veces más grande: \\[ \\mbox{SE}[(X_1+X_1+\\dots+X_1)/ n] = \\mbox{SE}[n X_1/ n] = \\sigma &gt; \\sigma/ \\sqrt{n} \\] Para construir una simulación más realista que la original que ejecutó su colega, supongamos que hay un evento global que afecta a todas las personas con hipotecas de alto riesgo y cambia su probabilidad. Asumiremos que con una probabilidad de 50-50, todas las probabilidades suben o bajan ligeramente a algún lugar entre 0.03 y 0.05. Pero le sucede a todos a la vez, no solo a una persona. Estos sorteos ya no son independientes. p &lt;- 0.04 x &lt;- 0.05*180000 profit &lt;- replicate(B, { new_p &lt;- 0.04 + sample(seq(-0.01, 0.01, length = 100), 1) draws &lt;- sample( c(x, loss_per_foreclosure), n, prob=c(1-new_p, new_p), replace = TRUE) sum(draws) }) Tenga en cuenta que nuestro beneficio esperado sigue siendo grande: mean(profit) #&gt; [1] 14173763 Sin embargo, la probabilidad de que el banco tenga ganancias negativas se dispara a: mean(profit&lt;0) #&gt; [1] 0.348 Aún más aterrador es que la probabilidad de perder más de 10 millones de dólares es: mean(profit &lt; -10000000) #&gt; [1] 0.24 Para entender cómo sucede esto, mire la distribución: data.frame(profit_in_millions=profit/10^6) %&gt;% ggplot(aes(profit_in_millions)) + geom_histogram(color=&quot;black&quot;, binwidth = 5) La teoría se rompe por completo y la variable aleatoria tiene mucha más variabilidad de lo esperado. El colapso financiero de 2007 se debió, entre otras cosas, a los “expertos” financieros que asumieron la independencia cuando no había ninguno. 14.12 Ejercicios 1. Crea una variable aleatoria \\(S\\) con las ganancias de su banco si otorga 10,000 préstamos, la tasa predeterminada es 0.3 y pierde $ 200,000 en cada ejecución hipotecaria. Sugerencia: use el código que mostramos en la sección anterior, pero cambie los parámetros. 2. Ejecute una simulación de Monte Carlo con 10.000 resultados para \\(S\\). Haz un histograma de los resultados. 3. ¿Cuál es el valor esperado de \\(S\\)? 4. ¿Cuál es el error estándar de \\(S\\)? 5. Supongamos que otorgamos préstamos por $ 180,000. ¿Cuál debería ser la tasa de interés para que nuestro valor esperado sea 0? 6. (Más difícil) ¿Cuál debería ser la tasa de interés para que la posibilidad de perder dinero sea 1 en 20? En notación matemática, ¿cuál debería ser la tasa de interés para que \\(\\mbox{Pr}(S&lt;0) = 0.05\\) ? 7. Si el banco quiere minimizar las probabilidades de perder dinero, ¿cuál de las siguientes opciones no hace subir las tasas de interés? Un grupo más pequeño de préstamos. Una mayor probabilidad de incumplimiento. Una menor probabilidad requerida de perder dinero. El número de simulaciones de Monte Carlo. https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008↩ https://en.wikipedia.org/w/index.php?title=Security_ (finance)↩ https://en.wikipedia.org/w/index.php?title=Binomial_distribution↩ https://en.wikipedia.org/w/index.php?title=Poisson_distribution↩ "]
]
