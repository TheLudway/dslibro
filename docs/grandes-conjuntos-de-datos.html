<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 33 Grandes conjuntos de datos | Introducción a la Ciencia de Datos</title>
  <meta name="description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 33 Grandes conjuntos de datos | Introducción a la Ciencia de Datos" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 33 Grandes conjuntos de datos | Introducción a la Ciencia de Datos" />
  
  <meta name="twitter:description" content="Este libro presenta conceptos y destrezas que les ayudarán abordar los retos de situaciones actuales del análisis de datos. Cubre conceptos de probabilidad, inferencia estadística, regresión lineal y machine learning. Además, les permitirá desarrollar destrezas como la programación R, el wrangling de datos con dplyr, la visualización de datos con ggplot2, la organización de archivos con Shell de UNIX / Linux, el control de versiones con GitHub y la preparación de documentos reproducibles con R markdown." />
  

<meta name="author" content="Rafael A. Irizarry" />


<meta name="date" content="2021-03-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machine-learning-en-la-práctica.html"/>
<link rel="next" href="clustering.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introducción a la Ciencia de Datos</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a></li>
<li class="chapter" data-level="" data-path="agradecimientos.html"><a href="agradecimientos.html"><i class="fa fa-check"></i>Agradecimientos</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html"><i class="fa fa-check"></i>Introducción</a><ul>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#los-casos-de-estudio"><i class="fa fa-check"></i>Los casos de estudio</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#quién-encontrará-útil-este-libro"><i class="fa fa-check"></i>¿Quién encontrará útil este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#que-cubre-este-libro"><i class="fa fa-check"></i>¿Que cubre este libro?</a></li>
<li class="chapter" data-level="" data-path="introducción.html"><a href="introducción.html#qué-no-cubre-este-libro"><i class="fa fa-check"></i>¿Qué no cubre este libro?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Comenzando con R y RStudio</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#por-qué-r"><i class="fa fa-check"></i><b>1.1</b> ¿Por qué R?</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#la-consola-r"><i class="fa fa-check"></i><b>1.2</b> La consola R</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>1.3</b> <em>Scripts</em></a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>1.4</b> RStudio</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#paneles"><i class="fa fa-check"></i><b>1.4.1</b> Paneles</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>1.4.2</b> <em>Key bindings</em></a></li>
<li class="chapter" data-level="1.4.3" data-path="getting-started.html"><a href="getting-started.html#cómo-ejecutar-comandos-mientras-edita-scripts"><i class="fa fa-check"></i><b>1.4.3</b> Cómo ejecutar comandos mientras edita <em>scripts</em></a></li>
<li class="chapter" data-level="1.4.4" data-path="getting-started.html"><a href="getting-started.html#cómo-cambiar-las-opciones-globales"><i class="fa fa-check"></i><b>1.4.4</b> Cómo cambiar las opciones globales</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#instalación-de-paquetes-de-r"><i class="fa fa-check"></i><b>1.5</b> Instalación de paquetes de R</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>2</b> Lo básico de R</a><ul>
<li class="chapter" data-level="2.1" data-path="r-basics.html"><a href="r-basics.html#caso-de-estudio-los-asesinatos-con-armas-en-ee.-uu."><i class="fa fa-check"></i><b>2.1</b> Caso de estudio: los asesinatos con armas en EE. UU.</a></li>
<li class="chapter" data-level="2.2" data-path="r-basics.html"><a href="r-basics.html#lo-básico"><i class="fa fa-check"></i><b>2.2</b> Lo básico</a><ul>
<li class="chapter" data-level="2.2.1" data-path="r-basics.html"><a href="r-basics.html#objetos"><i class="fa fa-check"></i><b>2.2.1</b> Objetos</a></li>
<li class="chapter" data-level="2.2.2" data-path="r-basics.html"><a href="r-basics.html#el-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.2</b> El espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.3" data-path="r-basics.html"><a href="r-basics.html#funciones"><i class="fa fa-check"></i><b>2.2.3</b> Funciones</a></li>
<li class="chapter" data-level="2.2.4" data-path="r-basics.html"><a href="r-basics.html#otros-objetos-predefinidos"><i class="fa fa-check"></i><b>2.2.4</b> Otros objetos predefinidos</a></li>
<li class="chapter" data-level="2.2.5" data-path="r-basics.html"><a href="r-basics.html#nombres-de-variables"><i class="fa fa-check"></i><b>2.2.5</b> Nombres de variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="r-basics.html"><a href="r-basics.html#cómo-guardar-su-espacio-de-trabajo"><i class="fa fa-check"></i><b>2.2.6</b> Cómo guardar su espacio de trabajo</a></li>
<li class="chapter" data-level="2.2.7" data-path="r-basics.html"><a href="r-basics.html#scripts-motivantes"><i class="fa fa-check"></i><b>2.2.7</b> <em>Scripts</em> motivantes</a></li>
<li class="chapter" data-level="2.2.8" data-path="r-basics.html"><a href="r-basics.html#cómo-comentar-su-código"><i class="fa fa-check"></i><b>2.2.8</b> Cómo comentar su código</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="r-basics.html"><a href="r-basics.html#ejercicios"><i class="fa fa-check"></i><b>2.3</b> Ejercicios</a></li>
<li class="chapter" data-level="2.4" data-path="r-basics.html"><a href="r-basics.html#tipos-de-datos"><i class="fa fa-check"></i><b>2.4</b> Tipos de datos</a><ul>
<li class="chapter" data-level="2.4.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>2.4.1</b> <em>data frames</em></a></li>
<li class="chapter" data-level="2.4.2" data-path="r-basics.html"><a href="r-basics.html#cómo-examinar-un-objeto"><i class="fa fa-check"></i><b>2.4.2</b> Cómo examinar un objeto</a></li>
<li class="chapter" data-level="2.4.3" data-path="r-basics.html"><a href="r-basics.html#el-operador-de-acceso"><i class="fa fa-check"></i><b>2.4.3</b> El operador de acceso: <code>$</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="r-basics.html"><a href="r-basics.html#vectores-numéricos-de-caracteres-y-lógicos"><i class="fa fa-check"></i><b>2.4.4</b> Vectores: numéricos, de caracteres y lógicos</a></li>
<li class="chapter" data-level="2.4.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>2.4.5</b> Factores</a></li>
<li class="chapter" data-level="2.4.6" data-path="r-basics.html"><a href="r-basics.html#listas"><i class="fa fa-check"></i><b>2.4.6</b> Listas</a></li>
<li class="chapter" data-level="2.4.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>2.4.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="r-basics.html"><a href="r-basics.html#ejercicios-1"><i class="fa fa-check"></i><b>2.5</b> Ejercicios</a></li>
<li class="chapter" data-level="2.6" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>2.6</b> Vectores</a><ul>
<li class="chapter" data-level="2.6.1" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-vectores"><i class="fa fa-check"></i><b>2.6.1</b> Cómo crear vectores</a></li>
<li class="chapter" data-level="2.6.2" data-path="r-basics.html"><a href="r-basics.html#nombres"><i class="fa fa-check"></i><b>2.6.2</b> Nombres</a></li>
<li class="chapter" data-level="2.6.3" data-path="r-basics.html"><a href="r-basics.html#secuencias"><i class="fa fa-check"></i><b>2.6.3</b> Secuencias</a></li>
<li class="chapter" data-level="2.6.4" data-path="r-basics.html"><a href="r-basics.html#cómo-crear-un-subconjunto"><i class="fa fa-check"></i><b>2.6.4</b> Cómo crear un subconjunto</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="r-basics.html"><a href="r-basics.html#la-conversión-forzada"><i class="fa fa-check"></i><b>2.7</b> La conversión forzada</a><ul>
<li class="chapter" data-level="2.7.1" data-path="r-basics.html"><a href="r-basics.html#not-available-na"><i class="fa fa-check"></i><b>2.7.1</b> <em>Not available</em> (NA)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="r-basics.html"><a href="r-basics.html#ejercicios-2"><i class="fa fa-check"></i><b>2.8</b> Ejercicios</a></li>
<li class="chapter" data-level="2.9" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>2.9</b> <em>Sorting</em></a><ul>
<li class="chapter" data-level="2.9.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>2.9.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="2.9.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>2.9.2</b> <code>order</code></a></li>
<li class="chapter" data-level="2.9.3" data-path="r-basics.html"><a href="r-basics.html#max-y-which.max"><i class="fa fa-check"></i><b>2.9.3</b> <code>max</code> y <code>which.max</code></a></li>
<li class="chapter" data-level="2.9.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>2.9.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="2.9.5" data-path="r-basics.html"><a href="r-basics.html#cuidado-con-el-reciclaje"><i class="fa fa-check"></i><b>2.9.5</b> Cuidado con el reciclaje</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="r-basics.html"><a href="r-basics.html#ejercicios-3"><i class="fa fa-check"></i><b>2.10</b> Ejercicios</a></li>
<li class="chapter" data-level="2.11" data-path="r-basics.html"><a href="r-basics.html#aritmética-de-vectores"><i class="fa fa-check"></i><b>2.11</b> Aritmética de vectores</a><ul>
<li class="chapter" data-level="2.11.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-un-vector"><i class="fa fa-check"></i><b>2.11.1</b> <em>Rescaling</em> un vector</a></li>
<li class="chapter" data-level="2.11.2" data-path="r-basics.html"><a href="r-basics.html#dos-vectores"><i class="fa fa-check"></i><b>2.11.2</b> Dos vectores</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="r-basics.html"><a href="r-basics.html#ejercicios-4"><i class="fa fa-check"></i><b>2.12</b> Ejercicios</a></li>
<li class="chapter" data-level="2.13" data-path="r-basics.html"><a href="r-basics.html#indexación"><i class="fa fa-check"></i><b>2.13</b> Indexación</a><ul>
<li class="chapter" data-level="2.13.1" data-path="r-basics.html"><a href="r-basics.html#crear-subconjuntos-con-lógicos"><i class="fa fa-check"></i><b>2.13.1</b> Crear subconjuntos con lógicos</a></li>
<li class="chapter" data-level="2.13.2" data-path="r-basics.html"><a href="r-basics.html#operadores-lógicos"><i class="fa fa-check"></i><b>2.13.2</b> Operadores lógicos</a></li>
<li class="chapter" data-level="2.13.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>2.13.3</b> <code>which</code></a></li>
<li class="chapter" data-level="2.13.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>2.13.4</b> <code>match</code></a></li>
<li class="chapter" data-level="2.13.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>2.13.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="r-basics.html"><a href="r-basics.html#ejercicios-5"><i class="fa fa-check"></i><b>2.14</b> Ejercicios</a></li>
<li class="chapter" data-level="2.15" data-path="r-basics.html"><a href="r-basics.html#gráficos-básicos"><i class="fa fa-check"></i><b>2.15</b> Gráficos básicos</a><ul>
<li class="chapter" data-level="2.15.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>2.15.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="2.15.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>2.15.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="2.15.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>2.15.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="2.15.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>2.15.4</b> <code>image</code></a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="r-basics.html"><a href="r-basics.html#ejercicios-6"><i class="fa fa-check"></i><b>2.16</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html"><i class="fa fa-check"></i><b>3</b> Conceptos básicos de programación</a><ul>
<li class="chapter" data-level="3.1" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#conditionals"><i class="fa fa-check"></i><b>3.1</b> Expresiones condicionales</a></li>
<li class="chapter" data-level="3.2" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#cómo-definir-funciones"><i class="fa fa-check"></i><b>3.2</b> Cómo definir funciones</a></li>
<li class="chapter" data-level="3.3" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#namespaces"><i class="fa fa-check"></i><b>3.3</b> <em>Namespaces</em></a></li>
<li class="chapter" data-level="3.4" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#bucles-for"><i class="fa fa-check"></i><b>3.4</b> Bucles-for</a></li>
<li class="chapter" data-level="3.5" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#vectorization"><i class="fa fa-check"></i><b>3.5</b> Vectorización y funcionales</a></li>
<li class="chapter" data-level="3.6" data-path="conceptos-básicos-de-programación.html"><a href="conceptos-básicos-de-programación.html#ejercicios-7"><i class="fa fa-check"></i><b>3.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>4.1</b> Data <em>tidy</em></a></li>
<li class="chapter" data-level="4.2" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-8"><i class="fa fa-check"></i><b>4.2</b> Ejercicios</a></li>
<li class="chapter" data-level="4.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-manipular-los-data-frames"><i class="fa fa-check"></i><b>4.3</b> Cómo manipular los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.3.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-añadir-una-columna-con-mutate"><i class="fa fa-check"></i><b>4.3.1</b> Cómo añadir una columna con <code>mutate</code></a></li>
<li class="chapter" data-level="4.3.2" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-subconjuntos-con-filter"><i class="fa fa-check"></i><b>4.3.2</b> Cómo crear subconjuntos con <code>filter</code></a></li>
<li class="chapter" data-level="4.3.3" data-path="tidyverse.html"><a href="tidyverse.html#cómo-seleccionar-columnas-con-select"><i class="fa fa-check"></i><b>4.3.3</b> Cómo seleccionar columnas con <code>select</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-9"><i class="fa fa-check"></i><b>4.4</b> Ejercicios</a></li>
<li class="chapter" data-level="4.5" data-path="tidyverse.html"><a href="tidyverse.html#el-pipe"><i class="fa fa-check"></i><b>4.5</b> El <em>pipe</em>: <code>%&gt;%</code></a></li>
<li class="chapter" data-level="4.6" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-10"><i class="fa fa-check"></i><b>4.6</b> Ejercicios</a></li>
<li class="chapter" data-level="4.7" data-path="tidyverse.html"><a href="tidyverse.html#cómo-resumir-datos"><i class="fa fa-check"></i><b>4.7</b> Cómo resumir datos</a><ul>
<li class="chapter" data-level="4.7.1" data-path="tidyverse.html"><a href="tidyverse.html#summarize"><i class="fa fa-check"></i><b>4.7.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="tidyverse.html"><a href="tidyverse.html#pull"><i class="fa fa-check"></i><b>4.7.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="tidyverse.html"><a href="tidyverse.html#group-by"><i class="fa fa-check"></i><b>4.7.3</b> Cómo agrupar y luego resumir con <code>group_by</code></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-los-data-frames"><i class="fa fa-check"></i><b>4.8</b> Cómo ordenar los <em>data frames</em></a><ul>
<li class="chapter" data-level="4.8.1" data-path="tidyverse.html"><a href="tidyverse.html#cómo-ordenar-anidadamente"><i class="fa fa-check"></i><b>4.8.1</b> Cómo ordenar anidadamente</a></li>
<li class="chapter" data-level="4.8.2" data-path="tidyverse.html"><a href="tidyverse.html#los-primeros-n"><i class="fa fa-check"></i><b>4.8.2</b> Los primeros <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-11"><i class="fa fa-check"></i><b>4.9</b> Ejercicios</a></li>
<li class="chapter" data-level="4.10" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>4.10</b> <em>Tibbles</em></a><ul>
<li class="chapter" data-level="4.10.1" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-ven-mejor"><i class="fa fa-check"></i><b>4.10.1</b> Los <em>tibbles</em> se ven mejor</a></li>
<li class="chapter" data-level="4.10.2" data-path="tidyverse.html"><a href="tidyverse.html#los-subconjuntos-de-tibbles-son-tibbles"><i class="fa fa-check"></i><b>4.10.2</b> Los subconjuntos de <em>tibbles</em> son <em>tibbles</em></a></li>
<li class="chapter" data-level="4.10.3" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-pueden-tener-entradas-complejas"><i class="fa fa-check"></i><b>4.10.3</b> Los <em>tibbles</em> pueden tener entradas complejas</a></li>
<li class="chapter" data-level="4.10.4" data-path="tidyverse.html"><a href="tidyverse.html#los-tibbles-se-pueden-agrupar"><i class="fa fa-check"></i><b>4.10.4</b> Los <em>tibbles</em> se pueden agrupar</a></li>
<li class="chapter" data-level="4.10.5" data-path="tidyverse.html"><a href="tidyverse.html#cómo-crear-un-tibble-usando-tibble-en-lugar-de-data.frame"><i class="fa fa-check"></i><b>4.10.5</b> Cómo crear un <em>tibble</em> usando <code>tibble</code> en lugar de <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="tidyverse.html"><a href="tidyverse.html#el-operador-punto"><i class="fa fa-check"></i><b>4.11</b> El operador punto</a></li>
<li class="chapter" data-level="4.12" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>4.12</b> <code>do</code></a></li>
<li class="chapter" data-level="4.13" data-path="tidyverse.html"><a href="tidyverse.html#el-paquete-purrr"><i class="fa fa-check"></i><b>4.13</b> El paquete <strong>purrr</strong></a></li>
<li class="chapter" data-level="4.14" data-path="tidyverse.html"><a href="tidyverse.html#los-condicionales-de-tidyverse"><i class="fa fa-check"></i><b>4.14</b> Los condicionales de <em>tidyverse</em></a><ul>
<li class="chapter" data-level="4.14.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>4.14.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="4.14.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>4.14.2</b> <code>between</code></a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="tidyverse.html"><a href="tidyverse.html#ejercicios-12"><i class="fa fa-check"></i><b>4.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>5</b> Importando datos</a><ul>
<li class="chapter" data-level="5.1" data-path="importing-data.html"><a href="importing-data.html#las-rutas-y-el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1</b> Las rutas y el directorio de trabajo</a><ul>
<li class="chapter" data-level="5.1.1" data-path="importing-data.html"><a href="importing-data.html#el-sistema-de-archivos"><i class="fa fa-check"></i><b>5.1.1</b> El sistema de archivos</a></li>
<li class="chapter" data-level="5.1.2" data-path="importing-data.html"><a href="importing-data.html#las-rutas-relativas-y-completas"><i class="fa fa-check"></i><b>5.1.2</b> Las rutas relativas y completas</a></li>
<li class="chapter" data-level="5.1.3" data-path="importing-data.html"><a href="importing-data.html#el-directorio-de-trabajo"><i class="fa fa-check"></i><b>5.1.3</b> El directorio de trabajo</a></li>
<li class="chapter" data-level="5.1.4" data-path="importing-data.html"><a href="importing-data.html#cómo-generar-los-nombres-de-ruta"><i class="fa fa-check"></i><b>5.1.4</b> Cómo generar los nombres de ruta</a></li>
<li class="chapter" data-level="5.1.5" data-path="importing-data.html"><a href="importing-data.html#cómo-copiar-los-archivos-usando-rutas"><i class="fa fa-check"></i><b>5.1.5</b> Cómo copiar los archivos usando rutas</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importing-data.html"><a href="importing-data.html#los-paquetes-readr-y-readxl"><i class="fa fa-check"></i><b>5.2</b> Los paquetes readr y readxl</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>5.2.1</b> readr</a></li>
<li class="chapter" data-level="5.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>5.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="importing-data.html"><a href="importing-data.html#ejercicios-13"><i class="fa fa-check"></i><b>5.3</b> Ejercicios</a></li>
<li class="chapter" data-level="5.4" data-path="importing-data.html"><a href="importing-data.html#cómo-descargar-archivos"><i class="fa fa-check"></i><b>5.4</b> Cómo descargar archivos</a></li>
<li class="chapter" data-level="5.5" data-path="importing-data.html"><a href="importing-data.html#las-funciones-de-importación-de-base-r"><i class="fa fa-check"></i><b>5.5</b> Las funciones de importación de base R</a><ul>
<li class="chapter" data-level="5.5.1" data-path="importing-data.html"><a href="importing-data.html#scan"><i class="fa fa-check"></i><b>5.5.1</b> <code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="importing-data.html"><a href="importing-data.html#archivos-de-texto-versus-archivos-binarios"><i class="fa fa-check"></i><b>5.6</b> Archivos de texto versus archivos binarios</a></li>
<li class="chapter" data-level="5.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>5.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="5.8" data-path="importing-data.html"><a href="importing-data.html#cómo-organizar-datos-con-hojas-de-cálculo"><i class="fa fa-check"></i><b>5.8</b> Cómo organizar datos con hojas de cálculo</a></li>
<li class="chapter" data-level="5.9" data-path="importing-data.html"><a href="importing-data.html#ejercicios-14"><i class="fa fa-check"></i><b>5.9</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>II Visualización de datos</b></span></li>
<li class="chapter" data-level="6" data-path="introducción-a-la-visualización-de-datos.html"><a href="introducción-a-la-visualización-de-datos.html"><i class="fa fa-check"></i><b>6</b> Introducción a la visualización de datos</a></li>
<li class="chapter" data-level="7" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>7</b> ggplot2</a><ul>
<li class="chapter" data-level="7.1" data-path="ggplot2.html"><a href="ggplot2.html#los-componentes-de-un-gráfico"><i class="fa fa-check"></i><b>7.1</b> Los componentes de un gráfico</a></li>
<li class="chapter" data-level="7.2" data-path="ggplot2.html"><a href="ggplot2.html#objetos-ggplot"><i class="fa fa-check"></i><b>7.2</b> objetos <code>ggplot</code></a></li>
<li class="chapter" data-level="7.3" data-path="ggplot2.html"><a href="ggplot2.html#geometrías"><i class="fa fa-check"></i><b>7.3</b> Geometrías</a></li>
<li class="chapter" data-level="7.4" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos"><i class="fa fa-check"></i><b>7.4</b> Mapeos estéticos</a></li>
<li class="chapter" data-level="7.5" data-path="ggplot2.html"><a href="ggplot2.html#capas"><i class="fa fa-check"></i><b>7.5</b> Capas</a><ul>
<li class="chapter" data-level="7.5.1" data-path="ggplot2.html"><a href="ggplot2.html#cómo-probar-varios-argumentos"><i class="fa fa-check"></i><b>7.5.1</b> Cómo probar varios argumentos</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ggplot2.html"><a href="ggplot2.html#mapeos-estéticos-globales-versus-locales"><i class="fa fa-check"></i><b>7.6</b> Mapeos estéticos globales versus locales</a></li>
<li class="chapter" data-level="7.7" data-path="ggplot2.html"><a href="ggplot2.html#escalas"><i class="fa fa-check"></i><b>7.7</b> Escalas</a></li>
<li class="chapter" data-level="7.8" data-path="ggplot2.html"><a href="ggplot2.html#etiquetas-y-títulos"><i class="fa fa-check"></i><b>7.8</b> Etiquetas y títulos</a></li>
<li class="chapter" data-level="7.9" data-path="ggplot2.html"><a href="ggplot2.html#categorías-como-colores"><i class="fa fa-check"></i><b>7.9</b> Categorías como colores</a></li>
<li class="chapter" data-level="7.10" data-path="ggplot2.html"><a href="ggplot2.html#anotación-formas-y-ajustes"><i class="fa fa-check"></i><b>7.10</b> Anotación, formas y ajustes</a></li>
<li class="chapter" data-level="7.11" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>7.11</b> Paquetes complementarios</a></li>
<li class="chapter" data-level="7.12" data-path="ggplot2.html"><a href="ggplot2.html#cómo-combinarlo-todo"><i class="fa fa-check"></i><b>7.12</b> Cómo combinarlo todo</a></li>
<li class="chapter" data-level="7.13" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>7.13</b> Gráficos rápidos con <code>qplot</code></a></li>
<li class="chapter" data-level="7.14" data-path="ggplot2.html"><a href="ggplot2.html#cuadrículas-de-gráficos"><i class="fa fa-check"></i><b>7.14</b> Cuadrículas de gráficos</a></li>
<li class="chapter" data-level="7.15" data-path="ggplot2.html"><a href="ggplot2.html#ejercicios-15"><i class="fa fa-check"></i><b>7.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>8</b> Cómo visualizar distribuciones de datos</a><ul>
<li class="chapter" data-level="8.1" data-path="distributions.html"><a href="distributions.html#tipos-de-variables"><i class="fa fa-check"></i><b>8.1</b> Tipos de variables</a></li>
<li class="chapter" data-level="8.2" data-path="distributions.html"><a href="distributions.html#estudio-de-caso-describiendo-alturas-de-estudiantes"><i class="fa fa-check"></i><b>8.2</b> Estudio de caso: describiendo alturas de estudiantes</a></li>
<li class="chapter" data-level="8.3" data-path="distributions.html"><a href="distributions.html#la-función-de-distribución"><i class="fa fa-check"></i><b>8.3</b> La función de distribución</a></li>
<li class="chapter" data-level="8.4" data-path="distributions.html"><a href="distributions.html#cdf-intro"><i class="fa fa-check"></i><b>8.4</b> Funciones de distribución acumulada</a></li>
<li class="chapter" data-level="8.5" data-path="distributions.html"><a href="distributions.html#histogramas"><i class="fa fa-check"></i><b>8.5</b> Histogramas</a></li>
<li class="chapter" data-level="8.6" data-path="distributions.html"><a href="distributions.html#densidad-suave"><i class="fa fa-check"></i><b>8.6</b> Densidad suave</a><ul>
<li class="chapter" data-level="8.6.1" data-path="distributions.html"><a href="distributions.html#cómo-interpretar-el-eje-y"><i class="fa fa-check"></i><b>8.6.1</b> Cómo interpretar el eje-y</a></li>
<li class="chapter" data-level="8.6.2" data-path="distributions.html"><a href="distributions.html#densidades-permiten-estratificación"><i class="fa fa-check"></i><b>8.6.2</b> Densidades permiten estratificación</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="distributions.html"><a href="distributions.html#ejercicios-16"><i class="fa fa-check"></i><b>8.7</b> Ejercicios</a></li>
<li class="chapter" data-level="8.8" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>8.8</b> La distribución normal</a></li>
<li class="chapter" data-level="8.9" data-path="distributions.html"><a href="distributions.html#unidades-estándar"><i class="fa fa-check"></i><b>8.9</b> Unidades estándar</a></li>
<li class="chapter" data-level="8.10" data-path="distributions.html"><a href="distributions.html#gráficos-q-q"><i class="fa fa-check"></i><b>8.10</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>8.11</b> Percentiles</a></li>
<li class="chapter" data-level="8.12" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja"><i class="fa fa-check"></i><b>8.12</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.13" data-path="distributions.html"><a href="distributions.html#stratification"><i class="fa fa-check"></i><b>8.13</b> Estratificación</a></li>
<li class="chapter" data-level="8.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>8.14</b> Estudio de caso: descripción de alturas de estudiantes (continuación)</a></li>
<li class="chapter" data-level="8.15" data-path="distributions.html"><a href="distributions.html#ejercicios-17"><i class="fa fa-check"></i><b>8.15</b> Ejercicios</a></li>
<li class="chapter" data-level="8.16" data-path="distributions.html"><a href="distributions.html#other-geometries"><i class="fa fa-check"></i><b>8.16</b> Geometrías ggplot2</a><ul>
<li class="chapter" data-level="8.16.1" data-path="distributions.html"><a href="distributions.html#diagramas-de-barras"><i class="fa fa-check"></i><b>8.16.1</b> Diagramas de barras</a></li>
<li class="chapter" data-level="8.16.2" data-path="distributions.html"><a href="distributions.html#histogramas-1"><i class="fa fa-check"></i><b>8.16.2</b> Histogramas</a></li>
<li class="chapter" data-level="8.16.3" data-path="distributions.html"><a href="distributions.html#gráficos-de-densidad"><i class="fa fa-check"></i><b>8.16.3</b> Gráficos de densidad</a></li>
<li class="chapter" data-level="8.16.4" data-path="distributions.html"><a href="distributions.html#diagramas-de-caja-1"><i class="fa fa-check"></i><b>8.16.4</b> Diagramas de caja</a></li>
<li class="chapter" data-level="8.16.5" data-path="distributions.html"><a href="distributions.html#gráficos-q-q-1"><i class="fa fa-check"></i><b>8.16.5</b> Gráficos Q-Q</a></li>
<li class="chapter" data-level="8.16.6" data-path="distributions.html"><a href="distributions.html#imágenes"><i class="fa fa-check"></i><b>8.16.6</b> Imágenes</a></li>
<li class="chapter" data-level="8.16.7" data-path="distributions.html"><a href="distributions.html#gráficos-rápidos"><i class="fa fa-check"></i><b>8.16.7</b> Gráficos rápidos</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="distributions.html"><a href="distributions.html#ejercicios-18"><i class="fa fa-check"></i><b>8.17</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>9</b> Visualización de datos en la práctica</a><ul>
<li class="chapter" data-level="9.1" data-path="gapminder.html"><a href="gapminder.html#estudio-de-caso-nuevas-ideas-sobre-la-pobreza"><i class="fa fa-check"></i><b>9.1</b> Estudio de caso: nuevas ideas sobre la pobreza</a><ul>
<li class="chapter" data-level="9.1.1" data-path="gapminder.html"><a href="gapminder.html#la-prueba-de-hans-rosling"><i class="fa fa-check"></i><b>9.1.1</b> La prueba de Hans Rosling</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-dispersión"><i class="fa fa-check"></i><b>9.2</b> Diagrama de dispersión</a></li>
<li class="chapter" data-level="9.3" data-path="gapminder.html"><a href="gapminder.html#separar-en-facetas"><i class="fa fa-check"></i><b>9.3</b> Separar en facetas</a><ul>
<li class="chapter" data-level="9.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>9.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="9.3.2" data-path="gapminder.html"><a href="gapminder.html#escalas-fijas-para-mejores-comparaciones"><i class="fa fa-check"></i><b>9.3.2</b> Escalas fijas para mejores comparaciones</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="gapminder.html"><a href="gapminder.html#gráficos-de-series-de-tiempo"><i class="fa fa-check"></i><b>9.4</b> Gráficos de series de tiempo</a><ul>
<li class="chapter" data-level="9.4.1" data-path="gapminder.html"><a href="gapminder.html#etiquetas-en-lugar-de-leyendas"><i class="fa fa-check"></i><b>9.4.1</b> Etiquetas en lugar de leyendas</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="gapminder.html"><a href="gapminder.html#transformaciones-de-datos"><i class="fa fa-check"></i><b>9.5</b> Transformaciones de datos</a><ul>
<li class="chapter" data-level="9.5.1" data-path="gapminder.html"><a href="gapminder.html#transformación-logarítmica"><i class="fa fa-check"></i><b>9.5.1</b> Transformación logarítmica</a></li>
<li class="chapter" data-level="9.5.2" data-path="gapminder.html"><a href="gapminder.html#qué-base"><i class="fa fa-check"></i><b>9.5.2</b> ¿Qué base?</a></li>
<li class="chapter" data-level="9.5.3" data-path="gapminder.html"><a href="gapminder.html#transformar-los-valores-o-la-escala"><i class="fa fa-check"></i><b>9.5.3</b> ¿Transformar los valores o la escala?</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="gapminder.html"><a href="gapminder.html#cómo-visualizar-distribuciones-multimodales"><i class="fa fa-check"></i><b>9.6</b> Cómo visualizar distribuciones multimodales</a></li>
<li class="chapter" data-level="9.7" data-path="gapminder.html"><a href="gapminder.html#cómo-comparar-múltiples-distribuciones-con-diagramas-de-caja-y-gráficos-ridge"><i class="fa fa-check"></i><b>9.7</b> Cómo comparar múltiples distribuciones con diagramas de caja y gráficos <em>ridge</em></a><ul>
<li class="chapter" data-level="9.7.1" data-path="gapminder.html"><a href="gapminder.html#diagrama-de-caja"><i class="fa fa-check"></i><b>9.7.1</b> Diagrama de caja</a></li>
<li class="chapter" data-level="9.7.2" data-path="gapminder.html"><a href="gapminder.html#gráficos-ridge"><i class="fa fa-check"></i><b>9.7.2</b> Gráficos <em>ridge</em></a></li>
<li class="chapter" data-level="9.7.3" data-path="gapminder.html"><a href="gapminder.html#ejemplo-distribuciones-de-ingresos-de-1970-versus-2010"><i class="fa fa-check"></i><b>9.7.3</b> Ejemplo: distribuciones de ingresos de 1970 versus 2010</a></li>
<li class="chapter" data-level="9.7.4" data-path="gapminder.html"><a href="gapminder.html#cómo-obtener-acceso-a-variables-calculadas"><i class="fa fa-check"></i><b>9.7.4</b> Cómo obtener acceso a variables calculadas</a></li>
<li class="chapter" data-level="9.7.5" data-path="gapminder.html"><a href="gapminder.html#densidades-ponderadas"><i class="fa fa-check"></i><b>9.7.5</b> Densidades ponderadas</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="gapminder.html"><a href="gapminder.html#la-falacia-ecológica-y-la-importancia-de-mostrar-los-datos"><i class="fa fa-check"></i><b>9.8</b> La falacia ecológica y la importancia de mostrar los datos</a><ul>
<li class="chapter" data-level="9.8.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>9.8.1</b> Transformación logística</a></li>
<li class="chapter" data-level="9.8.2" data-path="gapminder.html"><a href="gapminder.html#mostrar-los-datos"><i class="fa fa-check"></i><b>9.8.2</b> Mostrar los datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html"><i class="fa fa-check"></i><b>10</b> Principios de visualización de datos</a><ul>
<li class="chapter" data-level="10.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-datos-utilizando-señales-visuales"><i class="fa fa-check"></i><b>10.1</b> Cómo codificar datos utilizando señales visuales</a></li>
<li class="chapter" data-level="10.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#sepa-cuándo-incluir-0"><i class="fa fa-check"></i><b>10.2</b> Sepa cuándo incluir 0</a></li>
<li class="chapter" data-level="10.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#no-distorsionar-cantidades"><i class="fa fa-check"></i><b>10.3</b> No distorsionar cantidades</a></li>
<li class="chapter" data-level="10.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ordenar-categorías-por-un-valor-significativo"><i class="fa fa-check"></i><b>10.4</b> Ordenar categorías por un valor significativo</a></li>
<li class="chapter" data-level="10.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#mostrar-los-datos-1"><i class="fa fa-check"></i><b>10.5</b> Mostrar los datos</a></li>
<li class="chapter" data-level="10.6" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-facilitar-comparaciones"><i class="fa fa-check"></i><b>10.6</b> Cómo facilitar comparaciones</a><ul>
<li class="chapter" data-level="10.6.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-ejes-comunes"><i class="fa fa-check"></i><b>10.6.1</b> Use ejes comunes</a></li>
<li class="chapter" data-level="10.6.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#alinee-gráficos-verticalmente-para-ver-cambios-horizontales-y-horizontalmente-para-ver-cambios-verticales"><i class="fa fa-check"></i><b>10.6.2</b> Alinee gráficos verticalmente para ver cambios horizontales y horizontalmente para ver cambios verticales</a></li>
<li class="chapter" data-level="10.6.3" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#considere-transformaciones"><i class="fa fa-check"></i><b>10.6.3</b> Considere transformaciones</a></li>
<li class="chapter" data-level="10.6.4" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#señales-visuales-comparadas-deben-estar-adyacentes"><i class="fa fa-check"></i><b>10.6.4</b> Señales visuales comparadas deben estar adyacentes</a></li>
<li class="chapter" data-level="10.6.5" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#use-color"><i class="fa fa-check"></i><b>10.6.5</b> Use color</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#piense-en-los-daltónicos"><i class="fa fa-check"></i><b>10.7</b> Piense en los daltónicos</a></li>
<li class="chapter" data-level="10.8" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráficos-para-dos-variables"><i class="fa fa-check"></i><b>10.8</b> Gráficos para dos variables</a><ul>
<li class="chapter" data-level="10.8.1" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#slope-charts"><i class="fa fa-check"></i><b>10.8.1</b> Slope charts</a></li>
<li class="chapter" data-level="10.8.2" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#gráfico-bland-altman"><i class="fa fa-check"></i><b>10.8.2</b> Gráfico Bland-Altman</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#cómo-codificar-una-tercera-variable"><i class="fa fa-check"></i><b>10.9</b> Cómo codificar una tercera variable</a></li>
<li class="chapter" data-level="10.10" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-los-gráficos-pseudo-tridimensionales"><i class="fa fa-check"></i><b>10.10</b> Evite los gráficos pseudo-tridimensionales</a></li>
<li class="chapter" data-level="10.11" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#evite-demasiados-dígitos-significativos"><i class="fa fa-check"></i><b>10.11</b> Evite demasiados dígitos significativos</a></li>
<li class="chapter" data-level="10.12" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#conozca-a-su-audiencia"><i class="fa fa-check"></i><b>10.12</b> Conozca a su audiencia</a></li>
<li class="chapter" data-level="10.13" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-19"><i class="fa fa-check"></i><b>10.13</b> Ejercicios</a></li>
<li class="chapter" data-level="10.14" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#vaccines"><i class="fa fa-check"></i><b>10.14</b> Estudio de caso: las vacunas y las enfermedades infecciosas</a></li>
<li class="chapter" data-level="10.15" data-path="principios-de-visualización-de-datos.html"><a href="principios-de-visualización-de-datos.html#ejercicios-20"><i class="fa fa-check"></i><b>10.15</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>11</b> Resúmenes robustos</a><ul>
<li class="chapter" data-level="11.1" data-path="robust-summaries.html"><a href="robust-summaries.html#valores-atípicos"><i class="fa fa-check"></i><b>11.1</b> Valores atípicos</a></li>
<li class="chapter" data-level="11.2" data-path="robust-summaries.html"><a href="robust-summaries.html#mediana"><i class="fa fa-check"></i><b>11.2</b> Mediana</a></li>
<li class="chapter" data-level="11.3" data-path="robust-summaries.html"><a href="robust-summaries.html#el-rango-intercuartil-iqr"><i class="fa fa-check"></i><b>11.3</b> El rango intercuartil (IQR)</a></li>
<li class="chapter" data-level="11.4" data-path="robust-summaries.html"><a href="robust-summaries.html#la-definición-de-tukey-de-un-valor-atípico"><i class="fa fa-check"></i><b>11.4</b> La definición de Tukey de un valor atípico</a></li>
<li class="chapter" data-level="11.5" data-path="robust-summaries.html"><a href="robust-summaries.html#desviación-absoluta-mediana"><i class="fa fa-check"></i><b>11.5</b> Desviación absoluta mediana</a></li>
<li class="chapter" data-level="11.6" data-path="robust-summaries.html"><a href="robust-summaries.html#ejercicios-21"><i class="fa fa-check"></i><b>11.6</b> Ejercicios</a></li>
<li class="chapter" data-level="11.7" data-path="robust-summaries.html"><a href="robust-summaries.html#estudio-de-caso-alturas-autoreportadas-de-estudiantes"><i class="fa fa-check"></i><b>11.7</b> Estudio de caso: alturas autoreportadas de estudiantes</a></li>
</ul></li>
<li class="part"><span><b>III Estadísticas con R</b></span></li>
<li class="chapter" data-level="12" data-path="introducción-a-las-estadísticas-con-r.html"><a href="introducción-a-las-estadísticas-con-r.html"><i class="fa fa-check"></i><b>12</b> Introducción a las estadísticas con R</a></li>
<li class="chapter" data-level="13" data-path="probabilidad.html"><a href="probabilidad.html"><i class="fa fa-check"></i><b>13</b> Probabilidad</a><ul>
<li class="chapter" data-level="13.1" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-discreta"><i class="fa fa-check"></i><b>13.1</b> Probabilidad discreta</a><ul>
<li class="chapter" data-level="13.1.1" data-path="probabilidad.html"><a href="probabilidad.html#frecuencia-relativa"><i class="fa fa-check"></i><b>13.1.1</b> Frecuencia relativa</a></li>
<li class="chapter" data-level="13.1.2" data-path="probabilidad.html"><a href="probabilidad.html#notación"><i class="fa fa-check"></i><b>13.1.2</b> Notación</a></li>
<li class="chapter" data-level="13.1.3" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>13.1.3</b> Distribuciones de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-datos-categóricos"><i class="fa fa-check"></i><b>13.2</b> Simulaciones Monte Carlo para datos categóricos</a><ul>
<li class="chapter" data-level="13.2.1" data-path="probabilidad.html"><a href="probabilidad.html#fijar-la-semilla-aleatoria"><i class="fa fa-check"></i><b>13.2.1</b> Fijar la semilla aleatoria</a></li>
<li class="chapter" data-level="13.2.2" data-path="probabilidad.html"><a href="probabilidad.html#con-y-sin-reemplazo"><i class="fa fa-check"></i><b>13.2.2</b> Con y sin reemplazo</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="probabilidad.html"><a href="probabilidad.html#independencia"><i class="fa fa-check"></i><b>13.3</b> Independencia</a></li>
<li class="chapter" data-level="13.4" data-path="probabilidad.html"><a href="probabilidad.html#probabilidades-condicionales"><i class="fa fa-check"></i><b>13.4</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="13.5" data-path="probabilidad.html"><a href="probabilidad.html#reglas-de-la-adición-y-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5</b> Reglas de la adición y de la multiplicación</a><ul>
<li class="chapter" data-level="13.5.1" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación"><i class="fa fa-check"></i><b>13.5.1</b> Regla de la multiplicación</a></li>
<li class="chapter" data-level="13.5.2" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-multiplicación-bajo-independencia"><i class="fa fa-check"></i><b>13.5.2</b> Regla de la multiplicación bajo independencia</a></li>
<li class="chapter" data-level="13.5.3" data-path="probabilidad.html"><a href="probabilidad.html#regla-de-la-adición"><i class="fa fa-check"></i><b>13.5.3</b> Regla de la adición</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="probabilidad.html"><a href="probabilidad.html#combinaciones-y-permutaciones"><i class="fa fa-check"></i><b>13.6</b> Combinaciones y permutaciones</a><ul>
<li class="chapter" data-level="13.6.1" data-path="probabilidad.html"><a href="probabilidad.html#ejemplo-monte-carlo"><i class="fa fa-check"></i><b>13.6.1</b> Ejemplo Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="probabilidad.html"><a href="probabilidad.html#ejemplos"><i class="fa fa-check"></i><b>13.7</b> Ejemplos</a><ul>
<li class="chapter" data-level="13.7.1" data-path="probabilidad.html"><a href="probabilidad.html#problema-monty-hall"><i class="fa fa-check"></i><b>13.7.1</b> Problema Monty Hall</a></li>
<li class="chapter" data-level="13.7.2" data-path="probabilidad.html"><a href="probabilidad.html#problema-de-cumpleaños"><i class="fa fa-check"></i><b>13.7.2</b> Problema de cumpleaños</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="probabilidad.html"><a href="probabilidad.html#infinito-en-la-práctica"><i class="fa fa-check"></i><b>13.8</b> Infinito en la práctica</a></li>
<li class="chapter" data-level="13.9" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-22"><i class="fa fa-check"></i><b>13.9</b> Ejercicios</a></li>
<li class="chapter" data-level="13.10" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-continua"><i class="fa fa-check"></i><b>13.10</b> Probabilidad continua</a></li>
<li class="chapter" data-level="13.11" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-continuas"><i class="fa fa-check"></i><b>13.11</b> Distribuciones teóricas continuas</a><ul>
<li class="chapter" data-level="13.11.1" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-teóricas-como-aproximaciones"><i class="fa fa-check"></i><b>13.11.1</b> Distribuciones teóricas como aproximaciones</a></li>
<li class="chapter" data-level="13.11.2" data-path="probabilidad.html"><a href="probabilidad.html#la-densidad-de-probabilidad"><i class="fa fa-check"></i><b>13.11.2</b> La densidad de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="probabilidad.html"><a href="probabilidad.html#simulaciones-monte-carlo-para-variables-continuas"><i class="fa fa-check"></i><b>13.12</b> Simulaciones Monte Carlo para variables continuas</a></li>
<li class="chapter" data-level="13.13" data-path="probabilidad.html"><a href="probabilidad.html#distribuciones-continuas"><i class="fa fa-check"></i><b>13.13</b> Distribuciones continuas</a></li>
<li class="chapter" data-level="13.14" data-path="probabilidad.html"><a href="probabilidad.html#ejercicios-23"><i class="fa fa-check"></i><b>13.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html"><i class="fa fa-check"></i><b>14</b> Variables aleatorias</a><ul>
<li class="chapter" data-level="14.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#variables-aleatorias-1"><i class="fa fa-check"></i><b>14.1</b> Variables aleatorias</a></li>
<li class="chapter" data-level="14.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#modelos-de-muestreo"><i class="fa fa-check"></i><b>14.2</b> Modelos de muestreo</a></li>
<li class="chapter" data-level="14.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#la-distribución-de-probabilidad-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>14.3</b> La distribución de probabilidad de una variable aleatoria</a></li>
<li class="chapter" data-level="14.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#distribuciones-versus-distribuciones-de-probabilidad"><i class="fa fa-check"></i><b>14.4</b> Distribuciones versus distribuciones de probabilidad</a></li>
<li class="chapter" data-level="14.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#notación-para-variables-aleatorias"><i class="fa fa-check"></i><b>14.5</b> Notación para variables aleatorias</a></li>
<li class="chapter" data-level="14.6" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#el-valor-esperado-y-el-error-estándar"><i class="fa fa-check"></i><b>14.6</b> El valor esperado y el error estándar</a><ul>
<li class="chapter" data-level="14.6.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#población-sd-versus-la-muestra-sd"><i class="fa fa-check"></i><b>14.6.1</b> Población SD versus la muestra SD</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7</b> Teorema del límite central</a><ul>
<li class="chapter" data-level="14.7.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#cuán-grande-es-grande-en-el-teorema-del-límite-central"><i class="fa fa-check"></i><b>14.7.1</b> ¿Cuán grande es grande en el teorema del límite central?</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#propiedades-estadísticas-de-promedios"><i class="fa fa-check"></i><b>14.8</b> Propiedades estadísticas de promedios</a></li>
<li class="chapter" data-level="14.9" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ley-de-los-grandes-números"><i class="fa fa-check"></i><b>14.9</b> Ley de los grandes números</a><ul>
<li class="chapter" data-level="14.9.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#malinterpretando-la-ley-de-promedios"><i class="fa fa-check"></i><b>14.9.1</b> Malinterpretando la ley de promedios</a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-24"><i class="fa fa-check"></i><b>14.10</b> Ejercicios</a></li>
<li class="chapter" data-level="14.11" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#estudio-de-caso-the-big-short"><i class="fa fa-check"></i><b>14.11</b> Estudio de caso: The Big Short</a><ul>
<li class="chapter" data-level="14.11.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#tasas-de-interés-explicadas-con-modelo-de-oportunidad"><i class="fa fa-check"></i><b>14.11.1</b> Tasas de interés explicadas con modelo de oportunidad</a></li>
<li class="chapter" data-level="14.11.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#the-big-short"><i class="fa fa-check"></i><b>14.11.2</b> The Big Short</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#ejercicios-25"><i class="fa fa-check"></i><b>14.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>15</b> Inferencia estadística</a><ul>
<li class="chapter" data-level="15.1" data-path="inference.html"><a href="inference.html#encuestas"><i class="fa fa-check"></i><b>15.1</b> Encuestas</a><ul>
<li class="chapter" data-level="15.1.1" data-path="inference.html"><a href="inference.html#el-modelo-de-muestreo-para-encuestas"><i class="fa fa-check"></i><b>15.1.1</b> El modelo de muestreo para encuestas</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="inference.html"><a href="inference.html#poblaciones-muestras-parámetros-y-estimaciones"><i class="fa fa-check"></i><b>15.2</b> Poblaciones, muestras, parámetros y estimaciones</a><ul>
<li class="chapter" data-level="15.2.1" data-path="inference.html"><a href="inference.html#el-promedio-de-la-muestra"><i class="fa fa-check"></i><b>15.2.1</b> El promedio de la muestra</a></li>
<li class="chapter" data-level="15.2.2" data-path="inference.html"><a href="inference.html#parámetros"><i class="fa fa-check"></i><b>15.2.2</b> Parámetros</a></li>
<li class="chapter" data-level="15.2.3" data-path="inference.html"><a href="inference.html#encuesta-versus-pronóstico"><i class="fa fa-check"></i><b>15.2.3</b> Encuesta versus pronóstico</a></li>
<li class="chapter" data-level="15.2.4" data-path="inference.html"><a href="inference.html#propiedades-de-nuestra-estimación-valor-esperado-y-error-estándar"><i class="fa fa-check"></i><b>15.2.4</b> Propiedades de nuestra estimación: valor esperado y error estándar</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="inference.html"><a href="inference.html#ejercicios-26"><i class="fa fa-check"></i><b>15.3</b> Ejercicios</a></li>
<li class="chapter" data-level="15.4" data-path="inference.html"><a href="inference.html#clt"><i class="fa fa-check"></i><b>15.4</b> Teorema del límite central en la práctica</a><ul>
<li class="chapter" data-level="15.4.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo"><i class="fa fa-check"></i><b>15.4.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.4.2" data-path="inference.html"><a href="inference.html#la-diferencia"><i class="fa fa-check"></i><b>15.4.2</b> La diferencia</a></li>
<li class="chapter" data-level="15.4.3" data-path="inference.html"><a href="inference.html#sesgo-por-qué-no-realizar-una-encuesta-bien-grande"><i class="fa fa-check"></i><b>15.4.3</b> Sesgo: ¿por qué no realizar una encuesta bien grande?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="inference.html"><a href="inference.html#ejercicios-27"><i class="fa fa-check"></i><b>15.5</b> Ejercicios</a></li>
<li class="chapter" data-level="15.6" data-path="inference.html"><a href="inference.html#intervalos-de-confianza"><i class="fa fa-check"></i><b>15.6</b> Intervalos de confianza</a><ul>
<li class="chapter" data-level="15.6.1" data-path="inference.html"><a href="inference.html#una-simulación-monte-carlo-1"><i class="fa fa-check"></i><b>15.6.1</b> Una simulación Monte Carlo</a></li>
<li class="chapter" data-level="15.6.2" data-path="inference.html"><a href="inference.html#el-idioma-correcto"><i class="fa fa-check"></i><b>15.6.2</b> El idioma correcto</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="inference.html"><a href="inference.html#ejercicios-28"><i class="fa fa-check"></i><b>15.7</b> Ejercicios</a></li>
<li class="chapter" data-level="15.8" data-path="inference.html"><a href="inference.html#poder"><i class="fa fa-check"></i><b>15.8</b> Poder</a></li>
<li class="chapter" data-level="15.9" data-path="inference.html"><a href="inference.html#valores-p"><i class="fa fa-check"></i><b>15.9</b> valores p</a></li>
<li class="chapter" data-level="15.10" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>15.10</b> Pruebas de asociación</a><ul>
<li class="chapter" data-level="15.10.1" data-path="inference.html"><a href="inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>15.10.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="15.10.2" data-path="inference.html"><a href="inference.html#tablas-2x2"><i class="fa fa-check"></i><b>15.10.2</b> Tablas 2x2</a></li>
<li class="chapter" data-level="15.10.3" data-path="inference.html"><a href="inference.html#prueba-de-chi-cuadrado"><i class="fa fa-check"></i><b>15.10.3</b> Prueba de chi-cuadrado</a></li>
<li class="chapter" data-level="15.10.4" data-path="inference.html"><a href="inference.html#odds-ratio"><i class="fa fa-check"></i><b>15.10.4</b> Riesgo relativo</a></li>
<li class="chapter" data-level="15.10.5" data-path="inference.html"><a href="inference.html#intervalos-de-confianza-para-el-riesgo-relativo"><i class="fa fa-check"></i><b>15.10.5</b> Intervalos de confianza para el riesgo relativo</a></li>
<li class="chapter" data-level="15.10.6" data-path="inference.html"><a href="inference.html#corrección-de-recuento-pequeño"><i class="fa fa-check"></i><b>15.10.6</b> Corrección de recuento pequeño</a></li>
<li class="chapter" data-level="15.10.7" data-path="inference.html"><a href="inference.html#muestras-grandes-valores-p-pequeños"><i class="fa fa-check"></i><b>15.10.7</b> Muestras grandes, valores p pequeños</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="inference.html"><a href="inference.html#ejercicios-29"><i class="fa fa-check"></i><b>15.11</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>16</b> Modelos estadísticos</a><ul>
<li class="chapter" data-level="16.1" data-path="models.html"><a href="models.html#agregadores-de-encuestas"><i class="fa fa-check"></i><b>16.1</b> Agregadores de encuestas</a><ul>
<li class="chapter" data-level="16.1.1" data-path="models.html"><a href="models.html#datos-de-encuesta"><i class="fa fa-check"></i><b>16.1.1</b> Datos de encuesta</a></li>
<li class="chapter" data-level="16.1.2" data-path="models.html"><a href="models.html#sesgo-de-los-encuestadores"><i class="fa fa-check"></i><b>16.1.2</b> Sesgo de los encuestadores</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="models.html"><a href="models.html#data-driven-model"><i class="fa fa-check"></i><b>16.2</b> Modelos basados en datos</a></li>
<li class="chapter" data-level="16.3" data-path="models.html"><a href="models.html#ejercicios-30"><i class="fa fa-check"></i><b>16.3</b> Ejercicios</a></li>
<li class="chapter" data-level="16.4" data-path="models.html"><a href="models.html#bayesian-statistics"><i class="fa fa-check"></i><b>16.4</b> Estadísticas bayesianas</a><ul>
<li class="chapter" data-level="16.4.1" data-path="models.html"><a href="models.html#teorema-de-bayes"><i class="fa fa-check"></i><b>16.4.1</b> Teorema de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="models.html"><a href="models.html#simulación-del-teorema-de-bayes"><i class="fa fa-check"></i><b>16.5</b> Simulación del teorema de Bayes</a><ul>
<li class="chapter" data-level="16.5.1" data-path="models.html"><a href="models.html#bayes-en-la-práctica"><i class="fa fa-check"></i><b>16.5.1</b> Bayes en la práctica</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="models.html"><a href="models.html#modelos-jerárquicos"><i class="fa fa-check"></i><b>16.6</b> Modelos jerárquicos</a></li>
<li class="chapter" data-level="16.7" data-path="models.html"><a href="models.html#ejercicios-31"><i class="fa fa-check"></i><b>16.7</b> Ejercicios</a></li>
<li class="chapter" data-level="16.8" data-path="models.html"><a href="models.html#election-forecasting"><i class="fa fa-check"></i><b>16.8</b> Estudio de caso: pronóstico de elecciones</a><ul>
<li class="chapter" data-level="16.8.1" data-path="models.html"><a href="models.html#bayesian-approach"><i class="fa fa-check"></i><b>16.8.1</b> Enfoque bayesiano</a></li>
<li class="chapter" data-level="16.8.2" data-path="models.html"><a href="models.html#el-sesgo-general"><i class="fa fa-check"></i><b>16.8.2</b> El sesgo general</a></li>
<li class="chapter" data-level="16.8.3" data-path="models.html"><a href="models.html#representaciones-matemáticas-de-modelos"><i class="fa fa-check"></i><b>16.8.3</b> Representaciones matemáticas de modelos</a></li>
<li class="chapter" data-level="16.8.4" data-path="models.html"><a href="models.html#prediciendo-el-colegio-electoral"><i class="fa fa-check"></i><b>16.8.4</b> Prediciendo el colegio electoral</a></li>
<li class="chapter" data-level="16.8.5" data-path="models.html"><a href="models.html#pronósticos"><i class="fa fa-check"></i><b>16.8.5</b> Pronósticos</a></li>
</ul></li>
<li class="chapter" data-level="16.9" data-path="models.html"><a href="models.html#ejercicios-32"><i class="fa fa-check"></i><b>16.9</b> Ejercicios</a></li>
<li class="chapter" data-level="16.10" data-path="models.html"><a href="models.html#t-dist"><i class="fa fa-check"></i><b>16.10</b> La distribución t</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>17</b> Regresión</a><ul>
<li class="chapter" data-level="17.1" data-path="regression.html"><a href="regression.html#estudio-de-caso-la-altura-es-hereditaria"><i class="fa fa-check"></i><b>17.1</b> Estudio de caso: ¿la altura es hereditaria?</a></li>
<li class="chapter" data-level="17.2" data-path="regression.html"><a href="regression.html#corr-coef"><i class="fa fa-check"></i><b>17.2</b> El coeficiente de correlación</a><ul>
<li class="chapter" data-level="17.2.1" data-path="regression.html"><a href="regression.html#la-correlación-de-muestra-es-una-variable-aleatoria"><i class="fa fa-check"></i><b>17.2.1</b> La correlación de muestra es una variable aleatoria</a></li>
<li class="chapter" data-level="17.2.2" data-path="regression.html"><a href="regression.html#la-correlación-no-siempre-es-un-resumen-útil"><i class="fa fa-check"></i><b>17.2.2</b> La correlación no siempre es un resumen útil</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>17.3</b> Valor esperado condicional</a></li>
<li class="chapter" data-level="17.4" data-path="regression.html"><a href="regression.html#la-línea-de-regresión"><i class="fa fa-check"></i><b>17.4</b> La línea de regresión</a><ul>
<li class="chapter" data-level="17.4.1" data-path="regression.html"><a href="regression.html#regresión-mejora-precisión"><i class="fa fa-check"></i><b>17.4.1</b> Regresión mejora precisión</a></li>
<li class="chapter" data-level="17.4.2" data-path="regression.html"><a href="regression.html#distribución-normal-de-dos-variables-avanzada"><i class="fa fa-check"></i><b>17.4.2</b> Distribución normal de dos variables (avanzada)</a></li>
<li class="chapter" data-level="17.4.3" data-path="regression.html"><a href="regression.html#varianza-explicada"><i class="fa fa-check"></i><b>17.4.3</b> Varianza explicada</a></li>
<li class="chapter" data-level="17.4.4" data-path="regression.html"><a href="regression.html#advertencia-hay-dos-líneas-de-regresión"><i class="fa fa-check"></i><b>17.4.4</b> Advertencia: hay dos líneas de regresión</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="regression.html"><a href="regression.html#ejercicios-33"><i class="fa fa-check"></i><b>17.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>18</b> Modelos lineales</a><ul>
<li class="chapter" data-level="18.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball"><i class="fa fa-check"></i><b>18.1</b> Estudio de caso: Moneyball</a><ul>
<li class="chapter" data-level="18.1.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#sabermetrics"><i class="fa fa-check"></i><b>18.1.1</b> Sabermetrics</a></li>
<li class="chapter" data-level="18.1.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#conceptos-básicos-de-béisbol"><i class="fa fa-check"></i><b>18.1.2</b> Conceptos básicos de béisbol</a></li>
<li class="chapter" data-level="18.1.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#no-hay-premios-para-bb"><i class="fa fa-check"></i><b>18.1.3</b> No hay premios para BB</a></li>
<li class="chapter" data-level="18.1.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#base-por-bolas-o-bases-robadas"><i class="fa fa-check"></i><b>18.1.4</b> ¿Base por bolas o bases robadas?</a></li>
<li class="chapter" data-level="18.1.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-aplicada-a-las-estadísticas-de-béisbol"><i class="fa fa-check"></i><b>18.1.5</b> Regresión aplicada a las estadísticas de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#confusión"><i class="fa fa-check"></i><b>18.2</b> Confusión</a><ul>
<li class="chapter" data-level="18.2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#entender-confusión-a-través-de-estratificación"><i class="fa fa-check"></i><b>18.2.1</b> Entender confusión a través de estratificación</a></li>
<li class="chapter" data-level="18.2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-múltiple"><i class="fa fa-check"></i><b>18.2.2</b> Regresión lineal múltiple</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse"><i class="fa fa-check"></i><b>18.3</b> Estimaciones de mínimos cuadrados</a><ul>
<li class="chapter" data-level="18.3.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#interpretando-modelos-lineales"><i class="fa fa-check"></i><b>18.3.1</b> Interpretando modelos lineales</a></li>
<li class="chapter" data-level="18.3.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimaciones-de-mínimos-cuadrados-lse"><i class="fa fa-check"></i><b>18.3.2</b> Estimaciones de mínimos cuadrados (LSE)</a></li>
<li class="chapter" data-level="18.3.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-función-lm"><i class="fa fa-check"></i><b>18.3.3</b> La función <code>lm</code></a></li>
<li class="chapter" data-level="18.3.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#lse-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.4</b> LSE son variables aleatorias</a></li>
<li class="chapter" data-level="18.3.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-pronosticados-son-variables-aleatorias"><i class="fa fa-check"></i><b>18.3.5</b> Valores pronosticados son variables aleatorias</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-34"><i class="fa fa-check"></i><b>18.4</b> Ejercicios</a></li>
<li class="chapter" data-level="18.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresión-lineal-en-el-tidyverse"><i class="fa fa-check"></i><b>18.5</b> Regresión lineal en el tidyverse</a><ul>
<li class="chapter" data-level="18.5.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#el-paquete-broom"><i class="fa fa-check"></i><b>18.5.1</b> El paquete broom</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-35"><i class="fa fa-check"></i><b>18.6</b> Ejercicios</a></li>
<li class="chapter" data-level="18.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estudio-de-caso-moneyball-continuación"><i class="fa fa-check"></i><b>18.7</b> Estudio de caso: Moneyball (continuación)</a><ul>
<li class="chapter" data-level="18.7.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#añadiendo-información-sobre-salario-y-posición"><i class="fa fa-check"></i><b>18.7.1</b> Añadiendo información sobre salario y posición</a></li>
<li class="chapter" data-level="18.7.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#escoger-nueve-jugadores"><i class="fa fa-check"></i><b>18.7.2</b> Escoger nueve jugadores</a></li>
</ul></li>
<li class="chapter" data-level="18.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#la-falacia-de-la-regresión"><i class="fa fa-check"></i><b>18.8</b> La falacia de la regresión</a></li>
<li class="chapter" data-level="18.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-de-error-de-medición"><i class="fa fa-check"></i><b>18.9</b> Modelos de error de medición</a></li>
<li class="chapter" data-level="18.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ejercicios-36"><i class="fa fa-check"></i><b>18.10</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html"><i class="fa fa-check"></i><b>19</b> La asociación no implica causalidad</a><ul>
<li class="chapter" data-level="19.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#correlación-espuria"><i class="fa fa-check"></i><b>19.1</b> Correlación espuria</a></li>
<li class="chapter" data-level="19.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#valores-atípicos-1"><i class="fa fa-check"></i><b>19.2</b> Valores atípicos</a></li>
<li class="chapter" data-level="19.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#inversión-de-causa-y-efecto"><i class="fa fa-check"></i><b>19.3</b> Inversión de causa y efecto</a></li>
<li class="chapter" data-level="19.4" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#factores-de-confusión"><i class="fa fa-check"></i><b>19.4</b> Factores de confusión</a><ul>
<li class="chapter" data-level="19.4.1" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejemplo-admisiones-a-la-universidad-de-california-berkeley"><i class="fa fa-check"></i><b>19.4.1</b> Ejemplo: admisiones a la Universidad de California, Berkeley</a></li>
<li class="chapter" data-level="19.4.2" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#confusión-explicada-gráficamente"><i class="fa fa-check"></i><b>19.4.2</b> Confusión explicada gráficamente</a></li>
<li class="chapter" data-level="19.4.3" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#promedio-después-de-estratificar"><i class="fa fa-check"></i><b>19.4.3</b> Promedio después de estratificar</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#la-paradoja-de-simpson"><i class="fa fa-check"></i><b>19.5</b> La paradoja de Simpson</a></li>
<li class="chapter" data-level="19.6" data-path="la-asociación-no-implica-causalidad.html"><a href="la-asociación-no-implica-causalidad.html#ejercicios-37"><i class="fa fa-check"></i><b>19.6</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>IV <em>Wrangling</em> de datos</b></span></li>
<li class="chapter" data-level="20" data-path="introducción-al-wrangling-de-datos.html"><a href="introducción-al-wrangling-de-datos.html"><i class="fa fa-check"></i><b>20</b> Introducción al wrangling de datos</a></li>
<li class="chapter" data-level="21" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html"><i class="fa fa-check"></i><b>21</b> Cómo cambiar el formato de datos</a><ul>
<li class="chapter" data-level="21.1" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#gather"><i class="fa fa-check"></i><b>21.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="21.2" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#spread"><i class="fa fa-check"></i><b>21.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="21.3" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#separate"><i class="fa fa-check"></i><b>21.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="21.4" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#unite"><i class="fa fa-check"></i><b>21.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="21.5" data-path="cómo-cambiar-el-formato-de-datos.html"><a href="cómo-cambiar-el-formato-de-datos.html#ejercicios-38"><i class="fa fa-check"></i><b>21.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="unir-tablas.html"><a href="unir-tablas.html"><i class="fa fa-check"></i><b>22</b> Unir tablas</a><ul>
<li class="chapter" data-level="22.1" data-path="unir-tablas.html"><a href="unir-tablas.html#joins"><i class="fa fa-check"></i><b>22.1</b> Joins</a><ul>
<li class="chapter" data-level="22.1.1" data-path="unir-tablas.html"><a href="unir-tablas.html#left-join"><i class="fa fa-check"></i><b>22.1.1</b> Left join</a></li>
<li class="chapter" data-level="22.1.2" data-path="unir-tablas.html"><a href="unir-tablas.html#right-join"><i class="fa fa-check"></i><b>22.1.2</b> Right join</a></li>
<li class="chapter" data-level="22.1.3" data-path="unir-tablas.html"><a href="unir-tablas.html#inner-join"><i class="fa fa-check"></i><b>22.1.3</b> Inner join</a></li>
<li class="chapter" data-level="22.1.4" data-path="unir-tablas.html"><a href="unir-tablas.html#full-join"><i class="fa fa-check"></i><b>22.1.4</b> Full join</a></li>
<li class="chapter" data-level="22.1.5" data-path="unir-tablas.html"><a href="unir-tablas.html#semi-join"><i class="fa fa-check"></i><b>22.1.5</b> Semi join</a></li>
<li class="chapter" data-level="22.1.6" data-path="unir-tablas.html"><a href="unir-tablas.html#anti-join"><i class="fa fa-check"></i><b>22.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="unir-tablas.html"><a href="unir-tablas.html#binding"><i class="fa fa-check"></i><b>22.2</b> Binding</a><ul>
<li class="chapter" data-level="22.2.1" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-columnas"><i class="fa fa-check"></i><b>22.2.1</b> Pegando columnas</a></li>
<li class="chapter" data-level="22.2.2" data-path="unir-tablas.html"><a href="unir-tablas.html#pegando-filas"><i class="fa fa-check"></i><b>22.2.2</b> Pegando filas</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="unir-tablas.html"><a href="unir-tablas.html#operadores-de-sets"><i class="fa fa-check"></i><b>22.3</b> Operadores de sets</a><ul>
<li class="chapter" data-level="22.3.1" data-path="unir-tablas.html"><a href="unir-tablas.html#intersecar"><i class="fa fa-check"></i><b>22.3.1</b> Intersecar</a></li>
<li class="chapter" data-level="22.3.2" data-path="unir-tablas.html"><a href="unir-tablas.html#unión"><i class="fa fa-check"></i><b>22.3.2</b> Unión</a></li>
<li class="chapter" data-level="22.3.3" data-path="unir-tablas.html"><a href="unir-tablas.html#setdiff"><i class="fa fa-check"></i><b>22.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="22.3.4" data-path="unir-tablas.html"><a href="unir-tablas.html#setequal"><i class="fa fa-check"></i><b>22.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="unir-tablas.html"><a href="unir-tablas.html#ejercicios-39"><i class="fa fa-check"></i><b>22.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html"><i class="fa fa-check"></i><b>23</b> Extracción de la web</a><ul>
<li class="chapter" data-level="23.1" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#html"><i class="fa fa-check"></i><b>23.1</b> HTML</a></li>
<li class="chapter" data-level="23.2" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#el-paquete-rvest"><i class="fa fa-check"></i><b>23.2</b> El paquete rvest</a></li>
<li class="chapter" data-level="23.3" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#css-selectors"><i class="fa fa-check"></i><b>23.3</b> Selectores CSS</a></li>
<li class="chapter" data-level="23.4" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#json"><i class="fa fa-check"></i><b>23.4</b> JSON</a></li>
<li class="chapter" data-level="23.5" data-path="extracción-de-la-web.html"><a href="extracción-de-la-web.html#ejercicios-40"><i class="fa fa-check"></i><b>23.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html"><i class="fa fa-check"></i><b>24</b> Procesamiento de cadenas</a><ul>
<li class="chapter" data-level="24.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#stringr"><i class="fa fa-check"></i><b>24.1</b> El paquete stringr</a></li>
<li class="chapter" data-level="24.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-1-datos-de-asesinatos-en-ee.-uu."><i class="fa fa-check"></i><b>24.2</b> Estudio de caso 1: datos de asesinatos en EE. UU.</a></li>
<li class="chapter" data-level="24.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas"><i class="fa fa-check"></i><b>24.3</b> Estudio de caso 2: alturas autoreportadas</a></li>
<li class="chapter" data-level="24.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cómo-escapar-al-definir-cadenas"><i class="fa fa-check"></i><b>24.4</b> Cómo <em>escapar</em> al definir cadenas</a></li>
<li class="chapter" data-level="24.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#expresiones-regulares"><i class="fa fa-check"></i><b>24.5</b> Expresiones regulares</a><ul>
<li class="chapter" data-level="24.5.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#las-cadenas-son-una-expresión-regular"><i class="fa fa-check"></i><b>24.5.1</b> Las cadenas son una expresión regular</a></li>
<li class="chapter" data-level="24.5.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#caracteres-especiales"><i class="fa fa-check"></i><b>24.5.2</b> Caracteres especiales</a></li>
<li class="chapter" data-level="24.5.3" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#clases-de-caracteres"><i class="fa fa-check"></i><b>24.5.3</b> Clases de caracteres</a></li>
<li class="chapter" data-level="24.5.4" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#anclas"><i class="fa fa-check"></i><b>24.5.4</b> Anclas</a></li>
<li class="chapter" data-level="24.5.5" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores"><i class="fa fa-check"></i><b>24.5.5</b> Cuantificadores</a></li>
<li class="chapter" data-level="24.5.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#espacio-en-blanco-s"><i class="fa fa-check"></i><b>24.5.6</b> Espacio en blanco <code>\s</code></a></li>
<li class="chapter" data-level="24.5.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cuantificadores-1"><i class="fa fa-check"></i><b>24.5.7</b> Cuantificadores: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="24.5.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#no"><i class="fa fa-check"></i><b>24.5.8</b> No</a></li>
<li class="chapter" data-level="24.5.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#groups"><i class="fa fa-check"></i><b>24.5.9</b> Grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-con-expresiones-regulares"><i class="fa fa-check"></i><b>24.6</b> Buscar y reemplazar con expresiones regulares</a><ul>
<li class="chapter" data-level="24.6.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#buscar-y-reemplazar-usando-grupos"><i class="fa fa-check"></i><b>24.6.1</b> Buscar y reemplazar usando grupos</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#probar-y-mejorar"><i class="fa fa-check"></i><b>24.7</b> Probar y mejorar</a></li>
<li class="chapter" data-level="24.8" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#podar"><i class="fa fa-check"></i><b>24.8</b> Podar</a></li>
<li class="chapter" data-level="24.9" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#cambio-de-mayúsculas-o-minúsculas"><i class="fa fa-check"></i><b>24.9</b> Cambio de mayúsculas o minúsculas</a></li>
<li class="chapter" data-level="24.10" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-2-alturas-autoreportadas-continuación"><i class="fa fa-check"></i><b>24.10</b> Estudio de caso 2: alturas autoreportadas (continuación)</a><ul>
<li class="chapter" data-level="24.10.1" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#la-función-extract"><i class="fa fa-check"></i><b>24.10.1</b> La función <code>extract</code></a></li>
<li class="chapter" data-level="24.10.2" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#juntando-todas-la-piezas"><i class="fa fa-check"></i><b>24.10.2</b> Juntando todas la piezas</a></li>
</ul></li>
<li class="chapter" data-level="24.11" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#división-de-cadenas"><i class="fa fa-check"></i><b>24.11</b> División de cadenas</a></li>
<li class="chapter" data-level="24.12" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#estudio-de-caso-3-extracción-de-tablas-de-un-pdf"><i class="fa fa-check"></i><b>24.12</b> Estudio de caso 3: extracción de tablas de un PDF</a></li>
<li class="chapter" data-level="24.13" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#recode"><i class="fa fa-check"></i><b>24.13</b> Recodificación</a></li>
<li class="chapter" data-level="24.14" data-path="procesamiento-de-cadenas.html"><a href="procesamiento-de-cadenas.html#ejercicios-41"><i class="fa fa-check"></i><b>24.14</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html"><i class="fa fa-check"></i><b>25</b> Cómo leer y procesar fechas y horas</a><ul>
<li class="chapter" data-level="25.1" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#el-tipo-de-datos-de-fecha"><i class="fa fa-check"></i><b>25.1</b> El tipo de datos de fecha</a></li>
<li class="chapter" data-level="25.2" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#lubridate"><i class="fa fa-check"></i><b>25.2</b> El paquete lubridate</a></li>
<li class="chapter" data-level="25.3" data-path="cómo-leer-y-procesar-fechas-y-horas.html"><a href="cómo-leer-y-procesar-fechas-y-horas.html#ejercicios-42"><i class="fa fa-check"></i><b>25.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="minería-de-textos.html"><a href="minería-de-textos.html"><i class="fa fa-check"></i><b>26</b> Minería de textos</a><ul>
<li class="chapter" data-level="26.1" data-path="minería-de-textos.html"><a href="minería-de-textos.html#estudio-de-caso-tuits-de-trump"><i class="fa fa-check"></i><b>26.1</b> Estudio de caso: tuits de Trump</a></li>
<li class="chapter" data-level="26.2" data-path="minería-de-textos.html"><a href="minería-de-textos.html#texto-como-datos"><i class="fa fa-check"></i><b>26.2</b> Texto como datos</a></li>
<li class="chapter" data-level="26.3" data-path="minería-de-textos.html"><a href="minería-de-textos.html#análisis-de-sentimiento"><i class="fa fa-check"></i><b>26.3</b> Análisis de sentimiento</a></li>
<li class="chapter" data-level="26.4" data-path="minería-de-textos.html"><a href="minería-de-textos.html#ejercicios-43"><i class="fa fa-check"></i><b>26.4</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="27" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html"><i class="fa fa-check"></i><b>27</b> Introducción a <em>machine learning</em></a><ul>
<li class="chapter" data-level="27.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#notación-1"><i class="fa fa-check"></i><b>27.1</b> Notación</a></li>
<li class="chapter" data-level="27.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#un-ejemplo"><i class="fa fa-check"></i><b>27.2</b> Un ejemplo</a></li>
<li class="chapter" data-level="27.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-44"><i class="fa fa-check"></i><b>27.3</b> Ejercicios</a></li>
<li class="chapter" data-level="27.4" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#métricas-de-evaluación"><i class="fa fa-check"></i><b>27.4</b> Métricas de evaluación</a><ul>
<li class="chapter" data-level="27.4.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#sets-de-entrenamiento-y-de-evaluación"><i class="fa fa-check"></i><b>27.4.1</b> Sets de entrenamiento y de evaluación</a></li>
<li class="chapter" data-level="27.4.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#exactitud-general"><i class="fa fa-check"></i><b>27.4.2</b> Exactitud general</a></li>
<li class="chapter" data-level="27.4.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-matriz-de-confusión"><i class="fa fa-check"></i><b>27.4.3</b> La matriz de confusión</a></li>
<li class="chapter" data-level="27.4.4" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#sensibilidad-y-especificidad"><i class="fa fa-check"></i><b>27.4.4</b> Sensibilidad y especificidad</a></li>
<li class="chapter" data-level="27.4.5" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#exactitud-equilibrada-y-medida-f_1"><i class="fa fa-check"></i><b>27.4.5</b> Exactitud equilibrada y medida <span class="math inline">\(F_1\)</span></a></li>
<li class="chapter" data-level="27.4.6" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-prevalencia-importa-en-la-práctica"><i class="fa fa-check"></i><b>27.4.6</b> La prevalencia importa en la práctica</a></li>
<li class="chapter" data-level="27.4.7" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#curvas-roc-y-precision-recall"><i class="fa fa-check"></i><b>27.4.7</b> Curvas ROC y precision-recall</a></li>
<li class="chapter" data-level="27.4.8" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#loss-function"><i class="fa fa-check"></i><b>27.4.8</b> La función de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-45"><i class="fa fa-check"></i><b>27.5</b> Ejercicios</a></li>
<li class="chapter" data-level="27.6" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#probabilidades-y-expectativas-condicionales"><i class="fa fa-check"></i><b>27.6</b> Probabilidades y expectativas condicionales</a><ul>
<li class="chapter" data-level="27.6.1" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#probabilidades-condicionales-1"><i class="fa fa-check"></i><b>27.6.1</b> Probabilidades condicionales</a></li>
<li class="chapter" data-level="27.6.2" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#expectativas-condicionales"><i class="fa fa-check"></i><b>27.6.2</b> Expectativas condicionales</a></li>
<li class="chapter" data-level="27.6.3" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#la-expectativa-condicional-minimiza-la-función-de-pérdida-cuadrática"><i class="fa fa-check"></i><b>27.6.3</b> La expectativa condicional minimiza la función de pérdida cuadrática</a></li>
</ul></li>
<li class="chapter" data-level="27.7" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#ejercicios-46"><i class="fa fa-check"></i><b>27.7</b> Ejercicios</a></li>
<li class="chapter" data-level="27.8" data-path="introducción-a-machine-learning.html"><a href="introducción-a-machine-learning.html#two-or-seven"><i class="fa fa-check"></i><b>27.8</b> Estudio de caso: ¿es un 2 o un 7?</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="suavización.html"><a href="suavización.html"><i class="fa fa-check"></i><b>28</b> Suavización</a><ul>
<li class="chapter" data-level="28.1" data-path="suavización.html"><a href="suavización.html#suavización-de-compartimientos"><i class="fa fa-check"></i><b>28.1</b> Suavización de compartimientos</a></li>
<li class="chapter" data-level="28.2" data-path="suavización.html"><a href="suavización.html#kernels"><i class="fa fa-check"></i><b>28.2</b> Kernels</a></li>
<li class="chapter" data-level="28.3" data-path="suavización.html"><a href="suavización.html#regresión-ponderada-local-loess"><i class="fa fa-check"></i><b>28.3</b> Regresión ponderada local (loess)</a><ul>
<li class="chapter" data-level="28.3.1" data-path="suavización.html"><a href="suavización.html#ajustando-con-parábolas"><i class="fa fa-check"></i><b>28.3.1</b> Ajustando con parábolas</a></li>
<li class="chapter" data-level="28.3.2" data-path="suavización.html"><a href="suavización.html#cuidado-con-los-parámetros-de-suavización-predeterminados"><i class="fa fa-check"></i><b>28.3.2</b> Cuidado con los parámetros de suavización predeterminados</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="suavización.html"><a href="suavización.html#smoothing-ml-connection"><i class="fa fa-check"></i><b>28.4</b> Conectando la suavización al <em>machine learning</em></a></li>
<li class="chapter" data-level="28.5" data-path="suavización.html"><a href="suavización.html#ejercicios-47"><i class="fa fa-check"></i><b>28.5</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>29</b> Validación cruzada</a><ul>
<li class="chapter" data-level="29.1" data-path="cross-validation.html"><a href="cross-validation.html#knn-cv-intro"><i class="fa fa-check"></i><b>29.1</b> Motivación con k vecinos más cercanos</a><ul>
<li class="chapter" data-level="29.1.1" data-path="cross-validation.html"><a href="cross-validation.html#sobreentrenamiento"><i class="fa fa-check"></i><b>29.1.1</b> Sobreentrenamiento</a></li>
<li class="chapter" data-level="29.1.2" data-path="cross-validation.html"><a href="cross-validation.html#alisado-excesivo"><i class="fa fa-check"></i><b>29.1.2</b> Alisado excesivo</a></li>
<li class="chapter" data-level="29.1.3" data-path="cross-validation.html"><a href="cross-validation.html#escogiendo-el-k-en-knn"><i class="fa fa-check"></i><b>29.1.3</b> Escogiendo el <span class="math inline">\(k\)</span> en kNN</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="cross-validation.html"><a href="cross-validation.html#descripción-matemática-de-validación-cruzada"><i class="fa fa-check"></i><b>29.2</b> Descripción matemática de validación cruzada</a></li>
<li class="chapter" data-level="29.3" data-path="cross-validation.html"><a href="cross-validation.html#validación-cruzada-k-fold"><i class="fa fa-check"></i><b>29.3</b> Validación cruzada K-fold</a></li>
<li class="chapter" data-level="29.4" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-48"><i class="fa fa-check"></i><b>29.4</b> Ejercicios</a></li>
<li class="chapter" data-level="29.5" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>29.5</b> Bootstrap</a></li>
<li class="chapter" data-level="29.6" data-path="cross-validation.html"><a href="cross-validation.html#ejercicios-49"><i class="fa fa-check"></i><b>29.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>30</b> El paquete caret</a><ul>
<li class="chapter" data-level="30.1" data-path="caret.html"><a href="caret.html#la-función-train-de-caret"><i class="fa fa-check"></i><b>30.1</b> La función <code>train</code> de caret</a></li>
<li class="chapter" data-level="30.2" data-path="caret.html"><a href="caret.html#caret-cv"><i class="fa fa-check"></i><b>30.2</b> Validación cruzada</a></li>
<li class="chapter" data-level="30.3" data-path="caret.html"><a href="caret.html#ejemplo-ajuste-con-loess"><i class="fa fa-check"></i><b>30.3</b> Ejemplo: ajuste con loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html"><i class="fa fa-check"></i><b>31</b> Ejemplos de algoritmos</a><ul>
<li class="chapter" data-level="31.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-lineal"><i class="fa fa-check"></i><b>31.1</b> Regresión lineal</a><ul>
<li class="chapter" data-level="31.1.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#la-función-predict"><i class="fa fa-check"></i><b>31.1.1</b> La función <code>predict</code></a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-50"><i class="fa fa-check"></i><b>31.2</b> Ejercicios</a></li>
<li class="chapter" data-level="31.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística"><i class="fa fa-check"></i><b>31.3</b> Regresión logística</a><ul>
<li class="chapter" data-level="31.3.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-lineales-generalizados"><i class="fa fa-check"></i><b>31.3.1</b> Modelos lineales generalizados</a></li>
<li class="chapter" data-level="31.3.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#regresión-logística-con-más-de-un-predictor"><i class="fa fa-check"></i><b>31.3.2</b> Regresión logística con más de un predictor</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-51"><i class="fa fa-check"></i><b>31.4</b> Ejercicios</a></li>
<li class="chapter" data-level="31.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>31.5</b> k-nearest neighbors</a></li>
<li class="chapter" data-level="31.6" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-52"><i class="fa fa-check"></i><b>31.6</b> Ejercicios</a></li>
<li class="chapter" data-level="31.7" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#modelos-generativos"><i class="fa fa-check"></i><b>31.7</b> Modelos generativos</a><ul>
<li class="chapter" data-level="31.7.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#naive-bayes"><i class="fa fa-check"></i><b>31.7.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.7.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#controlando-la-prevalencia"><i class="fa fa-check"></i><b>31.7.2</b> Controlando la prevalencia</a></li>
<li class="chapter" data-level="31.7.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-cuadrático"><i class="fa fa-check"></i><b>31.7.3</b> Análisis discriminante cuadrático</a></li>
<li class="chapter" data-level="31.7.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#análisis-discriminante-lineal"><i class="fa fa-check"></i><b>31.7.4</b> Análisis discriminante lineal</a></li>
<li class="chapter" data-level="31.7.5" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#conexión-a-distancia"><i class="fa fa-check"></i><b>31.7.5</b> Conexión a distancia</a></li>
</ul></li>
<li class="chapter" data-level="31.8" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#estudio-de-caso-más-de-tres-clases"><i class="fa fa-check"></i><b>31.8</b> Estudio de caso: más de tres clases</a></li>
<li class="chapter" data-level="31.9" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-53"><i class="fa fa-check"></i><b>31.9</b> Ejercicios</a></li>
<li class="chapter" data-level="31.10" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-y-regresión-cart"><i class="fa fa-check"></i><b>31.10</b> Árboles de clasificación y regresión (CART)</a><ul>
<li class="chapter" data-level="31.10.1" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#la-maldición-de-la-dimensionalidad"><i class="fa fa-check"></i><b>31.10.1</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="31.10.2" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#carrera-motivación"><i class="fa fa-check"></i><b>31.10.2</b> CARRERA motivación</a></li>
<li class="chapter" data-level="31.10.3" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-regresión"><i class="fa fa-check"></i><b>31.10.3</b> Árboles de regresión</a></li>
<li class="chapter" data-level="31.10.4" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#árboles-de-clasificación-decisión"><i class="fa fa-check"></i><b>31.10.4</b> Árboles de clasificación (decisión)</a></li>
</ul></li>
<li class="chapter" data-level="31.11" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#bosques-aleatiruis"><i class="fa fa-check"></i><b>31.11</b> Bosques aleatiruis</a></li>
<li class="chapter" data-level="31.12" data-path="ejemplos-de-algoritmos.html"><a href="ejemplos-de-algoritmos.html#ejercicios-54"><i class="fa fa-check"></i><b>31.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html"><i class="fa fa-check"></i><b>32</b> Machine learning en la práctica</a><ul>
<li class="chapter" data-level="32.1" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#preprocesamiento"><i class="fa fa-check"></i><b>32.1</b> Preprocesamiento</a></li>
<li class="chapter" data-level="32.2" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#k-vecino-más-cercano-y-bosque-aleatorio"><i class="fa fa-check"></i><b>32.2</b> k-vecino más cercano y bosque aleatorio</a></li>
<li class="chapter" data-level="32.3" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#importancia-variable"><i class="fa fa-check"></i><b>32.3</b> Importancia variable</a></li>
<li class="chapter" data-level="32.4" data-path="machine-learning-en-la-práctica.html"><a href="machine-learning-en-la-práctica.html#evaluaciones-visuales"><i class="fa fa-check"></i><b>32.4</b> Evaluaciones visuales</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html"><i class="fa fa-check"></i><b>33</b> Grandes conjuntos de datos</a><ul>
<li class="chapter" data-level="33.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#matrix-algebra"><i class="fa fa-check"></i><b>33.1</b> Álgebra matricial</a><ul>
<li class="chapter" data-level="33.1.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#notación-2"><i class="fa fa-check"></i><b>33.1.1</b> Notación</a></li>
<li class="chapter" data-level="33.1.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#convertir-un-vector-en-una-matriz"><i class="fa fa-check"></i><b>33.1.2</b> Convertir un vector en una matriz</a></li>
<li class="chapter" data-level="33.1.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#resúmenes-de-filas-y-columnas"><i class="fa fa-check"></i><b>33.1.3</b> Resúmenes de filas y columnas</a></li>
<li class="chapter" data-level="33.1.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#apply"><i class="fa fa-check"></i><b>33.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="33.1.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#filtrar-columnas-basadas-en-resúmenes"><i class="fa fa-check"></i><b>33.1.5</b> Filtrar columnas basadas en resúmenes</a></li>
<li class="chapter" data-level="33.1.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#indexación-con-matrices"><i class="fa fa-check"></i><b>33.1.6</b> Indexación con matrices</a></li>
<li class="chapter" data-level="33.1.7" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#binarizar-los-datos"><i class="fa fa-check"></i><b>33.1.7</b> Binarizar los datos</a></li>
<li class="chapter" data-level="33.1.8" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#vectorización-para-matrices"><i class="fa fa-check"></i><b>33.1.8</b> Vectorización para matrices</a></li>
<li class="chapter" data-level="33.1.9" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#operaciones-de-álgebra-matricial"><i class="fa fa-check"></i><b>33.1.9</b> Operaciones de álgebra matricial</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-55"><i class="fa fa-check"></i><b>33.2</b> Ejercicios</a></li>
<li class="chapter" data-level="33.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia"><i class="fa fa-check"></i><b>33.3</b> Distancia</a><ul>
<li class="chapter" data-level="33.3.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.1</b> Distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia-en-dimensiones-superiores"><i class="fa fa-check"></i><b>33.3.2</b> Distancia en dimensiones superiores</a></li>
<li class="chapter" data-level="33.3.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejemplo-de-distancia-euclidiana"><i class="fa fa-check"></i><b>33.3.3</b> Ejemplo de distancia euclidiana</a></li>
<li class="chapter" data-level="33.3.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#predictor-space"><i class="fa fa-check"></i><b>33.3.4</b> Espacio predictor</a></li>
<li class="chapter" data-level="33.3.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#distancia-entre-predictores"><i class="fa fa-check"></i><b>33.3.5</b> Distancia entre predictores</a></li>
</ul></li>
<li class="chapter" data-level="33.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-56"><i class="fa fa-check"></i><b>33.4</b> Ejercicios</a></li>
<li class="chapter" data-level="33.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#reducción-de-dimensiones"><i class="fa fa-check"></i><b>33.5</b> Reducción de dimensiones</a><ul>
<li class="chapter" data-level="33.5.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#preservando-la-distancia"><i class="fa fa-check"></i><b>33.5.1</b> Preservando la distancia</a></li>
<li class="chapter" data-level="33.5.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#transformaciones-lineales-avanzado"><i class="fa fa-check"></i><b>33.5.2</b> Transformaciones lineales (avanzado)</a></li>
<li class="chapter" data-level="33.5.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#transformaciones-ortogonales-avanzado"><i class="fa fa-check"></i><b>33.5.3</b> Transformaciones ortogonales (avanzado)</a></li>
<li class="chapter" data-level="33.5.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#análisis-de-componentes-principales"><i class="fa fa-check"></i><b>33.5.4</b> Análisis de componentes principales</a></li>
<li class="chapter" data-level="33.5.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejemplo-de-iris"><i class="fa fa-check"></i><b>33.5.5</b> Ejemplo de Iris</a></li>
<li class="chapter" data-level="33.5.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejemplo-de-mnist"><i class="fa fa-check"></i><b>33.5.6</b> Ejemplo de MNIST</a></li>
</ul></li>
<li class="chapter" data-level="33.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-57"><i class="fa fa-check"></i><b>33.6</b> Ejercicios</a></li>
<li class="chapter" data-level="33.7" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#sistemas-de-recomendación"><i class="fa fa-check"></i><b>33.7</b> Sistemas de recomendación</a><ul>
<li class="chapter" data-level="33.7.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#datos-de-lente-de-película"><i class="fa fa-check"></i><b>33.7.1</b> Datos de lente de película</a></li>
<li class="chapter" data-level="33.7.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#sistemas-de-recomendación-como-un-desafío-de-aprendizaje-automático"><i class="fa fa-check"></i><b>33.7.2</b> Sistemas de recomendación como un desafío de aprendizaje automático</a></li>
<li class="chapter" data-level="33.7.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#netflix-loss-function"><i class="fa fa-check"></i><b>33.7.3</b> Función de pérdida</a></li>
<li class="chapter" data-level="33.7.4" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#un-primer-modelo"><i class="fa fa-check"></i><b>33.7.4</b> Un primer modelo</a></li>
<li class="chapter" data-level="33.7.5" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#modelado-de-efectos-de-películas"><i class="fa fa-check"></i><b>33.7.5</b> Modelado de efectos de películas</a></li>
<li class="chapter" data-level="33.7.6" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#efectos-de-usuario"><i class="fa fa-check"></i><b>33.7.6</b> Efectos de usuario</a></li>
</ul></li>
<li class="chapter" data-level="33.8" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-58"><i class="fa fa-check"></i><b>33.8</b> Ejercicios</a></li>
<li class="chapter" data-level="33.9" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#regularización"><i class="fa fa-check"></i><b>33.9</b> Regularización</a><ul>
<li class="chapter" data-level="33.9.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#motivación"><i class="fa fa-check"></i><b>33.9.1</b> Motivación</a></li>
<li class="chapter" data-level="33.9.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#mínimos-cuadrados-penalizados"><i class="fa fa-check"></i><b>33.9.2</b> Mínimos cuadrados penalizados</a></li>
<li class="chapter" data-level="33.9.3" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#elegir-los-términos-de-penalización"><i class="fa fa-check"></i><b>33.9.3</b> Elegir los términos de penalización</a></li>
</ul></li>
<li class="chapter" data-level="33.10" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-59"><i class="fa fa-check"></i><b>33.10</b> Ejercicios</a></li>
<li class="chapter" data-level="33.11" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#factorización-matricial"><i class="fa fa-check"></i><b>33.11</b> Factorización matricial</a><ul>
<li class="chapter" data-level="33.11.1" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#análisis-de-factores"><i class="fa fa-check"></i><b>33.11.1</b> Análisis de factores</a></li>
<li class="chapter" data-level="33.11.2" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#conexión-a-svd-y-pca"><i class="fa fa-check"></i><b>33.11.2</b> Conexión a SVD y PCA</a></li>
</ul></li>
<li class="chapter" data-level="33.12" data-path="grandes-conjuntos-de-datos.html"><a href="grandes-conjuntos-de-datos.html#ejercicios-60"><i class="fa fa-check"></i><b>33.12</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>34</b> Agrupación</a><ul>
<li class="chapter" data-level="34.1" data-path="clustering.html"><a href="clustering.html#agrupación-jerárquica"><i class="fa fa-check"></i><b>34.1</b> Agrupación jerárquica</a></li>
<li class="chapter" data-level="34.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>34.2</b> k-means</a></li>
<li class="chapter" data-level="34.3" data-path="clustering.html"><a href="clustering.html#mapas-de-calor"><i class="fa fa-check"></i><b>34.3</b> Mapas de calor</a></li>
<li class="chapter" data-level="34.4" data-path="clustering.html"><a href="clustering.html#características-de-filtrado"><i class="fa fa-check"></i><b>34.4</b> Características de filtrado</a></li>
<li class="chapter" data-level="34.5" data-path="clustering.html"><a href="clustering.html#ejercicios-61"><i class="fa fa-check"></i><b>34.5</b> Ejercicios</a></li>
</ul></li>
<li class="part"><span><b>VI Herramientas de productividad</b></span></li>
<li class="chapter" data-level="35" data-path="introducción-a-las-herramientas-de-productividad.html"><a href="introducción-a-las-herramientas-de-productividad.html"><i class="fa fa-check"></i><b>35</b> Introducción a las herramientas de productividad</a></li>
<li class="chapter" data-level="36" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>36</b> Instalación de R y RStudio</a><ul>
<li class="chapter" data-level="36.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#instalando-r"><i class="fa fa-check"></i><b>36.1</b> Instalando R</a></li>
<li class="chapter" data-level="36.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#instalación-de-rstudio"><i class="fa fa-check"></i><b>36.2</b> Instalación de RStudio</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html"><i class="fa fa-check"></i><b>37</b> Accediendo al terminal e instalando Git</a><ul>
<li class="chapter" data-level="37.1" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#terminal-on-mac"><i class="fa fa-check"></i><b>37.1</b> Accediendo al terminal en una Mac</a></li>
<li class="chapter" data-level="37.2" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#instalando-git-en-la-mac"><i class="fa fa-check"></i><b>37.2</b> Instalando Git en la Mac</a></li>
<li class="chapter" data-level="37.3" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#instalación-de-git-y-git-bash-en-windows"><i class="fa fa-check"></i><b>37.3</b> Instalación de Git y Git Bash en Windows</a></li>
<li class="chapter" data-level="37.4" data-path="accediendo-al-terminal-e-instalando-git.html"><a href="accediendo-al-terminal-e-instalando-git.html#terminal-on-windows"><i class="fa fa-check"></i><b>37.4</b> Accediendo a la terminal en Windows</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>38</b> Organizando con Unix</a><ul>
<li class="chapter" data-level="38.1" data-path="unix.html"><a href="unix.html#convenio-de-denominación"><i class="fa fa-check"></i><b>38.1</b> Convenio de denominación</a></li>
<li class="chapter" data-level="38.2" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>38.2</b> La terminal</a></li>
<li class="chapter" data-level="38.3" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>38.3</b> El sistema de archivos</a><ul>
<li class="chapter" data-level="38.3.1" data-path="unix.html"><a href="unix.html#directorios-y-subdirectorios"><i class="fa fa-check"></i><b>38.3.1</b> Directorios y subdirectorios</a></li>
<li class="chapter" data-level="38.3.2" data-path="unix.html"><a href="unix.html#el-directorio-de-inicio"><i class="fa fa-check"></i><b>38.3.2</b> El directorio de inicio</a></li>
<li class="chapter" data-level="38.3.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>38.3.3</b> Directorio de trabajo</a></li>
<li class="chapter" data-level="38.3.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>38.3.4</b> Rutas</a></li>
</ul></li>
<li class="chapter" data-level="38.4" data-path="unix.html"><a href="unix.html#comandos-de-unix"><i class="fa fa-check"></i><b>38.4</b> Comandos de Unix</a><ul>
<li class="chapter" data-level="38.4.1" data-path="unix.html"><a href="unix.html#ls-listado-de-contenido-del-directorio"><i class="fa fa-check"></i><b>38.4.1</b> <code>ls</code>: Listado de contenido del directorio</a></li>
<li class="chapter" data-level="38.4.2" data-path="unix.html"><a href="unix.html#mkdir-y-rmdir-crear-y-eliminar-un-directorio"><i class="fa fa-check"></i><b>38.4.2</b> <code>mkdir</code> y <code>rmdir</code>: crear y eliminar un directorio</a></li>
<li class="chapter" data-level="38.4.3" data-path="unix.html"><a href="unix.html#cd-navegando-por-el-sistema-de-archivos-cambiando-directorios"><i class="fa fa-check"></i><b>38.4.3</b> <code>cd</code>: navegando por el sistema de archivos cambiando directorios</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="unix.html"><a href="unix.html#algunos-ejemplos"><i class="fa fa-check"></i><b>38.5</b> Algunos ejemplos</a></li>
<li class="chapter" data-level="38.6" data-path="unix.html"><a href="unix.html#más-comandos-de-unix"><i class="fa fa-check"></i><b>38.6</b> Más comandos de Unix</a><ul>
<li class="chapter" data-level="38.6.1" data-path="unix.html"><a href="unix.html#mv-mover-archivos"><i class="fa fa-check"></i><b>38.6.1</b> <code>mv</code>: mover archivos</a></li>
<li class="chapter" data-level="38.6.2" data-path="unix.html"><a href="unix.html#cp-copiando-documentos"><i class="fa fa-check"></i><b>38.6.2</b> <code>cp</code>: copiando documentos</a></li>
<li class="chapter" data-level="38.6.3" data-path="unix.html"><a href="unix.html#rm-eliminar-archivos"><i class="fa fa-check"></i><b>38.6.3</b> <code>rm</code>: eliminar archivos</a></li>
<li class="chapter" data-level="38.6.4" data-path="unix.html"><a href="unix.html#less-mirando-un-archivo"><i class="fa fa-check"></i><b>38.6.4</b> <code>less</code>: mirando un archivo</a></li>
</ul></li>
<li class="chapter" data-level="38.7" data-path="unix.html"><a href="unix.html#prep-project"><i class="fa fa-check"></i><b>38.7</b> Preparación para un proyecto de ciencia de datos</a></li>
<li class="chapter" data-level="38.8" data-path="unix.html"><a href="unix.html#unix-avanzado"><i class="fa fa-check"></i><b>38.8</b> Unix avanzado</a><ul>
<li class="chapter" data-level="38.8.1" data-path="unix.html"><a href="unix.html#argumentos"><i class="fa fa-check"></i><b>38.8.1</b> Argumentos</a></li>
<li class="chapter" data-level="38.8.2" data-path="unix.html"><a href="unix.html#obteniendo-ayuda"><i class="fa fa-check"></i><b>38.8.2</b> Obteniendo ayuda</a></li>
<li class="chapter" data-level="38.8.3" data-path="unix.html"><a href="unix.html#tuberías"><i class="fa fa-check"></i><b>38.8.3</b> Tuberías</a></li>
<li class="chapter" data-level="38.8.4" data-path="unix.html"><a href="unix.html#comodines"><i class="fa fa-check"></i><b>38.8.4</b> Comodines</a></li>
<li class="chapter" data-level="38.8.5" data-path="unix.html"><a href="unix.html#variables-de-entorno"><i class="fa fa-check"></i><b>38.8.5</b> Variables de entorno</a></li>
<li class="chapter" data-level="38.8.6" data-path="unix.html"><a href="unix.html#conchas"><i class="fa fa-check"></i><b>38.8.6</b> Conchas</a></li>
<li class="chapter" data-level="38.8.7" data-path="unix.html"><a href="unix.html#ejecutables"><i class="fa fa-check"></i><b>38.8.7</b> Ejecutables</a></li>
<li class="chapter" data-level="38.8.8" data-path="unix.html"><a href="unix.html#permisos-y-tipos-de-archivo"><i class="fa fa-check"></i><b>38.8.8</b> Permisos y tipos de archivo</a></li>
<li class="chapter" data-level="38.8.9" data-path="unix.html"><a href="unix.html#comandos-que-debes-aprender"><i class="fa fa-check"></i><b>38.8.9</b> Comandos que debes aprender</a></li>
<li class="chapter" data-level="38.8.10" data-path="unix.html"><a href="unix.html#manipulación-de-archivos-en-r"><i class="fa fa-check"></i><b>38.8.10</b> Manipulación de archivos en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>39</b> Git y GitHub</a><ul>
<li class="chapter" data-level="39.1" data-path="git.html"><a href="git.html#por-qué-usar-git-y-github"><i class="fa fa-check"></i><b>39.1</b> ¿Por qué usar Git y GitHub?</a></li>
<li class="chapter" data-level="39.2" data-path="git.html"><a href="git.html#cuentas-github"><i class="fa fa-check"></i><b>39.2</b> Cuentas GitHub</a></li>
<li class="chapter" data-level="39.3" data-path="git.html"><a href="git.html#github-repos"><i class="fa fa-check"></i><b>39.3</b> Repositorios de GitHub</a></li>
<li class="chapter" data-level="39.4" data-path="git.html"><a href="git.html#git-overview"><i class="fa fa-check"></i><b>39.4</b> Descripción general de Git</a><ul>
<li class="chapter" data-level="39.4.1" data-path="git.html"><a href="git.html#clonar"><i class="fa fa-check"></i><b>39.4.1</b> Clonar</a></li>
</ul></li>
<li class="chapter" data-level="39.5" data-path="git.html"><a href="git.html#init"><i class="fa fa-check"></i><b>39.5</b> Inicializando un directorio Git</a></li>
<li class="chapter" data-level="39.6" data-path="git.html"><a href="git.html#rstudio-git"><i class="fa fa-check"></i><b>39.6</b> Usando Git y GitHub en RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><i class="fa fa-check"></i><b>40</b> Proyectos reproducibles con RStudio y R markdown</a><ul>
<li class="chapter" data-level="40.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#proyectos-de-rstudio"><i class="fa fa-check"></i><b>40.1</b> Proyectos de RStudio</a></li>
<li class="chapter" data-level="40.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#r-descuento"><i class="fa fa-check"></i><b>40.2</b> R descuento</a><ul>
<li class="chapter" data-level="40.2.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#el-encabezado"><i class="fa fa-check"></i><b>40.2.1</b> El encabezado</a></li>
<li class="chapter" data-level="40.2.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#trozos-de-código-r"><i class="fa fa-check"></i><b>40.2.2</b> Trozos de código R</a></li>
<li class="chapter" data-level="40.2.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#opciones-globales"><i class="fa fa-check"></i><b>40.2.3</b> Opciones globales</a></li>
<li class="chapter" data-level="40.2.4" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#knitr"><i class="fa fa-check"></i><b>40.2.4</b> knitR</a></li>
<li class="chapter" data-level="40.2.5" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#más-sobre-markdown-r"><i class="fa fa-check"></i><b>40.2.5</b> Más sobre Markdown R</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#organizing"><i class="fa fa-check"></i><b>40.3</b> Organizando un proyecto de ciencia de datos</a><ul>
<li class="chapter" data-level="40.3.1" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#crear-directorios-en-unix"><i class="fa fa-check"></i><b>40.3.1</b> Crear directorios en Unix</a></li>
<li class="chapter" data-level="40.3.2" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#crear-un-proyecto-rstudio"><i class="fa fa-check"></i><b>40.3.2</b> Crear un proyecto RStudio</a></li>
<li class="chapter" data-level="40.3.3" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#editar-algunos-scripts-r"><i class="fa fa-check"></i><b>40.3.3</b> Editar algunos scripts R</a></li>
<li class="chapter" data-level="40.3.4" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#cree-más-directorios-usando-unix"><i class="fa fa-check"></i><b>40.3.4</b> Cree más directorios usando Unix</a></li>
<li class="chapter" data-level="40.3.5" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#agregar-un-archivo-readme"><i class="fa fa-check"></i><b>40.3.5</b> Agregar un archivo README</a></li>
<li class="chapter" data-level="40.3.6" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#inicializando-un-directorio-git"><i class="fa fa-check"></i><b>40.3.6</b> Inicializando un directorio Git</a></li>
<li class="chapter" data-level="40.3.7" data-path="proyectos-reproducibles-con-rstudio-y-r-markdown.html"><a href="proyectos-reproducibles-con-rstudio-y-r-markdown.html#agregue-confirme-y-envíe-archivos-con-rstudio"><i class="fa fa-check"></i><b>40.3.7</b> Agregue, confirme y envíe archivos con RStudio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción a la Ciencia de Datos</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="grandes-conjuntos-de-datos" class="section level1">
<h1><span class="header-section-number">Capítulo 33</span> Grandes conjuntos de datos</h1>
<p>Los problemas de aprendizaje automático a menudo implican conjuntos de datos que son tan grandes o más grandes que el conjunto de datos MNIST. Existe una variedad de técnicas computacionales y conceptos estadísticos que son útiles para el análisis de grandes conjuntos de datos. En este capítulo, rascamos la superficie de estas técnicas y conceptos describiendo álgebra matricial, reducción de dimensiones, regularización y factorización matricial. Utilizamos sistemas de recomendación relacionados con las clasificaciones de películas como un ejemplo motivador.</p>
<div id="matrix-algebra" class="section level2">
<h2><span class="header-section-number">33.1</span> Álgebra matricial</h2>
<p>En el aprendizaje automático, las situaciones en las que todos los predictores son numéricos, o pueden convertirse a numéricos de manera significativa, son comunes. El conjunto de datos de dígitos es un ejemplo: cada píxel registra un número entre 0 y 255. Carguemos los datos:</p>
<div class="sourceCode" id="cb1206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1206-1"><a href="grandes-conjuntos-de-datos.html#cb1206-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1206-2"><a href="grandes-conjuntos-de-datos.html#cb1206-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1206-3"><a href="grandes-conjuntos-de-datos.html#cb1206-3"></a><span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()</span></code></pre></div>
<p>En estos casos, a menudo es conveniente guardar los predictores en una matriz y el resultado en un vector en lugar de utilizar un marco de datos. Puede ver que los predictores se guardan como una matriz:</p>
<div class="sourceCode" id="cb1207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1207-1"><a href="grandes-conjuntos-de-datos.html#cb1207-1"></a><span class="kw">class</span>(mnist<span class="op">$</span>train<span class="op">$</span>images)</span>
<span id="cb1207-2"><a href="grandes-conjuntos-de-datos.html#cb1207-2"></a><span class="co">#&gt; [1] &quot;matrix&quot; &quot;array&quot;</span></span></code></pre></div>
<p>Esta matriz representa 60,000 dígitos, por lo que para los ejemplos en este capítulo, tomaremos un subconjunto más manejable. Tomaremos los primeros 1,000 predictores <code>x</code> y etiquetas <code>y</code>:</p>
<div class="sourceCode" id="cb1208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1208-1"><a href="grandes-conjuntos-de-datos.html#cb1208-1"></a>x &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>images[<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>,]</span>
<span id="cb1208-2"><a href="grandes-conjuntos-de-datos.html#cb1208-2"></a>y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels[<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>]</span></code></pre></div>
<p>La razón principal para usar matrices es que ciertas operaciones matemáticas necesarias para desarrollar código eficiente se pueden realizar usando técnicas de una rama de las matemáticas llamada álgebra lineal. De hecho, el álgebra lineal y la notación matricial son elementos clave del lenguaje utilizado en trabajos académicos que describen técnicas de aprendizaje automático. No cubriremos el álgebra lineal en detalle aquí, pero demostraremos cómo usar matrices en R para que pueda aplicar las técnicas de álgebra lineal ya implementadas en la base R u otros paquetes.</p>
<p>Para motivar el uso de matrices, plantearemos cinco preguntas/ desafíos:</p>
<p>1. ¿Algunos dígitos requieren más tinta que otros? Estudie la distribución de la oscuridad total de píxeles y cómo varía según los dígitos.</p>
<p>2. ¿Algunos píxeles no son informativos? Estudie la variación de cada píxel y elimine los predictores (columnas) asociados con los píxeles que no cambian mucho y, por lo tanto, no pueden proporcionar mucha información para la clasificación.</p>
<p>3. ¿Podemos eliminar las manchas? Primero, observe la distribución de todos los valores de píxeles. Use esto para elegir un límite para definir el espacio no escrito. Luego, establezca cualquier cosa por debajo de ese límite en 0.</p>
<p>4. Binarizar los datos. Primero, observe la distribución de todos los valores de píxeles. Use esto para elegir un límite para distinguir entre escribir y no escribir. Luego, convierta todas las entradas en 1 o 0, respectivamente.</p>
<p>5. Escale cada uno de los predictores en cada entrada para tener el mismo promedio y desviación estándar.</p>
<p>Para completar esto, tendremos que realizar operaciones matemáticas que involucren varias variables. El <strong>tidyverse</strong> no está desarrollado para realizar este tipo de operaciones matemáticas. Para esta tarea, es conveniente usar matrices.</p>
<p>Antes de hacer esto, introduciremos la notación matricial y el código R básico para definir y operar en matrices.</p>
<div id="notación-2" class="section level3">
<h3><span class="header-section-number">33.1.1</span> Notación</h3>
<p>En álgebra matricial, tenemos tres tipos principales de objetos: escalares, vectores y matrices. Un escalar es solo un número, por ejemplo <span class="math inline">\(a = 1\)</span>. Para denotar escalares en notación matricial, generalmente usamos una letra minúscula y no en negrita.</p>
<p>Los vectores son como los vectores numéricos que definimos en R: incluyen varias entradas escalares. Por ejemplo, la columna que contiene el primer píxel:</p>
<div class="sourceCode" id="cb1209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1209-1"><a href="grandes-conjuntos-de-datos.html#cb1209-1"></a><span class="kw">length</span>(x[,<span class="dv">1</span>])</span>
<span id="cb1209-2"><a href="grandes-conjuntos-de-datos.html#cb1209-2"></a><span class="co">#&gt; [1] 1000</span></span></code></pre></div>
<p>tiene 1,000 entradas. En álgebra matricial, utilizamos la siguiente notación para un vector que representa una característica/ predictor:</p>
<p><span class="math display">\[
\begin{pmatrix}
x_1\\\
x_2\\\
\vdots\\\
x_N
\end{pmatrix}
\]</span></p>
<p>Del mismo modo, podemos usar la notación matemática para representar diferentes características
matemáticamente agregando un índice:</p>
<p><span class="math display">\[
\mathbf{X}_1 = \begin{pmatrix}
x_{1,1}\\
\vdots\\
x_{N,1}
\end{pmatrix} \mbox{ and }
\mathbf{X}_2 = \begin{pmatrix}
x_{1,2}\\
\vdots\\
x_{N,2}
\end{pmatrix}
\]</span></p>
<p>Si estamos escribiendo una columna, como <span class="math inline">\(\mathbf{X}_1\)</span>, en una oración a menudo usamos la notación: <span class="math inline">\(\mathbf{X}_1 = ( x_{1,1}, \dots x_{N,1})^\top\)</span> con <span class="math inline">\(^\top\)</span> la operación de transposición que convierte las columnas en filas y las filas en columnas.</p>
<p>Una matriz se puede definir como una serie de vectores del mismo tamaño unidos como columnas:</p>
<div class="sourceCode" id="cb1210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1210-1"><a href="grandes-conjuntos-de-datos.html#cb1210-1"></a>x_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span></span>
<span id="cb1210-2"><a href="grandes-conjuntos-de-datos.html#cb1210-2"></a>x_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="dv">6</span><span class="op">:</span><span class="dv">10</span></span>
<span id="cb1210-3"><a href="grandes-conjuntos-de-datos.html#cb1210-3"></a><span class="kw">cbind</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>)</span>
<span id="cb1210-4"><a href="grandes-conjuntos-de-datos.html#cb1210-4"></a><span class="co">#&gt;      x_1 x_2</span></span>
<span id="cb1210-5"><a href="grandes-conjuntos-de-datos.html#cb1210-5"></a><span class="co">#&gt; [1,]   1   6</span></span>
<span id="cb1210-6"><a href="grandes-conjuntos-de-datos.html#cb1210-6"></a><span class="co">#&gt; [2,]   2   7</span></span>
<span id="cb1210-7"><a href="grandes-conjuntos-de-datos.html#cb1210-7"></a><span class="co">#&gt; [3,]   3   8</span></span>
<span id="cb1210-8"><a href="grandes-conjuntos-de-datos.html#cb1210-8"></a><span class="co">#&gt; [4,]   4   9</span></span>
<span id="cb1210-9"><a href="grandes-conjuntos-de-datos.html#cb1210-9"></a><span class="co">#&gt; [5,]   5  10</span></span></code></pre></div>
<p>Matemáticamente, los representamos con letras mayúsculas en negrita:</p>
<p><span class="math display">\[
\mathbf{X} = [ \mathbf{X}_1 \mathbf{X}_2 ] = \begin{pmatrix}
x_{1,1}&amp;x_{1,2}\\
\vdots\\
x_{N,1}&amp;x_{N,2}
\end{pmatrix}
\]</span></p>
<p>La dimensión de una matriz a menudo es una característica importante necesaria para asegurar que se puedan realizar ciertas operaciones. La dimensión es un resumen de dos números definido como el número de filas. <span class="math inline">\(\times\)</span> el número de columnas. En R, podemos extraer la dimensión de una matriz con la función <code>dim</code>:</p>
<div class="sourceCode" id="cb1211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1211-1"><a href="grandes-conjuntos-de-datos.html#cb1211-1"></a><span class="kw">dim</span>(x)</span>
<span id="cb1211-2"><a href="grandes-conjuntos-de-datos.html#cb1211-2"></a><span class="co">#&gt; [1] 1000  784</span></span></code></pre></div>
<p>Los vectores pueden considerarse <span class="math inline">\(N\times 1\)</span> matrices Sin embargo, en R, un vector no tiene dimensiones:</p>
<div class="sourceCode" id="cb1212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1212-1"><a href="grandes-conjuntos-de-datos.html#cb1212-1"></a><span class="kw">dim</span>(x_<span class="dv">1</span>)</span>
<span id="cb1212-2"><a href="grandes-conjuntos-de-datos.html#cb1212-2"></a><span class="co">#&gt; NULL</span></span></code></pre></div>
<p>Sin embargo, explícitamente convertimos un vector en una matriz usando la función <code>as.matrix</code>:</p>
<div class="sourceCode" id="cb1213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1213-1"><a href="grandes-conjuntos-de-datos.html#cb1213-1"></a><span class="kw">dim</span>(<span class="kw">as.matrix</span>(x_<span class="dv">1</span>))</span>
<span id="cb1213-2"><a href="grandes-conjuntos-de-datos.html#cb1213-2"></a><span class="co">#&gt; [1] 5 1</span></span></code></pre></div>
<p>Podemos usar esta notación para denotar un número arbitrario de predictores con lo siguiente <span class="math inline">\(N\times p\)</span> matriz, por ejemplo, con <span class="math inline">\(p=784\)</span>:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix}
x_{1,1}&amp;\dots &amp; x_{1,p} \\
x_{2,1}&amp;\dots &amp; x_{2,p} \\
&amp; \vdots &amp; \\
x_{N,1}&amp;\dots &amp; x_{N,p}
\end{pmatrix}
\]</span></p>
<p>Almacenamos esta matriz en x:</p>
<div class="sourceCode" id="cb1214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1214-1"><a href="grandes-conjuntos-de-datos.html#cb1214-1"></a><span class="kw">dim</span>(x)</span>
<span id="cb1214-2"><a href="grandes-conjuntos-de-datos.html#cb1214-2"></a><span class="co">#&gt; [1] 1000  784</span></span></code></pre></div>
<p>Ahora aprenderemos varias operaciones útiles relacionadas con el álgebra matricial. Utilizamos tres de las preguntas motivadoras mencionadas anteriormente.</p>
</div>
<div id="convertir-un-vector-en-una-matriz" class="section level3">
<h3><span class="header-section-number">33.1.2</span> Convertir un vector en una matriz</h3>
<p>A menudo es útil convertir un vector en una matriz. Por ejemplo, debido a que las variables son píxeles en una cuadrícula, podemos convertir las filas de intensidades de píxeles en una matriz que representa esta cuadrícula.</p>
<p>Podemos convertir un vector en una matriz con el <code>matrix</code> función y especificando el número de filas y columnas que debe tener la matriz resultante. La matriz se llena <strong>por columna</strong>: la primera columna se llena primero, luego la segunda y así sucesivamente. Este ejemplo ayuda a ilustrar:</p>
<div class="sourceCode" id="cb1215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1215-1"><a href="grandes-conjuntos-de-datos.html#cb1215-1"></a>my_vector &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">15</span></span>
<span id="cb1215-2"><a href="grandes-conjuntos-de-datos.html#cb1215-2"></a>mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(my_vector, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb1215-3"><a href="grandes-conjuntos-de-datos.html#cb1215-3"></a>mat</span>
<span id="cb1215-4"><a href="grandes-conjuntos-de-datos.html#cb1215-4"></a><span class="co">#&gt;      [,1] [,2] [,3]</span></span>
<span id="cb1215-5"><a href="grandes-conjuntos-de-datos.html#cb1215-5"></a><span class="co">#&gt; [1,]    1    6   11</span></span>
<span id="cb1215-6"><a href="grandes-conjuntos-de-datos.html#cb1215-6"></a><span class="co">#&gt; [2,]    2    7   12</span></span>
<span id="cb1215-7"><a href="grandes-conjuntos-de-datos.html#cb1215-7"></a><span class="co">#&gt; [3,]    3    8   13</span></span>
<span id="cb1215-8"><a href="grandes-conjuntos-de-datos.html#cb1215-8"></a><span class="co">#&gt; [4,]    4    9   14</span></span>
<span id="cb1215-9"><a href="grandes-conjuntos-de-datos.html#cb1215-9"></a><span class="co">#&gt; [5,]    5   10   15</span></span></code></pre></div>
<p>Podemos llenar por fila usando el <code>byrow</code> argumento. Entonces, por ejemplo, para <em>transponer</em> la matriz <code>mat</code>, nosotros podemos usar:</p>
<div class="sourceCode" id="cb1216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1216-1"><a href="grandes-conjuntos-de-datos.html#cb1216-1"></a>mat_t &lt;-<span class="st"> </span><span class="kw">matrix</span>(my_vector, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</span>
<span id="cb1216-2"><a href="grandes-conjuntos-de-datos.html#cb1216-2"></a>mat_t</span>
<span id="cb1216-3"><a href="grandes-conjuntos-de-datos.html#cb1216-3"></a><span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]</span></span>
<span id="cb1216-4"><a href="grandes-conjuntos-de-datos.html#cb1216-4"></a><span class="co">#&gt; [1,]    1    2    3    4    5</span></span>
<span id="cb1216-5"><a href="grandes-conjuntos-de-datos.html#cb1216-5"></a><span class="co">#&gt; [2,]    6    7    8    9   10</span></span>
<span id="cb1216-6"><a href="grandes-conjuntos-de-datos.html#cb1216-6"></a><span class="co">#&gt; [3,]   11   12   13   14   15</span></span></code></pre></div>
<p>Cuando convertimos las columnas en filas, nos referimos a las operaciones como <em>transponer</em> la matriz. La función <code>t</code> se puede usar para transponer directamente una matriz:</p>
<div class="sourceCode" id="cb1217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1217-1"><a href="grandes-conjuntos-de-datos.html#cb1217-1"></a><span class="kw">identical</span>(<span class="kw">t</span>(mat), mat_t)</span>
<span id="cb1217-2"><a href="grandes-conjuntos-de-datos.html#cb1217-2"></a><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p><strong>Advertencia</strong>: El <code>matrix</code> la función recicla valores en el vector <strong>sin advertencia</strong> si el producto de columnas y filas no coincide con la longitud del vector:</p>
<div class="sourceCode" id="cb1218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1218-1"><a href="grandes-conjuntos-de-datos.html#cb1218-1"></a><span class="kw">matrix</span>(my_vector, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb1218-2"><a href="grandes-conjuntos-de-datos.html#cb1218-2"></a><span class="co">#&gt; Warning in matrix(my_vector, 4, 5): la longitud de los datos [15] no es</span></span>
<span id="cb1218-3"><a href="grandes-conjuntos-de-datos.html#cb1218-3"></a><span class="co">#&gt; un submúltiplo o múltiplo del número de filas [4] en la matriz</span></span>
<span id="cb1218-4"><a href="grandes-conjuntos-de-datos.html#cb1218-4"></a><span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]</span></span>
<span id="cb1218-5"><a href="grandes-conjuntos-de-datos.html#cb1218-5"></a><span class="co">#&gt; [1,]    1    5    9   13    2</span></span>
<span id="cb1218-6"><a href="grandes-conjuntos-de-datos.html#cb1218-6"></a><span class="co">#&gt; [2,]    2    6   10   14    3</span></span>
<span id="cb1218-7"><a href="grandes-conjuntos-de-datos.html#cb1218-7"></a><span class="co">#&gt; [3,]    3    7   11   15    4</span></span>
<span id="cb1218-8"><a href="grandes-conjuntos-de-datos.html#cb1218-8"></a><span class="co">#&gt; [4,]    4    8   12    1    5</span></span></code></pre></div>
<p>Para poner las intensidades de píxeles de nuestra, digamos, tercera entrada, que es un en la cuadrícula, podemos usar:</p>
<div class="sourceCode" id="cb1219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1219-1"><a href="grandes-conjuntos-de-datos.html#cb1219-1"></a>grid &lt;-<span class="st"> </span><span class="kw">matrix</span>(x[<span class="dv">3</span>,], <span class="dv">28</span>, <span class="dv">28</span>)</span></code></pre></div>
<p>Para confirmar que, de hecho, lo hemos hecho correctamente, podemos usar la función <code>image</code>, que muestra una imagen de su tercer argumento. La parte superior de este gráfico es el píxel 1, que se muestra en la parte inferior para que la imagen se voltee. Para codificar a continuación se incluye un código que muestra cómo voltearlo:</p>
<div class="sourceCode" id="cb1220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1220-1"><a href="grandes-conjuntos-de-datos.html#cb1220-1"></a><span class="kw">image</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, grid)</span>
<span id="cb1220-2"><a href="grandes-conjuntos-de-datos.html#cb1220-2"></a><span class="kw">image</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, grid[, <span class="dv">28</span><span class="op">:</span><span class="dv">1</span>])</span></code></pre></div>
<p><img src="libro_files/figure-html/matrix-image-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="resúmenes-de-filas-y-columnas" class="section level3">
<h3><span class="header-section-number">33.1.3</span> Resúmenes de filas y columnas</h3>
<p>Para la primera tarea, relacionada con la oscuridad total de píxeles, queremos sumar los valores de cada fila y luego visualizar cómo estos valores varían por dígito.</p>
<p>La función <code>rowSums</code> toma una matriz como entrada y calcula los valores deseados:</p>
<div class="sourceCode" id="cb1221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1221-1"><a href="grandes-conjuntos-de-datos.html#cb1221-1"></a>sums &lt;-<span class="st"> </span><span class="kw">rowSums</span>(x)</span></code></pre></div>
<p>También podemos calcular los promedios con <code>rowMeans</code> si queremos que los valores permanezcan entre 0 y 255:</p>
<div class="sourceCode" id="cb1222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1222-1"><a href="grandes-conjuntos-de-datos.html#cb1222-1"></a>avg &lt;-<span class="st"> </span><span class="kw">rowMeans</span>(x)</span></code></pre></div>
<p>Una vez que tengamos esto, simplemente podemos generar un diagrama de caja:</p>
<div class="sourceCode" id="cb1223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1223-1"><a href="grandes-conjuntos-de-datos.html#cb1223-1"></a><span class="kw">tibble</span>(<span class="dt">labels =</span> <span class="kw">as.factor</span>(y), <span class="dt">row_averages =</span> avg) <span class="op">%&gt;%</span></span>
<span id="cb1223-2"><a href="grandes-conjuntos-de-datos.html#cb1223-2"></a><span class="kw">qplot</span>(labels, row_averages, <span class="dt">data =</span> ., <span class="dt">geom =</span> <span class="st">&quot;boxplot&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/boxplot-of-digit-averages-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>De esta gráfica vemos que, como es lógico, 1s usan menos tinta que los otros dígitos.</p>
<p>Podemos calcular las sumas y promedios de la columna usando la función <code>colSums</code> y <code>colMeans</code>, respectivamente.</p>
<p>El paquete <strong>matrixStats</strong> agrega funciones que realizan operaciones en cada fila o columna de manera muy eficiente, incluidas las funciones <code>rowSds</code> y <code>colSds</code>.</p>
</div>
<div id="apply" class="section level3">
<h3><span class="header-section-number">33.1.4</span> <code>apply</code></h3>
<p>Las funciones que acabamos de describir están realizando una operación similar a la que <code>sapply</code> y la función <strong>purrr</strong> <code>map</code> hacer: aplica la misma función a una parte de tu objeto. En este caso, la función se aplica a cada fila o cada columna. los <code>apply</code> la función le permite aplicar cualquier función, no solo <code>sum</code> o <code>mean</code>, a una matriz. El primer argumento es la matriz, el segundo es la dimensión, 1 para las filas, 2 para las columnas y el tercero es la función. Así por ejemplo, <code>rowMeans</code> se puede escribir como:</p>
<div class="sourceCode" id="cb1224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1224-1"><a href="grandes-conjuntos-de-datos.html#cb1224-1"></a>avgs &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, mean)</span></code></pre></div>
<p>Pero note que al igual que con <code>sapply</code> y <code>map</code>, podemos realizar cualquier función. Entonces, si quisiéramos la desviación estándar para cada columna, podríamos escribir:</p>
<div class="sourceCode" id="cb1225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1225-1"><a href="grandes-conjuntos-de-datos.html#cb1225-1"></a>sds &lt;-<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">2</span>, sd)</span></code></pre></div>
<p>La desventaja de esta flexibilidad es que estas operaciones no son tan rápidas como las funciones dedicadas como <code>rowMeans</code>.</p>
</div>
<div id="filtrar-columnas-basadas-en-resúmenes" class="section level3">
<h3><span class="header-section-number">33.1.5</span> Filtrar columnas basadas en resúmenes</h3>
<p>Ahora pasamos a la tarea 2: estudiar la variación de cada píxel y eliminar las columnas asociadas con píxeles que no cambian mucho y, por lo tanto, no informan la clasificación. Aunque es un enfoque simplista, cuantificaremos la variación de cada píxel con su desviación estándar en todas las entradas. Como cada columna representa un píxel, utilizamos el <code>colSds</code> función del paquete <strong>matrixStats</strong>:</p>
<div class="sourceCode" id="cb1226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1226-1"><a href="grandes-conjuntos-de-datos.html#cb1226-1"></a><span class="kw">library</span>(matrixStats)</span>
<span id="cb1226-2"><a href="grandes-conjuntos-de-datos.html#cb1226-2"></a>sds &lt;-<span class="st"> </span><span class="kw">colSds</span>(x)</span></code></pre></div>
<p>Un vistazo rápido a la distribución de estos valores muestra que algunos píxeles tienen una variabilidad muy baja de entrada a entrada:</p>
<div class="sourceCode" id="cb1227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1227-1"><a href="grandes-conjuntos-de-datos.html#cb1227-1"></a><span class="kw">qplot</span>(sds, <span class="dt">bins =</span> <span class="st">&quot;30&quot;</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="libro_files/figure-html/sds-histogram-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Esto tiene sentido ya que no escribimos en algunas partes del cuadro. Aquí está la variación trazada por ubicación:</p>
<div class="sourceCode" id="cb1228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1228-1"><a href="grandes-conjuntos-de-datos.html#cb1228-1"></a><span class="kw">image</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="kw">matrix</span>(sds, <span class="dv">28</span>, <span class="dv">28</span>)[, <span class="dv">28</span><span class="op">:</span><span class="dv">1</span>])</span></code></pre></div>
<p><img src="libro_files/figure-html/pixel-variance-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Vemos que hay poca variación en las esquinas.</p>
<p>Podríamos eliminar características que no tienen variación ya que estas no pueden ayudarnos a predecir. En la sección <a href="r-basics.html#matrices">2.4.7</a>, describimos las operaciones utilizadas para extraer columnas:</p>
<div class="sourceCode" id="cb1229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1229-1"><a href="grandes-conjuntos-de-datos.html#cb1229-1"></a>x[ ,<span class="kw">c</span>(<span class="dv">351</span>,<span class="dv">352</span>)]</span></code></pre></div>
<p>y filas:</p>
<div class="sourceCode" id="cb1230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1230-1"><a href="grandes-conjuntos-de-datos.html#cb1230-1"></a>x[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>),]</span></code></pre></div>
<p>También podemos usar índices lógicos para determinar qué columnas o filas mantener. Entonces, si quisiéramos eliminar predictores no informativos de nuestra matriz, podríamos escribir esta línea de código:</p>
<div class="sourceCode" id="cb1231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1231-1"><a href="grandes-conjuntos-de-datos.html#cb1231-1"></a>new_x &lt;-<span class="st"> </span>x[ ,<span class="kw">colSds</span>(x) <span class="op">&gt;</span><span class="st"> </span><span class="dv">60</span>]</span>
<span id="cb1231-2"><a href="grandes-conjuntos-de-datos.html#cb1231-2"></a><span class="kw">dim</span>(new_x)</span>
<span id="cb1231-3"><a href="grandes-conjuntos-de-datos.html#cb1231-3"></a><span class="co">#&gt; [1] 1000  314</span></span></code></pre></div>
<p>Solo se mantienen las columnas para las que la desviación estándar es superior a 60, lo que elimina más de la mitad de los predictores.</p>
<p>Aquí agregamos una advertencia importante relacionada con el subconjunto de matrices: si selecciona una columna o una fila, el resultado ya no es una matriz sino un vector.</p>
<div class="sourceCode" id="cb1232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1232-1"><a href="grandes-conjuntos-de-datos.html#cb1232-1"></a><span class="kw">class</span>(x[,<span class="dv">1</span>])</span>
<span id="cb1232-2"><a href="grandes-conjuntos-de-datos.html#cb1232-2"></a><span class="co">#&gt; [1] &quot;integer&quot;</span></span>
<span id="cb1232-3"><a href="grandes-conjuntos-de-datos.html#cb1232-3"></a><span class="kw">dim</span>(x[<span class="dv">1</span>,])</span>
<span id="cb1232-4"><a href="grandes-conjuntos-de-datos.html#cb1232-4"></a><span class="co">#&gt; NULL</span></span></code></pre></div>
<p>Sin embargo, podemos preservar la clase de matriz usando el argumento <code>drop=FALSE</code>:</p>
<div class="sourceCode" id="cb1233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1233-1"><a href="grandes-conjuntos-de-datos.html#cb1233-1"></a><span class="kw">class</span>(x[ , <span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>])</span>
<span id="cb1233-2"><a href="grandes-conjuntos-de-datos.html#cb1233-2"></a><span class="co">#&gt; [1] &quot;matrix&quot; &quot;array&quot;</span></span>
<span id="cb1233-3"><a href="grandes-conjuntos-de-datos.html#cb1233-3"></a><span class="kw">dim</span>(x[, <span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>])</span>
<span id="cb1233-4"><a href="grandes-conjuntos-de-datos.html#cb1233-4"></a><span class="co">#&gt; [1] 1000    1</span></span></code></pre></div>
</div>
<div id="indexación-con-matrices" class="section level3">
<h3><span class="header-section-number">33.1.6</span> Indexación con matrices</h3>
<p>Podemos hacer rápidamente un histograma de todos los valores en nuestro conjunto de datos. Vimos cómo podemos convertir vectores en matrices. También podemos deshacer esto y convertir matrices en vectores. La operación se realizará por fila:</p>
<div class="sourceCode" id="cb1234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1234-1"><a href="grandes-conjuntos-de-datos.html#cb1234-1"></a>mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb1234-2"><a href="grandes-conjuntos-de-datos.html#cb1234-2"></a><span class="kw">as.vector</span>(mat)</span>
<span id="cb1234-3"><a href="grandes-conjuntos-de-datos.html#cb1234-3"></a><span class="co">#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15</span></span></code></pre></div>
<p>Para ver un histograma de todos nuestros datos predictores, podemos usar:</p>
<div class="sourceCode" id="cb1235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1235-1"><a href="grandes-conjuntos-de-datos.html#cb1235-1"></a><span class="kw">qplot</span>(<span class="kw">as.vector</span>(x), <span class="dt">bins =</span> <span class="dv">30</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="libro_files/figure-html/histogram-all-pixels-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Notamos una clara dicotomía que se explica como partes de la imagen con tinta y partes sin ella. Si creemos que los valores a continuación, digamos, 50 son manchas, podemos hacerlos rápidamente cero usando:</p>
<div class="sourceCode" id="cb1236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1236-1"><a href="grandes-conjuntos-de-datos.html#cb1236-1"></a>new_x &lt;-<span class="st"> </span>x</span>
<span id="cb1236-2"><a href="grandes-conjuntos-de-datos.html#cb1236-2"></a>new_x[new_x <span class="op">&lt;</span><span class="st"> </span><span class="dv">50</span>] &lt;-<span class="st"> </span><span class="dv">0</span></span></code></pre></div>
<p>Para ver qué hace esto, observamos una matriz más pequeña:</p>
<div class="sourceCode" id="cb1237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1237-1"><a href="grandes-conjuntos-de-datos.html#cb1237-1"></a>mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb1237-2"><a href="grandes-conjuntos-de-datos.html#cb1237-2"></a>mat[mat <span class="op">&lt;</span><span class="st"> </span><span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb1237-3"><a href="grandes-conjuntos-de-datos.html#cb1237-3"></a>mat</span>
<span id="cb1237-4"><a href="grandes-conjuntos-de-datos.html#cb1237-4"></a><span class="co">#&gt;      [,1] [,2] [,3]</span></span>
<span id="cb1237-5"><a href="grandes-conjuntos-de-datos.html#cb1237-5"></a><span class="co">#&gt; [1,]    0    6   11</span></span>
<span id="cb1237-6"><a href="grandes-conjuntos-de-datos.html#cb1237-6"></a><span class="co">#&gt; [2,]    0    7   12</span></span>
<span id="cb1237-7"><a href="grandes-conjuntos-de-datos.html#cb1237-7"></a><span class="co">#&gt; [3,]    3    8   13</span></span>
<span id="cb1237-8"><a href="grandes-conjuntos-de-datos.html#cb1237-8"></a><span class="co">#&gt; [4,]    4    9   14</span></span>
<span id="cb1237-9"><a href="grandes-conjuntos-de-datos.html#cb1237-9"></a><span class="co">#&gt; [5,]    5   10   15</span></span></code></pre></div>
<p>También podemos usar operaciones lógicas con matriz lógica:</p>
<div class="sourceCode" id="cb1238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1238-1"><a href="grandes-conjuntos-de-datos.html#cb1238-1"></a>mat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb1238-2"><a href="grandes-conjuntos-de-datos.html#cb1238-2"></a>mat[mat <span class="op">&gt;</span><span class="st"> </span><span class="dv">6</span> <span class="op">&amp;</span><span class="st"> </span>mat <span class="op">&lt;</span><span class="st"> </span><span class="dv">12</span>] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb1238-3"><a href="grandes-conjuntos-de-datos.html#cb1238-3"></a>mat</span>
<span id="cb1238-4"><a href="grandes-conjuntos-de-datos.html#cb1238-4"></a><span class="co">#&gt;      [,1] [,2] [,3]</span></span>
<span id="cb1238-5"><a href="grandes-conjuntos-de-datos.html#cb1238-5"></a><span class="co">#&gt; [1,]    1    6    0</span></span>
<span id="cb1238-6"><a href="grandes-conjuntos-de-datos.html#cb1238-6"></a><span class="co">#&gt; [2,]    2    0   12</span></span>
<span id="cb1238-7"><a href="grandes-conjuntos-de-datos.html#cb1238-7"></a><span class="co">#&gt; [3,]    3    0   13</span></span>
<span id="cb1238-8"><a href="grandes-conjuntos-de-datos.html#cb1238-8"></a><span class="co">#&gt; [4,]    4    0   14</span></span>
<span id="cb1238-9"><a href="grandes-conjuntos-de-datos.html#cb1238-9"></a><span class="co">#&gt; [5,]    5    0   15</span></span></code></pre></div>
</div>
<div id="binarizar-los-datos" class="section level3">
<h3><span class="header-section-number">33.1.7</span> Binarizar los datos</h3>
<p>El histograma anterior parece sugerir que estos datos son principalmente binarios. Un píxel tiene tinta o no. Usando lo que hemos aprendido, podemos binarizar los datos usando solo operaciones matriciales:</p>
<div class="sourceCode" id="cb1239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1239-1"><a href="grandes-conjuntos-de-datos.html#cb1239-1"></a>bin_x &lt;-<span class="st"> </span>x</span>
<span id="cb1239-2"><a href="grandes-conjuntos-de-datos.html#cb1239-2"></a>bin_x[bin_x <span class="op">&lt;</span><span class="st"> </span><span class="dv">255</span><span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb1239-3"><a href="grandes-conjuntos-de-datos.html#cb1239-3"></a>bin_x[bin_x <span class="op">&gt;</span><span class="st"> </span><span class="dv">255</span><span class="op">/</span><span class="dv">2</span>] &lt;-<span class="st"> </span><span class="dv">1</span></span></code></pre></div>
<p>También podemos convertir a una matriz de lógicas y luego coaccionar a números como este:</p>
<div class="sourceCode" id="cb1240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1240-1"><a href="grandes-conjuntos-de-datos.html#cb1240-1"></a>bin_X &lt;-<span class="st"> </span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">255</span><span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">1</span></span></code></pre></div>
<!--
Podemos ver que al menos la entrada que vimos antes no cambia mucho:

<img src="libro_files/figure-html/binarized-image-1.png" width="100%" style="display: block; margin: auto;" />
-->
</div>
<div id="vectorización-para-matrices" class="section level3">
<h3><span class="header-section-number">33.1.8</span> Vectorización para matrices</h3>
<p>En R, si restamos un vector de una matriz, el primer elemento del vector se resta de la primera fila, el segundo elemento de la segunda fila, y así sucesivamente. Usando la notación matemática, la escribiríamos de la siguiente manera:</p>
<p><span class="math display">\[
\begin{pmatrix}
X_{1,1}&amp;\dots &amp; X_{1,p} \\
X_{2,1}&amp;\dots &amp; X_{2,p} \\
&amp; \vdots &amp; \\
X_{N,1}&amp;\dots &amp; X_{N,p}
\end{pmatrix}
-
\begin{pmatrix}
a_1\\\
a_2\\\
\vdots\\\
a_N
\end{pmatrix}
=
\begin{pmatrix}
X_{1,1}-a_1&amp;\dots &amp; X_{1,p} -a_1\\
X_{2,1}-a_2&amp;\dots &amp; X_{2,p} -a_2\\
&amp; \vdots &amp; \\
X_{N,1}-a_n&amp;\dots &amp; X_{N,p} -a_n
\end{pmatrix}
\]</span></p>
<p>Lo mismo es válido para otras operaciones aritméticas. Esto implica que podemos escalar cada fila de una matriz como esta:</p>
<div class="sourceCode" id="cb1241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1241-1"><a href="grandes-conjuntos-de-datos.html#cb1241-1"></a>(x <span class="op">-</span><span class="st"> </span><span class="kw">rowMeans</span>(x))<span class="op">/</span><span class="st"> </span><span class="kw">rowSds</span>(x)</span></code></pre></div>
<p>Si desea escalar cada columna, tenga cuidado ya que este enfoque no funciona para las columnas. Para realizar una operación similar, convertimos las columnas en filas usando la transposición <code>t</code>, proceda como se indica arriba y luego transponga de regreso:</p>
<div class="sourceCode" id="cb1242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1242-1"><a href="grandes-conjuntos-de-datos.html#cb1242-1"></a><span class="kw">t</span>(<span class="kw">t</span>(X) <span class="op">-</span><span class="st"> </span><span class="kw">colMeans</span>(X))</span></code></pre></div>
<p>También podemos usar una función llamada <code>sweep</code> que funciona de manera similar a <code>apply</code>. Toma cada entrada de un vector y lo resta de la fila o columna correspondiente.</p>
<div class="sourceCode" id="cb1243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1243-1"><a href="grandes-conjuntos-de-datos.html#cb1243-1"></a>X_mean_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x))</span></code></pre></div>
<p>La función <code>sweep</code> en realidad tiene otro argumento que le permite definir la operación aritmética. Entonces, para dividir por la desviación estándar, hacemos lo siguiente:</p>
<div class="sourceCode" id="cb1244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1244-1"><a href="grandes-conjuntos-de-datos.html#cb1244-1"></a>x_mean_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x))</span>
<span id="cb1244-2"><a href="grandes-conjuntos-de-datos.html#cb1244-2"></a>x_standardized &lt;-<span class="st"> </span><span class="kw">sweep</span>(x_mean_<span class="dv">0</span>, <span class="dv">2</span>, <span class="kw">colSds</span>(x), <span class="dt">FUN =</span> <span class="st">&quot;/&quot;</span>)</span></code></pre></div>
</div>
<div id="operaciones-de-álgebra-matricial" class="section level3">
<h3><span class="header-section-number">33.1.9</span> Operaciones de álgebra matricial</h3>
<p>Finalmente, aunque no cubrimos las operaciones de álgebra matricial, como la multiplicación matricial, compartimos aquí los comandos relevantes para aquellos que conocen las matemáticas y quieren aprender el código:</p>
<p>1. La multiplicación de matrices se realiza con <code>%*%</code>. Por ejemplo, el producto cruzado es:</p>
<div class="sourceCode" id="cb1245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1245-1"><a href="grandes-conjuntos-de-datos.html#cb1245-1"></a><span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x</span></code></pre></div>
<p>2. Podemos calcular el producto cruzado directamente con la función:</p>
<div class="sourceCode" id="cb1246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1246-1"><a href="grandes-conjuntos-de-datos.html#cb1246-1"></a><span class="kw">crossprod</span>(x)</span></code></pre></div>
<p>3. Para calcular el inverso de una función, usamos <code>solve</code>. Aquí se aplica al producto cruzado:</p>
<div class="sourceCode" id="cb1247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1247-1"><a href="grandes-conjuntos-de-datos.html#cb1247-1"></a><span class="kw">solve</span>(<span class="kw">crossprod</span>(x))</span></code></pre></div>
<p>4. La descomposición QR está fácilmente disponible mediante el uso de <code>qr</code> función:</p>
<div class="sourceCode" id="cb1248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1248-1"><a href="grandes-conjuntos-de-datos.html#cb1248-1"></a><span class="kw">qr</span>(x)</span></code></pre></div>
</div>
</div>
<div id="ejercicios-55" class="section level2">
<h2><span class="header-section-number">33.2</span> Ejercicios</h2>
<p>1. Cree una matriz de 100 por 10 de números normales generados aleatoriamente. Pon el resultado en <code>x</code>.</p>
<p>2. Aplica las tres funciones R que te dan la dimensión de <code>x</code>, el número de filas de <code>x</code> y el número de columnas de <code>x</code>, respectivamente.</p>
<p>3. Agregue el escalar 1 a la fila 1, el escalar 2 a la fila 2, y así sucesivamente, a la matriz <code>x</code>.</p>
<p>4. Agregue el escalar 1 a la columna 1, el escalar 2 a la columna 2, y así sucesivamente, a la matriz <code>x</code>. Sugerencia: uso <code>sweep</code> con <code>FUN = "+"</code>.</p>
<p>5. Calcule el promedio de cada fila de <code>x</code>.</p>
<p>6. Calcule el promedio de cada columna de <code>x</code>.</p>
<p>7. Para cada dígito en los datos de entrenamiento MNIST, calcule la proporción de píxeles que se encuentran en un área gris, definida como valores entre 50 y 205.
Hacer diagrama de caja por clase de dígitos. Sugerencia: utilice operadores lógicos y <code>rowMeans</code>.</p>

</div>
<div id="distancia" class="section level2">
<h2><span class="header-section-number">33.3</span> Distancia</h2>
<p>Muchos de los análisis que realizamos con datos de alta dimensión se relacionan directa o indirectamente con la distancia. La mayoría de las técnicas de agrupamiento y aprendizaje automático se basan en la capacidad de definir la distancia entre observaciones, utilizando características o predictores.</p>
<div id="distancia-euclidiana" class="section level3">
<h3><span class="header-section-number">33.3.1</span> Distancia euclidiana</h3>
<p>Como revisión, definamos la distancia entre dos puntos, <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span>, en un plano cartesiano.</p>
<p><img src="libro_files/figure-html/euclidean-distance-1.png" width="35%" style="display: block; margin: auto;" /></p>
<p>La distancia euclidiana entre <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> es simple:</p>
<p><span class="math display">\[
\mbox{dist}(A,B) = \sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2}
\]</span></p>
<p>Esta definición se aplica al caso de una dimensión, en la que la distancia entre dos números es simplemente el valor absoluto de su diferencia. Entonces, si nuestros dos números unidimensionales son <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span>, la distancia es:</p>
<p><span class="math display">\[
\mbox{dist}(A,B) = \sqrt{ (A - B)^2 } = | A - B |
\]</span></p>
</div>
<div id="distancia-en-dimensiones-superiores" class="section level3">
<h3><span class="header-section-number">33.3.2</span> Distancia en dimensiones superiores</h3>
<p>Anteriormente presentamos un conjunto de datos de entrenamiento con mediciones de matriz de características para 784 características. Con fines ilustrativos, veremos una muestra aleatoria de 2s y 7s.</p>
<div class="sourceCode" id="cb1249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1249-1"><a href="grandes-conjuntos-de-datos.html#cb1249-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1249-2"><a href="grandes-conjuntos-de-datos.html#cb1249-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1249-3"><a href="grandes-conjuntos-de-datos.html#cb1249-3"></a></span>
<span id="cb1249-4"><a href="grandes-conjuntos-de-datos.html#cb1249-4"></a><span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()</span>
<span id="cb1249-5"><a href="grandes-conjuntos-de-datos.html#cb1249-5"></a></span>
<span id="cb1249-6"><a href="grandes-conjuntos-de-datos.html#cb1249-6"></a><span class="kw">set.seed</span>(<span class="dv">1995</span>)</span>
<span id="cb1249-7"><a href="grandes-conjuntos-de-datos.html#cb1249-7"></a>ind &lt;-<span class="st"> </span><span class="kw">which</span>(mnist<span class="op">$</span>train<span class="op">$</span>labels <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">7</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample</span>(<span class="dv">500</span>)</span>
<span id="cb1249-8"><a href="grandes-conjuntos-de-datos.html#cb1249-8"></a>x &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>images[ind,]</span>
<span id="cb1249-9"><a href="grandes-conjuntos-de-datos.html#cb1249-9"></a>y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels[ind]</span></code></pre></div>
<p>Los predictores están en <code>x</code> y las etiquetas en <code>y</code>.</p>
<p>A los fines de, por ejemplo, suavizado, estamos interesados en describir la distancia entre observaciones; En este caso, dígitos. Más adelante, con el fin de seleccionar características, también podríamos estar interesados en encontrar píxeles que se comporten de manera similar en todas las muestras.</p>
<p>Para definir la distancia, necesitamos saber qué son los puntos, ya que la distancia matemática se calcula entre puntos. Con datos de alta dimensión, los puntos ya no están en el plano cartesiano. En cambio, los puntos están en dimensiones más altas. Ya no podemos visualizarlos y necesitamos pensar de manera abstracta. Por ejemplo, predictores <span class="math inline">\(\mathbf{X}_i\)</span> se definen como un punto en el espacio dimensional 784: <span class="math inline">\(\mathbf{X}_i = (x_{i,1},\dots,x_{i,784})^\top\)</span>.</p>
<p>Una vez que definimos los puntos de esta manera, la distancia euclidiana se define de manera muy similar a la de dos dimensiones. Por ejemplo, la distancia entre los predictores para dos observaciones, digamos observaciones <span class="math inline">\(i=1\)</span> y <span class="math inline">\(i=2\)</span>, es:</p>
<p><span class="math display">\[
\mbox{dist}(1,2) = \sqrt{ \sum_{j=1}^{784} (x_{1,j}-x_{2,j })^2 }
\]</span></p>
<p>Este es solo un número no negativo, tal como lo es para dos dimensiones.</p>
</div>
<div id="ejemplo-de-distancia-euclidiana" class="section level3">
<h3><span class="header-section-number">33.3.3</span> Ejemplo de distancia euclidiana</h3>
<p>Las etiquetas para las tres primeras observaciones son:</p>
<div class="sourceCode" id="cb1250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1250-1"><a href="grandes-conjuntos-de-datos.html#cb1250-1"></a>y[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</span>
<span id="cb1250-2"><a href="grandes-conjuntos-de-datos.html#cb1250-2"></a><span class="co">#&gt; [1] 7 2 7</span></span></code></pre></div>
<p>Los vectores de predictores para cada una de estas observaciones son:</p>
<div class="sourceCode" id="cb1251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1251-1"><a href="grandes-conjuntos-de-datos.html#cb1251-1"></a>x_<span class="dv">1</span> &lt;-<span class="st"> </span>x[<span class="dv">1</span>,]</span>
<span id="cb1251-2"><a href="grandes-conjuntos-de-datos.html#cb1251-2"></a>x_<span class="dv">2</span> &lt;-<span class="st"> </span>x[<span class="dv">2</span>,]</span>
<span id="cb1251-3"><a href="grandes-conjuntos-de-datos.html#cb1251-3"></a>x_<span class="dv">3</span> &lt;-<span class="st"> </span>x[<span class="dv">3</span>,]</span></code></pre></div>
<p>Los primeros dos números son siete y el tercero es un 2. Esperamos las distancias entre el mismo número:</p>
<div class="sourceCode" id="cb1252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1252-1"><a href="grandes-conjuntos-de-datos.html#cb1252-1"></a><span class="kw">sqrt</span>(<span class="kw">sum</span>((x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb1252-2"><a href="grandes-conjuntos-de-datos.html#cb1252-2"></a><span class="co">#&gt; [1] 3273</span></span></code></pre></div>
<p>ser más pequeño que entre diferentes números:</p>
<div class="sourceCode" id="cb1253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1253-1"><a href="grandes-conjuntos-de-datos.html#cb1253-1"></a><span class="kw">sqrt</span>(<span class="kw">sum</span>((x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb1253-2"><a href="grandes-conjuntos-de-datos.html#cb1253-2"></a><span class="co">#&gt; [1] 2311</span></span>
<span id="cb1253-3"><a href="grandes-conjuntos-de-datos.html#cb1253-3"></a><span class="kw">sqrt</span>(<span class="kw">sum</span>((x_<span class="dv">2</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb1253-4"><a href="grandes-conjuntos-de-datos.html#cb1253-4"></a><span class="co">#&gt; [1] 2636</span></span></code></pre></div>
<p>Como se esperaba, los 7 están más cerca uno del otro.</p>
<p>Una forma más rápida de calcular esto es usar álgebra matricial:</p>
<div class="sourceCode" id="cb1254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1254-1"><a href="grandes-conjuntos-de-datos.html#cb1254-1"></a><span class="kw">sqrt</span>(<span class="kw">crossprod</span>(x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">2</span>))</span>
<span id="cb1254-2"><a href="grandes-conjuntos-de-datos.html#cb1254-2"></a><span class="co">#&gt;      [,1]</span></span>
<span id="cb1254-3"><a href="grandes-conjuntos-de-datos.html#cb1254-3"></a><span class="co">#&gt; [1,] 3273</span></span>
<span id="cb1254-4"><a href="grandes-conjuntos-de-datos.html#cb1254-4"></a><span class="kw">sqrt</span>(<span class="kw">crossprod</span>(x_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>))</span>
<span id="cb1254-5"><a href="grandes-conjuntos-de-datos.html#cb1254-5"></a><span class="co">#&gt;      [,1]</span></span>
<span id="cb1254-6"><a href="grandes-conjuntos-de-datos.html#cb1254-6"></a><span class="co">#&gt; [1,] 2311</span></span>
<span id="cb1254-7"><a href="grandes-conjuntos-de-datos.html#cb1254-7"></a><span class="kw">sqrt</span>(<span class="kw">crossprod</span>(x_<span class="dv">2</span> <span class="op">-</span><span class="st"> </span>x_<span class="dv">3</span>))</span>
<span id="cb1254-8"><a href="grandes-conjuntos-de-datos.html#cb1254-8"></a><span class="co">#&gt;      [,1]</span></span>
<span id="cb1254-9"><a href="grandes-conjuntos-de-datos.html#cb1254-9"></a><span class="co">#&gt; [1,] 2636</span></span></code></pre></div>
<p>También podemos calcular <strong>todas</strong> las distancias a la vez de manera relativamente rápida utilizando la función <code>dist</code>, que calcula la distancia entre cada fila y produce un objeto de clase <code>dist</code>:</p>
<div class="sourceCode" id="cb1255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1255-1"><a href="grandes-conjuntos-de-datos.html#cb1255-1"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(x)</span>
<span id="cb1255-2"><a href="grandes-conjuntos-de-datos.html#cb1255-2"></a><span class="kw">class</span>(d)</span>
<span id="cb1255-3"><a href="grandes-conjuntos-de-datos.html#cb1255-3"></a><span class="co">#&gt; [1] &quot;dist&quot;</span></span></code></pre></div>
<p>Hay varias funciones relacionadas con el aprendizaje automático en R que toman objetos de clase <code>dist</code> como entrada Para acceder a las entradas usando índices de fila y columna, necesitamos forzarlo en una matriz. Podemos ver la distancia que calculamos arriba de esta manera:</p>
<div class="sourceCode" id="cb1256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1256-1"><a href="grandes-conjuntos-de-datos.html#cb1256-1"></a><span class="kw">as.matrix</span>(d)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</span>
<span id="cb1256-2"><a href="grandes-conjuntos-de-datos.html#cb1256-2"></a><span class="co">#&gt;      1    2    3</span></span>
<span id="cb1256-3"><a href="grandes-conjuntos-de-datos.html#cb1256-3"></a><span class="co">#&gt; 1    0 3273 2311</span></span>
<span id="cb1256-4"><a href="grandes-conjuntos-de-datos.html#cb1256-4"></a><span class="co">#&gt; 2 3273    0 2636</span></span>
<span id="cb1256-5"><a href="grandes-conjuntos-de-datos.html#cb1256-5"></a><span class="co">#&gt; 3 2311 2636    0</span></span></code></pre></div>
<p>Podemos ver rápidamente una imagen de estas distancias usando este código:</p>
<div class="sourceCode" id="cb1257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1257-1"><a href="grandes-conjuntos-de-datos.html#cb1257-1"></a><span class="kw">image</span>(<span class="kw">as.matrix</span>(d))</span></code></pre></div>
<p>Si ordenamos esta distancia por las etiquetas, podemos ver que, en general, los dos están más cerca uno del otro y los sietes están más cerca el uno del otro:</p>
<div class="sourceCode" id="cb1258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1258-1"><a href="grandes-conjuntos-de-datos.html#cb1258-1"></a><span class="kw">image</span>(<span class="kw">as.matrix</span>(d)[<span class="kw">order</span>(y), <span class="kw">order</span>(y)])</span></code></pre></div>
<p><img src="libro_files/figure-html/diatance-image-ordered-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Una cosa que notamos aquí es que parece haber más uniformidad en la forma en que se dibujan los sietes, ya que parecen estar más cercanos (más rojos) a otros sietes que los dos a otros dos.</p>
</div>
<div id="predictor-space" class="section level3">
<h3><span class="header-section-number">33.3.4</span> Espacio predictor</h3>
<p><em>Predictor space</em> es un concepto que a menudo se usa para describir algoritmos de aprendizaje automático. El término espacio se refiere a una definición matemática que no describimos en detalle aquí. En cambio, proporcionamos una explicación simplificada para ayudar a comprender el término espacio predictivo cuando se usa en el contexto de algoritmos de aprendizaje automático.</p>
<p>El espacio del predictor puede considerarse como la colección de todos los posibles vectores de predictores que deben considerarse para el desafío de aprendizaje automático en cuestión. Cada miembro del espacio se conoce como <em>point</em>. Por ejemplo, en el conjunto de datos 2 o 7, el espacio predictivo consta de todos los pares <span class="math inline">\((x_1, x_2)\)</span> tal que ambos <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> están dentro de 0 y 1. Este espacio en particular puede representarse gráficamente como un cuadrado. En el conjunto de datos MNIST, el espacio predictivo consta de todos los vectores dimensionales 784 con cada elemento vectorial un número entero entre 0 y 256. Un elemento esencial de un espacio predictor es que necesitamos definir una función que proporcione la distancia entre dos puntos. En la mayoría de los casos usamos la distancia euclidiana, pero hay otras posibilidades. Un caso particular en el que no podemos simplemente usar la distancia euclidiana es cuando tenemos predictores categóricos.</p>
<p>Definir un espacio predictivo es útil en el aprendizaje automático porque hacemos cosas como definir vecindarios de puntos, como lo requieren muchas técnicas de suavizado. Por ejemplo, podemos definir un vecindario como todos los puntos que están dentro de 2 unidades de un centro predefinido. Si los puntos son bidimensionales y usamos la distancia euclidiana, esta vecindad se representa gráficamente como un círculo con radio 2. En tres dimensiones, la vecindad es una esfera. Pronto aprenderemos sobre algoritmos que dividen el espacio en regiones que no se superponen y luego hacen diferentes predicciones para cada región utilizando los datos de la región.</p>
</div>
<div id="distancia-entre-predictores" class="section level3">
<h3><span class="header-section-number">33.3.5</span> Distancia entre predictores</h3>
<p>También podemos calcular distancias entre predictores. Si <span class="math inline">\(N\)</span> es el número de observaciones, la distancia entre dos predictores, digamos 1 y 2, es:</p>
<p><span class="math display">\[
\mbox{dist}(1,2) = \sqrt{ \sum_{i=1}^{N} (x_{i,1}-x_{i,2})^2 }
\]</span></p>
<p>Para calcular la distancia entre todos los pares de los predictores 784, primero podemos transponer la matriz y luego usar <code>dist</code>:</p>
<div class="sourceCode" id="cb1259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1259-1"><a href="grandes-conjuntos-de-datos.html#cb1259-1"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(x))</span>
<span id="cb1259-2"><a href="grandes-conjuntos-de-datos.html#cb1259-2"></a><span class="kw">dim</span>(<span class="kw">as.matrix</span>(d))</span>
<span id="cb1259-3"><a href="grandes-conjuntos-de-datos.html#cb1259-3"></a><span class="co">#&gt; [1] 784 784</span></span></code></pre></div>
<!--
Una cosa interesante a tener en cuenta aquí es que si elegimos un predictor (un píxel), podemos ver qué píxeles están cerca. Es decir, el par de píxeles tiene tinta en las mismas imágenes (pequeña distancia) o no (gran distancia). La distancia entre, por ejemplo, y todos los demás píxeles viene dada por:


```r
d_492 <- as.matrix(d)[492,]
```

Ahora podemos ver el patrón espacial de estas distancias con el siguiente código:


```r
image(1:28, 1:28, matrix(d_492, 28, 28))
```

<img src="libro_files/figure-html/distnace-rows-1.png" width="70%" style="display: block; margin: auto;" />

No es sorprendente que los puntos físicamente cercanos estén matemáticamente más cerca.
-->
</div>
</div>
<div id="ejercicios-56" class="section level2">
<h2><span class="header-section-number">33.4</span> Ejercicios</h2>
<p>1. Cargue el siguiente conjunto de datos:</p>
<div class="sourceCode" id="cb1260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1260-1"><a href="grandes-conjuntos-de-datos.html#cb1260-1"></a><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)</span></code></pre></div>
<p>Este conjunto de datos incluye una matriz <code>x</code></p>
<div class="sourceCode" id="cb1261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1261-1"><a href="grandes-conjuntos-de-datos.html#cb1261-1"></a><span class="kw">dim</span>(tissue_gene_expression<span class="op">$</span>x)</span></code></pre></div>
<p>con la expresión génica medida en 500 genes para 189 muestras biológicas que representan siete tejidos diferentes. El tipo de tejido se almacena en <code>y</code></p>
<div class="sourceCode" id="cb1262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1262-1"><a href="grandes-conjuntos-de-datos.html#cb1262-1"></a><span class="kw">table</span>(tissue_gene_expression<span class="op">$</span>y)</span></code></pre></div>
<p>Calcule la distancia entre cada observación y almacénela en un objeto <code>d</code>.</p>
<p>2. Compare la distancia entre las dos primeras observaciones (ambos cerebelos), la 39 y la 40 (ambos colones) y la 73 y 74 (ambos endometrios). Vea si las observaciones del mismo tipo de tejido están más próximas entre sí.</p>
<p>3. Vemos que, de hecho, las observaciones del mismo tipo de tejido están más próximas entre sí en los seis ejemplos de tejido que acabamos de examinar. Haz un diagrama de todas las distancias usando el <code>image</code> función para ver si este patrón es general. Sugerencia: convertir <code>d</code> a una matriz primero.</p>

</div>
<div id="reducción-de-dimensiones" class="section level2">
<h2><span class="header-section-number">33.5</span> Reducción de dimensiones</h2>
<p>Un reto típico de <em>machinte learning</em> incluirá una gran cantidad de predictores, lo que hace que la visualización sea algo desafiante. Hemos mostrado métodos para visualizar datos univariados y emparejados, pero los gráficos que revelan relaciones entre muchas variables son más complicados en dimensiones más altas. Por ejemplo, para comparar cada una de las 784 características en nuestro ejemplo de predicción de dígitos, tendríamos que crear, por ejemplo, 306,936 diagramas de dispersión. La creación de un único diagrama de dispersión de los datos es imposible debido a la alta dimensionalidad.</p>
<p>Aquí describimos técnicas poderosas útiles para el análisis exploratorio de datos, entre otras cosas, generalmente conocidas como <em>reducción de dimensiones</em>. La idea general es reducir la dimensión del set de datos mientras se conservan características importantes, como la distancia entre características u observaciones. Con menos dimensiones, la visualización se vuelve más factible. La técnica detrás de todo, la descomposición de valores singulares, también es útil en otros contextos. El análisis de componentes principales (PCA) es el enfoque que mostraremos. Antes de aplicar PCA a conjuntos de datos de alta dimensión, motivaremos las ideas detrás con un ejemplo simple.</p>
<div id="preservando-la-distancia" class="section level3">
<h3><span class="header-section-number">33.5.1</span> Preservando la distancia</h3>
<p>Consideramos un ejemplo con alturas gemelas. Algunas parejas son adultas, otras son niños. Aquí simulamos 100 puntos bidimensionales que representan el número de desviaciones estándar que cada individuo tiene respecto a la altura media. Cada punto es un par de gemelos. Utilizamos el <code>mvrnorm</code> función del paquete <strong>MASS</strong> para simular datos normales bivariados.</p>
<div class="sourceCode" id="cb1263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1263-1"><a href="grandes-conjuntos-de-datos.html#cb1263-1"></a><span class="kw">set.seed</span>(<span class="dv">1988</span>)</span>
<span id="cb1263-2"><a href="grandes-conjuntos-de-datos.html#cb1263-2"></a><span class="kw">library</span>(MASS)</span>
<span id="cb1263-3"><a href="grandes-conjuntos-de-datos.html#cb1263-3"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb1263-4"><a href="grandes-conjuntos-de-datos.html#cb1263-4"></a>Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">9</span>, <span class="dv">9</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.9</span>, <span class="dv">9</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.92</span>, <span class="dv">9</span> <span class="op">*</span><span class="st"> </span><span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1263-5"><a href="grandes-conjuntos-de-datos.html#cb1263-5"></a>x &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">mvrnorm</span>(n<span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma),</span>
<span id="cb1263-6"><a href="grandes-conjuntos-de-datos.html#cb1263-6"></a><span class="kw">mvrnorm</span>(n<span class="op">/</span><span class="st"> </span><span class="dv">2</span>, <span class="kw">c</span>(<span class="dv">55</span>, <span class="dv">55</span>), Sigma))</span></code></pre></div>
<p>Un diagrama de dispersión revela rápidamente que la correlación es alta y que hay dos grupos de gemelos, los adultos (puntos superiores derechos) y los niños (puntos inferiores izquierdos):</p>
<!--
<img src="libro_files/figure-html/simulated-twin-heights-1.png" width="50%" style="display: block; margin: auto;" />
-->
<p><img src="libro_files/figure-html/distance-illustration-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Nuestras características son <span class="math inline">\(N\)</span> puntos bidimensionales, las dos alturas y, con fines ilustrativos, actuaremos como si visualizar dos dimensiones fuera demasiado difícil. Por lo tanto, queremos reducir las dimensiones de dos a uno, pero aún así poder comprender características importantes de los datos, por ejemplo, que las observaciones se agrupan en dos grupos: adultos y niños.</p>
<p>Consideremos un desafío específico: queremos un resumen unidimensional de nuestros predictores a partir del cual podamos aproximar la distancia entre dos observaciones. En la figura anterior, mostramos la distancia entre la observación 1 y 2 (azul) y la observación 1 y 51 (rojo). Tenga en cuenta que la línea azul es más corta, lo que implica que 1 y 2 están más cerca.</p>
<p>Podemos calcular estas distancias usando <code>dist</code>:</p>
<div class="sourceCode" id="cb1264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1264-1"><a href="grandes-conjuntos-de-datos.html#cb1264-1"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(x)</span>
<span id="cb1264-2"><a href="grandes-conjuntos-de-datos.html#cb1264-2"></a><span class="kw">as.matrix</span>(d)[<span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb1264-3"><a href="grandes-conjuntos-de-datos.html#cb1264-3"></a><span class="co">#&gt; [1] 1.98</span></span>
<span id="cb1264-4"><a href="grandes-conjuntos-de-datos.html#cb1264-4"></a><span class="kw">as.matrix</span>(d)[<span class="dv">2</span>, <span class="dv">51</span>]</span>
<span id="cb1264-5"><a href="grandes-conjuntos-de-datos.html#cb1264-5"></a><span class="co">#&gt; [1] 18.7</span></span></code></pre></div>
<p>Esta distancia se basa en dos dimensiones y necesitamos una aproximación de distancia basada en una sola.</p>
<p>Comencemos con el enfoque ingenuo de simplemente eliminar una de las dos dimensiones. Comparemos las distancias reales con la distancia calculada solo con la primera dimensión:</p>
<div class="sourceCode" id="cb1265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1265-1"><a href="grandes-conjuntos-de-datos.html#cb1265-1"></a>z &lt;-<span class="st"> </span>x[,<span class="dv">1</span>]</span></code></pre></div>
<p>Aquí están las distancias aproximadas versus las distancias originales:
<img src="libro_files/figure-html/one-dim-approx-to-dist-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>La trama se ve casi igual si usamos la segunda dimensión. Obtenemos una subestimación general. Esto es de esperarse porque estamos agregando más cantidades positivas en el cálculo de la distancia a medida que aumentamos el número de dimensiones. Si en cambio usamos un promedio, como este</p>
<p><span class="math display">\[\sqrt{ \frac{1}{2} \sum_{j=1}^2 (X_{i,j}-X_{i,j})^2 },\]</span></p>
<p>entonces la subestimación desaparece. Dividimos la distancia por <span class="math inline">\(\sqrt{2}\)</span> para lograr la corrección.</p>
<p><img src="libro_files/figure-html/distance-approx-1-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>En realidad, esto funciona bastante bien y obtenemos una diferencia típica de:</p>
<div class="sourceCode" id="cb1266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1266-1"><a href="grandes-conjuntos-de-datos.html#cb1266-1"></a><span class="kw">sd</span>(<span class="kw">dist</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(z)<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>))</span>
<span id="cb1266-2"><a href="grandes-conjuntos-de-datos.html#cb1266-2"></a><span class="co">#&gt; [1] 1.21</span></span></code></pre></div>
<p>Ahora, ¿podemos elegir un resumen unidimensional que haga que esta aproximación sea aún mejor?</p>
<p>Si miramos hacia atrás al diagrama de dispersión anterior y visualizamos una línea entre cualquier par de puntos, la longitud de esta línea es la distancia entre los dos puntos. Estas líneas tienden a ir a lo largo de la dirección de la diagonal. Tenga en cuenta que si en su lugar trazamos la diferencia frente al promedio:</p>
<div class="sourceCode" id="cb1267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1267-1"><a href="grandes-conjuntos-de-datos.html#cb1267-1"></a>z &lt;-<span class="st"> </span><span class="kw">cbind</span>((x[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>x[,<span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span>, x[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>x[,<span class="dv">1</span>])</span></code></pre></div>
<p>podemos ver cómo la distancia entre puntos se explica principalmente por la primera dimensión: el promedio.</p>
<p><img src="libro_files/figure-html/rotation-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Esto significa que podemos ignorar la segunda dimensión y no perder demasiada información. Si la línea es completamente plana, no perdemos ninguna información. Usando la primera dimensión de esta matriz transformada obtenemos una aproximación aún mejor:</p>
<p><img src="libro_files/figure-html/distance-approx-2-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>con la diferencia típica mejorada en aproximadamente un 35%:</p>
<div class="sourceCode" id="cb1268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1268-1"><a href="grandes-conjuntos-de-datos.html#cb1268-1"></a><span class="kw">sd</span>(<span class="kw">dist</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(z[,<span class="dv">1</span>])<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">2</span>))</span>
<span id="cb1268-2"><a href="grandes-conjuntos-de-datos.html#cb1268-2"></a><span class="co">#&gt; [1] 0.315</span></span></code></pre></div>
<p>Más tarde aprendemos que <code>z[,1]</code> es el primer componente principal de la matriz <code>x</code>.</p>
</div>
<div id="transformaciones-lineales-avanzado" class="section level3">
<h3><span class="header-section-number">33.5.2</span> Transformaciones lineales (avanzado)</h3>
<p>Tenga en cuenta que cada fila de <span class="math inline">\(X\)</span> fue transformado usando una transformación lineal. Para cualquier fila <span class="math inline">\(i\)</span>, la primera entrada fue:</p>
<p><span class="math display">\[Z_{i,1} = a_{1,1} X_{i,1} + a_{2,1} X_{i,2}\]</span></p>
<p>con <span class="math inline">\(a_{1,1} = 0.5\)</span> y <span class="math inline">\(a_{2,1} = 0.5\)</span>.</p>
<p>La segunda entrada también fue una transformación lineal:</p>
<p><span class="math display">\[Z_{i,2} = a_{1,2} X_{i,1} + a_{2,2} X_{i,2}\]</span></p>
<p>con <span class="math inline">\(a_{1,2} = 1\)</span> y <span class="math inline">\(a_{2,2} = -1\)</span>.</p>
<p>También podemos usar la transformación lineal para obtener <span class="math inline">\(X\)</span> de regreso de <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[X_{i,1} = b_{1,1} Z_{i,1} + b_{2,1} Z_{i,2}\]</span></p>
<p>con <span class="math inline">\(b_{1,2} = 1\)</span> y <span class="math inline">\(b_{2,1} = 0.5\)</span> y</p>
<p><span class="math display">\[X_{i,2} = b_{2,1} Z_{i,1} + b_{2,2} Z_{i,2}\]</span></p>
<p>con <span class="math inline">\(b_{2,1} = 1\)</span> y <span class="math inline">\(a_{1,2} = -0.5\)</span>.</p>
<p>Si está familiarizado con el álgebra lineal, podemos escribir la operación que acabamos de realizar de esta manera:</p>
<p><span class="math display">\[
Z = X A
\mbox{ with }
A = \,
\begin{pmatrix}
1/2&amp;1\\
1/2&amp;-1\\
\end{pmatrix}.
\]</span></p>
<p>Y que podemos transformar de nuevo simplemente multiplicando por <span class="math inline">\(A^{-1}\)</span> como sigue:</p>
<p><span class="math display">\[
X = Z A^{-1}
\mbox{ with }
A^{-1} = \,
\begin{pmatrix}
1&amp;1\\
1/2&amp;-1/2\\
\end{pmatrix}.
\]</span></p>
<p>La reducción de dimensiones a menudo se puede describir como la aplicación de una transformación <span class="math inline">\(A\)</span> a una matriz <span class="math inline">\(X\)</span> con muchas columnas que mueven la información contenida en <span class="math inline">\(X\)</span> a las primeras columnas de <span class="math inline">\(Z=AX\)</span>, manteniendo solo estas pocas columnas informativas, reduciendo así la dimensión de los vectores contenidos en las filas.</p>
</div>
<div id="transformaciones-ortogonales-avanzado" class="section level3">
<h3><span class="header-section-number">33.5.3</span> Transformaciones ortogonales (avanzado)</h3>
<p>Tenga en cuenta que dividimos lo anterior por <span class="math inline">\(\sqrt{2}\)</span> para tener en cuenta las diferencias en las dimensiones al comparar una distancia de 2 dimensiones con una distancia de 1 dimensión. De hecho, podemos garantizar que las escalas de distancia sigan siendo las mismas si volvemos a escalar las columnas de <span class="math inline">\(A\)</span> para asegurar que la suma de cuadrados es 1</p>
<p><span class="math display">\[a_{1,1}^2 + a_{2,1}^2 = 1\mbox{ and } a_{1,2}^2 + a_{2,2}^2=1,\]</span></p>
<p>y que la correlación de las columnas es 0:</p>
<p><span class="math display">\[
a_{1,1} a_{1,2} + a_{2,1} a_{2,2} = 0.
\]</span></p>
<p>Recuerde que si las columnas están centradas para tener un promedio de 0, entonces la suma de los cuadrados es equivalente a la varianza o desviación estándar al cuadrado.</p>
<p>En nuestro ejemplo, para lograr la ortogonalidad, multiplicamos el primer set de coeficientes (primera columna de <span class="math inline">\(A\)</span>) por <span class="math inline">\(\sqrt{2}\)</span> y el segundo por <span class="math inline">\(1/\sqrt{2}\)</span>, entonces obtenemos la misma distancia exacta si usamos ambas dimensiones:</p>
<div class="sourceCode" id="cb1269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1269-1"><a href="grandes-conjuntos-de-datos.html#cb1269-1"></a>z[,<span class="dv">1</span>] &lt;-<span class="st"> </span>(x[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>x[,<span class="dv">2</span>])<span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb1269-2"><a href="grandes-conjuntos-de-datos.html#cb1269-2"></a>z[,<span class="dv">2</span>] &lt;-<span class="st"> </span>(x[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>x[,<span class="dv">1</span>])<span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>)</span></code></pre></div>
<p>Esto nos da una transformación que preserva la distancia entre dos puntos:</p>
<div class="sourceCode" id="cb1270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1270-1"><a href="grandes-conjuntos-de-datos.html#cb1270-1"></a><span class="kw">max</span>(<span class="kw">dist</span>(z) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(x))</span>
<span id="cb1270-2"><a href="grandes-conjuntos-de-datos.html#cb1270-2"></a><span class="co">#&gt; [1] 3.24e-14</span></span></code></pre></div>
<!--
<img src="libro_files/figure-html/orthogonal-transformation-1.png" width="70%" style="display: block; margin: auto;" />
-->
<p>y una aproximación mejorada si usamos solo la primera dimensión:</p>
<div class="sourceCode" id="cb1271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1271-1"><a href="grandes-conjuntos-de-datos.html#cb1271-1"></a><span class="kw">sd</span>(<span class="kw">dist</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">dist</span>(z[,<span class="dv">1</span>]))</span>
<span id="cb1271-2"><a href="grandes-conjuntos-de-datos.html#cb1271-2"></a><span class="co">#&gt; [1] 0.315</span></span></code></pre></div>
<p>En este caso <span class="math inline">\(Z\)</span> se llama rotación ortogonal de <span class="math inline">\(X\)</span>: conserva las distancias entre filas.</p>
<p>Tenga en cuenta que al usar la transformación anterior podemos resumir la distancia entre dos pares de gemelos con una sola dimensión. Por ejemplo, exploración de datos unidimensionales de la primera dimensión de <span class="math inline">\(Z\)</span> muestra claramente que hay dos grupos, adultos y niños:</p>
<div class="sourceCode" id="cb1272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1272-1"><a href="grandes-conjuntos-de-datos.html#cb1272-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1272-2"><a href="grandes-conjuntos-de-datos.html#cb1272-2"></a><span class="kw">qplot</span>(z[,<span class="dv">1</span>], <span class="dt">bins =</span> <span class="dv">20</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="libro_files/figure-html/twins-pc-1-hist-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Redujimos con éxito el número de dimensiones de dos a uno con muy poca pérdida de información.</p>
<p>La razón por la que pudimos hacer esto es porque las columnas de <span class="math inline">\(X\)</span> estaban muy correlacionados:</p>
<div class="sourceCode" id="cb1273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1273-1"><a href="grandes-conjuntos-de-datos.html#cb1273-1"></a><span class="kw">cor</span>(x[,<span class="dv">1</span>], x[,<span class="dv">2</span>])</span>
<span id="cb1273-2"><a href="grandes-conjuntos-de-datos.html#cb1273-2"></a><span class="co">#&gt; [1] 0.988</span></span></code></pre></div>
<p>y la transformación produjo columnas no correlacionadas con información “independiente” en cada columna:</p>
<div class="sourceCode" id="cb1274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1274-1"><a href="grandes-conjuntos-de-datos.html#cb1274-1"></a><span class="kw">cor</span>(z[,<span class="dv">1</span>], z[,<span class="dv">2</span>])</span>
<span id="cb1274-2"><a href="grandes-conjuntos-de-datos.html#cb1274-2"></a><span class="co">#&gt; [1] 0.0876</span></span></code></pre></div>
<p>Una forma en que esta información puede ser útil en una aplicación de <em>machinte learning</em> es que podemos reducir la complejidad de un modelo utilizando solo <span class="math inline">\(Z_1\)</span> en lugar de ambos <span class="math inline">\(X_1\)</span> y <span class="math inline">\(X_2\)</span>.</p>
<p>En realidad, es común obtener datos con varios predictores altamente correlacionados. En estos casos, PCA, que describimos a continuación, puede ser bastante útil para reducir la complejidad del modelo que se está ajustando.</p>
</div>
<div id="análisis-de-componentes-principales" class="section level3">
<h3><span class="header-section-number">33.5.4</span> Análisis de componentes principales</h3>
<p>En el cálculo anterior, la variabilidad total en nuestros datos puede definirse como la suma de la suma de los cuadrados de las columnas. Suponemos que las columnas están centradas, por lo que esta suma es equivalente a la suma de las varianzas de cada columna:</p>
<p><span class="math display">\[
v_1 + v_2, \mbox{ with } v_1 = \frac{1}{N}\sum_{i=1}^N X_{i,1}^2 \mbox{ and } v_2 = \frac{1}{N}\sum_{i=1}^N X_{i,2}^2
\]</span></p>
<p>Podemos calcular <span class="math inline">\(v_1\)</span> y <span class="math inline">\(v_2\)</span> utilizando:</p>
<div class="sourceCode" id="cb1275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1275-1"><a href="grandes-conjuntos-de-datos.html#cb1275-1"></a><span class="kw">colMeans</span>(x<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb1275-2"><a href="grandes-conjuntos-de-datos.html#cb1275-2"></a><span class="co">#&gt; [1] 3904 3902</span></span></code></pre></div>
<p>y podemos mostrar matemáticamente que si aplicamos una transformación ortogonal como la anterior, la variación total sigue siendo la misma:</p>
<div class="sourceCode" id="cb1276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1276-1"><a href="grandes-conjuntos-de-datos.html#cb1276-1"></a><span class="kw">sum</span>(<span class="kw">colMeans</span>(x<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb1276-2"><a href="grandes-conjuntos-de-datos.html#cb1276-2"></a><span class="co">#&gt; [1] 7806</span></span>
<span id="cb1276-3"><a href="grandes-conjuntos-de-datos.html#cb1276-3"></a><span class="kw">sum</span>(<span class="kw">colMeans</span>(z<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb1276-4"><a href="grandes-conjuntos-de-datos.html#cb1276-4"></a><span class="co">#&gt; [1] 7806</span></span></code></pre></div>
<p>Sin embargo, mientras que la variabilidad en las dos columnas de <code>X</code> es casi lo mismo, en la versión transformada <span class="math inline">\(Z\)</span> el 99% de la variabilidad se incluye solo en la primera dimensión:</p>
<div class="sourceCode" id="cb1277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1277-1"><a href="grandes-conjuntos-de-datos.html#cb1277-1"></a>v &lt;-<span class="st"> </span><span class="kw">colMeans</span>(z<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb1277-2"><a href="grandes-conjuntos-de-datos.html#cb1277-2"></a>v<span class="op">/</span><span class="kw">sum</span>(v)</span>
<span id="cb1277-3"><a href="grandes-conjuntos-de-datos.html#cb1277-3"></a><span class="co">#&gt; [1] 1.00e+00 9.93e-05</span></span></code></pre></div>
<p>El <em>primer componente principal (PC) </em> de una matriz <span class="math inline">\(X\)</span> es la transformación ortogonal lineal de <span class="math inline">\(X\)</span> eso maximiza esta variabilidad. La función <code>prcomp</code> proporciona esta información:</p>
<div class="sourceCode" id="cb1278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1278-1"><a href="grandes-conjuntos-de-datos.html#cb1278-1"></a>pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(x)</span>
<span id="cb1278-2"><a href="grandes-conjuntos-de-datos.html#cb1278-2"></a>pca<span class="op">$</span>rotation</span>
<span id="cb1278-3"><a href="grandes-conjuntos-de-datos.html#cb1278-3"></a><span class="co">#&gt;         PC1    PC2</span></span>
<span id="cb1278-4"><a href="grandes-conjuntos-de-datos.html#cb1278-4"></a><span class="co">#&gt; [1,] -0.702  0.712</span></span>
<span id="cb1278-5"><a href="grandes-conjuntos-de-datos.html#cb1278-5"></a><span class="co">#&gt; [2,] -0.712 -0.702</span></span></code></pre></div>
<p>Tenga en cuenta que la primera PC es casi la misma que la proporcionada por el <span class="math inline">\((X_1 + X_2)/ \sqrt{2}\)</span> utilizamos anteriormente (excepto quizás por un cambio de signo que es arbitrario).</p>
<p>La función PCA devuelve la rotación necesaria para transformar <span class="math inline">\(X\)</span> para que la variabilidad de las columnas disminuya de más variable a menos (se accede con <code>$rotation</code>) as well as the resulting new matrix (accessed with <code>$ x</code>). Por defecto las columnas de <span class="math inline">\(X\)</span> están primero centrados.</p>
<p>Entonces, usando la multiplicación matricial que se muestra arriba, tenemos que lo siguiente es lo mismo (demostrado por una diferencia entre elementos esencialmente cero):</p>
<div class="sourceCode" id="cb1279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1279-1"><a href="grandes-conjuntos-de-datos.html#cb1279-1"></a>a &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x))</span>
<span id="cb1279-2"><a href="grandes-conjuntos-de-datos.html#cb1279-2"></a>b &lt;-<span class="st"> </span>pca<span class="op">$</span>x <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(pca<span class="op">$</span>rotation)</span>
<span id="cb1279-3"><a href="grandes-conjuntos-de-datos.html#cb1279-3"></a><span class="kw">max</span>(<span class="kw">abs</span>(a <span class="op">-</span><span class="st"> </span>b))</span>
<span id="cb1279-4"><a href="grandes-conjuntos-de-datos.html#cb1279-4"></a><span class="co">#&gt; [1] 3.55e-15</span></span></code></pre></div>
<p>La rotación es ortogonal, lo que significa que lo inverso es su transposición. Entonces también tenemos que estos dos son idénticos:</p>
<div class="sourceCode" id="cb1280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1280-1"><a href="grandes-conjuntos-de-datos.html#cb1280-1"></a>a &lt;-<span class="st"> </span><span class="kw">sweep</span>(x, <span class="dv">2</span>, <span class="kw">colMeans</span>(x)) <span class="op">%*%</span><span class="st"> </span>pca<span class="op">$</span>rotation</span>
<span id="cb1280-2"><a href="grandes-conjuntos-de-datos.html#cb1280-2"></a>b &lt;-<span class="st"> </span>pca<span class="op">$</span>x</span>
<span id="cb1280-3"><a href="grandes-conjuntos-de-datos.html#cb1280-3"></a><span class="kw">max</span>(<span class="kw">abs</span>(a <span class="op">-</span><span class="st"> </span>b))</span>
<span id="cb1280-4"><a href="grandes-conjuntos-de-datos.html#cb1280-4"></a><span class="co">#&gt; [1] 0</span></span></code></pre></div>
<p>Podemos visualizarlos para ver cómo el primer componente resume los datos. En el gráfico a continuación, el rojo representa valores altos y valores negativos azules (más adelante aprendemos por qué llamamos a estos pesos y patrones):</p>
<p><img src="libro_files/figure-html/illustrate-pca-twin-heights-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Resulta que podemos encontrar esta transformación lineal no solo para dos dimensiones sino también para matrices de cualquier dimensión <span class="math inline">\(p\)</span>.</p>
<p>Para una matriz multidimensional con <span class="math inline">\(X\)</span> con <span class="math inline">\(p\)</span> columnas, podemos encontrar una transformación que crea <span class="math inline">\(Z\)</span> que conserva la distancia entre filas, pero con la varianza de las columnas en orden decreciente. La segunda columna es el segundo componente principal, la tercera columna es el tercer componente principal, y así sucesivamente. Como en nuestro ejemplo, si después de un cierto número de columnas, digamos <span class="math inline">\(k\)</span>, las variaciones de las columnas de <span class="math inline">\(Z_j\)</span>, <span class="math inline">\(j&gt;k\)</span> son muy pequeños, significa que estas dimensiones tienen poco que contribuir a la distancia y podemos aproximar la distancia entre dos puntos con solo <span class="math inline">\(k\)</span> dimensiones. Si <span class="math inline">\(k\)</span> es mucho más pequeño que <span class="math inline">\(p\)</span>, entonces podemos lograr un resumen muy eficiente de nuestros datos.</p>
</div>
<div id="ejemplo-de-iris" class="section level3">
<h3><span class="header-section-number">33.5.5</span> Ejemplo de Iris</h3>
<p>Los datos del iris son un ejemplo ampliamente utilizado en los cursos de análisis de datos. Incluye cuatro medidas botánicas relacionadas con tres especies de flores:</p>
<div class="sourceCode" id="cb1281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1281-1"><a href="grandes-conjuntos-de-datos.html#cb1281-1"></a><span class="kw">names</span>(iris)</span>
<span id="cb1281-2"><a href="grandes-conjuntos-de-datos.html#cb1281-2"></a><span class="co">#&gt; [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; </span></span>
<span id="cb1281-3"><a href="grandes-conjuntos-de-datos.html#cb1281-3"></a><span class="co">#&gt; [5] &quot;Species&quot;</span></span></code></pre></div>
<p>Si imprime <code>iris$Species</code> verá que los datos están ordenados por especie.</p>
<p>Calculemos la distancia entre cada observación. Puede ver claramente las tres especies con una especie muy diferente de las otras dos:</p>
<div class="sourceCode" id="cb1282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1282-1"><a href="grandes-conjuntos-de-datos.html#cb1282-1"></a>x &lt;-<span class="st"> </span>iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()</span>
<span id="cb1282-2"><a href="grandes-conjuntos-de-datos.html#cb1282-2"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(x)</span>
<span id="cb1282-3"><a href="grandes-conjuntos-de-datos.html#cb1282-3"></a><span class="kw">image</span>(<span class="kw">as.matrix</span>(d), <span class="dt">col =</span> <span class="kw">rev</span>(RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dv">9</span>, <span class="st">&quot;RdBu&quot;</span>)))</span></code></pre></div>
<p><img src="libro_files/figure-html/iris-distances-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Nuestros predictores aquí tienen cuatro dimensiones, pero tres están muy correlacionadas:</p>
<div class="sourceCode" id="cb1283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1283-1"><a href="grandes-conjuntos-de-datos.html#cb1283-1"></a><span class="kw">cor</span>(x)</span>
<span id="cb1283-2"><a href="grandes-conjuntos-de-datos.html#cb1283-2"></a><span class="co">#&gt;              Sepal.Length Sepal.Width Petal.Length Petal.Width</span></span>
<span id="cb1283-3"><a href="grandes-conjuntos-de-datos.html#cb1283-3"></a><span class="co">#&gt; Sepal.Length        1.000      -0.118        0.872       0.818</span></span>
<span id="cb1283-4"><a href="grandes-conjuntos-de-datos.html#cb1283-4"></a><span class="co">#&gt; Sepal.Width        -0.118       1.000       -0.428      -0.366</span></span>
<span id="cb1283-5"><a href="grandes-conjuntos-de-datos.html#cb1283-5"></a><span class="co">#&gt; Petal.Length        0.872      -0.428        1.000       0.963</span></span>
<span id="cb1283-6"><a href="grandes-conjuntos-de-datos.html#cb1283-6"></a><span class="co">#&gt; Petal.Width         0.818      -0.366        0.963       1.000</span></span></code></pre></div>
<p>Si aplicamos PCA, deberíamos poder aproximar esta distancia con solo dos dimensiones, comprimiendo las dimensiones altamente correlacionadas. Utilizando la <code>summary</code> función podemos ver la variabilidad explicada por cada PC:</p>
<div class="sourceCode" id="cb1284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1284-1"><a href="grandes-conjuntos-de-datos.html#cb1284-1"></a>pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(x)</span>
<span id="cb1284-2"><a href="grandes-conjuntos-de-datos.html#cb1284-2"></a><span class="kw">summary</span>(pca)</span>
<span id="cb1284-3"><a href="grandes-conjuntos-de-datos.html#cb1284-3"></a><span class="co">#&gt; Importance of components:</span></span>
<span id="cb1284-4"><a href="grandes-conjuntos-de-datos.html#cb1284-4"></a><span class="co">#&gt;                          PC1    PC2    PC3     PC4</span></span>
<span id="cb1284-5"><a href="grandes-conjuntos-de-datos.html#cb1284-5"></a><span class="co">#&gt; Standard deviation     2.056 0.4926 0.2797 0.15439</span></span>
<span id="cb1284-6"><a href="grandes-conjuntos-de-datos.html#cb1284-6"></a><span class="co">#&gt; Proportion of Variance 0.925 0.0531 0.0171 0.00521</span></span>
<span id="cb1284-7"><a href="grandes-conjuntos-de-datos.html#cb1284-7"></a><span class="co">#&gt; Cumulative Proportion  0.925 0.9777 0.9948 1.00000</span></span></code></pre></div>
<p>Las dos primeras dimensiones representan el 97% de la variabilidad. Por lo tanto, deberíamos poder aproximar muy bien la distancia con dos dimensiones. Podemos visualizar los resultados de PCA:</p>
<p><img src="libro_files/figure-html/illustrate-pca-twin-heights-iris-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Y vea que el primer patrón es la longitud del sépalo, la longitud del pétalo y el ancho del pétalo (rojo) en una dirección y el ancho del sépalo (azul) en la otra. El segundo patrón es la longitud del sépalo y el ancho del pétalo en una dirección (azul) y la longitud y el ancho del pétalo en la otra (rojo). Puede ver en los pesos que la primera PC1 controla la mayor parte de la variabilidad y separa claramente el primer tercio de las muestras (setosa) de los dos tercios (versicolor y virginica). Si observa la segunda columna de las pesas, observa que separa algo versicolor (rojo) de virginica (azul).</p>
<p>Podemos ver esto mejor al trazar las dos primeras PC con el color que representa la especie:</p>
<div class="sourceCode" id="cb1285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1285-1"><a href="grandes-conjuntos-de-datos.html#cb1285-1"></a><span class="kw">data.frame</span>(pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dt">Species=</span>iris<span class="op">$</span>Species) <span class="op">%&gt;%</span></span>
<span id="cb1285-2"><a href="grandes-conjuntos-de-datos.html#cb1285-2"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(PC1,PC2, <span class="dt">fill =</span> Species))<span class="op">+</span></span>
<span id="cb1285-3"><a href="grandes-conjuntos-de-datos.html#cb1285-3"></a><span class="kw">geom_point</span>(<span class="dt">cex=</span><span class="dv">3</span>, <span class="dt">pch=</span><span class="dv">21</span>) <span class="op">+</span></span>
<span id="cb1285-4"><a href="grandes-conjuntos-de-datos.html#cb1285-4"></a><span class="kw">coord_fixed</span>(<span class="dt">ratio =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/iris-pca-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Vemos que las dos primeras dimensiones preservan la distancia:</p>
<div class="sourceCode" id="cb1286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1286-1"><a href="grandes-conjuntos-de-datos.html#cb1286-1"></a>d_approx &lt;-<span class="st"> </span><span class="kw">dist</span>(pca<span class="op">$</span>x[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>])</span>
<span id="cb1286-2"><a href="grandes-conjuntos-de-datos.html#cb1286-2"></a><span class="kw">qplot</span>(d, d_approx) <span class="op">+</span><span class="st"> </span><span class="kw">geom_abline</span>(<span class="dt">color=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/dist-approx-4-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>Este ejemplo es más realista que el primer ejemplo artificial que utilizamos, ya que mostramos cómo podemos visualizar los datos usando dos dimensiones cuando los datos eran de cuatro dimensiones.</p>
</div>
<div id="ejemplo-de-mnist" class="section level3">
<h3><span class="header-section-number">33.5.6</span> Ejemplo de MNIST</h3>
<p>El ejemplo de dígitos escritos tiene 784 características. ¿Hay espacio para la reducción de datos? ¿Podemos crear algoritmos sencillos de <em>machinte learning</em> con menos funciones?</p>
<p>Carguemos los datos:</p>
<div class="sourceCode" id="cb1287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1287-1"><a href="grandes-conjuntos-de-datos.html#cb1287-1"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1287-2"><a href="grandes-conjuntos-de-datos.html#cb1287-2"></a><span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()</span></code></pre></div>
<p>Debido a que los píxeles son tan pequeños, esperamos que los píxeles cercanos entre sí en la cuadrícula estén correlacionados, lo que significa que la reducción de dimensión debería ser posible.</p>
<p>Probemos PCA y exploremos la variación de las PC. Esto tomará unos segundos ya que es una matriz bastante grande.</p>
<div class="sourceCode" id="cb1288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1288-1"><a href="grandes-conjuntos-de-datos.html#cb1288-1"></a>col_means &lt;-<span class="st"> </span><span class="kw">colMeans</span>(mnist<span class="op">$</span>test<span class="op">$</span>images)</span>
<span id="cb1288-2"><a href="grandes-conjuntos-de-datos.html#cb1288-2"></a>pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(mnist<span class="op">$</span>train<span class="op">$</span>images)</span></code></pre></div>
<div class="sourceCode" id="cb1289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1289-1"><a href="grandes-conjuntos-de-datos.html#cb1289-1"></a>pc &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(mnist<span class="op">$</span>test<span class="op">$</span>images)</span>
<span id="cb1289-2"><a href="grandes-conjuntos-de-datos.html#cb1289-2"></a><span class="kw">qplot</span>(pc, pca<span class="op">$</span>sdev)</span></code></pre></div>
<p><img src="libro_files/figure-html/mnist-pca-variance-explained-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Podemos ver que las primeras PC ya explican un gran porcentaje de la variabilidad:</p>
<div class="sourceCode" id="cb1290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1290-1"><a href="grandes-conjuntos-de-datos.html#cb1290-1"></a><span class="kw">summary</span>(pca)<span class="op">$</span>importance[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</span>
<span id="cb1290-2"><a href="grandes-conjuntos-de-datos.html#cb1290-2"></a><span class="co">#&gt;                            PC1     PC2      PC3      PC4      PC5</span></span>
<span id="cb1290-3"><a href="grandes-conjuntos-de-datos.html#cb1290-3"></a><span class="co">#&gt; Standard deviation     576.823 493.238 459.8993 429.8562 408.5668</span></span>
<span id="cb1290-4"><a href="grandes-conjuntos-de-datos.html#cb1290-4"></a><span class="co">#&gt; Proportion of Variance   0.097   0.071   0.0617   0.0539   0.0487</span></span>
<span id="cb1290-5"><a href="grandes-conjuntos-de-datos.html#cb1290-5"></a><span class="co">#&gt; Cumulative Proportion    0.097   0.168   0.2297   0.2836   0.3323</span></span></code></pre></div>
<p>Y con solo mirar las dos primeras PC vemos información sobre la clase. Aquí hay una muestra aleatoria de 2,000 dígitos:</p>
<div class="sourceCode" id="cb1291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1291-1"><a href="grandes-conjuntos-de-datos.html#cb1291-1"></a><span class="kw">data.frame</span>(<span class="dt">PC1 =</span> pca<span class="op">$</span>x[,<span class="dv">1</span>], <span class="dt">PC2 =</span> pca<span class="op">$</span>x[,<span class="dv">2</span>],</span>
<span id="cb1291-2"><a href="grandes-conjuntos-de-datos.html#cb1291-2"></a><span class="dt">label=</span><span class="kw">factor</span>(mnist<span class="op">$</span>train<span class="op">$</span>label)) <span class="op">%&gt;%</span></span>
<span id="cb1291-3"><a href="grandes-conjuntos-de-datos.html#cb1291-3"></a><span class="kw">sample_n</span>(<span class="dv">2000</span>) <span class="op">%&gt;%</span></span>
<span id="cb1291-4"><a href="grandes-conjuntos-de-datos.html#cb1291-4"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(PC1, PC2, <span class="dt">fill=</span>label))<span class="op">+</span></span>
<span id="cb1291-5"><a href="grandes-conjuntos-de-datos.html#cb1291-5"></a><span class="kw">geom_point</span>(<span class="dt">cex=</span><span class="dv">3</span>, <span class="dt">pch=</span><span class="dv">21</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/mnist-pca-1-2-scatter-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>También podemos ver las combinaciones lineales en la cuadrícula para tener una idea de lo que se está ponderando:</p>
<p><img src="libro_files/figure-html/mnist-pca-1-4-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Las PC de menor varianza aparecen relacionadas con la variabilidad sin importancia en las esquinas:</p>
<p><img src="libro_files/figure-html/mnist-pca-last,-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Ahora apliquemos la transformación que aprendimos con los datos de entrenamiento a los datos de prueba, reduzca la dimensión y ejecute knn en solo un pequeño número de dimensiones.</p>
<p>Intentamos 36 dimensiones ya que esto explica aproximadamente el 80% de los datos. Primero ajuste el modelo:</p>
<div class="sourceCode" id="cb1292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1292-1"><a href="grandes-conjuntos-de-datos.html#cb1292-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1292-2"><a href="grandes-conjuntos-de-datos.html#cb1292-2"></a>k &lt;-<span class="st"> </span><span class="dv">36</span></span>
<span id="cb1292-3"><a href="grandes-conjuntos-de-datos.html#cb1292-3"></a>x_train &lt;-<span class="st"> </span>pca<span class="op">$</span>x[,<span class="dv">1</span><span class="op">:</span>k]</span>
<span id="cb1292-4"><a href="grandes-conjuntos-de-datos.html#cb1292-4"></a>y &lt;-<span class="st"> </span><span class="kw">factor</span>(mnist<span class="op">$</span>train<span class="op">$</span>labels)</span>
<span id="cb1292-5"><a href="grandes-conjuntos-de-datos.html#cb1292-5"></a>fit &lt;-<span class="st"> </span><span class="kw">knn3</span>(x_train, y)</span></code></pre></div>
<p>Ahora transforma el set de evaluación:</p>
<div class="sourceCode" id="cb1293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1293-1"><a href="grandes-conjuntos-de-datos.html#cb1293-1"></a>x_test &lt;-<span class="st"> </span><span class="kw">sweep</span>(mnist<span class="op">$</span>test<span class="op">$</span>images, <span class="dv">2</span>, col_means) <span class="op">%*%</span><span class="st"> </span>pca<span class="op">$</span>rotation</span>
<span id="cb1293-2"><a href="grandes-conjuntos-de-datos.html#cb1293-2"></a>x_test &lt;-<span class="st"> </span>x_test[,<span class="dv">1</span><span class="op">:</span>k]</span></code></pre></div>
<p>Y estamos listos para predecir y ver cómo lo hacemos:</p>
<div class="sourceCode" id="cb1294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1294-1"><a href="grandes-conjuntos-de-datos.html#cb1294-1"></a>y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, x_test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb1294-2"><a href="grandes-conjuntos-de-datos.html#cb1294-2"></a><span class="kw">confusionMatrix</span>(y_hat, <span class="kw">factor</span>(mnist<span class="op">$</span>test<span class="op">$</span>labels))<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb1294-3"><a href="grandes-conjuntos-de-datos.html#cb1294-3"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb1294-4"><a href="grandes-conjuntos-de-datos.html#cb1294-4"></a><span class="co">#&gt;    0.975</span></span></code></pre></div>
<p>Con solo 36 dimensiones, obtenemos una precisión muy superior a 0,95.</p>
</div>
</div>
<div id="ejercicios-57" class="section level2">
<h2><span class="header-section-number">33.6</span> Ejercicios</h2>
<p>1. Queremos explorar el <code>tissue_gene_expression</code> predictores al trazarlos.</p>
<div class="sourceCode" id="cb1295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1295-1"><a href="grandes-conjuntos-de-datos.html#cb1295-1"></a><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)</span>
<span id="cb1295-2"><a href="grandes-conjuntos-de-datos.html#cb1295-2"></a><span class="kw">dim</span>(tissue_gene_expression<span class="op">$</span>x)</span></code></pre></div>
<p>Queremos tener una idea de qué observaciones son cercanas entre sí, pero
los predictores son de 500 dimensiones, por lo que es difícil trazar. Trace los dos primeros componentes principales con un color que represente el tipo de tejido.</p>
<p>2. Los predictores para cada observación se miden en el mismo dispositivo y procedimiento experimental. Esto introduce sesgos que pueden afectar a todos los predictores de una observación. Para cada observación, calcule el promedio en todos los predictores y luego grafique esto contra la primera PC con el color que representa el tejido. Informe la correlación.</p>
<p>3. Vemos una asociación con la primera PC y los promedios de observación. Vuelva a hacer la PCA pero solo después de quitar el centro.</p>
<p>4. Para las primeras 10 PC, haga un diagrama de caja que muestre los valores para cada tejido.</p>
<p>5. Trace el porcentaje de varianza explicado por el número de PC. Sugerencia: use el <code>summary</code> función.</p>

</div>
<div id="sistemas-de-recomendación" class="section level2">
<h2><span class="header-section-number">33.7</span> Sistemas de recomendación</h2>
<p>Los sistemas de recomendación utilizan clasificaciones que <em>users_han dado_items</em> para hacer recomendaciones específicas. Las compañías que venden muchos productos a muchos clientes y permiten que estos clientes califiquen sus productos, como Amazon, pueden recopilar conjuntos de datos masivos que pueden usarse para predecir qué calificación otorgará un usuario en particular a un artículo específico. Los elementos para los que se predice una calificación alta para un usuario determinado se recomiendan a ese usuario.</p>
<p>Netflix utiliza un sistema de recomendación para predecir cuántas <em>estrellas</em> dará un usuario a una película específica. Una estrella sugiere que no es una buena película, mientras que cinco estrellas sugiere que es una película excelente. Aquí, proporcionamos los conceptos básicos de cómo se hacen estas recomendaciones, motivados por algunos de los enfoques adoptados por los ganadores de los <em>Netflix challenge</em>.</p>
<p>En octubre de 2006, Netflix ofreció un desafío a la comunidad de ciencia de datos: mejorar nuestro algoritmo de recomendación en un 10% y ganar un millón de dólares. En septiembre de 2009
los ganadores fueron anunciados<a href="#fn115" class="footnote-ref" id="fnref115"><sup>115</sup></a>. Puede leer un buen resumen de cómo se creó el algoritmo ganador aquí: [<a href="http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/font" class="uri">http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/font</a>&gt;(<a href="http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/" class="uri">http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/</a>)
y una explicación más detallada aquí:
[<a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdffont" class="uri">http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdffont</a>&gt;(<a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf" class="uri">http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf</a>). Ahora le mostraremos algunas de las estrategias de análisis de datos utilizadas por el equipo ganador.</p>
<div id="datos-de-lente-de-película" class="section level3">
<h3><span class="header-section-number">33.7.1</span> Datos de lente de película</h3>
<p>Los datos de Netflix no están disponibles públicamente, pero el laboratorio de investigación de GroupLens<a href="#fn116" class="footnote-ref" id="fnref116"><sup>116</sup></a> generó su propia base de datos con más de 20 millones de calificaciones para más de 27,000 películas de más de 138,000 usuarios. Ponemos a disposición un pequeño subconjunto de estos datos a través del paquete <strong>dslabs</strong>:</p>
<div class="sourceCode" id="cb1296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1296-1"><a href="grandes-conjuntos-de-datos.html#cb1296-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb1296-2"><a href="grandes-conjuntos-de-datos.html#cb1296-2"></a><span class="kw">library</span>(dslabs)</span>
<span id="cb1296-3"><a href="grandes-conjuntos-de-datos.html#cb1296-3"></a><span class="kw">data</span>(<span class="st">&quot;movielens&quot;</span>)</span></code></pre></div>
<p>Podemos ver que esta tabla está en formato ordenado con miles de filas:</p>
<div class="sourceCode" id="cb1297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1297-1"><a href="grandes-conjuntos-de-datos.html#cb1297-1"></a>movielens <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tibble</span>()</span>
<span id="cb1297-2"><a href="grandes-conjuntos-de-datos.html#cb1297-2"></a><span class="co">#&gt; # A tibble: 100,004 x 7</span></span>
<span id="cb1297-3"><a href="grandes-conjuntos-de-datos.html#cb1297-3"></a><span class="co">#&gt;   movieId title              year genres         userId rating timestamp</span></span>
<span id="cb1297-4"><a href="grandes-conjuntos-de-datos.html#cb1297-4"></a><span class="co">#&gt;     &lt;int&gt; &lt;chr&gt;             &lt;int&gt; &lt;fct&gt;           &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;</span></span>
<span id="cb1297-5"><a href="grandes-conjuntos-de-datos.html#cb1297-5"></a><span class="co">#&gt; 1      31 Dangerous Minds    1995 Drama               1    2.5    1.26e9</span></span>
<span id="cb1297-6"><a href="grandes-conjuntos-de-datos.html#cb1297-6"></a><span class="co">#&gt; 2    1029 Dumbo              1941 Animation|Chi…      1    3      1.26e9</span></span>
<span id="cb1297-7"><a href="grandes-conjuntos-de-datos.html#cb1297-7"></a><span class="co">#&gt; 3    1061 Sleepers           1996 Thriller            1    3      1.26e9</span></span>
<span id="cb1297-8"><a href="grandes-conjuntos-de-datos.html#cb1297-8"></a><span class="co">#&gt; 4    1129 Escape from New …  1981 Action|Advent…      1    2      1.26e9</span></span>
<span id="cb1297-9"><a href="grandes-conjuntos-de-datos.html#cb1297-9"></a><span class="co">#&gt; 5    1172 Cinema Paradiso …  1989 Drama               1    4      1.26e9</span></span>
<span id="cb1297-10"><a href="grandes-conjuntos-de-datos.html#cb1297-10"></a><span class="co">#&gt; # … with 99,999 more rows</span></span></code></pre></div>
<p>Cada fila representa una calificación dada por un usuario a una película.</p>
<p>Podemos ver la cantidad de usuarios únicos que proporcionaron calificaciones y cuántas películas únicas fueron calificadas:</p>
<div class="sourceCode" id="cb1298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1298-1"><a href="grandes-conjuntos-de-datos.html#cb1298-1"></a>movielens <span class="op">%&gt;%</span></span>
<span id="cb1298-2"><a href="grandes-conjuntos-de-datos.html#cb1298-2"></a><span class="kw">summarize</span>(<span class="dt">n_users =</span> <span class="kw">n_distinct</span>(userId),</span>
<span id="cb1298-3"><a href="grandes-conjuntos-de-datos.html#cb1298-3"></a><span class="dt">n_movies =</span> <span class="kw">n_distinct</span>(movieId))</span>
<span id="cb1298-4"><a href="grandes-conjuntos-de-datos.html#cb1298-4"></a><span class="co">#&gt;   n_users n_movies</span></span>
<span id="cb1298-5"><a href="grandes-conjuntos-de-datos.html#cb1298-5"></a><span class="co">#&gt; 1     671     9066</span></span></code></pre></div>
<p>Si multiplicamos esos dos números, obtenemos un número mayor de 5 millones, sin embargo, nuestra tabla de datos tiene aproximadamente 100,000 filas. Esto implica que no todos los usuarios calificaron todas las películas. Por lo tanto, podemos pensar en estos datos como una matriz muy grande, con usuarios en las filas y películas en las columnas, con muchas celdas vacías. los <code>gather</code> la función nos permite convertirlo a este formato, pero si lo probamos para toda la matriz, se bloqueará R. Vamos a mostrar la matriz para siete usuarios y cuatro películas.</p>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
userId
</th>
<th style="text-align:right;">
Forrest Gump
</th>
<th style="text-align:right;">
Pulp Fiction
</th>
<th style="text-align:right;">
Shawshank Redemption
</th>
<th style="text-align:right;">
Silence of the Lambs
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
3.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
5.0
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
NA
</td>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
2.5
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
4.5
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
4.0
</td>
<td style="text-align:right;">
3.0
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
4.5
</td>
<td style="text-align:right;">
0.5
</td>
</tr>
</tbody>
</table>
<p>Puede pensar en la tarea de un sistema de recomendación como completar el <code>NA</code> s en la tabla de arriba. Para ver qué tan dispersa es la matriz, aquí está la matriz para una muestra aleatoria de 100 películas y 100 usuarios con amarillo que indica una combinación de usuario/ película para la que tenemos una calificación.</p>
<p><img src="libro_files/figure-html/sparsity-of-movie-recs-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>Este desafío de aprendizaje automático es más complicado que lo que hemos estudiado hasta ahora porque cada resultado <span class="math inline">\(Y\)</span> tiene un conjunto diferente de predictores. Para ver esto, tenga en cuenta que si estamos prediciendo la calificación de la película <span class="math inline">\(i\)</span> por usuario <span class="math inline">\(u\)</span>, en principio, todas las otras clasificaciones relacionadas con la película <span class="math inline">\(i\)</span> y por usuario <span class="math inline">\(u\)</span> pueden usarse como predictores, pero diferentes usuarios califican diferentes películas y un número diferente de películas. Además, podemos usar información de otras películas que hemos determinado que son similares a las películas. <span class="math inline">\(i\)</span> o de usuarios que se consideran similares al usuario <span class="math inline">\(u\)</span>. En esencia, toda la matriz se puede utilizar como predictores para cada celda.</p>
<p>Veamos algunas de las propiedades generales de los datos para comprender mejor los desafíos.</p>
<p>Lo primero que notamos es que algunas películas se califican más que otras. A continuación se muestra la distribución. Esto no debería sorprendernos dado que hay películas de gran éxito vistas por millones y películas artísticas e independientes vistas por unos pocos. Nuestra segunda observación es que algunos usuarios son más activos que otros en la calificación de películas:</p>
<p><img src="libro_files/figure-html/movie-id-and-user-hists-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="sistemas-de-recomendación-como-un-desafío-de-aprendizaje-automático" class="section level3">
<h3><span class="header-section-number">33.7.2</span> Sistemas de recomendación como un desafío de aprendizaje automático</h3>
<p>Para ver cómo se trata de un tipo de aprendizaje automático, tenga en cuenta que necesitamos construir un algoritmo con los datos que hemos recopilado que luego se aplicarán fuera de nuestro control, a medida que los usuarios busquen recomendaciones de películas. Así que creemos un conjunto de pruebas para evaluar la precisión de los modelos que implementamos.</p>
<div class="sourceCode" id="cb1299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1299-1"><a href="grandes-conjuntos-de-datos.html#cb1299-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb1299-2"><a href="grandes-conjuntos-de-datos.html#cb1299-2"></a><span class="kw">set.seed</span>(<span class="dv">755</span>)</span>
<span id="cb1299-3"><a href="grandes-conjuntos-de-datos.html#cb1299-3"></a>test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> movielens<span class="op">$</span>rating, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.2</span>,</span>
<span id="cb1299-4"><a href="grandes-conjuntos-de-datos.html#cb1299-4"></a><span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb1299-5"><a href="grandes-conjuntos-de-datos.html#cb1299-5"></a>train_set &lt;-<span class="st"> </span>movielens[<span class="op">-</span>test_index,]</span>
<span id="cb1299-6"><a href="grandes-conjuntos-de-datos.html#cb1299-6"></a>test_set &lt;-<span class="st"> </span>movielens[test_index,]</span></code></pre></div>
<p>Para asegurarnos de que no incluimos usuarios y películas en el conjunto de prueba que no aparecen en el conjunto de entrenamiento, eliminamos estas entradas usando el <code>semi_join</code> función:</p>
<div class="sourceCode" id="cb1300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1300-1"><a href="grandes-conjuntos-de-datos.html#cb1300-1"></a>test_set &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span></span>
<span id="cb1300-2"><a href="grandes-conjuntos-de-datos.html#cb1300-2"></a><span class="kw">semi_join</span>(train_set, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1300-3"><a href="grandes-conjuntos-de-datos.html#cb1300-3"></a><span class="kw">semi_join</span>(train_set, <span class="dt">by =</span> <span class="st">&quot;userId&quot;</span>)</span></code></pre></div>
</div>
<div id="netflix-loss-function" class="section level3">
<h3><span class="header-section-number">33.7.3</span> Función de pérdida</h3>
<p>El desafío de Netflix usó la pérdida de error típica: decidieron un ganador basado en el error cuadrático medio residual (RMSE) en un conjunto de prueba. Definimos <span class="math inline">\(y_{u,i}\)</span> como la calificación de la película <span class="math inline">\(i\)</span> por usuario <span class="math inline">\(u\)</span> y denotar nuestra predicción con <span class="math inline">\(\hat{y}_{u,i}\)</span>. El RMSE se define entonces como:</p>
<p><span class="math display">\[
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
\]</span>
con <span class="math inline">\(N\)</span> es el número de combinaciones de usuario/ película y la suma que ocurre en todas estas combinaciones.</p>
<p>Recuerde que podemos interpretar el RMSE de manera similar a una desviación estándar: es el error típico que cometemos al predecir una calificación de película. Si este número es mayor que 1, significa que nuestro error típico es mayor que una estrella, lo cual no es bueno.</p>
<p>Escribamos una función que calcule el RMSE para vectores de clasificaciones y sus predictores correspondientes:</p>
<div class="sourceCode" id="cb1301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1301-1"><a href="grandes-conjuntos-de-datos.html#cb1301-1"></a>RMSE &lt;-<span class="st"> </span><span class="cf">function</span>(true_ratings, predicted_ratings){</span>
<span id="cb1301-2"><a href="grandes-conjuntos-de-datos.html#cb1301-2"></a><span class="kw">sqrt</span>(<span class="kw">mean</span>((true_ratings <span class="op">-</span><span class="st"> </span>predicted_ratings)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb1301-3"><a href="grandes-conjuntos-de-datos.html#cb1301-3"></a>}</span></code></pre></div>
</div>
<div id="un-primer-modelo" class="section level3">
<h3><span class="header-section-number">33.7.4</span> Un primer modelo</h3>
<p>Comencemos construyendo el sistema de recomendación más simple posible: predecimos la misma calificación para todas las películas, independientemente del usuario. ¿Qué número debería ser esta predicción? Podemos usar un enfoque basado en modelos para responder a esto. Un modelo que asume la misma calificación para todas las películas y usuarios con todas las diferencias explicadas por la variación aleatoria se vería así:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + \varepsilon_{u,i}
\]</span></p>
<p>con <span class="math inline">\(\varepsilon_{i,u}\)</span> errores independientes muestreados de la misma distribución centrada en 0 y <span class="math inline">\(\mu\)</span> la calificación “verdadera” para todas las películas. Sabemos que la estimación que minimiza el RMSE es la estimación de mínimos cuadrados de <span class="math inline">\(\mu\)</span> y, en este caso, es el promedio de todas las calificaciones:</p>
<div class="sourceCode" id="cb1302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1302-1"><a href="grandes-conjuntos-de-datos.html#cb1302-1"></a>mu_hat &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)</span>
<span id="cb1302-2"><a href="grandes-conjuntos-de-datos.html#cb1302-2"></a>mu_hat</span>
<span id="cb1302-3"><a href="grandes-conjuntos-de-datos.html#cb1302-3"></a><span class="co">#&gt; [1] 3.54</span></span></code></pre></div>
<p>Si predecimos todas las calificaciones desconocidas con <span class="math inline">\(\hat{\mu}\)</span> obtenemos el siguiente RMSE:</p>
<div class="sourceCode" id="cb1303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1303-1"><a href="grandes-conjuntos-de-datos.html#cb1303-1"></a>naive_rmse &lt;-<span class="st"> </span><span class="kw">RMSE</span>(test_set<span class="op">$</span>rating, mu_hat)</span>
<span id="cb1303-2"><a href="grandes-conjuntos-de-datos.html#cb1303-2"></a>naive_rmse</span>
<span id="cb1303-3"><a href="grandes-conjuntos-de-datos.html#cb1303-3"></a><span class="co">#&gt; [1] 1.05</span></span></code></pre></div>
<p>Tenga en cuenta que si conecta cualquier otro número, obtendrá un RMSE más alto. Por ejemplo:</p>
<div class="sourceCode" id="cb1304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1304-1"><a href="grandes-conjuntos-de-datos.html#cb1304-1"></a>predictions &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">3</span>, <span class="kw">nrow</span>(test_set))</span>
<span id="cb1304-2"><a href="grandes-conjuntos-de-datos.html#cb1304-2"></a><span class="kw">RMSE</span>(test_set<span class="op">$</span>rating, predictions)</span>
<span id="cb1304-3"><a href="grandes-conjuntos-de-datos.html#cb1304-3"></a><span class="co">#&gt; [1] 1.19</span></span></code></pre></div>
<p>Al observar la distribución de calificaciones, podemos visualizar que esta es la desviación estándar de esa distribución. Obtenemos un RMSE de aproximadamente 1. Para ganar el gran premio de $ 1,000,000, un equipo participante tuvo que obtener un RMSE de aproximadamente 0.857. ¡Así que definitivamente podemos hacerlo mejor!</p>
<p>A medida que avanzamos, compararemos diferentes enfoques. Comencemos creando una tabla de resultados con este enfoque ingenuo:</p>
<div class="sourceCode" id="cb1305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1305-1"><a href="grandes-conjuntos-de-datos.html#cb1305-1"></a>rmse_results &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">method =</span> <span class="st">&quot;Just the average&quot;</span>, <span class="dt">RMSE =</span> naive_rmse)</span></code></pre></div>
</div>
<div id="modelado-de-efectos-de-películas" class="section level3">
<h3><span class="header-section-number">33.7.5</span> Modelado de efectos de películas</h3>
<p>Sabemos por experiencia que algunas películas generalmente tienen una calificación más alta que otras. Esta
la intuición, que las diferentes películas se clasifican de manera diferente, se confirma por los datos. Podemos aumentar nuestro modelo anterior agregando el término <span class="math inline">\(b_i\)</span> para representar la clasificación promedio de la película <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]</span></p>
<p>Los libros de texto de estadísticas se refieren a <span class="math inline">\(b\)</span> s como efectos. Sin embargo, en los documentos de desafío de Netflix, se refieren a ellos como “sesgo”, por lo tanto, el <span class="math inline">\(b\)</span> notación.</p>
<p>De nuevo podemos usar mínimos cuadrados para estimar el <span class="math inline">\(b_i\)</span> de la siguiente manera:</p>
<div class="sourceCode" id="cb1306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1306-1"><a href="grandes-conjuntos-de-datos.html#cb1306-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(rating <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(movieId), <span class="dt">data =</span> movielens)</span></code></pre></div>
<p>Porque hay miles de <span class="math inline">\(b_i\)</span> a medida que cada película obtiene una, la <code>lm()</code> la función será muy lenta aquí. Por lo tanto, no recomendamos ejecutar el código anterior. Pero en esta situación particular, sabemos que los mínimos cuadrados estiman <span class="math inline">\(\hat{b}_i\)</span> es solo el promedio de <span class="math inline">\(Y_{u,i} - \hat{\mu}\)</span> para cada película <span class="math inline">\(i\)</span>. Entonces podemos calcularlos de esta manera (dejaremos caer el NA notación en el código para representar estimaciones en el futuro):</p>
<div class="sourceCode" id="cb1307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1307-1"><a href="grandes-conjuntos-de-datos.html#cb1307-1"></a>mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)</span>
<span id="cb1307-2"><a href="grandes-conjuntos-de-datos.html#cb1307-2"></a>movie_avgs &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1307-3"><a href="grandes-conjuntos-de-datos.html#cb1307-3"></a><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1307-4"><a href="grandes-conjuntos-de-datos.html#cb1307-4"></a><span class="kw">summarize</span>(<span class="dt">b_i =</span> <span class="kw">mean</span>(rating <span class="op">-</span><span class="st"> </span>mu))</span>
<span id="cb1307-5"><a href="grandes-conjuntos-de-datos.html#cb1307-5"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span></code></pre></div>
<p>Podemos ver que estas estimaciones varían sustancialmente:</p>
<div class="sourceCode" id="cb1308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1308-1"><a href="grandes-conjuntos-de-datos.html#cb1308-1"></a><span class="kw">qplot</span>(b_i, <span class="dt">data =</span> movie_avgs, <span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="kw">I</span>(<span class="st">&quot;black&quot;</span>))</span></code></pre></div>
<p><img src="libro_files/figure-html/movie-effects-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Recuerda <span class="math inline">\(\hat{\mu}=3.5\)</span> entonces un <span class="math inline">\(b_i = 1.5\)</span> implica una calificación perfecta de cinco estrellas.</p>
<p>Veamos cuánto mejora nuestra predicción una vez que usamos <span class="math inline">\(\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i\)</span>:</p>
<div class="sourceCode" id="cb1309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1309-1"><a href="grandes-conjuntos-de-datos.html#cb1309-1"></a>predicted_ratings &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>test_set <span class="op">%&gt;%</span></span>
<span id="cb1309-2"><a href="grandes-conjuntos-de-datos.html#cb1309-2"></a><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1309-3"><a href="grandes-conjuntos-de-datos.html#cb1309-3"></a><span class="kw">pull</span>(b_i)</span>
<span id="cb1309-4"><a href="grandes-conjuntos-de-datos.html#cb1309-4"></a><span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating)</span>
<span id="cb1309-5"><a href="grandes-conjuntos-de-datos.html#cb1309-5"></a><span class="co">#&gt; [1] 0.989</span></span></code></pre></div>
<p>Ya vemos una mejora. ¿Pero podemos hacerlo mejor?</p>
</div>
<div id="efectos-de-usuario" class="section level3">
<h3><span class="header-section-number">33.7.6</span> Efectos de usuario</h3>
<p>Calculemos la calificación promedio para el usuario <span class="math inline">\(u\)</span> para aquellos que han calificado más de 100 películas:</p>
<div class="sourceCode" id="cb1310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1310-1"><a href="grandes-conjuntos-de-datos.html#cb1310-1"></a>train_set <span class="op">%&gt;%</span></span>
<span id="cb1310-2"><a href="grandes-conjuntos-de-datos.html#cb1310-2"></a><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span></span>
<span id="cb1310-3"><a href="grandes-conjuntos-de-datos.html#cb1310-3"></a><span class="kw">summarize</span>(<span class="dt">b_u =</span> <span class="kw">mean</span>(rating)) <span class="op">%&gt;%</span></span>
<span id="cb1310-4"><a href="grandes-conjuntos-de-datos.html#cb1310-4"></a><span class="kw">filter</span>(<span class="kw">n</span>()<span class="op">&gt;=</span><span class="dv">100</span>) <span class="op">%&gt;%</span></span>
<span id="cb1310-5"><a href="grandes-conjuntos-de-datos.html#cb1310-5"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(b_u)) <span class="op">+</span></span>
<span id="cb1310-6"><a href="grandes-conjuntos-de-datos.html#cb1310-6"></a><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">30</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb1310-7"><a href="grandes-conjuntos-de-datos.html#cb1310-7"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span></code></pre></div>
<p><img src="libro_files/figure-html/user-effect-hist-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Tenga en cuenta que existe una variabilidad sustancial entre los usuarios
también: algunos usuarios son muy irritables y otros adoran cada película.
Esto implica que una mejora adicional de nuestro modelo puede ser:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\]</span></p>
<p>dónde <span class="math inline">\(b_u\)</span> es un efecto específico del usuario. Ahora si un usuario malhumorado (negativo <span class="math inline">\(b_u\)</span>) califica una gran película (positiva <span class="math inline">\(b_i\)</span>), los efectos se contrarrestan y podemos predecir correctamente que este usuario le dio a esta gran película un 3 en lugar de un 5.</p>
<p>Para ajustar este modelo, podríamos usar nuevamente <code>lm</code> me gusta esto:</p>
<div class="sourceCode" id="cb1311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1311-1"><a href="grandes-conjuntos-de-datos.html#cb1311-1"></a><span class="kw">lm</span>(rating <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(movieId) <span class="op">+</span><span class="st"> </span><span class="kw">as.factor</span>(userId))</span></code></pre></div>
<p>pero, por las razones descritas anteriormente, no lo haremos. En cambio, calcularemos una aproximación calculando <span class="math inline">\(\hat{\mu}\)</span> y <span class="math inline">\(\hat{b}_i\)</span> y estimar <span class="math inline">\(\hat{b}_u\)</span> como el promedio de <span class="math inline">\(y_{u,i} - \hat{\mu} - \hat{b}_i\)</span>:</p>
<div class="sourceCode" id="cb1312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1312-1"><a href="grandes-conjuntos-de-datos.html#cb1312-1"></a>user_avgs &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1312-2"><a href="grandes-conjuntos-de-datos.html#cb1312-2"></a><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1312-3"><a href="grandes-conjuntos-de-datos.html#cb1312-3"></a><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span></span>
<span id="cb1312-4"><a href="grandes-conjuntos-de-datos.html#cb1312-4"></a><span class="kw">summarize</span>(<span class="dt">b_u =</span> <span class="kw">mean</span>(rating <span class="op">-</span><span class="st"> </span>mu <span class="op">-</span><span class="st"> </span>b_i))</span>
<span id="cb1312-5"><a href="grandes-conjuntos-de-datos.html#cb1312-5"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span></code></pre></div>
<p>Ahora podemos construir predictores y ver cuánto mejora el RMSE:</p>
<div class="sourceCode" id="cb1313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1313-1"><a href="grandes-conjuntos-de-datos.html#cb1313-1"></a>predicted_ratings &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span></span>
<span id="cb1313-2"><a href="grandes-conjuntos-de-datos.html#cb1313-2"></a><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1313-3"><a href="grandes-conjuntos-de-datos.html#cb1313-3"></a><span class="kw">left_join</span>(user_avgs, <span class="dt">by=</span><span class="st">&#39;userId&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1313-4"><a href="grandes-conjuntos-de-datos.html#cb1313-4"></a><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i <span class="op">+</span><span class="st"> </span>b_u) <span class="op">%&gt;%</span></span>
<span id="cb1313-5"><a href="grandes-conjuntos-de-datos.html#cb1313-5"></a><span class="kw">pull</span>(pred)</span>
<span id="cb1313-6"><a href="grandes-conjuntos-de-datos.html#cb1313-6"></a><span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating)</span>
<span id="cb1313-7"><a href="grandes-conjuntos-de-datos.html#cb1313-7"></a><span class="co">#&gt; [1] 0.905</span></span></code></pre></div>
</div>
</div>
<div id="ejercicios-58" class="section level2">
<h2><span class="header-section-number">33.8</span> Ejercicios</h2>
<p>1. Carga el <code>movielens</code> datos.</p>
<div class="sourceCode" id="cb1314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1314-1"><a href="grandes-conjuntos-de-datos.html#cb1314-1"></a><span class="kw">data</span>(<span class="st">&quot;movielens&quot;</span>)</span></code></pre></div>
<p>Calcule el número de calificaciones para cada película y luego compárelo con el año en que salió la película. Use la transformación de raíz cuadrada en los recuentos.</p>
<p>2. Vemos que, en promedio, las películas que salieron después de 1993 obtienen más calificaciones. También vemos que con las películas más nuevas, a partir de 1993, el número de calificaciones disminuye con el año: cuanto más reciente es una película, menos tiempo han tenido los usuarios para calificarla.</p>
<p>Entre las películas que salieron en 1993 o más tarde, ¿cuáles son las 25 películas con más calificaciones por año? También informe su calificación promedio.</p>
<p>3. De la tabla construida en el ejemplo anterior, vemos que las películas mejor calificadas tienden a tener calificaciones superiores al promedio. Esto no es sorprendente: más personas miran películas populares. Para confirmar esto, estratifique las películas posteriores a 1993 por calificaciones por año y calcule sus calificaciones promedio. Haga un gráfico de calificación promedio versus calificaciones por año y muestre una estimación de la tendencia.</p>
<p>4. En el ejercicio anterior, vemos que cuanto más se califica una película, mayor es la calificación. Suponga que está haciendo un análisis predictivo en el que necesita completar las calificaciones faltantes con algún valor. ¿Cuál de las siguientes estrategias usarías?</p>
<ol style="list-style-type: lower-alpha">
<li>Complete los valores faltantes con la calificación promedio de todas las películas.
si. Complete los valores faltantes con 0.</li>
<li>Complete el valor con un valor más bajo que el promedio ya que la falta de calificación está asociada con calificaciones más bajas. Pruebe diferentes valores y evalúe la predicción en un conjunto de prueba.
re. Ninguna de las anteriores.</li>
</ol>
<p>5. Los <code>movielens</code> el conjunto de datos también incluye una marca de tiempo. Esta variable representa el tiempo y los datos en los que se proporcionó la calificación. Las unidades son segundos desde el 1 de enero de 1970. Cree una nueva columna <code>date</code> con la fecha Sugerencia: use el <code>as_datetime</code> funcionen en el paquete <strong>lubridate</strong>.</p>
<p>6. Calcule la calificación promedio de cada semana y calcule este promedio contra el día. Sugerencia: use el <code>round_date</code> funcionar ante ti <code>group_by</code>.</p>
<p>7. La trama muestra alguna evidencia de un efecto temporal. Si definimos <span class="math inline">\(d_{u,i}\)</span> como el día para el usuario <span class="math inline">\(u\)</span> calificación de la película <span class="math inline">\(i\)</span>, cuál de los siguientes modelos es el más apropiado:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i} + \varepsilon_{u,i}\)</span>.
si. <span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta + \varepsilon_{u,i}\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta_i + \varepsilon_{u,i}\)</span>.
re. <span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}\)</span>, con <span class="math inline">\(f\)</span> una función suave de <span class="math inline">\(d_{u,i}\)</span>.</li>
</ol>
<p>8. Los <code>movielens</code> los datos también tienen un <code>genres</code> columna. Esta columna incluye todos los géneros que se aplican a la película. Algunas películas pertenecen a varios géneros. Defina una categoría como cualquier combinación que aparezca en esta columna. Mantenga solo categorías con más de 1,000 calificaciones. Luego calcule el error promedio y estándar para cada categoría. Trace estos como diagramas de barras de error.</p>
<p>9. La trama muestra una fuerte evidencia de un efecto de género. Si definimos <span class="math inline">\(g_{u,i}\)</span> como el género para el usuario <span class="math inline">\(u\)</span> calificación de la película <span class="math inline">\(i\)</span>, cuál de los siguientes modelos es el más apropiado:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i} + \varepsilon_{u,i}\)</span>.
si. <span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + d_{u,i}\beta + \varepsilon_{u,i}\)</span>.</li>
<li><span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}\)</span>, con <span class="math inline">\(x^k_{u,i} = 1\)</span> si <span class="math inline">\(g_{u,i}\)</span> es genero <span class="math inline">\(k\)</span>.
re. <span class="math inline">\(Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}\)</span>, con <span class="math inline">\(f\)</span> una función suave de <span class="math inline">\(d_{u,i}\)</span>.</li>
</ol>
</div>
<div id="regularización" class="section level2">
<h2><span class="header-section-number">33.9</span> Regularización</h2>
<div id="motivación" class="section level3">
<h3><span class="header-section-number">33.9.1</span> Motivación</h3>
<p>A pesar de la gran variación de película a película, nuestra mejora en RMSE fue solo del 5%. Exploremos dónde cometimos errores en nuestro primer modelo, usando solo efectos de película <span class="math inline">\(b_i\)</span>. Aquí están los 10 errores más grandes:</p>
<div class="sourceCode" id="cb1315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1315-1"><a href="grandes-conjuntos-de-datos.html#cb1315-1"></a>test_set <span class="op">%&gt;%</span></span>
<span id="cb1315-2"><a href="grandes-conjuntos-de-datos.html#cb1315-2"></a><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1315-3"><a href="grandes-conjuntos-de-datos.html#cb1315-3"></a><span class="kw">mutate</span>(<span class="dt">residual =</span> rating <span class="op">-</span><span class="st"> </span>(mu <span class="op">+</span><span class="st"> </span>b_i)) <span class="op">%&gt;%</span></span>
<span id="cb1315-4"><a href="grandes-conjuntos-de-datos.html#cb1315-4"></a><span class="kw">arrange</span>(<span class="kw">desc</span>(<span class="kw">abs</span>(residual))) <span class="op">%&gt;%</span></span>
<span id="cb1315-5"><a href="grandes-conjuntos-de-datos.html#cb1315-5"></a><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1315-6"><a href="grandes-conjuntos-de-datos.html#cb1315-6"></a><span class="kw">pull</span>(title)</span>
<span id="cb1315-7"><a href="grandes-conjuntos-de-datos.html#cb1315-7"></a><span class="co">#&gt;  [1] &quot;Kingdom, The (Riget)&quot;            &quot;Heaven Knows, Mr. Allison&quot;      </span></span>
<span id="cb1315-8"><a href="grandes-conjuntos-de-datos.html#cb1315-8"></a><span class="co">#&gt;  [3] &quot;American Pimp&quot;                   &quot;Chinatown&quot;                      </span></span>
<span id="cb1315-9"><a href="grandes-conjuntos-de-datos.html#cb1315-9"></a><span class="co">#&gt;  [5] &quot;American Beauty&quot;                 &quot;Apocalypse Now&quot;                 </span></span>
<span id="cb1315-10"><a href="grandes-conjuntos-de-datos.html#cb1315-10"></a><span class="co">#&gt;  [7] &quot;Taxi Driver&quot;                     &quot;Wallace &amp; Gromit: A Close Shave&quot;</span></span>
<span id="cb1315-11"><a href="grandes-conjuntos-de-datos.html#cb1315-11"></a><span class="co">#&gt;  [9] &quot;Down in the Delta&quot;               &quot;Stalag 17&quot;</span></span></code></pre></div>
<p>Todo esto parece películas oscuras. Muchos de ellos tienen grandes predicciones. Echemos un vistazo a las 10 peores y mejores películas basadas en <span class="math inline">\(\hat{b}_i\)</span>. Primero, creemos una base de datos que se conecte <code>movieId</code> al título de la película:</p>
<div class="sourceCode" id="cb1316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1316-1"><a href="grandes-conjuntos-de-datos.html#cb1316-1"></a>movie_titles &lt;-<span class="st"> </span>movielens <span class="op">%&gt;%</span></span>
<span id="cb1316-2"><a href="grandes-conjuntos-de-datos.html#cb1316-2"></a><span class="kw">select</span>(movieId, title) <span class="op">%&gt;%</span></span>
<span id="cb1316-3"><a href="grandes-conjuntos-de-datos.html#cb1316-3"></a><span class="kw">distinct</span>()</span></code></pre></div>
<p>Aquí están las 10 mejores películas según nuestra estimación:</p>
<div class="sourceCode" id="cb1317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1317-1"><a href="grandes-conjuntos-de-datos.html#cb1317-1"></a>movie_avgs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1317-2"><a href="grandes-conjuntos-de-datos.html#cb1317-2"></a><span class="kw">arrange</span>(<span class="kw">desc</span>(b_i)) <span class="op">%&gt;%</span></span>
<span id="cb1317-3"><a href="grandes-conjuntos-de-datos.html#cb1317-3"></a><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1317-4"><a href="grandes-conjuntos-de-datos.html#cb1317-4"></a><span class="kw">pull</span>(title)</span>
<span id="cb1317-5"><a href="grandes-conjuntos-de-datos.html#cb1317-5"></a><span class="co">#&gt;  [1] &quot;When Night Is Falling&quot;                                  </span></span>
<span id="cb1317-6"><a href="grandes-conjuntos-de-datos.html#cb1317-6"></a><span class="co">#&gt;  [2] &quot;Lamerica&quot;                                               </span></span>
<span id="cb1317-7"><a href="grandes-conjuntos-de-datos.html#cb1317-7"></a><span class="co">#&gt;  [3] &quot;Mute Witness&quot;                                           </span></span>
<span id="cb1317-8"><a href="grandes-conjuntos-de-datos.html#cb1317-8"></a><span class="co">#&gt;  [4] &quot;Picture Bride (Bijo photo)&quot;                             </span></span>
<span id="cb1317-9"><a href="grandes-conjuntos-de-datos.html#cb1317-9"></a><span class="co">#&gt;  [5] &quot;Red Firecracker, Green Firecracker (Pao Da Shuang Deng)&quot;</span></span>
<span id="cb1317-10"><a href="grandes-conjuntos-de-datos.html#cb1317-10"></a><span class="co">#&gt;  [6] &quot;Paris, France&quot;                                          </span></span>
<span id="cb1317-11"><a href="grandes-conjuntos-de-datos.html#cb1317-11"></a><span class="co">#&gt;  [7] &quot;Faces&quot;                                                  </span></span>
<span id="cb1317-12"><a href="grandes-conjuntos-de-datos.html#cb1317-12"></a><span class="co">#&gt;  [8] &quot;Maya Lin: A Strong Clear Vision&quot;                        </span></span>
<span id="cb1317-13"><a href="grandes-conjuntos-de-datos.html#cb1317-13"></a><span class="co">#&gt;  [9] &quot;Heavy&quot;                                                  </span></span>
<span id="cb1317-14"><a href="grandes-conjuntos-de-datos.html#cb1317-14"></a><span class="co">#&gt; [10] &quot;Gate of Heavenly Peace, The&quot;</span></span></code></pre></div>
<p>Y aquí están los 10 peores:</p>
<div class="sourceCode" id="cb1318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1318-1"><a href="grandes-conjuntos-de-datos.html#cb1318-1"></a>movie_avgs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1318-2"><a href="grandes-conjuntos-de-datos.html#cb1318-2"></a><span class="kw">arrange</span>(b_i) <span class="op">%&gt;%</span></span>
<span id="cb1318-3"><a href="grandes-conjuntos-de-datos.html#cb1318-3"></a><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1318-4"><a href="grandes-conjuntos-de-datos.html#cb1318-4"></a><span class="kw">pull</span>(title)</span>
<span id="cb1318-5"><a href="grandes-conjuntos-de-datos.html#cb1318-5"></a><span class="co">#&gt;  [1] &quot;Children of the Corn IV: The Gathering&quot;           </span></span>
<span id="cb1318-6"><a href="grandes-conjuntos-de-datos.html#cb1318-6"></a><span class="co">#&gt;  [2] &quot;Barney&#39;s Great Adventure&quot;                         </span></span>
<span id="cb1318-7"><a href="grandes-conjuntos-de-datos.html#cb1318-7"></a><span class="co">#&gt;  [3] &quot;Merry War, A&quot;                                     </span></span>
<span id="cb1318-8"><a href="grandes-conjuntos-de-datos.html#cb1318-8"></a><span class="co">#&gt;  [4] &quot;Whiteboyz&quot;                                        </span></span>
<span id="cb1318-9"><a href="grandes-conjuntos-de-datos.html#cb1318-9"></a><span class="co">#&gt;  [5] &quot;Catfish in Black Bean Sauce&quot;                      </span></span>
<span id="cb1318-10"><a href="grandes-conjuntos-de-datos.html#cb1318-10"></a><span class="co">#&gt;  [6] &quot;Killer Shrews, The&quot;                               </span></span>
<span id="cb1318-11"><a href="grandes-conjuntos-de-datos.html#cb1318-11"></a><span class="co">#&gt;  [7] &quot;Horrors of Spider Island (Ein Toter Hing im Netz)&quot;</span></span>
<span id="cb1318-12"><a href="grandes-conjuntos-de-datos.html#cb1318-12"></a><span class="co">#&gt;  [8] &quot;Monkeybone&quot;                                       </span></span>
<span id="cb1318-13"><a href="grandes-conjuntos-de-datos.html#cb1318-13"></a><span class="co">#&gt;  [9] &quot;Arthur 2: On the Rocks&quot;                           </span></span>
<span id="cb1318-14"><a href="grandes-conjuntos-de-datos.html#cb1318-14"></a><span class="co">#&gt; [10] &quot;Red Heat&quot;</span></span></code></pre></div>
<p>Todos parecen ser bastante oscuros. Veamos con qué frecuencia son calificados.</p>
<div class="sourceCode" id="cb1319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1319-1"><a href="grandes-conjuntos-de-datos.html#cb1319-1"></a>train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1319-2"><a href="grandes-conjuntos-de-datos.html#cb1319-2"></a><span class="kw">left_join</span>(movie_avgs, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1319-3"><a href="grandes-conjuntos-de-datos.html#cb1319-3"></a><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1319-4"><a href="grandes-conjuntos-de-datos.html#cb1319-4"></a><span class="kw">arrange</span>(<span class="kw">desc</span>(b_i)) <span class="op">%&gt;%</span></span>
<span id="cb1319-5"><a href="grandes-conjuntos-de-datos.html#cb1319-5"></a><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1319-6"><a href="grandes-conjuntos-de-datos.html#cb1319-6"></a><span class="kw">pull</span>(n)</span>
<span id="cb1319-7"><a href="grandes-conjuntos-de-datos.html#cb1319-7"></a><span class="co">#&gt;  [1] 1 1 1 1 3 1 1 2 1 1</span></span>
<span id="cb1319-8"><a href="grandes-conjuntos-de-datos.html#cb1319-8"></a></span>
<span id="cb1319-9"><a href="grandes-conjuntos-de-datos.html#cb1319-9"></a>train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1319-10"><a href="grandes-conjuntos-de-datos.html#cb1319-10"></a><span class="kw">left_join</span>(movie_avgs) <span class="op">%&gt;%</span></span>
<span id="cb1319-11"><a href="grandes-conjuntos-de-datos.html#cb1319-11"></a><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1319-12"><a href="grandes-conjuntos-de-datos.html#cb1319-12"></a><span class="kw">arrange</span>(b_i) <span class="op">%&gt;%</span></span>
<span id="cb1319-13"><a href="grandes-conjuntos-de-datos.html#cb1319-13"></a><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1319-14"><a href="grandes-conjuntos-de-datos.html#cb1319-14"></a><span class="kw">pull</span>(n)</span>
<span id="cb1319-15"><a href="grandes-conjuntos-de-datos.html#cb1319-15"></a><span class="co">#&gt; Joining, by = &quot;movieId&quot;</span></span>
<span id="cb1319-16"><a href="grandes-conjuntos-de-datos.html#cb1319-16"></a><span class="co">#&gt;  [1] 1 1 1 1 1 1 1 1 1 1</span></span></code></pre></div>
<p>Las supuestas películas “mejores” y “peores” fueron calificadas por muy pocos usuarios, en la mayoría de los casos solo 1. Estas películas eran en su mayoría oscuras. Esto se debe a que con solo unos pocos usuarios, tenemos más incertidumbre. Por lo tanto, mayores estimaciones de <span class="math inline">\(b_i\)</span>, negativo o positivo, son más probables.</p>
<p>Estas son estimaciones ruidosas en las que no debemos confiar,
especialmente cuando se trata de predicciones. Grandes errores pueden
aumentar nuestro RMSE, por lo que preferimos ser conservadores
cuando no estoy seguro</p>
<p>En secciones anteriores, calculamos el error estándar y construimos intervalos de confianza para tener en cuenta los diferentes niveles de incertidumbre. Sin embargo, al hacer predicciones, necesitamos un número, una predicción, no un intervalo. Para esto, presentamos el concepto de regularización.</p>
<p>La regularización nos permite penalizar grandes estimaciones que
se forman utilizando pequeños tamaños de muestra. Tiene puntos en común con el
Enfoque bayesiano que redujo las predicciones descritas en la Sección <a href="models.html#bayesian-statistics">16.4</a>.</p>
</div>
<div id="mínimos-cuadrados-penalizados" class="section level3">
<h3><span class="header-section-number">33.9.2</span> Mínimos cuadrados penalizados</h3>
<p>La idea general detrás de la regularización es restringir la variabilidad total de los tamaños del efecto. ¿Por qué esto ayuda? Considere un caso en el que tenemos película <span class="math inline">\(i=1\)</span> con 100 valoraciones de usuarios y 4 películas <span class="math inline">\(i=2,3,4,5\)</span> con solo una calificación de usuario. Tenemos la intención de ajustar el modelo</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]</span></p>
<p>Supongamos que sabemos que la calificación promedio es, digamos, <span class="math inline">\(\mu = 3\)</span>. Si usamos mínimos cuadrados, la estimación para el primer efecto de película <span class="math inline">\(b_1\)</span> es el promedio de las 100 calificaciones de los usuarios, <span class="math inline">\(1/100 \sum_{i=1}^{100} (Y_{i,1} - \mu)\)</span>, que esperamos sea bastante preciso. Sin embargo, la estimación para las películas 2, 3, 4 y 5 será simplemente la desviación observada de la calificación promedio <span class="math inline">\(\hat{b}_i = Y_{u,i} - \hat{\mu}\)</span> que es una estimación basada en un solo número, por lo que no será precisa en absoluto. Tenga en cuenta que estas estimaciones hacen el error <span class="math inline">\(Y_{u,i} - \mu + \hat{b}_i\)</span> igual a 0 para <span class="math inline">\(i=2,3,4,5\)</span>, pero este es un caso de sobreentrenamiento. De hecho, ignorando al único usuario y adivinando que las películas 2,3,4 y 5 son solo películas promedio ( <span class="math inline">\(b_i = 0\)</span>) podría proporcionar una mejor predicción. La idea general de la regresión penalizada es controlar la variabilidad total de los efectos de la película: <span class="math inline">\(\sum_{i=1}^5 b_i^2\)</span>. Específicamente, en lugar de minimizar la ecuación de mínimos cuadrados, minimizamos una ecuación que agrega una penalización:</p>
<p><span class="math display">\[\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2\]</span>
El primer término es solo mínimos cuadrados y el segundo es una penalización que aumenta cuando muchos <span class="math inline">\(b_i\)</span> son grandes. Usando el cálculo podemos mostrar que los valores de <span class="math inline">\(b_i\)</span> que minimizan esta ecuación son:</p>
<p><span class="math display">\[
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
\]</span></p>
<p>dónde <span class="math inline">\(n_i\)</span> es la cantidad de clasificaciones hechas para la película <span class="math inline">\(i\)</span>. Este enfoque tendrá el efecto deseado: cuando nuestro tamaño de muestra <span class="math inline">\(n_i\)</span> es muy grande, un caso que nos dará una estimación estable, luego la penalización <span class="math inline">\(\lambda\)</span> es efectivamente ignorado desde <span class="math inline">\(n_i+\lambda \approx n_i\)</span>. Sin embargo, cuando el <span class="math inline">\(n_i\)</span> es pequeño, entonces la estimación <span class="math inline">\(\hat{b}_i(\lambda)\)</span> se encoge hacia 0. El más grande <span class="math inline">\(\lambda\)</span>, cuanto más nos encogemos.</p>
<p>Calculemos estas estimaciones regularizadas de <span class="math inline">\(b_i\)</span> utilizando
<span class="math inline">\(\lambda=3\)</span>. Más adelante, veremos por qué elegimos 3.</p>
<div class="sourceCode" id="cb1320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1320-1"><a href="grandes-conjuntos-de-datos.html#cb1320-1"></a>lambda &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb1320-2"><a href="grandes-conjuntos-de-datos.html#cb1320-2"></a>mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)</span>
<span id="cb1320-3"><a href="grandes-conjuntos-de-datos.html#cb1320-3"></a>movie_reg_avgs &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1320-4"><a href="grandes-conjuntos-de-datos.html#cb1320-4"></a><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1320-5"><a href="grandes-conjuntos-de-datos.html#cb1320-5"></a><span class="kw">summarize</span>(<span class="dt">b_i =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>mu)<span class="op">/</span>(<span class="kw">n</span>()<span class="op">+</span>lambda), <span class="dt">n_i =</span> <span class="kw">n</span>())</span>
<span id="cb1320-6"><a href="grandes-conjuntos-de-datos.html#cb1320-6"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span></code></pre></div>
<p>Para ver cómo se reducen las estimaciones, hagamos un gráfico de las estimaciones regularizadas versus las estimaciones de mínimos cuadrados.</p>
<div class="sourceCode" id="cb1321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1321-1"><a href="grandes-conjuntos-de-datos.html#cb1321-1"></a><span class="kw">tibble</span>(<span class="dt">original =</span> movie_avgs<span class="op">$</span>b_i,</span>
<span id="cb1321-2"><a href="grandes-conjuntos-de-datos.html#cb1321-2"></a><span class="dt">regularlized =</span> movie_reg_avgs<span class="op">$</span>b_i,</span>
<span id="cb1321-3"><a href="grandes-conjuntos-de-datos.html#cb1321-3"></a><span class="dt">n =</span> movie_reg_avgs<span class="op">$</span>n_i) <span class="op">%&gt;%</span></span>
<span id="cb1321-4"><a href="grandes-conjuntos-de-datos.html#cb1321-4"></a><span class="kw">ggplot</span>(<span class="kw">aes</span>(original, regularlized, <span class="dt">size=</span><span class="kw">sqrt</span>(n))) <span class="op">+</span></span>
<span id="cb1321-5"><a href="grandes-conjuntos-de-datos.html#cb1321-5"></a><span class="kw">geom_point</span>(<span class="dt">shape=</span><span class="dv">1</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/regularization-shrinkage-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Ahora, echemos un vistazo a las 10 mejores películas basadas en las estimaciones penalizadas <span class="math inline">\(\hat{b}_i(\lambda)\)</span>:</p>
<div class="sourceCode" id="cb1322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1322-1"><a href="grandes-conjuntos-de-datos.html#cb1322-1"></a>train_set <span class="op">%&gt;%</span></span>
<span id="cb1322-2"><a href="grandes-conjuntos-de-datos.html#cb1322-2"></a><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1322-3"><a href="grandes-conjuntos-de-datos.html#cb1322-3"></a><span class="kw">left_join</span>(movie_reg_avgs, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1322-4"><a href="grandes-conjuntos-de-datos.html#cb1322-4"></a><span class="kw">left_join</span>(movie_titles, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1322-5"><a href="grandes-conjuntos-de-datos.html#cb1322-5"></a><span class="kw">arrange</span>(<span class="kw">desc</span>(b_i)) <span class="op">%&gt;%</span></span>
<span id="cb1322-6"><a href="grandes-conjuntos-de-datos.html#cb1322-6"></a><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1322-7"><a href="grandes-conjuntos-de-datos.html#cb1322-7"></a><span class="kw">pull</span>(title)</span>
<span id="cb1322-8"><a href="grandes-conjuntos-de-datos.html#cb1322-8"></a><span class="co">#&gt;  [1] &quot;Paris Is Burning&quot;          &quot;Shawshank Redemption, The&quot;</span></span>
<span id="cb1322-9"><a href="grandes-conjuntos-de-datos.html#cb1322-9"></a><span class="co">#&gt;  [3] &quot;Godfather, The&quot;            &quot;African Queen, The&quot;       </span></span>
<span id="cb1322-10"><a href="grandes-conjuntos-de-datos.html#cb1322-10"></a><span class="co">#&gt;  [5] &quot;Band of Brothers&quot;          &quot;Paperman&quot;                 </span></span>
<span id="cb1322-11"><a href="grandes-conjuntos-de-datos.html#cb1322-11"></a><span class="co">#&gt;  [7] &quot;On the Waterfront&quot;         &quot;All About Eve&quot;            </span></span>
<span id="cb1322-12"><a href="grandes-conjuntos-de-datos.html#cb1322-12"></a><span class="co">#&gt;  [9] &quot;Usual Suspects, The&quot;       &quot;Ikiru&quot;</span></span></code></pre></div>
<p>¡Esto tiene mucho más sentido! Estas películas se ven más y tienen más calificaciones. Aquí están las 10 peores películas principales:</p>
<div class="sourceCode" id="cb1323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1323-1"><a href="grandes-conjuntos-de-datos.html#cb1323-1"></a>train_set <span class="op">%&gt;%</span></span>
<span id="cb1323-2"><a href="grandes-conjuntos-de-datos.html#cb1323-2"></a><span class="kw">count</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1323-3"><a href="grandes-conjuntos-de-datos.html#cb1323-3"></a><span class="kw">left_join</span>(movie_reg_avgs, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1323-4"><a href="grandes-conjuntos-de-datos.html#cb1323-4"></a><span class="kw">left_join</span>(movie_titles, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1323-5"><a href="grandes-conjuntos-de-datos.html#cb1323-5"></a><span class="kw">arrange</span>(b_i) <span class="op">%&gt;%</span></span>
<span id="cb1323-6"><a href="grandes-conjuntos-de-datos.html#cb1323-6"></a><span class="kw">select</span>(title, b_i, n) <span class="op">%&gt;%</span></span>
<span id="cb1323-7"><a href="grandes-conjuntos-de-datos.html#cb1323-7"></a><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span></span>
<span id="cb1323-8"><a href="grandes-conjuntos-de-datos.html#cb1323-8"></a><span class="kw">pull</span>(title)</span>
<span id="cb1323-9"><a href="grandes-conjuntos-de-datos.html#cb1323-9"></a><span class="co">#&gt;  [1] &quot;Battlefield Earth&quot;                      </span></span>
<span id="cb1323-10"><a href="grandes-conjuntos-de-datos.html#cb1323-10"></a><span class="co">#&gt;  [2] &quot;Joe&#39;s Apartment&quot;                        </span></span>
<span id="cb1323-11"><a href="grandes-conjuntos-de-datos.html#cb1323-11"></a><span class="co">#&gt;  [3] &quot;Super Mario Bros.&quot;                      </span></span>
<span id="cb1323-12"><a href="grandes-conjuntos-de-datos.html#cb1323-12"></a><span class="co">#&gt;  [4] &quot;Speed 2: Cruise Control&quot;                </span></span>
<span id="cb1323-13"><a href="grandes-conjuntos-de-datos.html#cb1323-13"></a><span class="co">#&gt;  [5] &quot;Dungeons &amp; Dragons&quot;                     </span></span>
<span id="cb1323-14"><a href="grandes-conjuntos-de-datos.html#cb1323-14"></a><span class="co">#&gt;  [6] &quot;Batman &amp; Robin&quot;                         </span></span>
<span id="cb1323-15"><a href="grandes-conjuntos-de-datos.html#cb1323-15"></a><span class="co">#&gt;  [7] &quot;Police Academy 6: City Under Siege&quot;     </span></span>
<span id="cb1323-16"><a href="grandes-conjuntos-de-datos.html#cb1323-16"></a><span class="co">#&gt;  [8] &quot;Cats &amp; Dogs&quot;                            </span></span>
<span id="cb1323-17"><a href="grandes-conjuntos-de-datos.html#cb1323-17"></a><span class="co">#&gt;  [9] &quot;Disaster Movie&quot;                         </span></span>
<span id="cb1323-18"><a href="grandes-conjuntos-de-datos.html#cb1323-18"></a><span class="co">#&gt; [10] &quot;Mighty Morphin Power Rangers: The Movie&quot;</span></span></code></pre></div>
<p>¿Mejoramos nuestros resultados?</p>
<div class="sourceCode" id="cb1324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1324-1"><a href="grandes-conjuntos-de-datos.html#cb1324-1"></a>predicted_ratings &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span></span>
<span id="cb1324-2"><a href="grandes-conjuntos-de-datos.html#cb1324-2"></a><span class="kw">left_join</span>(movie_reg_avgs, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1324-3"><a href="grandes-conjuntos-de-datos.html#cb1324-3"></a><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i) <span class="op">%&gt;%</span></span>
<span id="cb1324-4"><a href="grandes-conjuntos-de-datos.html#cb1324-4"></a><span class="kw">pull</span>(pred)</span>
<span id="cb1324-5"><a href="grandes-conjuntos-de-datos.html#cb1324-5"></a><span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating)</span>
<span id="cb1324-6"><a href="grandes-conjuntos-de-datos.html#cb1324-6"></a><span class="co">#&gt; [1] 0.97</span></span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 2
#&gt;   method                          RMSE
#&gt;   &lt;chr&gt;                          &lt;dbl&gt;
#&gt; 1 Just the average               1.05 
#&gt; 2 Movie Effect Model             0.989
#&gt; 3 Movie + User Effects Model     0.905
#&gt; 4 Regularized Movie Effect Model 0.970</code></pre>
<p>Las estimaciones penalizadas proporcionan una gran mejora sobre las estimaciones de mínimos cuadrados.</p>
</div>
<div id="elegir-los-términos-de-penalización" class="section level3">
<h3><span class="header-section-number">33.9.3</span> Elegir los términos de penalización</h3>
<p>Tenga en cuenta que <span class="math inline">\(\lambda\)</span> es un parámetro de ajuste. Podemos usar validación cruzada para elegirlo.</p>
<div class="sourceCode" id="cb1326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1326-1"><a href="grandes-conjuntos-de-datos.html#cb1326-1"></a>lambdas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.25</span>)</span>
<span id="cb1326-2"><a href="grandes-conjuntos-de-datos.html#cb1326-2"></a></span>
<span id="cb1326-3"><a href="grandes-conjuntos-de-datos.html#cb1326-3"></a>mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)</span>
<span id="cb1326-4"><a href="grandes-conjuntos-de-datos.html#cb1326-4"></a>just_the_sum &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1326-5"><a href="grandes-conjuntos-de-datos.html#cb1326-5"></a><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1326-6"><a href="grandes-conjuntos-de-datos.html#cb1326-6"></a><span class="kw">summarize</span>(<span class="dt">s =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>mu), <span class="dt">n_i =</span> <span class="kw">n</span>())</span>
<span id="cb1326-7"><a href="grandes-conjuntos-de-datos.html#cb1326-7"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1326-8"><a href="grandes-conjuntos-de-datos.html#cb1326-8"></a></span>
<span id="cb1326-9"><a href="grandes-conjuntos-de-datos.html#cb1326-9"></a>rmses &lt;-<span class="st"> </span><span class="kw">sapply</span>(lambdas, <span class="cf">function</span>(l){</span>
<span id="cb1326-10"><a href="grandes-conjuntos-de-datos.html#cb1326-10"></a>predicted_ratings &lt;-<span class="st"> </span>test_set <span class="op">%&gt;%</span></span>
<span id="cb1326-11"><a href="grandes-conjuntos-de-datos.html#cb1326-11"></a><span class="kw">left_join</span>(just_the_sum, <span class="dt">by=</span><span class="st">&#39;movieId&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1326-12"><a href="grandes-conjuntos-de-datos.html#cb1326-12"></a><span class="kw">mutate</span>(<span class="dt">b_i =</span> s<span class="op">/</span>(n_i<span class="op">+</span>l)) <span class="op">%&gt;%</span></span>
<span id="cb1326-13"><a href="grandes-conjuntos-de-datos.html#cb1326-13"></a><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i) <span class="op">%&gt;%</span></span>
<span id="cb1326-14"><a href="grandes-conjuntos-de-datos.html#cb1326-14"></a><span class="kw">pull</span>(pred)</span>
<span id="cb1326-15"><a href="grandes-conjuntos-de-datos.html#cb1326-15"></a><span class="kw">return</span>(<span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating))</span>
<span id="cb1326-16"><a href="grandes-conjuntos-de-datos.html#cb1326-16"></a>})</span>
<span id="cb1326-17"><a href="grandes-conjuntos-de-datos.html#cb1326-17"></a><span class="kw">qplot</span>(lambdas, rmses)</span>
<span id="cb1326-18"><a href="grandes-conjuntos-de-datos.html#cb1326-18"></a>lambdas[<span class="kw">which.min</span>(rmses)]</span>
<span id="cb1326-19"><a href="grandes-conjuntos-de-datos.html#cb1326-19"></a><span class="co">#&gt; [1] 3</span></span></code></pre></div>
<p><img src="libro_files/figure-html/best-penalty-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Sin embargo, si bien mostramos esto como una ilustración, en la práctica deberíamos usar validación cruzada completa solo en el conjunto del tren, sin usar el conjunto de prueba hasta la evaluación final. El conjunto de prueba nunca debe utilizarse para la sintonización</p>
<p>También podemos utilizar la regularización para estimar los efectos del usuario. Estamos minimizando:</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{u,i} \left(y_{u,i} - \mu - b_i - b_u \right)^2 +
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)
\]</span></p>
<p>Las estimaciones que minimizan esto se pueden encontrar de manera similar a lo que hicimos anteriormente. Aquí usamos validación cruzada para elegir un <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb1327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1327-1"><a href="grandes-conjuntos-de-datos.html#cb1327-1"></a>lambdas &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.25</span>)</span>
<span id="cb1327-2"><a href="grandes-conjuntos-de-datos.html#cb1327-2"></a></span>
<span id="cb1327-3"><a href="grandes-conjuntos-de-datos.html#cb1327-3"></a>rmses &lt;-<span class="st"> </span><span class="kw">sapply</span>(lambdas, <span class="cf">function</span>(l){</span>
<span id="cb1327-4"><a href="grandes-conjuntos-de-datos.html#cb1327-4"></a></span>
<span id="cb1327-5"><a href="grandes-conjuntos-de-datos.html#cb1327-5"></a>mu &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>rating)</span>
<span id="cb1327-6"><a href="grandes-conjuntos-de-datos.html#cb1327-6"></a></span>
<span id="cb1327-7"><a href="grandes-conjuntos-de-datos.html#cb1327-7"></a>b_i &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1327-8"><a href="grandes-conjuntos-de-datos.html#cb1327-8"></a><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1327-9"><a href="grandes-conjuntos-de-datos.html#cb1327-9"></a><span class="kw">summarize</span>(<span class="dt">b_i =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>mu)<span class="op">/</span>(<span class="kw">n</span>()<span class="op">+</span>l))</span>
<span id="cb1327-10"><a href="grandes-conjuntos-de-datos.html#cb1327-10"></a></span>
<span id="cb1327-11"><a href="grandes-conjuntos-de-datos.html#cb1327-11"></a>b_u &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span></span>
<span id="cb1327-12"><a href="grandes-conjuntos-de-datos.html#cb1327-12"></a><span class="kw">left_join</span>(b_i, <span class="dt">by=</span><span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1327-13"><a href="grandes-conjuntos-de-datos.html#cb1327-13"></a><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span></span>
<span id="cb1327-14"><a href="grandes-conjuntos-de-datos.html#cb1327-14"></a><span class="kw">summarize</span>(<span class="dt">b_u =</span> <span class="kw">sum</span>(rating <span class="op">-</span><span class="st"> </span>b_i <span class="op">-</span><span class="st"> </span>mu)<span class="op">/</span>(<span class="kw">n</span>()<span class="op">+</span>l))</span>
<span id="cb1327-15"><a href="grandes-conjuntos-de-datos.html#cb1327-15"></a></span>
<span id="cb1327-16"><a href="grandes-conjuntos-de-datos.html#cb1327-16"></a>predicted_ratings &lt;-</span>
<span id="cb1327-17"><a href="grandes-conjuntos-de-datos.html#cb1327-17"></a>test_set <span class="op">%&gt;%</span></span>
<span id="cb1327-18"><a href="grandes-conjuntos-de-datos.html#cb1327-18"></a><span class="kw">left_join</span>(b_i, <span class="dt">by =</span> <span class="st">&quot;movieId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1327-19"><a href="grandes-conjuntos-de-datos.html#cb1327-19"></a><span class="kw">left_join</span>(b_u, <span class="dt">by =</span> <span class="st">&quot;userId&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb1327-20"><a href="grandes-conjuntos-de-datos.html#cb1327-20"></a><span class="kw">mutate</span>(<span class="dt">pred =</span> mu <span class="op">+</span><span class="st"> </span>b_i <span class="op">+</span><span class="st"> </span>b_u) <span class="op">%&gt;%</span></span>
<span id="cb1327-21"><a href="grandes-conjuntos-de-datos.html#cb1327-21"></a><span class="kw">pull</span>(pred)</span>
<span id="cb1327-22"><a href="grandes-conjuntos-de-datos.html#cb1327-22"></a></span>
<span id="cb1327-23"><a href="grandes-conjuntos-de-datos.html#cb1327-23"></a><span class="kw">return</span>(<span class="kw">RMSE</span>(predicted_ratings, test_set<span class="op">$</span>rating))</span>
<span id="cb1327-24"><a href="grandes-conjuntos-de-datos.html#cb1327-24"></a>})</span>
<span id="cb1327-25"><a href="grandes-conjuntos-de-datos.html#cb1327-25"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-26"><a href="grandes-conjuntos-de-datos.html#cb1327-26"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-27"><a href="grandes-conjuntos-de-datos.html#cb1327-27"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-28"><a href="grandes-conjuntos-de-datos.html#cb1327-28"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-29"><a href="grandes-conjuntos-de-datos.html#cb1327-29"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-30"><a href="grandes-conjuntos-de-datos.html#cb1327-30"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-31"><a href="grandes-conjuntos-de-datos.html#cb1327-31"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-32"><a href="grandes-conjuntos-de-datos.html#cb1327-32"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-33"><a href="grandes-conjuntos-de-datos.html#cb1327-33"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-34"><a href="grandes-conjuntos-de-datos.html#cb1327-34"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-35"><a href="grandes-conjuntos-de-datos.html#cb1327-35"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-36"><a href="grandes-conjuntos-de-datos.html#cb1327-36"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-37"><a href="grandes-conjuntos-de-datos.html#cb1327-37"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-38"><a href="grandes-conjuntos-de-datos.html#cb1327-38"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-39"><a href="grandes-conjuntos-de-datos.html#cb1327-39"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-40"><a href="grandes-conjuntos-de-datos.html#cb1327-40"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-41"><a href="grandes-conjuntos-de-datos.html#cb1327-41"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-42"><a href="grandes-conjuntos-de-datos.html#cb1327-42"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-43"><a href="grandes-conjuntos-de-datos.html#cb1327-43"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-44"><a href="grandes-conjuntos-de-datos.html#cb1327-44"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-45"><a href="grandes-conjuntos-de-datos.html#cb1327-45"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-46"><a href="grandes-conjuntos-de-datos.html#cb1327-46"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-47"><a href="grandes-conjuntos-de-datos.html#cb1327-47"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-48"><a href="grandes-conjuntos-de-datos.html#cb1327-48"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-49"><a href="grandes-conjuntos-de-datos.html#cb1327-49"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-50"><a href="grandes-conjuntos-de-datos.html#cb1327-50"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-51"><a href="grandes-conjuntos-de-datos.html#cb1327-51"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-52"><a href="grandes-conjuntos-de-datos.html#cb1327-52"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-53"><a href="grandes-conjuntos-de-datos.html#cb1327-53"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-54"><a href="grandes-conjuntos-de-datos.html#cb1327-54"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-55"><a href="grandes-conjuntos-de-datos.html#cb1327-55"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-56"><a href="grandes-conjuntos-de-datos.html#cb1327-56"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-57"><a href="grandes-conjuntos-de-datos.html#cb1327-57"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-58"><a href="grandes-conjuntos-de-datos.html#cb1327-58"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-59"><a href="grandes-conjuntos-de-datos.html#cb1327-59"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-60"><a href="grandes-conjuntos-de-datos.html#cb1327-60"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-61"><a href="grandes-conjuntos-de-datos.html#cb1327-61"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-62"><a href="grandes-conjuntos-de-datos.html#cb1327-62"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-63"><a href="grandes-conjuntos-de-datos.html#cb1327-63"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-64"><a href="grandes-conjuntos-de-datos.html#cb1327-64"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-65"><a href="grandes-conjuntos-de-datos.html#cb1327-65"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-66"><a href="grandes-conjuntos-de-datos.html#cb1327-66"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-67"><a href="grandes-conjuntos-de-datos.html#cb1327-67"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-68"><a href="grandes-conjuntos-de-datos.html#cb1327-68"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-69"><a href="grandes-conjuntos-de-datos.html#cb1327-69"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-70"><a href="grandes-conjuntos-de-datos.html#cb1327-70"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-71"><a href="grandes-conjuntos-de-datos.html#cb1327-71"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-72"><a href="grandes-conjuntos-de-datos.html#cb1327-72"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-73"><a href="grandes-conjuntos-de-datos.html#cb1327-73"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-74"><a href="grandes-conjuntos-de-datos.html#cb1327-74"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-75"><a href="grandes-conjuntos-de-datos.html#cb1327-75"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-76"><a href="grandes-conjuntos-de-datos.html#cb1327-76"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-77"><a href="grandes-conjuntos-de-datos.html#cb1327-77"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-78"><a href="grandes-conjuntos-de-datos.html#cb1327-78"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-79"><a href="grandes-conjuntos-de-datos.html#cb1327-79"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-80"><a href="grandes-conjuntos-de-datos.html#cb1327-80"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-81"><a href="grandes-conjuntos-de-datos.html#cb1327-81"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-82"><a href="grandes-conjuntos-de-datos.html#cb1327-82"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-83"><a href="grandes-conjuntos-de-datos.html#cb1327-83"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-84"><a href="grandes-conjuntos-de-datos.html#cb1327-84"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-85"><a href="grandes-conjuntos-de-datos.html#cb1327-85"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-86"><a href="grandes-conjuntos-de-datos.html#cb1327-86"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-87"><a href="grandes-conjuntos-de-datos.html#cb1327-87"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-88"><a href="grandes-conjuntos-de-datos.html#cb1327-88"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-89"><a href="grandes-conjuntos-de-datos.html#cb1327-89"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-90"><a href="grandes-conjuntos-de-datos.html#cb1327-90"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-91"><a href="grandes-conjuntos-de-datos.html#cb1327-91"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-92"><a href="grandes-conjuntos-de-datos.html#cb1327-92"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-93"><a href="grandes-conjuntos-de-datos.html#cb1327-93"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-94"><a href="grandes-conjuntos-de-datos.html#cb1327-94"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-95"><a href="grandes-conjuntos-de-datos.html#cb1327-95"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-96"><a href="grandes-conjuntos-de-datos.html#cb1327-96"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-97"><a href="grandes-conjuntos-de-datos.html#cb1327-97"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-98"><a href="grandes-conjuntos-de-datos.html#cb1327-98"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-99"><a href="grandes-conjuntos-de-datos.html#cb1327-99"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-100"><a href="grandes-conjuntos-de-datos.html#cb1327-100"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-101"><a href="grandes-conjuntos-de-datos.html#cb1327-101"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-102"><a href="grandes-conjuntos-de-datos.html#cb1327-102"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-103"><a href="grandes-conjuntos-de-datos.html#cb1327-103"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-104"><a href="grandes-conjuntos-de-datos.html#cb1327-104"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-105"><a href="grandes-conjuntos-de-datos.html#cb1327-105"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-106"><a href="grandes-conjuntos-de-datos.html#cb1327-106"></a><span class="co">#&gt; `summarise()` ungrouping output (override with `.groups` argument)</span></span>
<span id="cb1327-107"><a href="grandes-conjuntos-de-datos.html#cb1327-107"></a></span>
<span id="cb1327-108"><a href="grandes-conjuntos-de-datos.html#cb1327-108"></a><span class="kw">qplot</span>(lambdas, rmses)</span></code></pre></div>
<p><img src="libro_files/figure-html/best-lambdas-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Para el modelo completo, el óptimo <span class="math inline">\(\lambda\)</span> es:</p>
<div class="sourceCode" id="cb1328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1328-1"><a href="grandes-conjuntos-de-datos.html#cb1328-1"></a>lambda &lt;-<span class="st"> </span>lambdas[<span class="kw">which.min</span>(rmses)]</span>
<span id="cb1328-2"><a href="grandes-conjuntos-de-datos.html#cb1328-2"></a>lambda</span>
<span id="cb1328-3"><a href="grandes-conjuntos-de-datos.html#cb1328-3"></a><span class="co">#&gt; [1] 3.25</span></span></code></pre></div>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
method
</th>
<th style="text-align:right;">
RMSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Just the average
</td>
<td style="text-align:right;">
1.053
</td>
</tr>
<tr>
<td style="text-align:left;">
Movie Effect Model
</td>
<td style="text-align:right;">
0.989
</td>
</tr>
<tr>
<td style="text-align:left;">
Movie + User Effects Model
</td>
<td style="text-align:right;">
0.905
</td>
</tr>
<tr>
<td style="text-align:left;">
Regularized Movie Effect Model
</td>
<td style="text-align:right;">
0.970
</td>
</tr>
<tr>
<td style="text-align:left;">
Regularized Movie + User Effect Model
</td>
<td style="text-align:right;">
0.881
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="ejercicios-59" class="section level2">
<h2><span class="header-section-number">33.10</span> Ejercicios</h2>
<p>Un experto en educación aboga por escuelas más pequeñas. El experto basa esta recomendación en el hecho de que entre las mejores escuelas, muchas son escuelas pequeñas. Simulemos un conjunto de datos para 100 escuelas. Primero, simulemos el número de estudiantes en cada escuela.</p>
<div class="sourceCode" id="cb1329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1329-1"><a href="grandes-conjuntos-de-datos.html#cb1329-1"></a><span class="kw">set.seed</span>(<span class="dv">1986</span>)</span>
<span id="cb1329-2"><a href="grandes-conjuntos-de-datos.html#cb1329-2"></a>n &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="dv">2</span><span class="op">^</span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dv">8</span>, <span class="dv">1</span>))</span></code></pre></div>
<p>Ahora asignemos una calidad verdadera para cada escuela completamente independiente del tamaño. Este es el parámetro que queremos estimar.</p>
<div class="sourceCode" id="cb1330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1330-1"><a href="grandes-conjuntos-de-datos.html#cb1330-1"></a>mu &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="dv">80</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">rt</span>(<span class="dv">1000</span>, <span class="dv">5</span>))</span>
<span id="cb1330-2"><a href="grandes-conjuntos-de-datos.html#cb1330-2"></a><span class="kw">range</span>(mu)</span>
<span id="cb1330-3"><a href="grandes-conjuntos-de-datos.html#cb1330-3"></a>schools &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> <span class="kw">paste</span>(<span class="st">&quot;PS&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>),</span>
<span id="cb1330-4"><a href="grandes-conjuntos-de-datos.html#cb1330-4"></a><span class="dt">size =</span> n,</span>
<span id="cb1330-5"><a href="grandes-conjuntos-de-datos.html#cb1330-5"></a><span class="dt">quality =</span> mu,</span>
<span id="cb1330-6"><a href="grandes-conjuntos-de-datos.html#cb1330-6"></a><span class="dt">rank =</span> <span class="kw">rank</span>(<span class="op">-</span>mu))</span></code></pre></div>
<p>Podemos ver que las 10 mejores escuelas son:</p>
<div class="sourceCode" id="cb1331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1331-1"><a href="grandes-conjuntos-de-datos.html#cb1331-1"></a>schools <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">top_n</span>(<span class="dv">10</span>, quality) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(quality))</span></code></pre></div>
<p>Ahora hagamos que los estudiantes en la escuela tomen un examen. Existe una variabilidad aleatoria en la toma de exámenes, por lo que simularemos los puntajes de los exámenes distribuidos normalmente con el promedio determinado por la calidad de la escuela y las desviaciones estándar de 30 puntos porcentuales:</p>
<div class="sourceCode" id="cb1332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1332-1"><a href="grandes-conjuntos-de-datos.html#cb1332-1"></a>scores &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(schools), <span class="cf">function</span>(i){</span>
<span id="cb1332-2"><a href="grandes-conjuntos-de-datos.html#cb1332-2"></a>scores &lt;-<span class="st"> </span><span class="kw">rnorm</span>(schools<span class="op">$</span>size[i], schools<span class="op">$</span>quality[i], <span class="dv">30</span>)</span>
<span id="cb1332-3"><a href="grandes-conjuntos-de-datos.html#cb1332-3"></a>scores</span>
<span id="cb1332-4"><a href="grandes-conjuntos-de-datos.html#cb1332-4"></a>})</span>
<span id="cb1332-5"><a href="grandes-conjuntos-de-datos.html#cb1332-5"></a>schools &lt;-<span class="st"> </span>schools <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">score =</span> <span class="kw">sapply</span>(scores, mean))</span></code></pre></div>
<p>1. ¿Cuáles son las mejores escuelas según el puntaje promedio? Muestra solo la identificación, el tamaño y el puntaje promedio.</p>
<p>2. Compare el tamaño medio de la escuela con el tamaño medio de las 10 mejores escuelas según el puntaje.</p>
<p>3. Según esta prueba, parece que las escuelas pequeñas son mejores que las grandes. Cinco de las 10 mejores escuelas tienen 100 estudiantes o menos. ¿Pero como puede ser ésto? Construimos la simulación para que la calidad y el tamaño sean independientes. Repita el ejercicio para las peores 10 escuelas.</p>
<p>4. ¡Lo mismo es cierto para las peores escuelas! También son pequeños. Trace el puntaje promedio versus el tamaño de la escuela para ver qué está pasando. Destaque las 10 mejores escuelas según la calidad <em>verdadera</em>. Use la transformación de escala logarítmica para el tamaño.</p>
<p>5. Podemos ver que el error estándar de la puntuación tiene una mayor variabilidad cuando la escuela es más pequeña. Esta es una realidad estadística básica que aprendimos en las secciones de probabilidad e inferencia. De hecho, tenga en cuenta que 4 de las 10 mejores escuelas se encuentran en las 10 mejores escuelas según el puntaje del examen.</p>
<p>Usemos la regularización para elegir las mejores escuelas. Recuerde las desviaciones de la regularización <em>desviaciones</em> del promedio hacia 0. Entonces, para aplicar la regularización aquí, primero debemos definir el promedio general para todas las escuelas:</p>
<div class="sourceCode" id="cb1333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1333-1"><a href="grandes-conjuntos-de-datos.html#cb1333-1"></a>overall &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">sapply</span>(scores, mean))</span></code></pre></div>
<p>y luego defina, para cada escuela, cómo se desvía de ese promedio. Escriba un código que calcule el puntaje por encima del promedio de cada escuela pero dividiéndolo por <span class="math inline">\(n + \lambda\)</span> en lugar de <span class="math inline">\(n\)</span>, con <span class="math inline">\(n\)</span> el tamaño de la escuela y <span class="math inline">\(\lambda\)</span> un parámetro de regularización. Tratar <span class="math inline">\(\lambda = 3\)</span>.</p>
<p>6. Tenga en cuenta que esto mejora un poco las cosas. El número de escuelas pequeñas que no están altamente clasificadas es ahora 4. ¿Existe una mejor <span class="math inline">\(\lambda\)</span>? Encuentra el <span class="math inline">\(\lambda\)</span> que minimiza el RMSE = <span class="math inline">\(1/100 \sum_{i=1}^{100} (\mbox{quality} - \mbox{estimate})^2\)</span>.</p>
<p>7. Clasifique las escuelas según el promedio obtenido con los mejores <span class="math inline">\(\alpha\)</span>. Tenga en cuenta que ninguna escuela pequeña se incluye incorrectamente.</p>
<p>8. Un error común al usar la regularización es reducir los valores hacia 0 que no están centrados alrededor de 0. Por ejemplo, si no restamos el promedio general antes de reducir, en realidad obtenemos un resultado muy similar. Confirme esto volviendo a ejecutar el código del ejercicio 6 pero sin eliminar la media general.</p>
</div>
<div id="factorización-matricial" class="section level2">
<h2><span class="header-section-number">33.11</span> Factorización matricial</h2>
<p>La factorización matricial es un concepto ampliamente utilizado en el aprendizaje automático. Está muy relacionado con el análisis factorial, la descomposición de valores singulares (SVD) y el análisis de componentes principales (PCA). Aquí describimos el concepto en el contexto de los sistemas de recomendación de películas.</p>
<p>Hemos descrito cómo el modelo:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\]</span></p>
<p>explica las diferencias de película a película a través del <span class="math inline">\(b_i\)</span> y las diferencias de usuario a usuario a través de la <span class="math inline">\(b_u\)</span>. Pero este modelo omite una fuente importante de variación relacionada con el hecho de que los grupos de películas tienen patrones de calificación similares y los grupos de usuarios también tienen patrones de calificación similares. Descubriremos estos patrones estudiando los residuos:</p>
<p><span class="math display">\[
r_{u,i} = y_{u,i} - \hat{b}_i - \hat{b}_u
\]</span></p>
<p>Para ver esto, convertiremos los datos en una matriz para que cada usuario obtenga una fila, cada película obtenga una columna y <span class="math inline">\(y_{u,i}\)</span> es la entrada en fila <span class="math inline">\(u\)</span> y columna <span class="math inline">\(i\)</span>. Con fines ilustrativos, solo consideraremos un pequeño subconjunto de películas con muchas calificaciones y usuarios que han calificado muchas películas. También conservamos el aroma de una mujer ( <code>movieId == 3252</code>) porque lo usamos para un ejemplo específico:</p>
<div class="sourceCode" id="cb1334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1334-1"><a href="grandes-conjuntos-de-datos.html#cb1334-1"></a>train_small &lt;-<span class="st"> </span>movielens <span class="op">%&gt;%</span></span>
<span id="cb1334-2"><a href="grandes-conjuntos-de-datos.html#cb1334-2"></a><span class="kw">group_by</span>(movieId) <span class="op">%&gt;%</span></span>
<span id="cb1334-3"><a href="grandes-conjuntos-de-datos.html#cb1334-3"></a><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">50</span> <span class="op">|</span><span class="st"> </span>movieId <span class="op">==</span><span class="st"> </span><span class="dv">3252</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></span>
<span id="cb1334-4"><a href="grandes-conjuntos-de-datos.html#cb1334-4"></a><span class="kw">group_by</span>(userId) <span class="op">%&gt;%</span></span>
<span id="cb1334-5"><a href="grandes-conjuntos-de-datos.html#cb1334-5"></a><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">50</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ungroup</span>()</span>
<span id="cb1334-6"><a href="grandes-conjuntos-de-datos.html#cb1334-6"></a></span>
<span id="cb1334-7"><a href="grandes-conjuntos-de-datos.html#cb1334-7"></a>y &lt;-<span class="st"> </span>train_small <span class="op">%&gt;%</span></span>
<span id="cb1334-8"><a href="grandes-conjuntos-de-datos.html#cb1334-8"></a><span class="kw">select</span>(userId, movieId, rating) <span class="op">%&gt;%</span></span>
<span id="cb1334-9"><a href="grandes-conjuntos-de-datos.html#cb1334-9"></a><span class="kw">spread</span>(movieId, rating) <span class="op">%&gt;%</span></span>
<span id="cb1334-10"><a href="grandes-conjuntos-de-datos.html#cb1334-10"></a><span class="kw">as.matrix</span>()</span></code></pre></div>
<p>Agregamos nombres de fila y columna:</p>
<div class="sourceCode" id="cb1335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1335-1"><a href="grandes-conjuntos-de-datos.html#cb1335-1"></a><span class="kw">rownames</span>(y)&lt;-<span class="st"> </span>y[,<span class="dv">1</span>]</span>
<span id="cb1335-2"><a href="grandes-conjuntos-de-datos.html#cb1335-2"></a>y &lt;-<span class="st"> </span>y[,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1335-3"><a href="grandes-conjuntos-de-datos.html#cb1335-3"></a></span>
<span id="cb1335-4"><a href="grandes-conjuntos-de-datos.html#cb1335-4"></a>movie_titles &lt;-<span class="st"> </span>movielens <span class="op">%&gt;%</span></span>
<span id="cb1335-5"><a href="grandes-conjuntos-de-datos.html#cb1335-5"></a><span class="kw">select</span>(movieId, title) <span class="op">%&gt;%</span></span>
<span id="cb1335-6"><a href="grandes-conjuntos-de-datos.html#cb1335-6"></a><span class="kw">distinct</span>()</span>
<span id="cb1335-7"><a href="grandes-conjuntos-de-datos.html#cb1335-7"></a></span>
<span id="cb1335-8"><a href="grandes-conjuntos-de-datos.html#cb1335-8"></a><span class="kw">colnames</span>(y) &lt;-<span class="st"> </span><span class="kw">with</span>(movie_titles, title[<span class="kw">match</span>(<span class="kw">colnames</span>(y), movieId)])</span></code></pre></div>
<p>y convertirlos en residuos eliminando los efectos de columna y fila:</p>
<div class="sourceCode" id="cb1336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1336-1"><a href="grandes-conjuntos-de-datos.html#cb1336-1"></a>y &lt;-<span class="st"> </span><span class="kw">sweep</span>(y, <span class="dv">2</span>, <span class="kw">colMeans</span>(y, <span class="dt">na.rm=</span><span class="ot">TRUE</span>))</span>
<span id="cb1336-2"><a href="grandes-conjuntos-de-datos.html#cb1336-2"></a>y &lt;-<span class="st"> </span><span class="kw">sweep</span>(y, <span class="dv">1</span>, <span class="kw">rowMeans</span>(y, <span class="dt">na.rm=</span><span class="ot">TRUE</span>))</span></code></pre></div>
<p>Si el modelo anterior explica todas las señales, y el <span class="math inline">\(\varepsilon\)</span> son solo ruido, entonces los residuos para diferentes películas deben ser independientes entre sí. Pero no lo son. Aquí hay unos ejemplos:</p>
<div class="sourceCode" id="cb1337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1337-1"><a href="grandes-conjuntos-de-datos.html#cb1337-1"></a>m_<span class="dv">1</span> &lt;-<span class="st"> &quot;Godfather, The&quot;</span></span>
<span id="cb1337-2"><a href="grandes-conjuntos-de-datos.html#cb1337-2"></a>m_<span class="dv">2</span> &lt;-<span class="st"> &quot;Godfather: Part II, The&quot;</span></span>
<span id="cb1337-3"><a href="grandes-conjuntos-de-datos.html#cb1337-3"></a>p1 &lt;-<span class="st"> </span><span class="kw">qplot</span>(y[ ,m_<span class="dv">1</span>], y[,m_<span class="dv">2</span>], <span class="dt">xlab =</span> m_<span class="dv">1</span>, <span class="dt">ylab =</span> m_<span class="dv">2</span>)</span>
<span id="cb1337-4"><a href="grandes-conjuntos-de-datos.html#cb1337-4"></a></span>
<span id="cb1337-5"><a href="grandes-conjuntos-de-datos.html#cb1337-5"></a>m_<span class="dv">1</span> &lt;-<span class="st"> &quot;Godfather, The&quot;</span></span>
<span id="cb1337-6"><a href="grandes-conjuntos-de-datos.html#cb1337-6"></a>m_<span class="dv">3</span> &lt;-<span class="st"> &quot;Goodfellas&quot;</span></span>
<span id="cb1337-7"><a href="grandes-conjuntos-de-datos.html#cb1337-7"></a>p2 &lt;-<span class="st"> </span><span class="kw">qplot</span>(y[ ,m_<span class="dv">1</span>], y[,m_<span class="dv">3</span>], <span class="dt">xlab =</span> m_<span class="dv">1</span>, <span class="dt">ylab =</span> m_<span class="dv">3</span>)</span>
<span id="cb1337-8"><a href="grandes-conjuntos-de-datos.html#cb1337-8"></a></span>
<span id="cb1337-9"><a href="grandes-conjuntos-de-datos.html#cb1337-9"></a>m_<span class="dv">4</span> &lt;-<span class="st"> &quot;You&#39;ve Got Mail&quot;</span></span>
<span id="cb1337-10"><a href="grandes-conjuntos-de-datos.html#cb1337-10"></a>m_<span class="dv">5</span> &lt;-<span class="st"> &quot;Sleepless in Seattle&quot;</span></span>
<span id="cb1337-11"><a href="grandes-conjuntos-de-datos.html#cb1337-11"></a>p3 &lt;-<span class="st"> </span><span class="kw">qplot</span>(y[ ,m_<span class="dv">4</span>], y[,m_<span class="dv">5</span>], <span class="dt">xlab =</span> m_<span class="dv">4</span>, <span class="dt">ylab =</span> m_<span class="dv">5</span>)</span>
<span id="cb1337-12"><a href="grandes-conjuntos-de-datos.html#cb1337-12"></a></span>
<span id="cb1337-13"><a href="grandes-conjuntos-de-datos.html#cb1337-13"></a>gridExtra<span class="op">::</span><span class="kw">grid.arrange</span>(p1, p2 ,p3, <span class="dt">ncol =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/movie-cor-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Esta trama dice que a los usuarios que les gustó The Godfather más de lo que el modelo espera de ellos, según la película y los efectos del usuario, también les gustó The Godfather II más de lo esperado. Se observa una relación similar al comparar El Padrino y Goodfellas. Aunque no es tan fuerte, todavía hay correlación. También vemos correlaciones entre You Have Got Mail y Sleepless en Seattle</p>
<p>Al observar la correlación entre películas, podemos ver un patrón (cambiamos el nombre de las columnas para ahorrar espacio de impresión):</p>
<div class="sourceCode" id="cb1338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1338-1"><a href="grandes-conjuntos-de-datos.html#cb1338-1"></a>x &lt;-<span class="st"> </span>y[, <span class="kw">c</span>(m_<span class="dv">1</span>, m_<span class="dv">2</span>, m_<span class="dv">3</span>, m_<span class="dv">4</span>, m_<span class="dv">5</span>)]</span>
<span id="cb1338-2"><a href="grandes-conjuntos-de-datos.html#cb1338-2"></a>short_names &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Godfather&quot;</span>, <span class="st">&quot;Godfather2&quot;</span>, <span class="st">&quot;Goodfellas&quot;</span>,</span>
<span id="cb1338-3"><a href="grandes-conjuntos-de-datos.html#cb1338-3"></a><span class="st">&quot;You&#39;ve Got&quot;</span>, <span class="st">&quot;Sleepless&quot;</span>)</span>
<span id="cb1338-4"><a href="grandes-conjuntos-de-datos.html#cb1338-4"></a><span class="kw">colnames</span>(x) &lt;-<span class="st"> </span>short_names</span>
<span id="cb1338-5"><a href="grandes-conjuntos-de-datos.html#cb1338-5"></a><span class="kw">cor</span>(x, <span class="dt">use=</span><span class="st">&quot;pairwise.complete&quot;</span>)</span>
<span id="cb1338-6"><a href="grandes-conjuntos-de-datos.html#cb1338-6"></a><span class="co">#&gt;            Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span></span>
<span id="cb1338-7"><a href="grandes-conjuntos-de-datos.html#cb1338-7"></a><span class="co">#&gt; Godfather      1.000      0.829      0.444     -0.440    -0.378</span></span>
<span id="cb1338-8"><a href="grandes-conjuntos-de-datos.html#cb1338-8"></a><span class="co">#&gt; Godfather2     0.829      1.000      0.521     -0.331    -0.358</span></span>
<span id="cb1338-9"><a href="grandes-conjuntos-de-datos.html#cb1338-9"></a><span class="co">#&gt; Goodfellas     0.444      0.521      1.000     -0.481    -0.402</span></span>
<span id="cb1338-10"><a href="grandes-conjuntos-de-datos.html#cb1338-10"></a><span class="co">#&gt; You&#39;ve Got    -0.440     -0.331     -0.481      1.000     0.533</span></span>
<span id="cb1338-11"><a href="grandes-conjuntos-de-datos.html#cb1338-11"></a><span class="co">#&gt; Sleepless     -0.378     -0.358     -0.402      0.533     1.000</span></span></code></pre></div>
<p>Parece que hay personas a las que les gustan las comedias románticas más de lo esperado, mientras que otras a las que les gustan las películas de gángsters más de lo esperado.</p>
<p>Estos resultados nos dicen que hay estructura en los datos. Pero, ¿cómo podemos modelar esto?</p>
<div id="análisis-de-factores" class="section level3">
<h3><span class="header-section-number">33.11.1</span> Análisis de factores</h3>
<p>Aquí hay una ilustración, usando una simulación, de cómo podemos usar alguna estructura para predecir el <span class="math inline">\(r_{u,i}\)</span>. Supongamos nuestros residuos <code>r</code> se parece a esto:</p>
<div class="sourceCode" id="cb1339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1339-1"><a href="grandes-conjuntos-de-datos.html#cb1339-1"></a><span class="kw">round</span>(r, <span class="dv">1</span>)</span>
<span id="cb1339-2"><a href="grandes-conjuntos-de-datos.html#cb1339-2"></a><span class="co">#&gt;    Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span></span>
<span id="cb1339-3"><a href="grandes-conjuntos-de-datos.html#cb1339-3"></a><span class="co">#&gt; 1        2.0        2.3        2.2       -1.8      -1.9</span></span>
<span id="cb1339-4"><a href="grandes-conjuntos-de-datos.html#cb1339-4"></a><span class="co">#&gt; 2        2.0        1.7        2.0       -1.9      -1.7</span></span>
<span id="cb1339-5"><a href="grandes-conjuntos-de-datos.html#cb1339-5"></a><span class="co">#&gt; 3        1.9        2.4        2.1       -2.3      -2.0</span></span>
<span id="cb1339-6"><a href="grandes-conjuntos-de-datos.html#cb1339-6"></a><span class="co">#&gt; 4       -0.3        0.3        0.3       -0.4      -0.3</span></span>
<span id="cb1339-7"><a href="grandes-conjuntos-de-datos.html#cb1339-7"></a><span class="co">#&gt; 5       -0.3       -0.4        0.3        0.2       0.3</span></span>
<span id="cb1339-8"><a href="grandes-conjuntos-de-datos.html#cb1339-8"></a><span class="co">#&gt; 6       -0.1        0.1        0.2       -0.3       0.2</span></span>
<span id="cb1339-9"><a href="grandes-conjuntos-de-datos.html#cb1339-9"></a><span class="co">#&gt; 7       -0.1        0.0       -0.2       -0.2       0.3</span></span>
<span id="cb1339-10"><a href="grandes-conjuntos-de-datos.html#cb1339-10"></a><span class="co">#&gt; 8        0.2        0.2        0.1        0.0       0.4</span></span>
<span id="cb1339-11"><a href="grandes-conjuntos-de-datos.html#cb1339-11"></a><span class="co">#&gt; 9       -1.7       -2.1       -1.8        2.0       2.4</span></span>
<span id="cb1339-12"><a href="grandes-conjuntos-de-datos.html#cb1339-12"></a><span class="co">#&gt; 10      -2.3       -1.8       -1.7        1.8       1.7</span></span>
<span id="cb1339-13"><a href="grandes-conjuntos-de-datos.html#cb1339-13"></a><span class="co">#&gt; 11      -1.7       -2.0       -2.1        1.9       2.3</span></span>
<span id="cb1339-14"><a href="grandes-conjuntos-de-datos.html#cb1339-14"></a><span class="co">#&gt; 12      -1.8       -1.7       -2.1        2.3       2.0</span></span></code></pre></div>
<p>Parece que hay un patrón aquí. De hecho, podemos ver patrones de correlación muy fuertes:</p>
<div class="sourceCode" id="cb1340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1340-1"><a href="grandes-conjuntos-de-datos.html#cb1340-1"></a><span class="kw">cor</span>(r)</span>
<span id="cb1340-2"><a href="grandes-conjuntos-de-datos.html#cb1340-2"></a><span class="co">#&gt;            Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span></span>
<span id="cb1340-3"><a href="grandes-conjuntos-de-datos.html#cb1340-3"></a><span class="co">#&gt; Godfather      1.000      0.980      0.978     -0.974    -0.966</span></span>
<span id="cb1340-4"><a href="grandes-conjuntos-de-datos.html#cb1340-4"></a><span class="co">#&gt; Godfather2     0.980      1.000      0.983     -0.987    -0.992</span></span>
<span id="cb1340-5"><a href="grandes-conjuntos-de-datos.html#cb1340-5"></a><span class="co">#&gt; Goodfellas     0.978      0.983      1.000     -0.986    -0.989</span></span>
<span id="cb1340-6"><a href="grandes-conjuntos-de-datos.html#cb1340-6"></a><span class="co">#&gt; You&#39;ve Got    -0.974     -0.987     -0.986      1.000     0.986</span></span>
<span id="cb1340-7"><a href="grandes-conjuntos-de-datos.html#cb1340-7"></a><span class="co">#&gt; Sleepless     -0.966     -0.992     -0.989      0.986     1.000</span></span></code></pre></div>
<p>Podemos crear vectores <code>q</code> y <code>p</code>, eso puede explicar gran parte de la estructura que vemos. los <code>q</code> se vería así:</p>
<div class="sourceCode" id="cb1341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1341-1"><a href="grandes-conjuntos-de-datos.html#cb1341-1"></a><span class="kw">t</span>(q)</span>
<span id="cb1341-2"><a href="grandes-conjuntos-de-datos.html#cb1341-2"></a><span class="co">#&gt;      Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless</span></span>
<span id="cb1341-3"><a href="grandes-conjuntos-de-datos.html#cb1341-3"></a><span class="co">#&gt; [1,]         1          1          1         -1        -1</span></span></code></pre></div>
<p>y reduce las películas a dos grupos: gángster (codificado con 1) y romance (codificado con -1). También podemos reducir los usuarios a tres grupos:</p>
<div class="sourceCode" id="cb1342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1342-1"><a href="grandes-conjuntos-de-datos.html#cb1342-1"></a><span class="kw">t</span>(p)</span>
<span id="cb1342-2"><a href="grandes-conjuntos-de-datos.html#cb1342-2"></a><span class="co">#&gt;      1 2 3 4 5 6 7 8  9 10 11 12</span></span>
<span id="cb1342-3"><a href="grandes-conjuntos-de-datos.html#cb1342-3"></a><span class="co">#&gt; [1,] 2 2 2 0 0 0 0 0 -2 -2 -2 -2</span></span></code></pre></div>
<p>los que les gustan las películas de gángsters y no les gustan las películas románticas (codificadas como 2), las que les gustan las películas románticas y no les gustan las películas de gángsters (codificadas como -2), y las que no les importa (codificadas como 0). El punto principal aquí es que casi podemos reconstruir <span class="math inline">\(r\)</span>, que tiene 60 valores, con un par de vectores que totalizan 17 valores. Si <span class="math inline">\(r\)</span> contiene los residuos para usuarios <span class="math inline">\(u=1,\dots,12\)</span> para peliculas <span class="math inline">\(i=1,\dots,5\)</span> podemos escribir la siguiente fórmula matemática para nuestros residuos <span class="math inline">\(r_{u,i}\)</span>.</p>
<p><span class="math display">\[
r_{u,i} \approx p_u q_i
\]</span></p>
<p>Esto implica que podemos explicar más variabilidad modificando nuestro modelo anterior para recomendaciones de películas para:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_u q_i + \varepsilon_{u,i}
\]</span></p>
<p>Sin embargo, motivamos la necesidad de <span class="math inline">\(p_u q_i\)</span> término con una simulación simple. La estructura que se encuentra en los datos suele ser más compleja. Por ejemplo, en esta primera simulación supusimos que solo había un factor <span class="math inline">\(p_u\)</span> eso determinó cuál de las dos películas de géneros <span class="math inline">\(u\)</span> pertenece a. Pero la estructura en nuestros datos de películas parece ser mucho más complicada que la película de gángsters versus el romance. Podemos tener muchos otros factores. Aquí presentamos una simulación un poco más compleja. Ahora agregamos una sexta película.</p>
<div class="sourceCode" id="cb1343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1343-1"><a href="grandes-conjuntos-de-datos.html#cb1343-1"></a><span class="kw">round</span>(r, <span class="dv">1</span>)</span>
<span id="cb1343-2"><a href="grandes-conjuntos-de-datos.html#cb1343-2"></a><span class="co">#&gt;    Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless Scent</span></span>
<span id="cb1343-3"><a href="grandes-conjuntos-de-datos.html#cb1343-3"></a><span class="co">#&gt; 1        0.5        0.6        1.6       -0.5      -0.5  -1.6</span></span>
<span id="cb1343-4"><a href="grandes-conjuntos-de-datos.html#cb1343-4"></a><span class="co">#&gt; 2        1.5        1.4        0.5       -1.5      -1.4  -0.4</span></span>
<span id="cb1343-5"><a href="grandes-conjuntos-de-datos.html#cb1343-5"></a><span class="co">#&gt; 3        1.5        1.6        0.5       -1.6      -1.5  -0.5</span></span>
<span id="cb1343-6"><a href="grandes-conjuntos-de-datos.html#cb1343-6"></a><span class="co">#&gt; 4       -0.1        0.1        0.1       -0.1      -0.1   0.1</span></span>
<span id="cb1343-7"><a href="grandes-conjuntos-de-datos.html#cb1343-7"></a><span class="co">#&gt; 5       -0.1       -0.1        0.1        0.0       0.1  -0.1</span></span>
<span id="cb1343-8"><a href="grandes-conjuntos-de-datos.html#cb1343-8"></a><span class="co">#&gt; 6        0.5        0.5       -0.4       -0.6      -0.5   0.5</span></span>
<span id="cb1343-9"><a href="grandes-conjuntos-de-datos.html#cb1343-9"></a><span class="co">#&gt; 7        0.5        0.5       -0.5       -0.6      -0.4   0.4</span></span>
<span id="cb1343-10"><a href="grandes-conjuntos-de-datos.html#cb1343-10"></a><span class="co">#&gt; 8        0.5        0.6       -0.5       -0.5      -0.4   0.4</span></span>
<span id="cb1343-11"><a href="grandes-conjuntos-de-datos.html#cb1343-11"></a><span class="co">#&gt; 9       -0.9       -1.0       -0.9        1.0       1.1   0.9</span></span>
<span id="cb1343-12"><a href="grandes-conjuntos-de-datos.html#cb1343-12"></a><span class="co">#&gt; 10      -1.6       -1.4       -0.4        1.5       1.4   0.5</span></span>
<span id="cb1343-13"><a href="grandes-conjuntos-de-datos.html#cb1343-13"></a><span class="co">#&gt; 11      -1.4       -1.5       -0.5        1.5       1.6   0.6</span></span>
<span id="cb1343-14"><a href="grandes-conjuntos-de-datos.html#cb1343-14"></a><span class="co">#&gt; 12      -1.4       -1.4       -0.5        1.6       1.5   0.6</span></span></code></pre></div>
<p>Al explorar la estructura de correlación de este nuevo conjunto de datos</p>
<div class="sourceCode" id="cb1344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1344-1"><a href="grandes-conjuntos-de-datos.html#cb1344-1"></a><span class="kw">colnames</span>(r)[<span class="dv">4</span><span class="op">:</span><span class="dv">6</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;YGM&quot;</span>, <span class="st">&quot;SS&quot;</span>, <span class="st">&quot;SW&quot;</span>)</span>
<span id="cb1344-2"><a href="grandes-conjuntos-de-datos.html#cb1344-2"></a><span class="kw">cor</span>(r)</span>
<span id="cb1344-3"><a href="grandes-conjuntos-de-datos.html#cb1344-3"></a><span class="co">#&gt;            Godfather Godfather2 Goodfellas    YGM     SS     SW</span></span>
<span id="cb1344-4"><a href="grandes-conjuntos-de-datos.html#cb1344-4"></a><span class="co">#&gt; Godfather      1.000      0.997      0.562 -0.997 -0.996 -0.571</span></span>
<span id="cb1344-5"><a href="grandes-conjuntos-de-datos.html#cb1344-5"></a><span class="co">#&gt; Godfather2     0.997      1.000      0.577 -0.998 -0.999 -0.583</span></span>
<span id="cb1344-6"><a href="grandes-conjuntos-de-datos.html#cb1344-6"></a><span class="co">#&gt; Goodfellas     0.562      0.577      1.000 -0.552 -0.583 -0.994</span></span>
<span id="cb1344-7"><a href="grandes-conjuntos-de-datos.html#cb1344-7"></a><span class="co">#&gt; YGM           -0.997     -0.998     -0.552  1.000  0.998  0.558</span></span>
<span id="cb1344-8"><a href="grandes-conjuntos-de-datos.html#cb1344-8"></a><span class="co">#&gt; SS            -0.996     -0.999     -0.583  0.998  1.000  0.588</span></span>
<span id="cb1344-9"><a href="grandes-conjuntos-de-datos.html#cb1344-9"></a><span class="co">#&gt; SW            -0.571     -0.583     -0.994  0.558  0.588  1.000</span></span></code></pre></div>
<p>Observamos que quizás necesitemos un segundo factor para tener en cuenta el hecho de que a algunos usuarios les gusta Al Pacino, mientras que a otros no les gusta o no les importa. Observe que la estructura general de la correlación obtenida de los datos simulados no está tan lejos de la correlación real:</p>
<div class="sourceCode" id="cb1345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1345-1"><a href="grandes-conjuntos-de-datos.html#cb1345-1"></a>six_movies &lt;-<span class="st"> </span><span class="kw">c</span>(m_<span class="dv">1</span>, m_<span class="dv">2</span>, m_<span class="dv">3</span>, m_<span class="dv">4</span>, m_<span class="dv">5</span>, m_<span class="dv">6</span>)</span>
<span id="cb1345-2"><a href="grandes-conjuntos-de-datos.html#cb1345-2"></a>x &lt;-<span class="st"> </span>y[, six_movies]</span>
<span id="cb1345-3"><a href="grandes-conjuntos-de-datos.html#cb1345-3"></a><span class="kw">colnames</span>(x) &lt;-<span class="st"> </span><span class="kw">colnames</span>(r)</span>
<span id="cb1345-4"><a href="grandes-conjuntos-de-datos.html#cb1345-4"></a><span class="kw">cor</span>(x, <span class="dt">use=</span><span class="st">&quot;pairwise.complete&quot;</span>)</span>
<span id="cb1345-5"><a href="grandes-conjuntos-de-datos.html#cb1345-5"></a><span class="co">#&gt;            Godfather Godfather2 Goodfellas    YGM     SS      SW</span></span>
<span id="cb1345-6"><a href="grandes-conjuntos-de-datos.html#cb1345-6"></a><span class="co">#&gt; Godfather     1.0000      0.829      0.444 -0.440 -0.378  0.0589</span></span>
<span id="cb1345-7"><a href="grandes-conjuntos-de-datos.html#cb1345-7"></a><span class="co">#&gt; Godfather2    0.8285      1.000      0.521 -0.331 -0.358  0.1186</span></span>
<span id="cb1345-8"><a href="grandes-conjuntos-de-datos.html#cb1345-8"></a><span class="co">#&gt; Goodfellas    0.4441      0.521      1.000 -0.481 -0.402 -0.1230</span></span>
<span id="cb1345-9"><a href="grandes-conjuntos-de-datos.html#cb1345-9"></a><span class="co">#&gt; YGM          -0.4397     -0.331     -0.481  1.000  0.533 -0.1699</span></span>
<span id="cb1345-10"><a href="grandes-conjuntos-de-datos.html#cb1345-10"></a><span class="co">#&gt; SS           -0.3781     -0.358     -0.402  0.533  1.000 -0.1822</span></span>
<span id="cb1345-11"><a href="grandes-conjuntos-de-datos.html#cb1345-11"></a><span class="co">#&gt; SW            0.0589      0.119     -0.123 -0.170 -0.182  1.0000</span></span></code></pre></div>
<p>Para explicar esta estructura más complicada, necesitamos dos factores. Por ejemplo algo como esto:</p>
<div class="sourceCode" id="cb1346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1346-1"><a href="grandes-conjuntos-de-datos.html#cb1346-1"></a><span class="kw">t</span>(q)</span>
<span id="cb1346-2"><a href="grandes-conjuntos-de-datos.html#cb1346-2"></a><span class="co">#&gt;      Godfather Godfather2 Goodfellas You&#39;ve Got Sleepless Scent</span></span>
<span id="cb1346-3"><a href="grandes-conjuntos-de-datos.html#cb1346-3"></a><span class="co">#&gt; [1,]         1          1          1         -1        -1    -1</span></span>
<span id="cb1346-4"><a href="grandes-conjuntos-de-datos.html#cb1346-4"></a><span class="co">#&gt; [2,]         1          1         -1         -1        -1     1</span></span></code></pre></div>
<p>Con el primer factor (la primera fila) utilizado para codificar los grupos de gángster versus romance y un segundo factor (la segunda fila) para explicar los grupos Al Pacino versus ningún grupo Al Pacino. También necesitaremos dos conjuntos de coeficientes para explicar la variabilidad introducida por el <span class="math inline">\(3\times 3\)</span> tipos de grupos:</p>
<div class="sourceCode" id="cb1347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1347-1"><a href="grandes-conjuntos-de-datos.html#cb1347-1"></a><span class="kw">t</span>(p)</span>
<span id="cb1347-2"><a href="grandes-conjuntos-de-datos.html#cb1347-2"></a><span class="co">#&gt;         1   2   3 4 5   6   7   8  9   10   11   12</span></span>
<span id="cb1347-3"><a href="grandes-conjuntos-de-datos.html#cb1347-3"></a><span class="co">#&gt; [1,]  1.0 1.0 1.0 0 0 0.0 0.0 0.0 -1 -1.0 -1.0 -1.0</span></span>
<span id="cb1347-4"><a href="grandes-conjuntos-de-datos.html#cb1347-4"></a><span class="co">#&gt; [2,] -0.5 0.5 0.5 0 0 0.5 0.5 0.5  0 -0.5 -0.5 -0.5</span></span></code></pre></div>
<p>El modelo con dos factores tiene 36 parámetros que pueden usarse para explicar gran parte de la variabilidad en las 72 clasificaciones:</p>
<p><span class="math display">\[
Y_{u,i} = \mu + b_i + b_u + p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \varepsilon_{u,i}
\]</span></p>
<p>Tenga en cuenta que en una aplicación de datos real, necesitamos ajustar este modelo a los datos. Para explicar la compleja correlación que observamos en datos reales, generalmente permitimos las entradas de <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> ser valores continuos, en lugar de discretos, como los que usamos en la simulación. Por ejemplo, en lugar de dividir las películas en gángster o romance, definimos un continuo. También tenga en cuenta que este no es un modelo lineal y para ajustarlo necesitamos usar un algoritmo diferente al usado por <code>lm</code> para encontrar los parámetros que minimizan los mínimos cuadrados. Los algoritmos ganadores para el desafío de Netflix se ajustan a un modelo similar al anterior y utilizan la regularización para penalizar por grandes valores de <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span>, en lugar de usar mínimos cuadrados. Implementando esto
enfoque está más allá del alcance de este libro.</p>
</div>
<div id="conexión-a-svd-y-pca" class="section level3">
<h3><span class="header-section-number">33.11.2</span> Conexión a SVD y PCA</h3>
<p>La descomposición:</p>
<p><span class="math display">\[
r_{u,i} \approx p_{u,1} q_{1,i} + p_{u,2} q_{2,i}
\]</span></p>
<p>está muy relacionado con SVD y PCA. SVD y PCA son conceptos complicados, pero una forma de entenderlos es que SVD es un algoritmo que encuentra los vectores <span class="math inline">\(p\)</span> y <span class="math inline">\(q\)</span> que nos permiten reescribir la matriz <span class="math inline">\(\mbox{r}\)</span> con <span class="math inline">\(m\)</span> filas y <span class="math inline">\(n\)</span> columnas como:</p>
<p><span class="math display">\[
r_{u,i} = p_{u,1} q_{1,i} + p_{u,2} q_{2,i} + \dots + p_{u,m} q_{m,i}
\]</span></p>
<p>con la variabilidad de cada término disminuyendo y con el <span class="math inline">\(p\)</span> s no correlacionado. El algoritmo también calcula esta variabilidad para que podamos saber cuánto de las matrices, la variabilidad total se explica a medida que agregamos nuevos términos. Esto puede permitirnos ver que, con solo unos pocos términos, podemos explicar la mayor parte de la variabilidad.</p>
<p>Veamos un ejemplo con los datos de la película. Para calcular la descomposición, haremos que los residuos con NA sean iguales a 0:</p>
<div class="sourceCode" id="cb1348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1348-1"><a href="grandes-conjuntos-de-datos.html#cb1348-1"></a>y[<span class="kw">is.na</span>(y)] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb1348-2"><a href="grandes-conjuntos-de-datos.html#cb1348-2"></a>pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(y)</span></code></pre></div>
<p>Los <span class="math inline">\(q\)</span> los vectores se denominan componentes principales y se almacenan en esta matriz:</p>
<div class="sourceCode" id="cb1349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1349-1"><a href="grandes-conjuntos-de-datos.html#cb1349-1"></a><span class="kw">dim</span>(pca<span class="op">$</span>rotation)</span>
<span id="cb1349-2"><a href="grandes-conjuntos-de-datos.html#cb1349-2"></a><span class="co">#&gt; [1] 454 292</span></span></code></pre></div>
<p>Mientras que la <span class="math inline">\(p\)</span>, o los efectos del usuario, están aquí:</p>
<div class="sourceCode" id="cb1350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1350-1"><a href="grandes-conjuntos-de-datos.html#cb1350-1"></a><span class="kw">dim</span>(pca<span class="op">$</span>x)</span>
<span id="cb1350-2"><a href="grandes-conjuntos-de-datos.html#cb1350-2"></a><span class="co">#&gt; [1] 292 292</span></span></code></pre></div>
<p>Podemos ver la variabilidad de cada uno de los vectores:</p>
<div class="sourceCode" id="cb1351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1351-1"><a href="grandes-conjuntos-de-datos.html#cb1351-1"></a><span class="kw">qplot</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(x), pca<span class="op">$</span>sdev, <span class="dt">xlab =</span> <span class="st">&quot;PC&quot;</span>)</span></code></pre></div>
<p><img src="libro_files/figure-html/pca-sds-1.png" width="70%" style="display: block; margin: auto;" /></p>
<!--
y vemos que solo los primeros ya explican un gran porcentaje:


```r
var_explained <- cumsum(pca$sdev^2/ sum(pca$sdev^2))
qplot(1:nrow(x), var_explained, xlab = "PC")
```

<img src="libro_files/figure-html/var-expained-pca-1.png" width="70%" style="display: block; margin: auto;" />
-->
<p>También notamos que los dos primeros componentes principales están relacionados con la estructura en las opiniones sobre películas:</p>
<pre><code>#&gt; Warning: ggrepel: 3 unlabeled data points (too many overlaps). Consider
#&gt; increasing max.overlaps</code></pre>
<p><img src="libro_files/figure-html/movies-pca-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Con solo mirar los 10 primeros en cada dirección, vemos un patrón significativo. La primera PC muestra la diferencia entre las películas aclamadas por la crítica en un lado:</p>
<pre><code>#&gt;  [1] &quot;Pulp Fiction&quot;              &quot;Seven (a.k.a. Se7en)&quot;     
#&gt;  [3] &quot;Fargo&quot;                     &quot;2001: A Space Odyssey&quot;    
#&gt;  [5] &quot;Silence of the Lambs, The&quot; &quot;Clockwork Orange, A&quot;      
#&gt;  [7] &quot;Taxi Driver&quot;               &quot;Being John Malkovich&quot;     
#&gt;  [9] &quot;Royal Tenenbaums, The&quot;     &quot;Shining, The&quot;</code></pre>
<p>y éxitos de taquilla de Hollywood por el otro:</p>
<pre><code>#&gt;  [1] &quot;Independence Day (a.k.a. ID4)&quot;  &quot;Shrek&quot;                         
#&gt;  [3] &quot;Spider-Man&quot;                     &quot;Titanic&quot;                       
#&gt;  [5] &quot;Twister&quot;                        &quot;Armageddon&quot;                    
#&gt;  [7] &quot;Harry Potter and the Sorcer...&quot; &quot;Forrest Gump&quot;                  
#&gt;  [9] &quot;Lord of the Rings: The Retu...&quot; &quot;Enemy of the State&quot;</code></pre>
<p>Mientras que la segunda PC parece ir de películas artísticas e independientes:</p>
<pre><code>#&gt;  [1] &quot;Shawshank Redemption, The&quot;      &quot;Truman Show, The&quot;              
#&gt;  [3] &quot;Little Miss Sunshine&quot;           &quot;Slumdog Millionaire&quot;           
#&gt;  [5] &quot;Amelie (Fabuleux destin d&#39;A...&quot; &quot;Kill Bill: Vol. 1&quot;             
#&gt;  [7] &quot;American Beauty&quot;                &quot;City of God (Cidade de Deus)&quot;  
#&gt;  [9] &quot;Mars Attacks!&quot;                  &quot;Beautiful Mind, A&quot;</code></pre>
<p>para nerd favoritos:</p>
<pre><code>#&gt;  [1] &quot;Lord of the Rings: The Two ...&quot; &quot;Lord of the Rings: The Fell...&quot;
#&gt;  [3] &quot;Lord of the Rings: The Retu...&quot; &quot;Matrix, The&quot;                   
#&gt;  [5] &quot;Star Wars: Episode IV - A N...&quot; &quot;Star Wars: Episode VI - Ret...&quot;
#&gt;  [7] &quot;Star Wars: Episode V - The ...&quot; &quot;Spider-Man 2&quot;                  
#&gt;  [9] &quot;Dark Knight, The&quot;               &quot;Speed&quot;</code></pre>
<p>Ajustar un modelo que incorpora estas estimaciones es complicado. Para aquellos interesados en implementar un enfoque que incorpore estas ideas, recomendamos probar el paquete <strong>recommenderlab</strong>. Los detalles están más allá del alcance de este libro.</p>
</div>
</div>
<div id="ejercicios-60" class="section level2">
<h2><span class="header-section-number">33.12</span> Ejercicios</h2>
<p>En este conjunto de ejercicios, trataremos un tema útil para comprender la factorización matricial: la descomposición de valores singulares (SVD). SVD es un resultado matemático que se usa ampliamente en el aprendizaje automático, tanto en la práctica como para comprender las propiedades matemáticas de algunos algoritmos. Este es un tema bastante avanzado y para completar este conjunto de ejercicios tendrá que estar familiarizado con los conceptos de álgebra lineal, como la multiplicación de matrices, las matrices ortogonales y las matrices diagonales.</p>
<p>La SVD nos dice que podemos <em>descomponer</em> un <span class="math inline">\(N\times p\)</span> matriz <span class="math inline">\(Y\)</span> con <span class="math inline">\(p &lt; N\)</span> como</p>
<p><span class="math display">\[ Y = U D V^{\top} \]</span></p>
<p>Con <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> <em>ortogonal</em> de dimensiones <span class="math inline">\(N\times p\)</span> y <span class="math inline">\(p\times p\)</span>, respectivamente, y <span class="math inline">\(D\)</span> un <span class="math inline">\(p \times p\)</span> matriz <em>diagonal</em> con los valores de la diagonal decreciente:</p>
<p><span class="math display">\[d_{1,1} \geq d_{2,2} \geq \dots d_{p,p}.\]</span></p>
<p>En este ejercicio, veremos una de las formas en que esta descomposición puede ser útil. Para hacer esto, construiremos un conjunto de datos que represente las calificaciones de 100 estudiantes en 24 materias diferentes. El promedio general se ha eliminado, por lo que estos datos representan el punto porcentual que cada estudiante recibió por encima o por debajo del puntaje promedio de la prueba. Entonces, un 0 representa una calificación promedio (C), un 25 es una calificación alta (A +) y un -25 representa una calificación baja (F). Puede simular los datos de esta manera:</p>
<div class="sourceCode" id="cb1357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1357-1"><a href="grandes-conjuntos-de-datos.html#cb1357-1"></a><span class="kw">set.seed</span>(<span class="dv">1987</span>)</span>
<span id="cb1357-2"><a href="grandes-conjuntos-de-datos.html#cb1357-2"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb1357-3"><a href="grandes-conjuntos-de-datos.html#cb1357-3"></a>k &lt;-<span class="st"> </span><span class="dv">8</span></span>
<span id="cb1357-4"><a href="grandes-conjuntos-de-datos.html#cb1357-4"></a>Sigma &lt;-<span class="st"> </span><span class="dv">64</span> <span class="op">*</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">.75</span>, <span class="fl">.5</span>, <span class="fl">.75</span>, <span class="dv">1</span>, <span class="fl">.5</span>, <span class="fl">.5</span>, <span class="fl">.5</span>, <span class="dv">1</span>), <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb1357-5"><a href="grandes-conjuntos-de-datos.html#cb1357-5"></a>m &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(n, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), Sigma)</span>
<span id="cb1357-6"><a href="grandes-conjuntos-de-datos.html#cb1357-6"></a>m &lt;-<span class="st"> </span>m[<span class="kw">order</span>(<span class="kw">rowMeans</span>(m), <span class="dt">decreasing =</span> <span class="ot">TRUE</span>),]</span>
<span id="cb1357-7"><a href="grandes-conjuntos-de-datos.html#cb1357-7"></a>y &lt;-<span class="st"> </span>m <span class="op">%x%</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">1</span>, k), <span class="dt">nrow =</span> <span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb1357-8"><a href="grandes-conjuntos-de-datos.html#cb1357-8"></a><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="kw">matrix</span>(n <span class="op">*</span><span class="st"> </span>k <span class="op">*</span><span class="st"> </span><span class="dv">3</span>)), n, k <span class="op">*</span><span class="st"> </span><span class="dv">3</span>)</span>
<span id="cb1357-9"><a href="grandes-conjuntos-de-datos.html#cb1357-9"></a><span class="kw">colnames</span>(y) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste</span>(<span class="kw">rep</span>(<span class="st">&quot;Math&quot;</span>,k), <span class="dv">1</span><span class="op">:</span>k, <span class="dt">sep=</span><span class="st">&quot;_&quot;</span>),</span>
<span id="cb1357-10"><a href="grandes-conjuntos-de-datos.html#cb1357-10"></a><span class="kw">paste</span>(<span class="kw">rep</span>(<span class="st">&quot;Science&quot;</span>,k), <span class="dv">1</span><span class="op">:</span>k, <span class="dt">sep=</span><span class="st">&quot;_&quot;</span>),</span>
<span id="cb1357-11"><a href="grandes-conjuntos-de-datos.html#cb1357-11"></a><span class="kw">paste</span>(<span class="kw">rep</span>(<span class="st">&quot;Arts&quot;</span>,k), <span class="dv">1</span><span class="op">:</span>k, <span class="dt">sep=</span><span class="st">&quot;_&quot;</span>))</span></code></pre></div>
<p>Nuestro objetivo es describir las actuaciones de los estudiantes de la manera más sucinta posible. Por ejemplo, queremos saber si los resultados de estas pruebas son solo números independientes aleatorios. ¿Todos los estudiantes son igual de buenos? ¿Ser bueno en un tema implica que serás bueno en otro? ¿Cómo ayuda la SVD con todo esto? Iremos paso a paso para mostrar que con solo tres pares relativamente pequeños de vectores podemos explicar gran parte de la variabilidad en este <span class="math inline">\(100 \times 24\)</span> conjunto de datos</p>
<p>Puede visualizar los 24 puntajes de las pruebas para los 100 estudiantes al trazar una imagen:</p>
<div class="sourceCode" id="cb1358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1358-1"><a href="grandes-conjuntos-de-datos.html#cb1358-1"></a>my_image &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">zlim =</span> <span class="kw">range</span>(x), ...){</span>
<span id="cb1358-2"><a href="grandes-conjuntos-de-datos.html#cb1358-2"></a>colors =<span class="st"> </span><span class="kw">rev</span>(RColorBrewer<span class="op">::</span><span class="kw">brewer.pal</span>(<span class="dv">9</span>, <span class="st">&quot;RdBu&quot;</span>))</span>
<span id="cb1358-3"><a href="grandes-conjuntos-de-datos.html#cb1358-3"></a>cols &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x)</span>
<span id="cb1358-4"><a href="grandes-conjuntos-de-datos.html#cb1358-4"></a>rows &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(x)</span>
<span id="cb1358-5"><a href="grandes-conjuntos-de-datos.html#cb1358-5"></a><span class="kw">image</span>(cols, rows, <span class="kw">t</span>(x[<span class="kw">rev</span>(rows),,<span class="dt">drop=</span><span class="ot">FALSE</span>]), <span class="dt">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">yaxt =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb1358-6"><a href="grandes-conjuntos-de-datos.html#cb1358-6"></a><span class="dt">xlab=</span><span class="st">&quot;&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;&quot;</span>, <span class="dt">col =</span> colors, <span class="dt">zlim =</span> zlim, ...)</span>
<span id="cb1358-7"><a href="grandes-conjuntos-de-datos.html#cb1358-7"></a><span class="kw">abline</span>(<span class="dt">h=</span>rows <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dt">v =</span> cols <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span>)</span>
<span id="cb1358-8"><a href="grandes-conjuntos-de-datos.html#cb1358-8"></a><span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">1</span>, cols, <span class="kw">colnames</span>(x), <span class="dt">las =</span> <span class="dv">2</span>)</span>
<span id="cb1358-9"><a href="grandes-conjuntos-de-datos.html#cb1358-9"></a>}</span>
<span id="cb1358-10"><a href="grandes-conjuntos-de-datos.html#cb1358-10"></a></span>
<span id="cb1358-11"><a href="grandes-conjuntos-de-datos.html#cb1358-11"></a><span class="kw">my_image</span>(y)</span></code></pre></div>
<p>1. ¿Cómo describirías los datos basados en esta figura?</p>
<ol style="list-style-type: lower-alpha">
<li>Los puntajes de las pruebas son independientes entre sí.
si. Los estudiantes que evalúan bien están en la parte superior de la imagen y parece que hay tres agrupaciones por materia.</li>
<li>Los estudiantes que son buenos en matemáticas no son buenos en ciencias.
re. Los estudiantes que son buenos en matemáticas no son buenos en humanidades.</li>
</ol>
<p>2. Puede examinar la correlación entre los puntajes de la prueba directamente de esta manera:</p>
<div class="sourceCode" id="cb1359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1359-1"><a href="grandes-conjuntos-de-datos.html#cb1359-1"></a><span class="kw">my_image</span>(<span class="kw">cor</span>(y), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb1359-2"><a href="grandes-conjuntos-de-datos.html#cb1359-2"></a><span class="kw">range</span>(<span class="kw">cor</span>(y))</span>
<span id="cb1359-3"><a href="grandes-conjuntos-de-datos.html#cb1359-3"></a><span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>¿Cuál de las siguientes opciones describe mejor lo que ves?</p>
<ol style="list-style-type: lower-alpha">
<li>Los puntajes de las pruebas son independientes.
si. Las matemáticas y las ciencias están altamente correlacionadas, pero las humanidades no.</li>
<li>Existe una alta correlación entre las pruebas en el mismo sujeto pero no hay correlación entre los sujetos.
re. Hay una correlación entre todas las pruebas, pero es mayor si las pruebas son de ciencias y matemáticas e incluso más altas dentro de cada materia.</li>
</ol>
<p>3. Recuerda que la ortogonalidad significa que <span class="math inline">\(U^{\top}U\)</span> y <span class="math inline">\(V^{\top}V\)</span> son iguales a la matriz de identidad. Esto implica que también podemos reescribir la descomposición como</p>
<p><span class="math display">\[ Y V = U D \mbox{ or } U^{\top}Y = D V^{\top}\]</span></p>
<p>Podemos pensar en <span class="math inline">\(YV\)</span> y <span class="math inline">\(U^{\top}V\)</span> como dos transformaciones de Y que preservan la variabilidad total de <span class="math inline">\(Y\)</span> ya que <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> son ortogonales</p>
<p>Usa la función <code>svd</code> para calcular la SVD de <code>y</code>. Esta función regresará <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> y las entradas diagonales de <span class="math inline">\(D\)</span>.</p>
<div class="sourceCode" id="cb1360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1360-1"><a href="grandes-conjuntos-de-datos.html#cb1360-1"></a>s &lt;-<span class="st"> </span><span class="kw">svd</span>(y)</span>
<span id="cb1360-2"><a href="grandes-conjuntos-de-datos.html#cb1360-2"></a><span class="kw">names</span>(s)</span></code></pre></div>
<p>Puede verificar que la SVD funciona escribiendo:</p>
<div class="sourceCode" id="cb1361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1361-1"><a href="grandes-conjuntos-de-datos.html#cb1361-1"></a>y_svd &lt;-<span class="st"> </span>s<span class="op">$</span>u <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(s<span class="op">$</span>d) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(s<span class="op">$</span>v)</span>
<span id="cb1361-2"><a href="grandes-conjuntos-de-datos.html#cb1361-2"></a><span class="kw">max</span>(<span class="kw">abs</span>(y <span class="op">-</span><span class="st"> </span>y_svd))</span></code></pre></div>
<p>Calcule la suma de cuadrados de las columnas de <span class="math inline">\(Y\)</span> y guardarlos en <code>ss_y</code>. Luego calcule la suma de cuadrados de columnas del transformado <span class="math inline">\(YV\)</span> y guardarlos en NA. Confirma eso NA es igual a NA.</p>
<p>4. Vemos que se conserva la suma total de cuadrados. Esto es porque <span class="math inline">\(V\)</span> es ortogonal Ahora para comenzar a entender cómo <span class="math inline">\(YV\)</span> es útil,
trama <code>ss_y</code> contra el número de columna y luego hacer lo mismo para <code>ss_yv</code>. Que observas</p>
<p>5. Vemos que la variabilidad de las columnas de <span class="math inline">\(YV\)</span> está disminuyendo. Además, vemos que, en relación con los tres primeros, la variabilidad de las columnas más allá del tercero es casi 0. Ahora observe que no tuvimos que calcular <code>ss_yv</code> porque ya tenemos la respuesta ¿Cómo? Recuérdalo <span class="math inline">\(YV = UD\)</span> y porqué <span class="math inline">\(U\)</span> es ortogonal, sabemos que la suma de cuadrados de las columnas de <span class="math inline">\(UD\)</span> son las entradas diagonales de <span class="math inline">\(D\)</span> al cuadrado Confirme esto trazando la raíz cuadrada de <code>ss_yv</code> frente a las entradas diagonales de <span class="math inline">\(D\)</span>.</p>
<p>6. De lo anterior sabemos que la suma de cuadrados de las columnas de <span class="math inline">\(Y\)</span> (la suma total de cuadrados) se suman a la suma de <code>s $d^2</code> and that the transformation $ YV $ gives us columns with sums of squares equal to <code>s$ d^2</code>. Ahora calcule qué porcentaje de la variabilidad total se explica solo por las tres primeras columnas de <span class="math inline">\(YV\)</span>.</p>
<p>7. Vemos que casi el 99% de la variabilidad se explica por las primeras tres columnas de <span class="math inline">\(YV = UD\)</span>. Entonces tenemos la sensación de que deberíamos poder explicar gran parte de la variabilidad y estructura que encontramos al explorar los datos con unas pocas columnas. Antes de continuar, vamos a mostrar un truco computacional útil para evitar crear la matriz <code>diag (s) $d)</code>. To motivate this, we note that if we write $ U $ out in its columns $[U_1, U_2, , U_p] $ then $ UD $ es igual a</p>
<p><span class="math display">\[UD = [U_1 d_{1,1}, U_2 d_{2,2}, \dots, U_p d_{p,p}]\]</span></p>
<p>Utilizar el <code>sweep</code> función para calcular <span class="math inline">\(UD\)</span> sin construir NA ni la multiplicación de matrices.</p>
<p>8. Lo sabemos <span class="math inline">\(U_1 d_{1,1}\)</span>, la primera columna de <span class="math inline">\(UD\)</span>, tiene la mayor variabilidad de todas las columnas de <span class="math inline">\(UD\)</span>. Anteriormente vimos una imagen de <span class="math inline">\(Y\)</span>:</p>
<div class="sourceCode" id="cb1362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1362-1"><a href="grandes-conjuntos-de-datos.html#cb1362-1"></a><span class="kw">my_image</span>(y)</span></code></pre></div>
<p>en el que podemos ver que la variabilidad de estudiante a estudiante es bastante grande y que parece que los estudiantes que son buenos en una materia son buenos en todos. Esto implica que el promedio (en todas las asignaturas) de cada alumno debe explicar en gran medida la variabilidad. Calcule el puntaje promedio de cada estudiante y compárelo con <span class="math inline">\(U_1 d_{1,1}\)</span> y describe lo que encuentras.</p>
<p>9. Notamos que los signos en SVD son arbitrarios porque:</p>
<p><span class="math display">\[ U D V^{\top} = (-U) D (-V)^{\top} \]</span></p>
<p>Con esto en mente, vemos que la primera columna de <span class="math inline">\(UD\)</span> es casi idéntico al puntaje promedio de cada estudiante, excepto por el signo.</p>
<p>Esto implica que multiplicar <span class="math inline">\(Y\)</span> por la primera columna de <span class="math inline">\(V\)</span> debe estar realizando una operación similar a tomar el promedio. Haz un diagrama de imagen de <span class="math inline">\(V\)</span> y describa la primera columna en relación con otros y cómo se relaciona esto con tomar un promedio.</p>
<p>10. Ya vimos que podemos reescribir <span class="math inline">\(UD\)</span> como</p>
<p><span class="math display">\[U_1 d_{1,1} + U_2 d_{2,2} + \dots + U_p d_{p,p}\]</span></p>
<p>con <span class="math inline">\(U_j\)</span> la columna j-ésima de <span class="math inline">\(U\)</span>. Esto implica que podemos reescribir toda la SVD como:</p>
<p><span class="math display">\[Y = U_1 d_{1,1} V_1 ^{\top} + U_2 d_{2,2} V_2 ^{\top} + \dots + U_p d_{p,p} V_p ^{\top}\]</span></p>
<p>con <span class="math inline">\(V_j\)</span> la jésima columna de <span class="math inline">\(V\)</span>. Trama <span class="math inline">\(U_1\)</span>, luego trazar <span class="math inline">\(V_1^{\top}\)</span> usando el mismo rango para los límites del eje y, luego haga una imagen de <span class="math inline">\(U_1 d_{1,1} V_1 ^{\top}\)</span> y compararlo con la imagen de <span class="math inline">\(Y\)</span>. Sugerencia: use el <code>my_image</code> función definida anteriormente y usar el <code>drop=FALSE</code> argumento para asegurar que los subconjuntos de matrices son matrices.</p>
<p>11. Vemos que con solo un vector de longitud 100, un escalar y un vector de longitud 24, en realidad nos acercamos a reconstruir el original <span class="math inline">\(100 \times 24\)</span> matriz. Esta es nuestra primera factorización matricial:</p>
<p><span class="math display">\[ Y \approx d_{1,1} U_1 V_1^{\top}\]</span></p>
<p>Sabemos que explica <code>s $d[1]^2/sum(s$ d^2) * 100</code> por ciento de la variabilidad total. Nuestra aproximación solo explica la observación de que los buenos estudiantes tienden a ser buenos en todas las materias. Pero otro aspecto de los datos originales que nuestra aproximación no explica fue la mayor similitud que observamos en los sujetos. Podemos ver esto calculando la diferencia entre nuestra aproximación y los datos originales y luego calculando las correlaciones. Puede ver esto ejecutando este código:</p>
<div class="sourceCode" id="cb1363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1363-1"><a href="grandes-conjuntos-de-datos.html#cb1363-1"></a>resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="kw">with</span>(s,(u[,<span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>]<span class="op">*</span>d[<span class="dv">1</span>]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v[,<span class="dv">1</span>, <span class="dt">drop=</span><span class="ot">FALSE</span>]))</span>
<span id="cb1363-2"><a href="grandes-conjuntos-de-datos.html#cb1363-2"></a><span class="kw">my_image</span>(<span class="kw">cor</span>(resid), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb1363-3"><a href="grandes-conjuntos-de-datos.html#cb1363-3"></a><span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>Ahora que hemos eliminado el efecto general del estudiante, la gráfica de correlación revela que todavía no hemos explicado la correlación interna de la asignatura ni el hecho de que las matemáticas y las ciencias están más cercanas entre sí que con las artes. Así que exploremos la segunda columna de la SVD. Repita el ejercicio anterior pero para la segunda columna: Trazar <span class="math inline">\(U_2\)</span>, luego trazar <span class="math inline">\(V_2^{\top}\)</span> usando el mismo rango para los límites del eje y, luego haga una imagen de <span class="math inline">\(U_2 d_{2,2} V_2 ^{\top}\)</span> y compararlo con la imagen de <code>resid</code>.</p>
<p>12. La segunda columna se relaciona claramente con la diferencia de habilidad del estudiante en matemáticas/ ciencias versus las artes. Podemos ver esto más claramente en la trama de <code>s$v[,2]</code>. Agregar la matriz que obtenemos con estas dos columnas ayudará con nuestra aproximación:</p>
<p><span class="math display">\[ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} \]</span></p>
<p>Sabemos que explicará</p>
<div class="sourceCode" id="cb1364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1364-1"><a href="grandes-conjuntos-de-datos.html#cb1364-1"></a><span class="kw">sum</span>(s<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></span></code></pre></div>
<p>porcentaje de la variabilidad total. Podemos calcular nuevos residuos como este:</p>
<div class="sourceCode" id="cb1365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1365-1"><a href="grandes-conjuntos-de-datos.html#cb1365-1"></a>resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="kw">with</span>(s,<span class="kw">sweep</span>(u[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dv">2</span>, d[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], <span class="dt">FUN=</span><span class="st">&quot;*&quot;</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]))</span>
<span id="cb1365-2"><a href="grandes-conjuntos-de-datos.html#cb1365-2"></a><span class="kw">my_image</span>(<span class="kw">cor</span>(resid), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb1365-3"><a href="grandes-conjuntos-de-datos.html#cb1365-3"></a><span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>y ver que la estructura que queda es impulsada por las diferencias entre matemáticas y ciencias. Confirme esto trazando <span class="math inline">\(U_3\)</span>, luego trazar <span class="math inline">\(V_3^{\top}\)</span> usando el mismo rango para los límites del eje y, luego haga una imagen de <span class="math inline">\(U_3 d_{3,3} V_3 ^{\top}\)</span> y compararlo con la imagen de <code>resid</code>.</p>
<p>13. La tercera columna se relaciona claramente con la diferencia de habilidad del estudiante en matemáticas y ciencias. Podemos ver esto más claramente en la trama de <code>s$v[,3]</code>. Agregar la matriz que obtenemos con estas dos columnas ayudará con nuestra aproximación:</p>
<p><span class="math display">\[ Y \approx d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}\]</span></p>
<p>Sabemos que explicará:</p>
<div class="sourceCode" id="cb1366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1366-1"><a href="grandes-conjuntos-de-datos.html#cb1366-1"></a><span class="kw">sum</span>(s<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></span></code></pre></div>
<p>porcentaje de la variabilidad total. Podemos calcular nuevos residuos como este:</p>
<div class="sourceCode" id="cb1367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1367-1"><a href="grandes-conjuntos-de-datos.html#cb1367-1"></a>resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="kw">with</span>(s,<span class="kw">sweep</span>(u[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dv">2</span>, d[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>], <span class="dt">FUN=</span><span class="st">&quot;*&quot;</span>) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]))</span>
<span id="cb1367-2"><a href="grandes-conjuntos-de-datos.html#cb1367-2"></a><span class="kw">my_image</span>(<span class="kw">cor</span>(resid), <span class="dt">zlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb1367-3"><a href="grandes-conjuntos-de-datos.html#cb1367-3"></a><span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(y), <span class="kw">rev</span>(<span class="kw">colnames</span>(y)), <span class="dt">las =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>Ya no vemos estructura en los residuos: parecen ser independientes entre sí. Esto implica que podemos describir los datos con el siguiente modelo:</p>
<p><span class="math display">\[ Y = d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top} + \varepsilon\]</span></p>
<p>con <span class="math inline">\(\varepsilon\)</span> una matriz de errores independientes idénticamente distribuidos. Este modelo es útil porque resumimos <span class="math inline">\(100 \times 24\)</span> observaciones con <span class="math inline">\(3 \times (100+24+1) = 375\)</span> números. Además, los tres componentes del modelo tienen interpretaciones útiles: 1) la capacidad general de un estudiante, 2) la diferencia en la habilidad entre las matemáticas/ ciencias y las artes, y 3) las diferencias restantes entre las tres materias. Los tamaños <span class="math inline">\(d_{1,1}, d_{2,2}\)</span> y <span class="math inline">\(d_{3,3}\)</span> cuéntanos la variabilidad explicada por cada componente. Finalmente, tenga en cuenta que los componentes <span class="math inline">\(d_{j,j} U_j V_j^{\top}\)</span> son equivalentes al componente principal j.</p>
<p>Termine el ejercicio trazando una imagen de <span class="math inline">\(Y\)</span>, una imagen de <span class="math inline">\(d_{1,1} U_1 V_1^{\top} + d_{2,2} U_2 V_2^{\top} + d_{3,3} U_3 V_3^{\top}\)</span> y una imagen de los residuos, todos con el mismo <code>zlim</code>.</p>
<p>14. Avanzado. los <code>movielens</code> el conjunto de datos incluido en el paquete <strong>dslabs</strong> es un pequeño subconjunto de un conjunto de datos más grande con millones de clasificaciones. Puede encontrar el conjunto de datos más reciente aquí [<a href="https://grouplens.org/datasets/movielens/20m/font" class="uri">https://grouplens.org/datasets/movielens/20m/font</a>&gt;(<a href="https://grouplens.org/datasets/movielens/20m/" class="uri">https://grouplens.org/datasets/movielens/20m/</a>). Cree su propio sistema de recomendaciones utilizando todas las herramientas que le hemos mostrado.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="115">
<li id="fn115"><p><a href="http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/" class="uri">http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/</a><a href="grandes-conjuntos-de-datos.html#fnref115" class="footnote-back">↩︎</a></p></li>
<li id="fn116"><p><a href="https://grouplens.org/" class="uri">https://grouplens.org/</a><a href="grandes-conjuntos-de-datos.html#fnref116" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machine-learning-en-la-práctica.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dslibro/edit/master/ml/matrix.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
